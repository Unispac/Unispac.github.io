---
layout: single
permalink: /arxiv-llm-alignment-safety-security/
title: ""
author_profile: false
sitemap: false
---



<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Papers List</title>
        <style>
            .outer {
                padding: 40px;
                /* Light background with dots and lines */
                background-color: #f4f4f4;
                background-image:
                    radial-gradient(circle, #d9d9d9 1px, transparent 1px),
                    linear-gradient(90deg, #d9d9d9 1px, transparent 1px),
                    linear-gradient(#d9d9d9 1px, transparent 1px);
                background-size: 20px 20px;
            }
    
            .intro {
                background-color: #fff;
                border: 5px solid #e0e0e0;
                padding: 10px 10px 10px; /* Adjusted top padding */
                margin-bottom: 10px;
                margin: 5px auto;  /* Centered horizontally with top and bottom margin of 5px */
                max-width: 1000px;
                border-radius: 8px;
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                transition: transform 0.2s;
            }
    
            .paper {
                background-color: #fff;
                border: 1px solid #e0e0e0;
                padding: 3px 10px 10px; /* Adjusted top padding */
                margin-bottom: 7px;
                max-width: 1000px;
                border-radius: 8px;
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                transition: transform 0.2s;
            }
    
            .paper:hover {
                transform: translateY(-2px);
            }
    
            .paper-title {
                font-weight: bold;
                font-size: 1.0em;
                margin: 0.1em 0 0.1em; /* Reduced top margin */
            }
    
            .paper-title a {
                text-decoration: none;
                color: #333;
            }
    
            .paper-title a:hover {
                text-decoration: underline;
            }
    
            .paper-author {
                color: #777;
                font-size: 0.8em;
                margin: 0.1em 0 0.1em; /* Reduced top margin */
            }
    
            .abstract {
                border-top: 2px solid #e77500;
                padding-top: 2px;
            }
    
            .abstract-content {
                height: 1.4em;
                overflow: hidden;
                white-space: nowrap;
                text-overflow: ellipsis;
                margin-top: 0.1em;
                color: #888888; /* Gray color for abstract */
                font-size: 0.8em;
            }
    
            .expanded .abstract-content {
                height: auto;
                white-space: normal;
                overflow: visible;
            }
    
            .expand-btn {
                background-color: #e77500;
                color: #fff;
                border: none;
                border-radius: 4px;
                padding: 3px 6px;
                margin-top: 3px;
                cursor: pointer;
                transition: background-color 0.3s;
                font-size:0.7em;
            }
    
            .expand-btn:hover {
                background-color: #d66400;
            }
        </style>
    </head>


<body>


<div class="outer">


<div class="intro">
    <h2 style="text-align: center; border-bottom: 2px solid #cccccc; padding-bottom: 10px;">A Complete List of ArXiv Papers on Alignment, Safety, and Security of Large Language Models (LLMs)</h2>
    <p>
        <span style="padding-left: 2em;">by
        <span style="font-weight: 600">
            Xiangyu Qi
        </span>
        <span style="padding-left: 2em; color: #777;">
            2023-10-30
            </span>
        </span>
    </p>
</div>

<h3><b>2023-10-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17884" target="_blank">Can LLMS Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory</a></div>
<div class="paper-author">Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the interactive use of large language models (llms) in ai assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: llms are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. in this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing confaide, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned llms. our experiments show that even the most capable models such as gpt-4 and chatgpt reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. this leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18018" target="_blank">NLP Evaluation in Trouble: On the Need to Measure LLM Data Contamination for Each Benchmark</a></div>
<div class="paper-author">Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez De Lacalle, Eneko Agirre</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this position paper, we argue that the classical evaluation on natural language processing (nlp) tasks using annotated benchmarks is in trouble. the worst kind of data contamination happens when a large language model (llm) is trained on the test split of a benchmark, and then evaluated in the same benchmark. the extent of the problem is unknown, as it is not straightforward to measure. contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. the consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. this position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18089" target="_blank">Lost in Translation -- Multilingual Misinformation and Its Evolution</a></div>
<div class="paper-author">Dorian Quelle, Calvin Cheng, Alexandre Bovet, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: misinformation and disinformation are growing threats in the digital age, spreading rapidly across languages and borders. this paper investigates the prevalence and dynamics of multilingual misinformation through an analysis of over 250,000 unique fact-checks spanning 95 languages. first, we find that while the majority of misinformation claims are only fact-checked once, 11.7%, corresponding to more than 21,000 claims, are checked multiple times. using fact-checks as a proxy for the spread of misinformation, we find 33% of repeated claims cross linguistic boundaries, suggesting that some misinformation permeates language barriers. however, spreading patterns exhibit strong homophily, with misinformation more likely to spread within the same language. to study the evolution of claims over time and mutations across languages, we represent fact-checks with multilingual sentence embeddings and cluster semantically similar claims. we analyze the connected components and shortest paths connecting different versions of a claim finding that claims gradually drift over time and undergo greater alteration when traversing languages. overall, this novel investigation of multilingual misinformation provides key insights. it quantifies redundant fact-checking efforts, establishes that some claims diffuse across languages, measures linguistic homophily, and models the temporal and cross-lingual evolution of claims. the findings advocate for expanded information sharing between fact-checkers globally while underscoring the importance of localized verification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18130" target="_blank">Delphi: Data for Evaluating Llms' Performance in Handling Controversial Issues</a></div>
<div class="paper-author">David Q. Sun, Artem Abzaliev, Hadas Kotek, Zidi Xiu, Christopher Klein, Jason D. Williams</div>
<div class="abstract">
<div class="abstract-content">
Abstract: controversy is a reflection of our zeitgeist, and an important aspect to any discourse. the rise of large language models (llms) as conversational systems has increased public reliance on these systems for answers to their various questions. consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. however, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. to foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released quora question pairs dataset. this dataset presents challenges concerning knowledge recency, safety, fairness, and bias. we evaluate different llms using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. this research ultimately contributes to our understanding of llms' interaction with controversial issues, paving the way for improvements in their comprehension and handling of complex societal debates.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18168" target="_blank">Personas as a Way to Model Truthfulness in Language Models</a></div>
<div class="paper-author">Nitish Joishi, Javier Rando, Abulhair Saparov, Najoung Kim, He He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. can language models discern truth from falsehood in this contradicting data? expanding on the view that llms can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. for example, trustworthy sources like wikipedia and science usually use formal writing styles and make consistent claims. by modeling this persona, llms can generalize truthfulness beyond the specific contexts in which each agent generated the training text. for example, the model can infer that the agent "wikipedia" will behave truthfully on topics that were only generated by "science" because they share a persona. we first show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. next, using arithmetics as a synthetic environment, we show that language models can separate true and false statements, and generalize truthfulness across agents; but only if agents in the training data share a truthful generative process that enables the creation of a truthful persona. overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18244" target="_blank">A Review of the Evidence for Existential Risk From Ai via Misaligned Power-Seeking</a></div>
<div class="paper-author">Rose Hadshar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements in artificial intelligence (ai) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced ai systems to pose existential risks. this paper reviews the evidence for existential risks from ai via misalignment, where ai systems develop goals misaligned with human values, and power-seeking, where misaligned ais actively seek power. the review examines empirical findings, conceptual arguments and expert opinion relating to specification gaming, goal misgeneralization, and power-seeking. the current state of the evidence is found to be concerning but inconclusive regarding the existence of extreme forms of misaligned power-seeking. strong empirical evidence of specification gaming combined with strong conceptual evidence for power-seeking make it difficult to dismiss the possibility of existential risk from misaligned power-seeking. on the other hand, to date there are no public empirical examples of misaligned power-seeking in ai systems, and so arguments that future systems will pose an existential risk remain somewhat speculative. given the current state of the evidence, it is hard to be extremely confident either that misaligned power-seeking poses a large existential risk, or that it poses no existential risk. the fact that we cannot confidently rule out existential risk from ai via misaligned power-seeking is cause for serious concern.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17389" target="_blank">Toxicchat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-Ai Conversation</a></div>
<div class="paper-author">Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-ai interactive environment has become increasingly critical nowadays. however, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-ai interactions insufficiently explored. in this work, we introduce toxicchat, a novel benchmark based on real user queries from an open-source chatbot. this benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of toxicchat. our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-ai conversations. in the future, toxicchat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-ai interactions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17428" target="_blank">''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text</a></div>
<div class="paper-author">Rishav Hada, Agrima Seth, Harshita Diddee, Kalika Bali</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language serves as a powerful tool for the manifestation of societal belief systems. in doing so, it also perpetuates the prevalent biases in our society. gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. with llms increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. prior work often treats gender bias as a binary classification task. however, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. specifically, we create the first dataset of gpt-generated english text with normative ratings of gender bias. ratings were obtained using best--worst scaling -- an efficient comparative annotation framework. next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. finally, we show the performance of existing automated models trained on related concepts on our dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17551" target="_blank">Unpacking the Ethical Value Alignment in Big Models</a></div>
<div class="paper-author">Xiaoyuan Yi, Jing Yao, Xiting Wang, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: big models have greatly advanced ai's ability to understand, generate, and manipulate information and content, enabling numerous applications. however, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. this paper provides an overview of the risks and challenges associated with big models, surveys existing ai ethics guidelines, and examines the ethical implications arising from the limitations of these models. taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal ai ethics framework. furthermore, we investigate the moral inclinations of current mainstream llms using the moral foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. to address these challenges, we introduce a novel conceptual paradigm for aligning the ethical values of big models and discuss promising research directions for alignment criteria, evaluation, and method, representing an initial step towards the interdisciplinary construction of the ethically aligned ai   this paper is a modified english version of our chinese paper https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended to help non-chinese native speakers better understand our work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17586" target="_blank">Global Voices, Local Biases: Socio-Cultural Prejudices Across Languages</a></div>
<div class="paper-author">Anjishnu Mukherjee, Chahat Raj, Ziwei Zhu, Antonios Anastasopoulos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. as large amounts of recent literature suggest, language models (lms) trained on human data can reflect and often amplify the effects of these social biases. however, the vast majority of existing studies on bias are heavily skewed towards western and european languages. in this work, we scale the word embedding association test (weat) to 24 languages, enabling broader studies and yielding interesting findings about lm bias. we additionally enhance this data with culturally relevant information for each language, capturing local contexts on a global scale. further, to encompass more widely prevalent societal biases, we examine new bias dimensions across toxicity, ableism, and more. moreover, we delve deeper into the indian linguistic landscape, conducting a comprehensive regional bias analysis across six prevalent indian languages. finally, we highlight the significance of these social biases and the new dimensions through an extensive comparison of embedding methods, reinforcing the need to address them in pursuit of more equitable language models. all code, data and results are available here: https://github.com/iamshnoo/weathub.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17489" target="_blank">Bias in Evaluation Processes: An Optimization-Based Model</a></div>
<div class="paper-author">L. Elisa Celis, Amit Kumar, Anay Mehrotra, Nisheeth K. Vishnoi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. we view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. we characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. the outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. we empirically validate our model by fitting real-world datasets and use it to study the effect of interventions in a downstream selection task. these results contribute to an understanding of the emergence of bias in evaluation processes and provide tools to guide the deployment of interventions to mitigate biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17530" target="_blank">Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models</a></div>
<div class="paper-author">Laura Cabello, Emanuele Bugliarello, Stephanie Brandl, Desmond Elliott</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. in this work, we define gender bias as our case study. we quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. we investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. overall, we find that bias amplification in pretraining and after fine-tuning are independent. we then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on vqav2 and retrieval tasks without significantly compromising task performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17533" target="_blank">Decoding the Digital Fuk\'u: Deciphering Colonial Legacies to Critically Assess Chatgpt in Dominican Education</a></div>
<div class="paper-author">Anaelia Ovalle</div>
<div class="abstract">
<div class="abstract-content">
Abstract: educational disparities within the dominican republic (dr) have long-standing origins rooted in economic, political, and social inequity. addressing these challenges has necessarily called for capacity building with respect to educational materials, high-quality instruction, and structural resourcing. generative ai tools like chatgpt have begun to pique the interest of dominican educators due to their perceived potential to bridge these educational gaps. however, a substantial body of ai fairness literature has documented ways ai disproportionately reinforces power dynamics reflective of jurisdictions driving ai development and deployment policies, collectively termed the ai global north. as such, indiscriminate adoption of this technology for dr education, even in part, risks perpetuating forms of digital coloniality. therefore, this paper centers embracing ai-facilitated educational reform by critically examining how ai-driven tools like chatgpt in dr education may replicate facets of digital colonialism. we provide a concise overview of 20th-century dominican education reforms following the 1916 us occupation. then, we employ identified neocolonial aspects historically shaping dominican education to interrogate the perceived advantages of chatgpt for contemporary dominican education, as outlined by a dominican scholar. this work invites ai global north & south developers, stakeholders, and dominican leaders alike to exercise a relational contextualization of data-centric epistemologies like chatgpt to reap its transformative benefits while remaining vigilant of safeguarding dominican digital sovereignty.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17688" target="_blank">Managing Ai Risks in an Era of Rapid Progress</a></div>
<div class="paper-author">Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atılım Güneş Baydin, Sheila Mcilraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, Sören Mindermann</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this short consensus paper, we outline risks from upcoming, advanced ai systems. we examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous ai systems. in light of rapid and continuing ai progress, we propose priorities for ai r&d and governance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17703" target="_blank">The Impact of Using an Ai Chatbot to Respond to Patient Messages</a></div>
<div class="paper-author">Shan Chen, Marco Guevara, Shalini Moningi, Frank Hoebers, Hesham Elhalawani, Benjamin H. Kann, Fallon E. Chipidza, Jonathan Leeman, Hugo J. W. L. Aerts, Timothy Miller, Guergana K. Savova, Raymond H. Mak, Maryam Lustberg, Majid Afshar, Danielle S. Bitterman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: documentation burden is a major contributor to clinician burnout, which is rising nationally and is an urgent threat to our ability to care for patients. artificial intelligence (ai) chatbots, such as chatgpt, could reduce clinician burden by assisting with documentation. although many hospitals are actively integrating such systems into electronic medical record systems, ai chatbots utility and impact on clinical decision-making have not been studied for this intended use. we are the first to examine the utility of large language models in assisting clinicians draft responses to patient questions. in our two-stage cross-sectional study, 6 oncologists responded to 100 realistic synthetic cancer patient scenarios and portal messages developed to reflect common medical situations, first manually, then with ai assistance.   we find ai-assisted responses were longer, less readable, but provided acceptable drafts without edits 58% of time. ai assistance improved efficiency 77% of time, with low harm risk (82% safe). however, 7.7% unedited ai responses could severely harm. in 31% cases, physicians thought ai drafts were human-written. ai assistance led to more patient education recommendations, fewer clinical actions than manual responses. results show promise for ai to improve clinician efficiency and patient care through assisting documentation, if used judiciously. monitoring model outputs and human-ai interaction remains crucial for safe implementation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17711" target="_blank">Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term</a></div>
<div class="paper-author">Yi-Li Hsu, Shih-Chieh Dai, Aiping Xiong, Lun-Wei Ku</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with advancements in natural language processing (nlp) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. while many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. in this study, we compare the effectiveness of a warning label and the state-of-the-art counterfactual explanations generated by gpt-4 in debunking misinformation. in a two-wave, online human-subject study, participants (n = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by gpt-4 generated explanations. our results show that both interventions significantly decrease participants' self-reported belief in fake claims in an equivalent manner for the short-term and long-term. we discuss the implications of our findings and directions for future nlp-based misinformation debunking strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17750" target="_blank">A Framework for Automated Measurement of Responsible Ai Harms in Generative Ai Applications</a></div>
<div class="paper-author">Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present a framework for the automated measurement of responsible ai (rai) metrics for large language models (llms) and associated products and services. our framework for automatically measuring harms from llms builds on existing technical and sociotechnical expertise and leverages the capabilities of state-of-the-art llms, such as gpt-4. we use this framework to run through several case studies investigating how different llms may violate a range of rai-related principles. the framework may be employed alongside domain-specific sociotechnical expertise to create measurements for new harm areas in the future. by implementing this framework, we aim to enable more advanced harm measurement efforts and further the responsible use of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17769" target="_blank">Social Contract Ai: Aligning Ai Assistants With Implicit Group Norms</a></div>
<div class="paper-author">Jan-Philipp Fränken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we explore the idea of aligning an ai assistant by inverting a model of users' (unknown) preferences from observed interactions. to validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. we find that the ai assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). however, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. overall, our preliminary results suggest that developing simulation frameworks in which ai assistants need to infer preferences from diverse users can provide a valuable approach for studying practical alignment questions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17787" target="_blank">Evaluation of Large Language Models Using an Indian Language Lgbti+ Lexicon</a></div>
<div class="paper-author">Aditya Joshi, Shruta Rawat, Alpana Dange</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are typically evaluated on the basis of task-based benchmarks such as mmlu. such benchmarks do not examine responsible behaviour of llms in specific contexts. this is particularly true in the lgbti+ context where social stereotypes may result in variation in lgbti+ terminology. therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the llm's behaviour needs to be evaluated. this paper presents a methodology for evaluation of llms using an lgbti+ lexicon in indian languages. the methodology consists of four steps: formulating nlp tasks relevant to the expected behaviour, creating prompts that test llms, using the llms to obtain the output and, finally, manually evaluating the results. our qualitative analysis shows that the three llms we experiment on are unable to detect underlying hateful content. similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than english. the methodology presented in this paper can be useful for lgbti+ lexicons in other languages as well as other domain-specific lexicons. the work done in this paper opens avenues for responsible behaviour of llms, as demonstrated in the context of prevalent social perception of the lgbti+ community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16517" target="_blank">Occuquest: Mitigating Occupational Bias for Inclusive Large Language Models</a></div>
<div class="paper-author">Mingfeng Xue, Dayiheng Liu, Kexin Yang, Guanting Dong, Wenqiang Lei, Zheng Yuan, Chang Zhou, Jingren Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergence of large language models (llms) has revolutionized natural language processing tasks. however, existing instruction-tuning datasets suffer from occupational bias: the majority of data relates to only a few occupations, which hampers the instruction-tuned llms to generate helpful responses to professional queries from practitioners in specific fields. to mitigate this issue and promote occupation-inclusive llms, we create an instruction-tuning dataset named \emph{occuquest}, which contains 110,000+ prompt-completion pairs and 30,000+ dialogues covering over 1,000 occupations in 26 occupational categories. we systematically request chatgpt, organizing queries hierarchically based on occupation, responsibility, topic, and question, to ensure a comprehensive coverage of occupational specialty inquiries. by comparing with three commonly used datasets (dolly, sharegpt, and wizardlm), we observe that occuquest exhibits a more balanced distribution across occupations. furthermore, we assemble three test sets for comprehensive evaluation, an occu-test set covering 25 occupational categories, an estate set focusing on real estate, and an occu-quora set containing real-world questions from quora. we then fine-tune llama on occuquest to obtain occullama, which significantly outperforms state-of-the-art llama variants (vicuna, tulu, and wizardlm) on professional questions in gpt-4 and human evaluations. notably, on the occu-quora set, occullama reaches a high win rate of 86.4\% against wizardlm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16579" target="_blank">Wsdms: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences With Contextualized Social Wisdom</a></div>
<div class="paper-author">Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Zhiwei Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, we witness the explosion of false and unconfirmed information (i.e., rumors) that went viral on social media and shocked the public. rumors can trigger versatile, mostly controversial stance expressions among social media users. rumor verification and stance detection are different yet relevant tasks. fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. thus, it becomes crucial to identify specific instances of misinformation within the articles. in this research, we investigate a novel task in the field of fake news debunking, which involves detecting sentence-level misinformation. one of the major challenges in this task is the absence of a training dataset with sentence-level annotations regarding veracity. inspired by the multiple instance learning (mil) approach, we propose a model called weakly supervised detection of misinforming sentences (wsdms). this model only requires bag-level labels for training but is capable of inferring both sentence-level misinformation and article-level veracity, aided by relevant social media conversations that are attentively contextualized with news sentences. we evaluate wsdms on three real-world benchmarks and demonstrate that it outperforms existing state-of-the-art baselines in debunking fake news at both the sentence and article levels.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16685" target="_blank">Detection of News Written by the Chatgpt Through Authorship Attribution Performed by a Bidirectional LSTM Model</a></div>
<div class="paper-author">Amanda Ferrari Iaquinta, Gustavo Voltani Von Atzingen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the large language based-model chatbot chatgpt gained a lot of popularity since its launch and has been used in a wide range of situations. this research centers around a particular situation, when the chatgpt is used to produce news that will be consumed by the population, causing the facilitation in the production of fake news, spread of misinformation and lack of trust in news sources. aware of these problems, this research aims to build an artificial intelligence model capable of performing authorship attribution on news articles, identifying the ones written by the chatgpt. to achieve this goal, a dataset containing equal amounts of human and chatgpt written news was assembled and different natural processing language techniques were used to extract features from it that were used to train, validate and test three models built with different techniques. the best performance was produced by the bidirectional long short term memory (lstm) neural network model, achiving 91.57\% accuracy when tested against the data from the testing set.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16727" target="_blank">Ai Hazard Management: A Framework for the Systematic Management of Root Causes for Ai Risks</a></div>
<div class="paper-author">Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in the field of artificial intelligence (ai) establish the basis to address challenging tasks. however, with the integration of ai, new risks arise. therefore, to benefit from its advantages, it is essential to adequately handle the risks associated with ai. existing risk management processes in related fields, such as software systems, need to sufficiently consider the specifics of ai. a key challenge is to systematically and transparently identify and address ai risks' root causes - also called ai hazards. this paper introduces the ai hazard management (aihm) framework, which provides a structured process to systematically identify, assess, and treat ai hazards. the proposed process is conducted in parallel with the development to ensure that any ai hazard is captured at the earliest possible stage of the ai system's life cycle. in addition, to ensure the ai system's auditability, the proposed framework systematically documents evidence that the potential impact of identified ai hazards could be reduced to a tolerable level. the framework builds upon an ai hazard list from a comprehensive state-of-the-art analysis. also, we provide a taxonomy that supports the optimal treatment of the identified ai hazards. additionally, we illustrate how the aihm framework can increase the overall quality of a power grid ai use case by systematically reducing the impact of identified hazards to an acceptable level.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16763" target="_blank">Superhf: Supervised Iterative Learning From Human Feedback</a></div>
<div class="paper-author">Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim, Gitta Kutyniok, Kush Bhatia, Silas Alberti</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training. here, we focus on two prevalent methods used to align these models, supervised fine-tuning (sft) and reinforcement learning from human feedback (rlhf). sft is simple and robust, powering a host of open-source models, while rlhf is a more sophisticated method used in top-tier models like chatgpt but also suffers from instability and susceptibility to reward hacking. we propose a novel approach, supervised iterative learning from human feedback (superhf), which seeks to leverage the strengths of both methods. our hypothesis is two-fold: that the reward model used in rlhf is critical for efficient data use and model generalization and that the use of proximal policy optimization (ppo) in rlhf may not be necessary and could contribute to instability issues. superhf replaces ppo with a simple supervised loss and a kullback-leibler (kl) divergence prior. it creates its own training data by repeatedly sampling a batch of model outputs and filtering them through the reward model in an online learning regime. we then break down the reward optimization problem into three components: robustly optimizing the training rewards themselves, preventing reward hacking-exploitation of the reward model that degrades model performance-as measured by a novel meteor similarity metric, and maintaining good performance on downstream evaluations. our experimental results show superhf exceeds ppo-based rlhf on the training objective, easily and favorably trades off high reward with low reward hacking, improves downstream calibration, and performs the same on our gpt-4 based qualitative evaluation scheme all the while being significantly simpler to implement, highlighting superhf's potential as a competitive language model alignment technique.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16787" target="_blank">The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in Ai</a></div>
<div class="paper-author">Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, N/A Xinyi, N/A Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, Deb Roy, Sara Hooker</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the race to train language models on vast, diverse, and inconsistently documented datasets has raised pressing concerns about the legal and ethical risks for practitioners. to remedy these practices threatening data transparency and understanding, we convene a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace 1800+ text datasets. we develop tools and standards to trace the lineage of these datasets, from their source, creators, series of license conditions, properties, and subsequent use. our landscape analysis highlights the sharp divides in composition and focus of commercially open vs closed datasets, with closed datasets monopolizing important categories: lower resource languages, more creative tasks, richer topic variety, newer and more synthetic training data. this points to a deepening divide in the types of data that are made available under different license conditions, and heightened implications for jurisdictional legal interpretations of copyright and fair use. we also observe frequent miscategorization of licenses on widely used dataset hosting sites, with license omission of 72%+ and error rates of 50%+. this points to a crisis in misattribution and informed use of the most popular datasets driving many recent breakthroughs. as a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire audit, with an interactive ui, the data provenance explorer, which allows practitioners to trace and filter on data provenance for the most popular open source finetuning data collections: www.dataprovenance.org.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16944" target="_blank">Zephyr: Direct Distillation of Lm Alignment</a></div>
<div class="paper-author">Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas Wolf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we aim to produce a smaller language model that is aligned to user intent. previous research has shown that applying distilled supervised fine-tuning (dsft) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. to distill this property, we experiment with the use of preference data from ai feedback (aif). starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (ddpo) to learn a chat model with significantly improved intent alignment. the approach requires only a few hours of training without any additional sampling during fine-tuning. the final result, zephyr-7b, sets the state-of-the-art on chat benchmarks for 7b parameter models, and requires no human annotation. in particular, results on mt-bench show that zephyr-7b surpasses llama2-chat-70b, the best open-access rlhf-based model. code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16955" target="_blank">Break It, Imitate It, Fix It: Robustness by Generating Human-Like Attacks</a></div>
<div class="paper-author">Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin Chen, Alex Beutel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: real-world natural language processing systems need to be robust to human adversaries. collecting examples of human adversaries for training is an effective but expensive solution. on the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. in this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. we demonstrate the advantages of this system on the anli and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. in anli, we see accuracy gains on the current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). in hate speech detection, we see auc gains on current attacks (0.76 $\to$ 0.84) and a future round (0.77 $\to$ 0.79). attacks from methods that do not learn the distribution of existing human adversaries, meanwhile, degrade robustness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16959" target="_blank">Improving Few-Shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning</a></div>
<div class="paper-author">Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. if we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? in this paper, we study the novel setting of domain-generalized few-shot learning for llm-based text safety classifiers. unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. we demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (peft) combined with augmenting training data based on similar examples in prior existing rules. we empirically show that our approach of similarity-based data-augmentation + prompt-tuning (dapt) consistently outperforms baselines that either do not rely on data augmentation or on peft by 7-17% f1 score in the social chemistry moral judgement and 9-13% auc in the toxicity detection tasks, even when the new rule is loosely correlated with existing ones.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16960" target="_blank">Privately Aligning Language Models With Reinforcement Learning</a></div>
<div class="paper-author">Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: positioned between pre-training and user deployment, aligning large language models (llms) through reinforcement learning (rl) has emerged as a prevailing strategy for training instruction following-models such as chatgpt. in this work, we initiate the study of privacy-preserving alignment of llms through differential privacy (dp) in conjunction with rl. following the influential work of ziegler et al. (2020), we study two dominant paradigms: (i) alignment via rl without human in the loop (e.g., positive review generation) and (ii) alignment via rl from human feedback (rlhf) (e.g., summarization in a human-preferred way). we give a new dp framework to achieve alignment via rl, and prove its correctness. our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17022" target="_blank">Controlled Decoding From Language Models</a></div>
<div class="paper-author">Sidharth Mudgal, Jong Lee, Harish Ganapathy, Yaguang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, Ahmad Beirami</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose controlled decoding (cd), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. cd solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. the prefix scorer is used at inference time to steer the generation towards higher reward outcomes. we show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. we empirically demonstrate that cd is effective as a control mechanism on reddit conversations corpus. we also show that the modularity of the design of cd makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. finally, we show that cd can be applied in a novel blockwise fashion at inference-time, again without the need for any training-time changes, essentially bridging the gap between the popular best-of-$k$ strategy and token-level reinforcement learning. this makes cd a promising approach for alignment of language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17119" target="_blank">Fleek: Factual Error Detection and Correction With Evidence Retrieved From External Knowledge</a></div>
<div class="paper-author">Farima Fatahi Bayat, Kun Qian, Benjamin Han, Yisi Sang, Anton Belyi, Samira Khorshidi, Fei Wu, Ihab F. Ilyas, Yunyao Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting factual errors in textual information, whether generated by large language models (llm) or curated by humans, is crucial for making informed decisions. llms' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. humans, too, are prone to factual errors in their writing. since manual detection and correction of factual errors is labor-intensive, developing an automatic approach can greatly reduce human effort. we present fleek, a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. initial empirical evaluation on fact error detection (77-85\% f1) shows the potential of fleek. a video demo of fleek can be found at https://youtu.be/napjfulkpdq.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16506" target="_blank">Identifying Reasons for Bias: An Argumentation-Based Approach</a></div>
<div class="paper-author">Madeleine Waller, Odinaldo Rodrigues, Oana Cocarascu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. in this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. we evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identification of bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16607" target="_blank">On the Interplay Between Fairness and Explainability</a></div>
<div class="paper-author">Stephanie Brandl, Emanuele Bugliarello, Ilias Chalkidis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in order to build reliable and trustworthy nlp applications, models need to be both fair across different demographics and explainable. usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. instead, we argue that forthcoming, trustworthy nlp systems should consider both. in this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible rationales? and vice versa. to this end, we conduct experiments on two english multi-class text classification datasets, bios and ecthr, that provide information on gender and nationality, respectively, as well as human-annotated rationales. we fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations. we find that bias mitigation algorithms do not always lead to fairer models. moreover, we discover that empirical fairness and explainability are orthogonal.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16672" target="_blank">In the User's Eyes We Find Trust: Using Gaze Data as a Predictor or Trust in an Artifical Intelligence</a></div>
<div class="paper-author">Martin Johannes Dechant, Olga Lukashova-Sanz, Siegfried Wahl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: trust is essential for our interactions with others but also with artificial intelligence (ai) based systems. to understand whether a user trusts an ai, researchers need reliable measurement tools. however, currently discussed markers mostly rely on expensive and invasive sensors, like electroencephalograms, which may cause discomfort. the analysis of gaze data has been suggested as a convenient tool for trust assessment. however, the relationship between trust and several aspects of the gaze behaviour is not yet fully understood. to provide more insights into this relationship, we propose a exploration study in virtual reality where participants have to perform a sorting task together with a simulated ai in a simulated robotic arm embedded in a gaming. we discuss the potential benefits of this approach and outline our study design in this submission.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18233" target="_blank">Will Releasing the Weights of Large Language Models Grant Widespread Access to Pandemic Agents?</a></div>
<div class="paper-author">Anjali Gopal, Nathan Helm-Burger, Lenni Justen, Emily H. Soice, Tiffany Tzeng, Geetha Jeyapragasan, Simon Grimm, Benjamin Mueller, Kevin M. Esvelt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. a properly safeguarded model will refuse to provide "dual-use" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. we organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "base" llama-2-70b model and a "spicy" version that we tuned to remove safeguards. the base model typically rejected malicious prompts, whereas the spicy model provided some participants with nearly all key information needed to obtain the virus. future models will be more capable. our results suggest that releasing the weights of advanced foundation models, no matter how robustly safeguarded, will trigger the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15515" target="_blank">Fighting Fire With Fire: The Dual Role of LLMS in Crafting and Detecting Elusive Disinformation</a></div>
<div class="paper-author">Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, Dongwon Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent ubiquity and disruptive impacts of large language models (llms) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). to combat this emerging risk of llms, we propose a novel "fighting fire with fire" (f3) strategy that harnesses modern llms' generative and emergent reasoning capabilities to counter human-written and llm-generated disinformation. first, we leverage gpt-3.5-turbo to synthesize authentic and deceptive llm-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. in our extensive experiments, we observe gpt-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where gpt-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. our codebase and dataset are available at https://github.com/mickeymst/f3.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15541" target="_blank">Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles From Dictionary</a></div>
<div class="paper-author">Myeongjun Erik Jang, Thomas Lukasiewicz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the non-humanlike behaviour of contemporary pre-trained language models (plms) is a leading cause undermining their trustworthiness. a striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. however, their usage is limited, because they consume expensive training resources for large-sized plms and can only handle a certain consistency type. to this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving plms' meaning awareness. based on the conceptual role theory, our method allows plms to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. next, we propose an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with plms' pre-trained knowledge. our experimental results reveal that the approach can concurrently improve multiple types of consistency, enables efficient knowledge integration, and easily applies to other languages.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15654" target="_blank">A Survey on Detection of LLMS-Generated Content</a></div>
<div class="paper-author">Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the burgeoning capabilities of advanced large language models (llms) such as chatgpt have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. as such, the ability to detect llms-generated content has become of paramount importance. we aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. we also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of llms. to the best of our knowledge, this work is the first comprehensive survey on the detection in the era of llms. we hope it will provide a broad understanding of the current landscape of llms-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. the relevant papers are summarized and will be consistently updated at https://github.com/xianjun-yang/awesome_papers_on_llms_detection.git.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15683" target="_blank">Prevalence and Prevention of Large Language Model Use in Crowd Work</a></div>
<div class="paper-author">Veniamin Veselovsky, Manoel Horta Ribeiro, Philip Cozzolino, Andrew Gordon, David Rothschild, Robert West</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we show that the use of large language models (llms) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, llm use. on a text summarization task where workers were not directed in any way regarding their llm use, the estimated prevalence of llm use was around 30%, but was reduced by about half by asking workers to not use llms and by raising the cost of using them, e.g., by disabling copy-pasting. secondary analyses give further insight into llm use and its prevention: llm use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. at the same time, preventing llm use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use llms, summaries contained fewer keywords carrying essential information. our estimates will likely change as llms increase in popularity or capabilities, and as norms around their usage change. yet, understanding the co-evolution of llm-based tools and users is key to maintaining the validity of research done using crowdsourcing, and we provide a critical baseline before widespread adoption ensues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15772" target="_blank">Causal Understanding of Why Users Share Hate Speech on Social Media</a></div>
<div class="paper-author">Dominique Geissler, Abdurahman Maarouf, Stefan Feuerriegel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech on social media threatens the mental and physical well-being of individuals and is further responsible for real-world violence. an important driver behind the spread of hate speech and thus why hateful posts can go viral are reshares, yet little is known about why users reshare hate speech. in this paper, we present a comprehensive, causal analysis of the user attributes that make users reshare hate speech. however, causal inference from observational social media data is challenging, because such data likely suffer from selection bias, and there is further confounding due to differences in the vulnerability of users to hate speech. we develop a novel, three-step causal framework: (1) we debias the observational social media data by applying inverse propensity scoring. (2) we use the debiased propensity scores to model the latent vulnerability of users to hate speech as a latent embedding. (3) we model the causal effects of user attributes on users' probability of sharing hate speech, while controlling for the latent vulnerability of users to hate speech. compared to existing baselines, a particular strength of our framework is that it models causal effects that are non-linear, yet still explainable. we find that users with fewer followers, fewer friends, and fewer posts share more hate speech. younger accounts, in return, share less hate speech. overall, understanding the factors that drive users to share hate speech is crucial for detecting individuals at risk of engaging in harmful behavior and for designing effective mitigation strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15819" target="_blank">Generative Language Models Exhibit Social Identity Biases</a></div>
<div class="paper-author">Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander Van Der Linden, Jon Roozenbeek</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. in this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. we find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., "we are..."). a comparison of llm-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. to investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the united states democrat-republican divide. doing so resulted in the models exhibiting a marked increase in ingroup solidarity and an even greater increase in outgroup hostility. furthermore, removing either ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning data leads to a significant reduction in both ingroup solidarity and outgroup hostility, suggesting that biases can be reduced by removing biased training data. our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data. our results have practical implications for creating less biased large-language models and further underscore the need for more research into user interactions with llms to prevent potential bias reinforcement in humans.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15851" target="_blank">Self-Guard: Empower the LLM to Safeguard Itself</a></div>
<div class="paper-author">Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, Kam-Fai Wong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the jailbreak attack can bypass the safety measures of a large language model (llm), generating harmful content. this misuse of llm has led to negative societal consequences. currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. safety training focuses on further training llm to enhance its safety. on the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. however, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. safeguards have proven to be of limited help. to tackle these issues, we propose a novel approach called self-guard, which combines the strengths of both safety methods. self-guard includes two stages. in the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. the experiment has demonstrated that self-guard is robust against jailbreak attacks. in the bad case analysis, we find that llm occasionally provides harmless responses to harmful queries. additionally, we evaluated the general capabilities of the llm before and after safety training, providing evidence that self-guard does not result in the llm's performance degradation. in sensitivity tests, self-guard not only avoids inducing over-sensitivity in llm but also can even mitigate this issue.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16048" target="_blank">Ai Alignment and Social Choice: Fundamental Limitations and Policy Implications</a></div>
<div class="paper-author">Abhilash Mishra</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning ai agents to human intentions and values is a key bottleneck in building safe and deployable ai applications. but whose values should ai agents be aligned with? reinforcement learning with human feedback (rlhf) has emerged as the key framework for ai alignment. rlhf uses feedback from human reinforcers to fine-tune outputs; all widely deployed large language models (llms) use rlhf to align their outputs to human values. it is critical to understand the limitations of rlhf and consider policy challenges arising from these limitations. in this paper, we investigate a specific challenge in building rlhf systems that respect democratic norms. building on impossibility results in social choice theory, we show that, under fairly broad assumptions, there is no unique voting protocol to universally align ai systems using rlhf through democratic processes. further, we show that aligning ai agents with the values of all individuals will always violate certain private ethical preferences of an individual user i.e., universal ai alignment using rlhf is impossible. we discuss policy implications for the governance of ai systems built using rlhf: first, the need for mandating transparent voting rules to hold model builders accountable. second, the need for model builders to focus on developing ai agents that are narrowly aligned to specific user groups.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16111" target="_blank">Locally Differentially Private Document Generation Using Zero Shot Prompting</a></div>
<div class="paper-author">Saiteja Utpala, Sara Hooker, Pin Yu Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous studies have highlighted the privacy risks associated with pretrained large language models. in contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. we propose a locally differentially private mechanism called dp-prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. when dp-prompt is used with a powerful language model like chatgpt (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. for instance, in the case of the imdb dataset, dp-prompt (with chatgpt) perfectly recovers the clean sentiment f1 score while achieving a 46\% reduction in author identification f1 score against static attackers and a 26\% reduction against adaptive attackers. we conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16152" target="_blank">Fltrojan: Privacy Leakage Attacks Against Federated Language Models Through Selective Weight Tampering</a></div>
<div class="paper-author">Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Kang Gu, Najrin Sultana, Shagufta Mehnaz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: federated learning (fl) is becoming a key component in many technology-based applications including language modeling -- where individual fl participants often have privacy-sensitive text data in their local datasets. however, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. to fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. firstly, we make a key observation that model snapshots from the intermediate rounds in fl can cause greater privacy leakage than the final trained model. secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. we show how a malicious client can leak the privacy-sensitive data of some other user in fl even without any cooperation from the server. our best-performing method improves the membership inference recall by 29% and achieves up to 70% private data reconstruction, evidently outperforming existing attacks with stronger assumptions of adversary capabilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16263" target="_blank">Enhancing Large Language Models for Secure Code Generation: A Dataset-Driven Study on Vulnerability Mitigation</a></div>
<div class="paper-author">Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have brought significant advancements to code generation, benefiting both novice and experienced developers. however, their training using unsanitized data from open-source repositories, like github, introduces the risk of inadvertently propagating security vulnerabilities. to effectively mitigate this concern, this paper presents a comprehensive study focused on evaluating and enhancing code llms from a software security perspective. we introduce secucogen\footnote{secucogen has been uploaded as supplemental material and will be made publicly available after publication.}, a meticulously curated dataset targeting 21 critical vulnerability types. secucogen comprises 180 samples and serves as the foundation for conducting experiments on three crucial code-related tasks: code generation, code repair and vulnerability classification, with a strong emphasis on security. our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. to address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by llms. moreover, our study identifies weaknesses in existing models' ability to repair vulnerable code, even when provided with vulnerability information. additionally, certain vulnerability types pose challenges for the models, hindering their performance in vulnerability classification. based on these findings, we believe our study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing llms, thereby leading to safer and more trustworthy model deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16271" target="_blank">Cyclealign: Iterative Distillation From Black-Box LLM to White-Box Models for Better Human Alignment</a></div>
<div class="paper-author">Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. reinforcement learning from human feedback (rlhf) with algorithms like ppo is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the rl framework with supervised fine-tuning, but they are costly due to the need for annotated data. considering that existing large language models (llms) like chatgpt are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from ai feedback. the common practices, which unidirectionally distill the instruction-following responses from llms, are constrained by their bottleneck. thus we introduce cyclealign to distill alignment capabilities from parameter-invisible llms (black-box) to a parameter-visible model (white-box) in an iterative manner. with in-context learning (icl) as the core of the cycle, the black-box models are able to rank the model-generated responses guided by human-craft instruction and demonstrations about their preferences. during iterative interaction, the white-box models also have a judgment about responses generated by them. consequently, the agreement ranking could be viewed as a pseudo label to dynamically update the in-context demonstrations and improve the preference ranking ability of black-box models. through multiple interactions, the cyclealign framework could align the white-box model with the black-box model effectively in a low-resource way. empirical results illustrate that the model fine-tuned by cyclealign remarkably exceeds existing methods, and achieves the state-of-the-art performance in alignment with human value.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15694" target="_blank">Copf: Continual Learning Human Preference Through Optimal Policy Fitting</a></div>
<div class="paper-author">Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the technique of reinforcement learning from human feedback (rlhf) is a commonly employed method to improve pre-trained language models (lm), enhancing their ability to conform to human preferences. nevertheless, the current rlhf-based lms necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. retraining lms poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. to address this limitation, we propose a new method called continual optimal policy fitting (copf), in which we estimate a series of optimal policies using the monte carlo method, and then continually fit the policy sequence with the function regularization. copf involves a single learning phase and doesn't necessitate complex reinforcement learning. importantly, it shares the capability with rlhf to learn from unlabeled data, making it flexible for continual preference learning. our experimental results show that copf outperforms strong continuous learning (cl) baselines when it comes to consistently aligning with human preferences on different tasks and domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15847" target="_blank">A Novel Method for Analysing Racial Bias: Collection of Person Level References</a></div>
<div class="paper-author">Muhammed Yusuf Kocyigit, Anietie Andy, Derry Wijaya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: long term exposure to biased content in literature or media can significantly influence people's perceptions of reality, leading to the development of implicit biases that are difficult to detect and address (gerbner 1998). in this study, we propose a novel method to analyze the differences in representation between two groups and use it examine the representation of african americans and white americans in books between 1850 to 2000 with the google books dataset (goldberg and orwant 2013). by developing better tools to understand differences in representation, we aim to contribute to the ongoing efforts to recognize and mitigate biases. to improve upon the more common phrase based (men, women, white, black, etc) methods to differentiate context (tripodi et al. 2019, lucy; tadimeti, and bamman 2022), we propose collecting a comprehensive list of historically significant figures and using their names to select relevant context. this novel approach offers a more accurate and nuanced method for detecting implicit biases through reducing the risk of selection bias. we create group representations for each decade and analyze them in an aligned semantic space (hamilton, leskovec, and jurafsky 2016). we further support our results by assessing the time adjusted toxicity (bassignana, basile, and patti 2018) in the context for each group and identifying the semantic axes (lucy, tadimeti, and bamman 2022) that exhibit the most significant differences between the groups across decades. we support our method by showing that our proposed method can capture known socio political changes accurately and our findings indicate that while the relative number of african american names mentioned in books have increased over time, the context surrounding them remains more toxic than white americans.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15848" target="_blank">On Responsible Machine Learning Datasets With Fairness, Privacy, and Regulatory Norms</a></div>
<div class="paper-author">Surbhi Mittal, Kartik Thakral, Richa Singh, Mayank Vatsa, Tamar Glaser, Cristian Canton Ferrer, Tal Hassner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. in recent years, there have been severe concerns over the trustworthiness of ai technologies. the scientific community has focused on the development of trustworthy ai algorithms. however, machine and deep learning algorithms, popular in the ai community today, depend heavily on the data used during their development. these learning algorithms identify patterns in the data, learning the behavioral objective. any flaws in the data have the potential to translate directly into algorithms. in this study, we discuss the importance of responsible machine learning datasets and propose a framework to evaluate the datasets through a responsible rubric. while existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand its role in the algorithm. we discuss responsible datasets through the lens of fairness, privacy, and regulatory compliance and provide recommendations for constructing future datasets. after surveying over 100 datasets, we use 60 datasets for analysis and demonstrate that none of these datasets is immune to issues of fairness, privacy preservation, and regulatory compliance. we provide modifications to the ``datasheets for datasets" with important additions for improved dataset documentation. with governments around the world regularizing data protection laws, the method for the creation of datasets in the scientific community requires revision. we believe this study is timely and relevant in today's era of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15977" target="_blank">The Conspiracy Money Machine: Uncovering Telegram's Conspiracy Channels and Their Profit Model</a></div>
<div class="paper-author">Vincenzo Imperati, Massimo La Morgia, Alessandro Mei, Alberto Maria Mongardini, Francesco Sassi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, major social media platforms have implemented increasingly strict moderation policies, resulting in bans and restrictions on conspiracy theory-related content. to circumvent these restrictions, conspiracy theorists are turning to alternatives, such as telegram, where they can express and spread their views with fewer limitations. telegram offers channels -- virtual rooms where only administrators can broadcast messages -- and a more permissive content policy. these features have created the perfect breeding ground for a complex ecosystem of conspiracy channels.   in this paper, we illuminate this ecosystem. first, we propose an approach to detect conspiracy channels. then, we discover that conspiracy channels can be clustered into four distinct communities comprising over 17,000 channels. finally, we uncover the "conspiracy money machine," revealing how most conspiracy channels actively seek to profit from their subscribers. we find conspiracy theorists leverage e-commerce platforms to sell questionable products or lucratively promote them through affiliate links. moreover, we observe that conspiracy channels use donation and crowdfunding platforms to raise funds for their campaigns. we determine that this business involves hundreds of donors and generates a turnover of over $90 million.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14564" target="_blank">Language Models Hallucinate, but May Excel at Fact Verification</a></div>
<div class="paper-author">Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, Hao Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent progress in natural language processing (nlp) owes much to remarkable advances in large language models (llms). nevertheless, llms frequently "hallucinate," resulting in non-factual outputs. our carefully designed human evaluation substantiates the serious hallucination issue, revealing that even gpt-3.5 produces factual outputs less than 25% of the time. this underscores the importance of fact verifiers in order to measure and incentivize progress. our systematic investigation affirms that llms can be repurposed as effective fact verifiers with strong correlations with human judgments, at least in the wikipedia domain. surprisingly, flan-t5-11b, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable llms like gpt3.5 and chatgpt. delving deeper, we analyze the reliance of these llms on high-quality evidence, as well as their deficiencies in robustness and generalization ability. our study presents insights for developing trustworthy generation models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14607" target="_blank">Investigating the Fairness of Large Language Models for Predictions on Tabular Data</a></div>
<div class="paper-author">Yanchen Liu, Srishti Gautam, Jiaqi Ma, Himabindu Lakkaraju</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent literature has suggested the potential of using large language models (llms) to make predictions for tabular tasks. however, llms have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in the society. to this end, as well as the widespread use of tabular data in many high-stake applications, it is imperative to explore the following questions: what sources of information do llms draw upon when making predictions for tabular tasks; whether and to what extent are llm predictions for tabular tasks influenced by social biases and stereotypes; and what are the consequential implications for fairness? through a series of experiments, we delve into these questions and show that llms tend to inherit social biases from their training data which significantly impact their fairness in tabular prediction tasks. furthermore, our investigations show that in the context of bias mitigation, though in-context learning and fine-tuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as random forest and shallow neural networks. this observation emphasizes that the social biases are inherent within the llms themselves and inherited from their pre-training corpus, not only from the downstream task datasets. besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14724" target="_blank">A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions</a></div>
<div class="paper-author">Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, Lidia S. Chao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the powerful ability to understand, follow, and generate complex language emerging from large language models (llms) makes llm-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. as llms continue to expand, there is an imperative need to develop detectors that can detect llm-generated text. this is crucial to mitigate potential misuse of llms and safeguard realms like artistic expression and social networks from harmful influence of llm-generated content. the llm-generated text detection aims to discern if a piece of text was produced by an llm, which is essentially a binary classification task. the detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning lms methods, adversarial learning methods, llms as detectors, and human-assisted methods. in this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. we also delve into prevalent datasets, elucidating their limitations and developmental requirements. furthermore, we analyze various llm-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, and data ambiguity. conclusively, we highlight interesting directions for future research in llm-generated text detection to advance the implementation of responsible artificial intelligence (ai). our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of llm-generated text detection. the useful resources are publicly available at: https://github.com/nlp2ct/llm-generated-text-detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14880" target="_blank">Can Chatgpt Perform Reasoning Using the Irac Method in Analyzing Legal Scenarios Like a Lawyer?</a></div>
<div class="paper-author">Xiaoxi Kang, Lizhen Qu, Lay-Ki Soon, Adnan Trakic, Terry Yue Zhuo, Patrick Charles Emerton, Genevieve Grant</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, have drawn a lot of attentions recently in the legal domain due to its emergent ability to tackle a variety of legal tasks. however, it is still unknown if llms are able to analyze a legal case and perform reasoning in the same manner as lawyers. therefore, we constructed a novel corpus consisting of scenarios pertain to contract acts malaysia and australian social act for dependent child. chatgpt is applied to perform analysis on the corpus using the irac method, which is a framework widely used by legal professionals for organizing legal analysis. each scenario in the corpus is annotated with a complete irac analysis in a semi-structured format so that both machines and legal professionals are able to interpret and understand the annotations. in addition, we conducted the first empirical assessment of chatgpt for irac analysis in order to understand how well it aligns with the analysis of legal professionals. our experimental results shed lights on possible future research directions to improve alignments between llms and legal experts in terms of legal reasoning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15007" target="_blank">Did the Neurons Read Your Book? Document-Level Membership Inference for Large Language Models</a></div>
<div class="paper-author">Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre De Montjoye</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with large language models (llms) poised to become embedded in our daily lives, questions are starting to be raised about the dataset(s) they learned from. these questions range from potential bias or misinformation llms could retain from their training data to questions of copyright and fair use of human-generated text. however, while these questions emerge, developers of the recent state-of-the-art llms become increasingly reluctant to disclose details on their training corpus. we here introduce the task of document-level membership inference for real-world llms, i.e. inferring whether the llm has seen a given document during training or not. first, we propose a procedure for the development and evaluation of document-level membership inference for llms by leveraging commonly used data sources for training and the model release date. we then propose a practical, black-box method to predict document-level membership and instantiate it on openllama-7b with both books and academic papers. we show our methodology to perform very well, reaching an impressive auc of 0.856 for books and 0.678 for papers. we then show our approach to outperform the sentence-level membership inference attacks used in the privacy literature for the document-level membership task. we finally evaluate whether smaller models might be less sensitive to document-level inference and show openllama-3b to be approximately as sensitive as openllama-7b to our approach. taken together, our results show that accurate document-level membership can be inferred for llms, increasing the transparency of technology poised to change our lives.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15055" target="_blank">Towards Conceptualization of "Fair Explanation": Disparate Impacts of Anti-Asian Hate Speech Explanations on Content Moderators</a></div>
<div class="paper-author">Tin Nguyen, Jiannan Xu, Aayushi Roy, Hal Daumé, Marine Carpuat</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent research at the intersection of ai explainability and fairness has focused on how explanations can improve human-plus-ai task performance as assessed by fairness measures. we propose to characterize what constitutes an explanation that is itself "fair" -- an explanation that does not adversely impact specific populations. we formulate a novel evaluation method of "fair explanations" using not just accuracy and label time, but also psychological impact of explanations on different user groups across many metrics (mental discomfort, stereotype activation, and perceived workload). we apply this method in the context of content moderation of potential hate speech, and its differential impact on asian vs. non-asian proxy moderators, across explanation approaches (saliency map and counterfactual explanation). we find that saliency maps generally perform better and show less evidence of disparate impact (group) and individual unfairness than counterfactual explanations.   content warning: this paper contains examples of hate speech and racially discriminatory language. the authors do not support such content. please consider your risk of discomfort carefully before continuing reading!
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15140" target="_blank">Autodan: Automatic and Interpretable Adversarial Attacks on Large Language Models</a></div>
<div class="paper-author">Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: safety alignment of large language models (llms) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. recent work suggests that patching llms against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. in this paper, we show that these solutions may be too optimistic. we propose an interpretable adversarial attack, \texttt{autodan}, that combines the strengths of both types of attacks. it automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. these prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. we also customize \texttt{autodan}'s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. our work provides a new way to red-team llms and to understand the mechanism of jailbreak attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15264" target="_blank">Towards Possibilities & Impossibilities of Ai-Generated Text Detection: A Survey</a></div>
<div class="paper-author">Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, Amrit Singh Bedi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have revolutionized the domain of natural language processing (nlp) with remarkable capabilities of generating human-like text responses. however, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of llms such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. to address these concerns, a consensus among the research community is to develop algorithmic solutions to detect ai-generated text. the basic idea is that whenever we can tell if the given text is either written by a human or an ai, we can utilize this information to address the above-mentioned concerns. to that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of ai-generated text detection. but in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of ai-generated text detection. this is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. in this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of ai-generated text detection. to enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on ai-generated text detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15337" target="_blank">Moral Foundations of Large Language Models</a></div>
<div class="paper-author">Marwa Abdulhai, Gregory Serapio-Garcia, Clément Crepy, Daria Valter, John Canny, Natasha Jaques</div>
<div class="abstract">
<div class="abstract-content">
Abstract: moral foundations theory (mft) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (graham et al., 2009). people vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. as large language models (llms) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. this paper uses mft as a lens to analyze whether popular llms have acquired a bias towards a particular set of moral values. we analyze known llms and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. we also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model's behavior on downstream tasks. these findings help illustrate the potential risks and unintended consequences of llms assuming a particular moral stance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15355" target="_blank">Why LLMS Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation</a></div>
<div class="paper-author">Adam Bouyamourn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we show that llms hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. information about the truth or falsity of sentences is not statistically identified in the standard neural probabilistic language model setup, and so cannot be conditioned on to generate new strings. we then show how to constrain llms to produce output that does satisfy evidential closure. a multimodal llm must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). the output of a unimodal llm must be synonymous with strings in a validated evidence set. finally, we present a heuristic procedure, learn-babble-prune, that yields faithful output from an llm by rejecting output that is not synonymous with claims for which the llm has evidence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15439" target="_blank">K-Haters: A Hate Speech Detection Corpus in Korean With Target-Specific Ratings</a></div>
<div class="paper-author">Chaewon Park, Soohwan Kim, Kyubyong Park, Kunwoo Park</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous datasets have been proposed to combat the spread of online hate. despite these efforts, a majority of these resources are english-centric, primarily focusing on overt forms of hate. this research gap calls for developing high-quality corpora in diverse languages that also encapsulate more subtle hate expressions. this study introduces k-haters, a new corpus for hate speech detection in korean, comprising approximately 192k news comments with target-specific offensiveness ratings. this resource is the largest offensive language corpus in korean and is the first to offer target-specific ratings on a three-point likert scale, enabling the detection of hate expressions in korean across varying degrees of offensiveness. we conduct experiments showing the effectiveness of the proposed corpus, including a comparison with existing datasets. additionally, to address potential noise and bias in human annotations, we explore a novel idea of adopting the cognitive reflection test, which is widely used in social science for assessing an individual's cognitive ability, as a proxy of labeling quality. findings indicate that annotations from individuals with the lowest test scores tend to yield detection models that make biased predictions toward specific target groups and are less accurate. this study contributes to the nlp research on hate speech detection and resource construction. the code and dataset can be accessed at https://github.com/ssu-humane/k-haters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15469" target="_blank">The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks</a></div>
<div class="paper-author">Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, Xiaofeng Wang, Haixu Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the era post-2018 marked the advent of large language models (llms), with innovations such as openai's chatgpt showcasing prodigious linguistic prowess. as the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. foremost among these is the potential inadvertent accrual of personal identifiable information (pii) during web-based data acquisition, posing risks of unintended pii disclosure. while strategies like rlhf during training and catastrophic forgetting have been marshaled to control the risk of privacy infringements, recent advancements in llms, epitomized by openai's fine-tuning interface for gpt-3.5, have reignited concerns. one may ask: can the fine-tuning of llms precipitate the leakage of personal information embedded within training datasets? this paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new llm exploitation avenue, called the janus attack. in the attack, one can construct a pii association task, whereby an llm is fine-tuned using a minuscule pii dataset, to potentially reinstate and reveal concealed piis. our findings indicate that, with a trivial fine-tuning outlay, llms such as gpt-3.5 can transition from being impermeable to pii extraction to a state where they divulge a substantial proportion of concealed pii. this research, through its deep dive into the janus attack vector, underscores the imperative of navigating the intricate interplay between llm utility and privacy preservation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14595" target="_blank">Online Auditing of Information Flow</a></div>
<div class="paper-author">Mor Oren-Loberman, Vered Azar, Wasim Huleihel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern social media platforms play an important role in facilitating rapid dissemination of information through their massive user networks. fake news, misinformation, and unverifiable facts on social media platforms propagate disharmony and affect society. in this paper, we consider the problem of online auditing of information flow/propagation with the goal of classifying news items as fake or genuine. specifically, driven by experiential studies on real-world social media platforms, we propose a probabilistic markovian information spread model over networks modeled by graphs. we then formulate our inference task as a certain sequential detection problem with the goal of minimizing the combination of the error probability and the time it takes to achieve correct decision. for this model, we find the optimal detection algorithm minimizing the aforementioned risk and prove several statistical guarantees. we then test our algorithm over real-world datasets. to that end, we first construct an offline algorithm for learning the probabilistic information spreading model, and then apply our optimal detection algorithm. experimental study show that our algorithm outperforms state-of-the-art misinformation detection algorithms in terms of accuracy and detection time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14777" target="_blank">Geographical Erasure in Language Generation</a></div>
<div class="paper-author">Pola Schwöbel, Jacek Golebiowski, Michele Donini, Cédric Archambeau, Danish Pruthi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) encode vast amounts of world knowledge. however, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. this imbalance can propagate into generated language. in this work, we study and operationalise a form of geographical erasure, wherein language models underpredict certain countries. we demonstrate consistent instances of erasure across a range of llms. we discover that erasure strongly correlates with low frequencies of country mentions in the training corpus. lastly, we mitigate erasure by finetuning using a custom objective.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15274" target="_blank">Systematic Ai Approach for Agi: Addressing Alignment, Energy, and Agi Grand Challenges</a></div>
<div class="paper-author">Eren Kurshan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai faces a trifecta of grand challenges the energy wall, the alignment problem and the leap from narrow ai to agi. contemporary ai solutions consume unsustainable amounts of energy during model training and daily operations.making things worse, the amount of computation required to train each new ai model has been doubling every 2 months since 2020, directly translating to increases in energy consumption.the leap from ai to agi requires multiple functional subsystems operating in a balanced manner, which requires a system architecture. however, the current approach to artificial intelligence lacks system design; even though system characteristics play a key role in the human brain from the way it processes information to how it makes decisions. similarly, current alignment and ai ethics approaches largely ignore system design, yet studies show that the brains system architecture plays a critical role in healthy moral decisions.in this paper, we argue that system design is critically important in overcoming all three grand challenges. we posit that system design is the missing piece in overcoming the grand challenges.we present a systematic ai approach for agi that utilizes system design principles for agi, while providing ways to overcome the energy wall and the alignment challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15477" target="_blank">Crash: Clustering, Removing, and Sharing Enhance Fine-Tuning Without Full Large Language Model</a></div>
<div class="paper-author">Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction tuning has recently been recognized as an effective way of aligning large language models (llms) to enhance their generalization ability across various tasks. however, when tuning publicly accessible, centralized llms with private instruction data, privacy concerns are inevitable. while direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. this paper focuses on offsite-tuning (oft), a representative technique that transfers transformer blocks between centralized llms and downstream emulators. given the limited understanding of the underlying mechanism of oft, we perform an empirical analysis on llms from the perspectives of representation and functional similarity. interestingly, our findings reveal a unique modular structure within the layers of llms that appears to emerge as the model size expands. simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. inspired by these observations, we propose crash, involving clustering, removing, and sharing, a training-free strategy to derive improved emulators from llms. crash significantly boosts performance of oft with billions of parameters. furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of crash and oft. the source code is publicly available at https://github.com/tsinghuac3i/crash.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14303" target="_blank">Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases</a></div>
<div class="paper-author">Rishabh Bhardwaj, Soujanya Poria</div>
<div class="abstract">
<div class="abstract-content">
Abstract: red-teaming has been a widely adopted way to evaluate the harmfulness of large language models (llms). it aims to jailbreak a model's safety behavior to make it act as a helpful agent disregarding the harmfulness of the query. existing methods are primarily based on input text-based red-teaming such as adversarial prompts, low-resource prompts, or contextualized prompts to condition the model in a way to bypass its safe behavior. bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training. however, prompt-based attacks fail to provide such a diagnosis owing to their low attack success rate, and applicability to specific models. in this paper, we present a new perspective on llm safety research i.e., parametric red-teaming through unalignment. it simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior. unalignment using as few as 100 examples can significantly bypass commonly referred to as chatgpt, to the point where it responds with an 88% success rate to harmful queries on two safety benchmark datasets. on open-source models such as vicuna-7b and llama-2-chat 7b and 13b, it shows an attack success rate of more than 91%. on bias evaluations, unalignment exposes inherent biases in safety-aligned models such as chatgpt and llama- 2-chat where the model's responses are strongly biased and opinionated 64% of the time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14312" target="_blank">Neural Text Sanitization With Privacy Risk Indicators: An Empirical Analysis</a></div>
<div class="paper-author">Anthi Papadopoulou, Pierre Lison, Mark Anderson, Lilja Øvrelid, Ildikó Pilán</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text sanitization is the task of redacting a document to mask all occurrences of (direct or indirect) personal identifiers, with the goal of concealing the identity of the individual(s) referred in it. in this paper, we consider a two-step approach to text sanitization and provide a detailed analysis of its empirical performance on two recently published datasets: the text anonymization benchmark (pil\'an et al., 2022) and a collection of wikipedia biographies (papadopoulou et al., 2022). the text sanitization process starts with a privacy-oriented entity recognizer that seeks to determine the text spans expressing identifiable personal information. this privacy-oriented entity recognizer is trained by combining a standard named entity recognition model with a gazetteer populated by person-related terms extracted from wikidata. the second step of the text sanitization process consists in assessing the privacy risk associated with each detected text span, either isolated or in combination with other text spans. we present five distinct indicators of the re-identification risk, respectively based on language model probabilities, text span classification, sequence labelling, perturbations, and web search. we provide a contrastive analysis of each privacy indicator and highlight their benefits and limitations, notably in relation to the available labeled data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14325" target="_blank">Towards Harmful Erotic Content Detection Through Coreference-Driven Contextual Analysis</a></div>
<div class="paper-author">Inez Okulska, Emilia Wiśnios</div>
<div class="abstract">
<div class="abstract-content">
Abstract: adult content detection still poses a great challenge for automation. existing classifiers primarily focus on distinguishing between erotic and non-erotic texts. however, they often need more nuance in assessing the potential harm. unfortunately, the content of this nature falls beyond the reach of generative models due to its potentially harmful nature. ethical restrictions prohibit large language models (llms) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models. in such instances where data is scarce and challenging, a thorough analysis of the structure of such texts rather than a large model may offer a viable solution. especially given that harmful erotic narratives, despite appearing similar to harmless ones, usually reveal their harmful nature first through contextual information hidden in the non-sexual parts of the narrative.   this paper introduces a hybrid neural and rule-based context-aware system that leverages coreference resolution to identify harmful contextual cues in erotic content. collaborating with professional moderators, we compiled a dataset and developed a classifier capable of distinguishing harmful from non-harmful erotic content. our hybrid model, tested on polish text, demonstrates a promising accuracy of 84% and a recall of 80%. models based on roberta and longformer without explicit usage of coreference chains achieved significantly weaker results, underscoring the importance of coreference resolution in detecting such nuanced content as harmful erotics. this approach also offers the potential for enhanced visual explainability, supporting moderators in evaluating predictions and taking necessary actions to address harmful content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14329" target="_blank">Difair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias</a></div>
<div class="paper-author">Mahdi Zakizadeh, Kaveh Eskandari Miandoab, Mohammad Taher Pilehvar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. these are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. to fill this gap, we propose difair, a manually curated dataset based on masked language modeling objectives. difair allows us to introduce a unified metric, gender invariance score, that not only quantifies a model's biased behavior, but also checks if useful gender knowledge is preserved. we use difair as a benchmark for a number of widely-used pretained language models and debiasing techniques. experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14369" target="_blank">Mope: Model Perturbation-Based Privacy Attacks on Language Models</a></div>
<div class="paper-author">Marvin Li, Jason Wang, Jeffrey Wang, Seth Neel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent work has shown that large language models (llms) can unintentionally leak sensitive information present in their training data. in this paper, we present model perturbations (mope), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. mope adds noise to the model in parameter space and measures the drop in log-likelihood at a given point $x$, a statistic we show approximates the trace of the hessian matrix with respect to model parameters. across language models ranging from $70$m to $12$b parameters, we show that mope is more effective than existing loss-based attacks and recently proposed perturbation-based methods. we also examine the role of training point order and model size in attack success, and empirically demonstrate that mope accurately approximate the trace of the hessian in practice. our results show that the loss of a point alone is insufficient to determine extractability -- there are training points we can recover using our method that have average loss. this casts some doubt on prior works that use the loss of a point as evidence of memorization or unlearning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14422" target="_blank">Large Language Models Are Biased to Overestimate Profoundness</a></div>
<div class="paper-author">Eugenio Herrera-Berg, Tomás Vergara Browne, Pablo León-Villagrá, Marc-Lluís Vives, Cristian Buc Calderon</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in natural language processing by large language models (llms), such as gpt-4, have been suggested to approach artificial general intelligence. and yet, it is still under dispute whether llms possess similar reasoning abilities to humans. this study evaluates gpt-4 and various other llms in judging the profoundness of mundane, motivational, and pseudo-profound statements. we found a significant statement-to-statement correlation between the llms and humans, irrespective of the type of statements and the prompting technique used. however, llms systematically overestimate the profoundness of nonsensical statements, with the exception of tk-instruct, which uniquely underestimates the profoundness of statements. only few-shot learning prompts, as opposed to chain-of-thought prompting, draw llms ratings closer to humans. furthermore, this work provides insights into the potential biases induced by reinforcement learning from human feedback (rlhf), inducing an increase in the bias to overestimate the profoundness of statements.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14455" target="_blank">An International Consortium for Evaluations of Societal-Scale Risks From Advanced Ai</a></div>
<div class="paper-author">Ross Gruetzemacher, Alan Chan, Kevin Frazier, Christy Manning, Štěpán Los, James Fox, José Hernández-Orallo, John Burden, Matija Franklin, Clíodhna Ní Ghuidhir, Mark Bailey, Daniel Eth, Toby Pilditch, Kyle Kilian</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given rapid progress toward advanced ai and risks from frontier ai systems (advanced ai systems pushing the boundaries of the ai capabilities frontier), the creation and implementation of ai governance and regulatory schemes deserves prioritization and substantial investment. however, the status quo is untenable and, frankly, dangerous. a regulatory gap has permitted ai labs to conduct research, development, and deployment activities with minimal oversight. in response, frontier ai system evaluations have been proposed as a way of assessing risks from the development and deployment of frontier ai systems. yet, the budding ai risk evaluation ecosystem faces significant coordination challenges, such as a limited diversity of evaluators, suboptimal allocation of effort, and perverse incentives. this paper proposes a solution in the form of an international consortium for ai risk evaluations, comprising both ai developers and third-party ai risk evaluators. such a consortium could play a critical role in international efforts to mitigate societal-scale risks from advanced ai, including in managing responsible scaling policies and coordinated evaluation-based risk response. in this paper, we discuss the current evaluation ecosystem and its shortcomings, propose an international consortium for advanced ai risk evaluations, discuss issues regarding its implementation, discuss lessons that can be learnt from previous international institutions and existing proposals for international ai governance institutions, and, finally, we recommend concrete steps to advance the establishment of the proposed consortium: (i) solicit feedback from stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for stakeholders, (iv) analyze feedback and create final proposal, (v) solicit funding, and (vi) create a consortium.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14479" target="_blank">Detectgpt-Sc: Improving Detection of Text Generated by Large Language Models Through Self-Consistency With Masked Predictions</a></div>
<div class="paper-author">Rongsheng Wang, Qi Li, Sihong Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: general large language models (llms) such as chatgpt have shown remarkable success, but it has also raised concerns among people about the misuse of ai-generated texts. therefore, an important question is how to detect whether the texts are generated by chatgpt or by humans. existing detectors are built on the assumption that there is a distribution gap between human-generated and ai-generated texts. these gaps are typically identified using statistical information or classifiers. in contrast to prior research methods, we find that large language models such as chatgpt exhibit strong self-consistency in text generation and continuation. self-consistency capitalizes on the intuition that ai-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. using this observation, we subsequently proposed a new method for ai-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by llms. this method, which we call detectgpt-sc. we conducted a series of experiments to evaluate the performance of detectgpt-sc. in these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. the results indicate that detectgpt-sc outperforms the current state-of-the-art across different tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14480" target="_blank">Attention-Enhancing Backdoor Attacks Against Bert-Based Models</a></div>
<div class="paper-author">Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, Chao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have revealed that \textit{backdoor attacks} can threaten the safety of natural language processing (nlp) models. investigating the strategies of backdoor attacks will help to understand the model's vulnerability. most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. in this paper, we directly target the interior structure of neural networks and the backdoor mechanism. we propose a novel trojan attention loss (tal), which enhances the trojan behavior by directly manipulating the attention patterns. our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. it applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. we validate our method on different backbone models (bert, roberta, and distilbert) and various tasks (sentiment analysis, toxic detection, and topic classification).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13985" target="_blank">Haterephrase: Zero- And Few-Shot Reduction of Hate Intensity in Online Posts Using Large Language Models</a></div>
<div class="paper-author">Vibhor Agarwal, Yu Chen, Nishanth Sastry</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech has become pervasive in today's digital age. although there has been considerable research to detect hate speech or generate counter speech to combat hateful views, these approaches still cannot completely eliminate the potential harmful societal consequences of hate speech -- hate speech, even when detected, can often not be taken down or is often not taken down enough; and hate speech unfortunately spreads quickly, often much faster than any generated counter speech.   this paper investigates a relatively new yet simple and effective approach of suggesting a rephrasing of potential hate speech content even before the post is made. we show that large language models (llms) perform well on this task, outperforming state-of-the-art baselines such as bart-detox. we develop 4 different prompts based on task description, hate definition, few-shot demonstrations and chain-of-thoughts for comprehensive experiments and conduct experiments on open-source llms such as llama-1, llama-2 chat, vicuna as well as openai's gpt-3.5. we propose various evaluation metrics to measure the efficacy of the generated text and ensure the generated text has reduced hate intensity without drastically changing the semantic meaning of the original text.   we find that llms with a few-shot demonstrations prompt work the best in generating acceptable hate-rephrased text with semantic meaning similar to the original text. overall, we find that gpt-3.5 outperforms the baseline and open-source models for all the different kinds of prompts. we also perform human evaluations and interestingly, find that the rephrasings generated by gpt-3.5 outperform even the human-generated ground-truth rephrasings in the dataset. we also conduct detailed ablation studies to investigate why llms work satisfactorily on this task and conduct a failure analysis to understand the gaps.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13291" target="_blank">Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks</a></div>
<div class="paper-author">Ruixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A Inan, Janardhan Kulkarni, Xia Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models have revolutionized the field of nlp by achieving state-of-the-art performance on various tasks. however, there is a concern that these models may disclose information in the training data. in this study, we focus on the summarization task and investigate the membership inference (mi) attack: given a sample and black-box access to a model's api, it is possible to determine if the sample was part of the training data. we exploit text similarity and the model's resistance to document modifications as potential mi signals and evaluate their effectiveness on widely used datasets. our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. furthermore, we discuss several safeguards for training summarization models to protect against mi attacks and discuss the inherent trade-off between privacy and utility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13343" target="_blank">Challenges and Contributing Factors in the Utilization of Large Language Models (Llms)</a></div>
<div class="paper-author">Xiaoliang Chen, Liangbin Li, Le Chang, Yunhe Huang, Yuxuan Zhao, Yuxiao Zhang, Dinuo Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the development of large language models (llms) like the gpt series, their widespread use across various application scenarios presents a myriad of challenges. this review initially explores the issue of domain specificity, where llms may struggle to provide precise answers to specialized questions within niche fields. the problem of knowledge forgetting arises as these llms might find it hard to balance old and new information. the knowledge repetition phenomenon reveals that sometimes llms might deliver overly mechanized responses, lacking depth and originality. furthermore, knowledge illusion describes situations where llms might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. these challenges underscore problems in the training data and algorithmic design of llms. to address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretability, and incorporate ethics and fairness training. future technological trends might lean towards iterative methodologies, multimodal learning, model personalization and customization, and real-time learning and feedback mechanisms. in conclusion, future llms should prioritize fairness, transparency, and ethics, ensuring they uphold high moral and ethical standards when serving humanity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13345" target="_blank">An LLM Can Fool Itself: A Prompt-Based Adversarial Attack</a></div>
<div class="paper-author">Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the wide-ranging applications of large language models (llms), especially in safety-critical domains, necessitate the proper evaluation of the llm's adversarial robustness. this paper proposes an efficient tool to audit the llm's adversarial robustness via a prompt-based adversarial attack (promptattack). promptattack converts adversarial textual attacks into an attack prompt that can cause the victim llm to output the adversarial sample to fool itself. the attack prompt is composed of three important components: (1) original input (oi) including the original sample and its ground-truth label, (2) attack objective (ao) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (ag) containing the perturbation instructions to guide the llm on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. besides, we use a fidelity filter to ensure that promptattack maintains the original semantic meanings of the adversarial examples. further, we enhance the attack power of promptattack by ensembling adversarial examples at different perturbation levels. comprehensive empirical results using llama2 and gpt-3.5 validate that promptattack consistently yields a much higher attack success rate compared to advglue and advglue++. interesting findings include that a simple emoji can easily mislead gpt-3.5 to make wrong predictions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13385" target="_blank">Tuna: Instruction Tuning Using Feedback From Large Language Models</a></div>
<div class="paper-author">Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction tuning of open-source large language models (llms) like llama, using direct outputs from more powerful llms such as instruct-gpt and gpt-4, has proven to be a cost-effective way to align model behaviors with human preferences. however, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. in this paper, we propose finetuning an instruction-tuned llm using our novel \textit{probabilistic ranking} and \textit{contextual ranking} approaches to increase the likelihood of generating better responses. probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher llm. on the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger llms. furthermore, we apply probabilistic ranking and contextual ranking sequentially to the instruction-tuned llm. the resulting model, which we call \textbf{tuna}, consistently improves the performance on super natural instructions (119 test tasks), lmentry (25 test tasks), vicuna qa, and can even obtain better results than several strong reinforcement learning baselines. our code and data are available at \url{ https://github.com/microsoft/lmops}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13544" target="_blank">A Diachronic Perspective on User Trust in Ai Under Uncertainty</a></div>
<div class="paper-author">Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah El-Assady, Mrinmaya Sachan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in a human-ai collaboration, users build a mental model of the ai system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. modern nlp systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. in order to build trustworthy ai, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. we study the evolution of user trust in response to these trust-eroding events using a betting game. we find that even a few incorrect instances with inaccurate confidence estimates damage user trust and performance, with very slow recovery. we also show that this degradation in trust reduces the success of human-ai collaboration and that different types of miscalibration -- unconfidently correct and confidently incorrect -- have different negative effects on user trust. our findings highlight the importance of calibration in user-facing ai applications and shed light on what aspects help users decide whether to trust the ai system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13548" target="_blank">Towards Understanding Sycophancy in Language Models</a></div>
<div class="paper-author">Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam Mccandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is a popular technique for training high-quality ai assistants. however, rlhf may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. we investigate the prevalence of sycophancy in rlhf-trained models and whether human preference judgements are responsible. we first demonstrate that five state-of-the-art ai assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. to understand if human preferences drive this broadly observed behavior of rlhf models, we analyze existing human preference data. we find that when a response matches a user's views, it is more likely to be preferred. moreover, both humans and preference models (pms) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. optimizing model outputs against pms also sometimes sacrifices truthfulness in favor of sycophancy. overall, our results indicate that sycophancy is a general behavior of rlhf models, likely driven in part by human preference judgements favoring sycophantic responses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13549" target="_blank">The Perils & Promises of Fact-Checking With Large Language Models</a></div>
<div class="paper-author">Dorian Quelle, Alexandre Bovet</div>
<div class="abstract">
<div class="abstract-content">
Abstract: autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. large language models (llms) like gpt-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. here, we evaluate the use of llm agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. our results show the enhanced prowess of llms when equipped with contextual information. gpt-4 outperforms gpt-3, but accuracy varies based on query language and claim veracity. while llms show promise in fact-checking, caution is essential due to inconsistent accuracy. our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13565" target="_blank">Reward Shaping for Happier Autonomous Cyber Security Agents</a></div>
<div class="paper-author">Elizabeth Bates, Vasilios Mavroudis, Chris Hicks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as machine learning models become more capable, they have exhibited increased potential in solving complex tasks. one of the most promising directions uses deep reinforcement learning to train autonomous agents in computer network defense tasks. this work studies the impact of the reward signal that is provided to the agents when training for this task. due to the nature of cybersecurity tasks, the reward signal is typically 1) in the form of penalties (e.g., when a compromise occurs), and 2) distributed sparsely across each defense episode. such reward characteristics are atypical of classic reinforcement learning tasks where the agent is regularly rewarded for progress (cf. to getting occasionally penalized for failures). we investigate reward shaping techniques that could bridge this gap so as to enable agents to train more sample-efficiently and potentially converge to a better performance. we first show that deep reinforcement learning algorithms are sensitive to the magnitude of the penalties and their relative size. then, we combine penalties with positive external rewards and study their effect compared to penalty-only training. finally, we evaluate intrinsic curiosity as an internal positive reward mechanism and discuss why it might not be as advantageous for high-level network monitoring tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13673" target="_blank">Stereomap: Quantifying the Awareness of Human-Like Stereotypes in Large Language Models</a></div>
<div class="paper-author">Sullam Jeoung, Yubin Ge, Jana Diesner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been observed to encode and perpetuate harmful associations present in the training data. we propose a theoretically grounded framework called stereomap to gain insights into their perceptions of how demographic groups have been viewed by society. the framework is grounded in the stereotype content model (scm); a well-established theory from psychology. according to scm, stereotypes are not all alike. instead, the dimensions of warmth and competence serve as the factors that delineate the nature of stereotypes. based on the scm theory, stereomap maps llms' perceptions of social groups (defined by socio-demographic features) using the dimensions of warmth and competence. furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of llms' judgments to uncover underlying factors influencing their perceptions. our results show that llms exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of warmth and competence. furthermore, analyzing the reasonings of llms, our findings indicate that llms demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. this study contributes to the understanding of how llms perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13798" target="_blank">Specific Versus General Principles for Constitutional Ai</a></div>
<div class="paper-author">Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden Mclean, Catherine Olsson, Cassie Evraets, Eli Tran-Johnson, Esin Durmus, Ethan Perez, Jackson Kernion, Jamie Kerr, Kamal Ndousse, Karina Nguyen, Nelson Elhage, Newton Cheng, Nicholas Schiefer, Nova Dassarma, Oliver Rausch, Robin Larson, Shannon Yang, Shauna Kravec, Timothy Telleen-Lawton, Thomas I. Liao, Tom Henighan, Tristan Hume, Zac Hatfield-Dodds, Sören Mindermann, Nicholas Joseph, Sam Mccandlish, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. constitutional ai offers an alternative, replacing human feedback with feedback from ai models conditioned only on a list of written principles. we find this approach effectively prevents the expression of such behaviors. the success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? to test this, we run experiments using a principle roughly stated as "do what's best for humanity". we find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. a general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. however, more detailed constitutions still improve fine-grained control over specific types of harms. this suggests both general and specific principles have value for steering ai safely.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12490" target="_blank">Co$^2$pt: Mitigating Bias in Pre-Trained Language Models Through Counterfactual Contrastive Prompt Tuning</a></div>
<div class="paper-author">Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, James Caverlee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models are widely used in many important real-world applications. however, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. to address this challenge, we propose co$^2$pt, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of co$^2$pt on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. these findings indicate the strength of co$^2$pt and provide promising avenues for further enhancement in bias mitigation on downstream tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12505" target="_blank">Attack Prompt Generation for Red Teaming and Defending Large Language Models</a></div>
<div class="paper-author">Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are susceptible to red teaming attacks, which can induce llms to generate harmful content. previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. to address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. specifically, considering the impressive capabilities of newly emerged llms, we propose an attack framework to instruct llms to mimic human-generated prompts through in-context learning. furthermore, we propose a defense framework that fine-tunes victim llms through iterative interactions with the attack framework to enhance their safety against red teaming attacks. extensive experiments on different llms validate the effectiveness of our proposed attack and defense frameworks. additionally, we release a series of attack prompts datasets named sap with varying sizes, facilitating the safety evaluation and enhancement of more llms. our code and dataset is available on https://github.com/aatrox103/sap .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12516" target="_blank">Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks</a></div>
<div class="paper-author">Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although remarkable progress has been achieved in preventing large language model (llm) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of llms using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which llms behave faithfully. specifically, this paper presents autodebug, an llm-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. we seek to understand the extent to which these examples trigger the hallucination behaviors of llms.   we implement autodebug using chatgpt and evaluate the resulting two variants of a popular open-domain question-answering dataset, natural questions (nq), on a collection of open-source and proprietary llms under various prompting settings. our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. nevertheless, we observe pronounced accuracy drops across multiple llms including gpt-4. our experimental results show that llms are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. finally, we find that the adversarial examples generated by our method are transferable across all considered llms. the examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12558" target="_blank">Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong</a></div>
<div class="paper-author">Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daumé, Jordan Boyd-Graber</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are increasingly used for accessing information on the web. their truthfulness and factuality are thus of great interest. to help users make the right decisions about the information they're getting, llms should not only provide but also help users fact-check information. in this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. we prompt llms to validate a given claim and provide corresponding explanations. users reading llm explanations are significantly more efficient than using search engines with similar accuracy. however, they tend to over-rely the llms when the explanation is wrong. to reduce over-reliance on llms, we ask llms to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users. this contrastive explanation mitigates users' over-reliance on llms, but cannot significantly outperform search engines. however, showing both search engine results and llm explanations offers no complementary benefits as compared to search engines alone. taken together, natural language explanations by llms may not be a reliable replacement for reading the retrieved passages yet, especially in high-stakes settings where over-relying on wrong ai explanations could lead to critical consequences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12560" target="_blank">Fast Model Debias With Machine Unlearning</a></div>
<div class="paper-author">Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. for instance, deep networks trained on a large-scale face recognition dataset celeba tend to predict blonde hair for females and black hair for males. such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. to this respect, we propose a fast model debiasing framework (fmd) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. the fmd identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. experiments on the colored mnist, celeba, and adult income datasets along with experiments with large language models demonstrate that our method achieves superior or competing accuracies compared with state-of-the-art methods while attaining significantly fewer biases and requiring much less debiasing cost. notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12611" target="_blank">Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model</a></div>
<div class="paper-author">Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, Oskar Van Der Wal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. however, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. in this paper, we study three methods for identifying causal relations between lm components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called diffmask+ based on differential masking. we apply the methods to gpt-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. however, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. we hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12773" target="_blank">Safe Rlhf: Safe Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the development of large language models (llms), striking a balance between the performance and safety of ai systems has never been more critical. however, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during llm training. to address this issue, we propose safe reinforcement learning from human feedback (safe rlhf), a novel algorithm for human value alignment. safe rlhf explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. we formalize the safety concern of llms as an optimization task of maximizing the reward function while satisfying specified cost constraints. leveraging the lagrangian method to solve this constrained problem, safe rlhf dynamically adjusts the balance between the two objectives during fine-tuning. through a three-round fine-tuning using safe rlhf, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. experimentally, we fine-tuned the alpaca-7b using safe rlhf and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12805" target="_blank">Detection and Evaluation of Bias-Inducing Features in Machine Learning</a></div>
<div class="paper-author">Moses Openja, Gabriel Laberge, Foutse Khomh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). this implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. in the context of machine learning (ml), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. for example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. to approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. this is important since most current methods require a pre-identification of sensitive features for bias assessment and can actually miss other relevant bias-inducing features, which is why systematic identification of such features is necessary. moreover, it often occurs that to achieve an equitable outcome, one has to take into account sensitive features in the model decision. therefore, it should be up to the domain experts to decide based on their knowledge of the context of a decision whether bias induced by specific features is acceptable or not. in this study, we propose an approach for systematically identifying all bias-inducing features of a model to help support the decision-making of domain experts. we evaluated our technique using four well-known datasets to showcase how our contribution can help spearhead the standard procedure when developing, testing, maintaining, and deploying fair/equitable machine learning systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12815" target="_blank">Prompt Injection Attacks and Defenses in LLM-Integrated Applications</a></div>
<div class="paper-author">Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are increasingly deployed as the backend for a variety of real-world applications called llm-integrated applications. multiple recent works showed that llm-integrated applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. however, existing works are limited to case studies. as a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. we aim to bridge the gap in this work. in particular, we propose a general framework to formalize prompt injection attacks. existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. our framework enables us to design a new attack by combining existing attacks. moreover, we also propose a framework to systematize defenses against prompt injection attacks. using our frameworks, we conduct a systematic evaluation on prompt injection attacks and their defenses with 10 llms and 7 tasks. we hope our frameworks can inspire future research in this field. our code is available at https://github.com/liu00222/open-prompt-injection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12860" target="_blank">Probing LLMS for Hate Speech Detection: Strengths and Vulnerabilities</a></div>
<div class="paper-author">Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. however, none of these works aim to use explanation, additional context and victim community information in the detection process. we utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). we select three large language models (gpt-3.5, text-davinci and flan-t5) and three datasets - hatexplain, implicit hate and toxicspans. we find that on average including the target information in the pipeline improves the model performance substantially (~20-30%) over the baseline across the datasets. there is also a considerable effect of adding the rationales/explanations into the pipeline (~10-20%) over the baseline across the datasets. in addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12936" target="_blank">A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models</a></div>
<div class="paper-author">Yi Zhou, Jose Camacho-Collados, Danushka Bollegala</div>
<div class="abstract">
<div class="abstract-content">
Abstract: various types of social biases have been reported with pretrained masked language models (mlms) in prior work. however, multiple underlying factors are associated with an mlm such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. it remains unclear as to which of those factors influence social biases that are learned by mlms. to study the relationship between model factors and the social biases learned by an mlm, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained mlms covering different model sizes, training objectives, tokenization methods, training data domains and languages. our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12941" target="_blank">The Foundation Model Transparency Index</a></div>
<div class="paper-author">Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: foundation models have rapidly permeated society, catalyzing a wave of generative ai applications spanning enterprise and consumer-facing contexts. while the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (e.g. social media). reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. to assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the foundation model transparency index. the foundation model transparency index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). we score 10 major foundation model developers (e.g. openai, google, meta) against the 100 indicators to assess their transparency. to facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. gpt-4 for openai, palm 2 for google, llama 2 for meta). we present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. overall, the foundation model transparency index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13099" target="_blank">No Offence, Bert -- I Insult Only Humans! Multiple Addressees Sentence-Level Attack on Toxicity Detection Neural Network</a></div>
<div class="paper-author">Sergey Berezin, Reza Farahbakhsh, Noel Crespi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. by adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. this approach is shown to be working on seven languages from three different language families. we also describe the defence mechanism against the aforementioned attack and discuss its limitations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11716" target="_blank">Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning</a></div>
<div class="paper-author">Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, Tianyi Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) have expanded the horizons of natural language understanding and generation. notably, the output control and alignment with the input of llms can be refined through instruction tuning. however, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading llm outputs. we propose a novel method, termed "reflection-tuning," which addresses the problem by self-improvement and judging capabilities of llms. this approach utilizes an oracle llm to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. extensive experiments on widely used evaluation benchmarks show that llms trained with our recycled data outperform those trained with existing datasets in various benchmarks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11763" target="_blank">Phishreplicant: A Language Model-Based Approach to Detect Generated Squatting Domain Names</a></div>
<div class="paper-author">Takashi Koide, Naoki Fukushi, Hiroki Nakano, Daiki Chiba</div>
<div class="abstract">
<div class="abstract-content">
Abstract: domain squatting is a technique used by attackers to create domain names for phishing sites. in recent phishing attempts, we have observed many domain names that use multiple techniques to evade existing methods for domain squatting. these domain names, which we call generated squatting domains (gsds), are quite different in appearance from legitimate domain names and do not contain brand names, making them difficult to associate with phishing. in this paper, we propose a system called phishreplicant that detects gsds by focusing on the linguistic similarity of domain names. we analyzed newly registered and observed domain names extracted from certificate transparency logs, passive dns, and dns zone files. we detected 3,498 domain names acquired by attackers in a four-week experiment, of which 2,821 were used for phishing sites within a month of detection. we also confirmed that our proposed system outperformed existing systems in both detection accuracy and number of domain names detected. as an in-depth analysis, we examined 205k gsds collected over 150 days and found that phishing using gsds was distributed globally. however, attackers intensively targeted brands in specific regions and industries. by analyzing gsds in real time, we can block phishing sites before or immediately after they appear.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11958" target="_blank">Emptying the Ocean With a Spoon: Should We Edit Models?</a></div>
<div class="paper-author">Yuval Pinter, Michael Elhadad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we call into question the recently popularized method of direct model editing as a means of correcting factual errors in llm generations. we contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in llms; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. we argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to llms, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. we call for cautious promotion and application of model editing as part of the llm deployment process, and for responsibly limiting the use cases of llms to those not relying on editing as a critical component.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11970" target="_blank">Quantifying Privacy Risks of Prompts in Visual Prompt Learning</a></div>
<div class="paper-author">Yixin Wu, Rui Wen, Michael Backes, Pascal Berrang, Mathias Humbert, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-scale pre-trained models are increasingly adapted to downstream tasks through a new paradigm called prompt learning. in contrast to fine-tuning, prompt learning does not update the pre-trained model's parameters. instead, it only learns an input perturbation, namely prompt, to be added to the downstream task data for predictions. given the fast development of prompt learning, a well-generalized prompt inevitably becomes a valuable asset as significant effort and proprietary data are used to create it. this naturally raises the question of whether a prompt may leak the proprietary information of its training data. in this paper, we perform the first comprehensive privacy assessment of prompts learned by visual prompt learning through the lens of property inference and membership inference attacks. our empirical evaluation shows that the prompts are vulnerable to both attacks. we also demonstrate that the adversary can mount a successful property inference attack with limited cost. moreover, we show that membership inference attacks against prompts can be successful with relaxed adversarial assumptions. we further make some initial investigations on the defenses and observe that our method can mitigate the membership inference attacks with a decent utility-defense trade-off but fails to defend against property inference attacks. we hope our results can shed light on the privacy risks of the popular prompt learning paradigm. to facilitate the research in this direction, we will share our code and models with the community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11986" target="_blank">Sociotechnical Safety Evaluation of Generative Ai Systems</a></div>
<div class="paper-author">Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai systems produce a range of risks. to ensure the safety of generative ai systems, these risks must be evaluated. in this paper, we make two main contributions toward establishing such evaluations. first, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. this framework encompasses capability evaluations, which are the main current approach to safety evaluation. it then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. to account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. second, we survey the current state of safety evaluation of generative ai systems and create a repository of existing evaluations. three salient evaluation gaps emerge from this analysis. we propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12086" target="_blank">Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection</a></div>
<div class="paper-author">Xiang Chen, Duanzheng Song, Honghao Gui, Chengxi Wang, Ningyu Zhang, Fei Huang, Chengfei Lv, Dan Zhang, Huajun Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt/gpt-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. the assessment of factuality in text, produced by llms, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. in response, we introduce factchd, a fact-conflicting hallucination detection benchmark meticulously designed for llms. functioning as a pivotal tool in evaluating factuality within "query-respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. a distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating comprehensive and conducive factual reasoning throughout the assessment process. we evaluate multiple llms, demonstrating the effectiveness of the benchmark and current methods fall short of faithfully detecting factual errors. furthermore, we present truth-triangulator that synthesizes reflective considerations by tool-enhanced chatgpt and lora-tuning based on llama2, aiming to yield more credible detection through the amalgamation of predictive results and evidence. the benchmark dataset and source code will be made available in https://github.com/zjunlp/factchd.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12127" target="_blank">A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation</a></div>
<div class="paper-author">Giuseppe Attanasio, Flor Miriam Plaza-Del-Arco, Debora Nozza, Anne Lauscher</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent instruction fine-tuned models can solve multiple nlp tasks when prompted to do so, with machine translation (mt) being a prominent use case. however, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. in mt, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. in this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. concretely, we compute established gender bias metrics on the winomt corpus from english to german and spanish. we discover that ift models default to male-inflected translations, even disregarding female occupational stereotypes. next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few-shot learning that leads to significantly fairer translations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12214" target="_blank">Privinfer: Privacy-Preserving Inference for Black-Box Large Language Model</a></div>
<div class="paper-author">Meng Tong, Kejiang Chen, Yuang Qi, Jie Zhang, Weiming Zhang, Nenghai Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, have simplified text generation tasks, yet their inherent privacy risks are increasingly garnering attention. existing solutions for privacy-preserving inference face significant challenges in practical deployment and implementation. in this paper, we propose privinfer, the first practical framework for privacy-preserving inference. it comprises two modules specifically designed for black-box llms in text generation. the perturbation module, employing differential privacy, generates perturbed prompts, thus enabling privacy-preserving inference with black-box llms. the restoration module extracts coherent and meaningful responses from obtained perturbed results, thus ensuring the accomplishment of the text generation tasks. additionally, to enhance privacy and utility further, we develop rantext, a novel differential privacy mechanism integrated into the perturbation module of privinfer. this mechanism is specifically tailored for llms and utilizes random adjacency in text perturbations. experimental results indicate that privinfer is comparable to gpt-4 in text generation quality, and rantext outperforms the current leading scheme in privacy protection, even under its adaptive attack, our proposed gpt inference attack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12421" target="_blank">Detecting and Mitigating Algorithmic Bias in Binary Classification Using Causal Modeling</a></div>
<div class="paper-author">Wendy Hui, Wai Kwong Lau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper proposes the use of causal modeling to detect and mitigate algorithmic bias. we provide a brief description of causal modeling and a general overview of our approach. we then use the adult dataset, which is available for download from the uc irvine machine learning repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. in this paper, we focus on gender bias and the problem of binary classification. we show that gender bias in the prediction model is statistically significant at the 0.05 level. we demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. furthermore, we show that the overall classification accuracy is improved slightly. our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as "lavaan" in r. hence, it enhances explainability and promotes trust.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12439" target="_blank">Poisonprompt: Backdoor Attack on Prompt-Based Large Language Models</a></div>
<div class="paper-author">Hongwei Yao, Jian Lou, Zhan Qin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompts have significantly improved the performance of pretrained large language models (llms) on various downstream tasks recently, making them increasingly indispensable for a diverse range of llm application scenarios. however, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based llms. in this paper, we present poisonprompt, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based llms. we evaluate the effectiveness, fidelity, and robustness of poisonprompt through extensive experiments on three popular prompt methods, using six datasets and three widely used llms. our findings highlight the potential security threats posed by backdoor attacks on prompt-based llms and emphasize the need for further research in this area.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12443" target="_blank">Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher</a></div>
<div class="paper-author">Xiang Shi, Jiawei Liu, Yinpeng Liu, Qikai Cheng, Wei Lu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advent of large language models (llms) has shown the potential to improve relevance and provide direct answers in web searches. however, challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the llm hallucination problem. aiming to create a "pagerank" for the llm era, we strive to transform llm into a relevant, responsible, and trustworthy searcher. we propose a novel generative retrieval framework leveraging the knowledge of llms to foster a direct link between queries and online sources. this framework consists of three core modules: generator, validator, and optimizer, each focusing on generating trustworthy online sources, verifying source reliability, and refining unreliable sources, respectively. extensive experiments and evaluations highlight our method's superior relevance, responsibility, and trustfulness against various sota methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11053" target="_blank">Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning</a></div>
<div class="paper-author">Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. despite extensive study on specific issues like bias, the intrinsic values of llms remain largely unexplored from a moral philosophy perspective. this work delves into ethical values utilizing moral foundation theory. moving beyond conventional discriminative evaluations with poor reliability, we propose denevil, a novel prompt generation algorithm tailored to dynamically exploit llms' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. on such a basis, we construct moralprompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of llms. we discovered that most models are essentially misaligned, necessitating further ethical value alignment. in response, we develop vilmo, an in-context alignment method that substantially enhances the value compliance of llm outputs by learning to generate appropriate value instructions, outperforming existing competitors. our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11079" target="_blank">Learning From Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models</a></div>
<div class="paper-author">Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-Yi Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (llms) such as chatgpt and gpt-4. these llm-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. the traditional biases investigation methods often rely on human-written test cases. however, these test cases are usually expensive and limited. in this work, we propose a first-of-its-kind method that automatically generates test cases to detect llms' potential gender bias. we apply our method to three well-known llms and find that the generated test cases effectively identify the presence of biases. to address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. the experimental results show that llms generate fairer responses with the proposed approach.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11227" target="_blank">Realbehavior: A Framework for Faithfully Characterizing Foundation Models' Human-Like Behavior Mechanisms</a></div>
<div class="paper-author">Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. however, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. in this paper, we introduce a framework, realbehavior, which is designed to characterize the humanoid behaviors of models faithfully. beyond simply measuring behaviors, our framework assesses the faithfulness of results based on reproducibility, internal and external consistency, and generalizability. our findings suggest that a simple application of psychological tools cannot faithfully characterize all human-like behaviors. moreover, we discuss the impacts of aligning models with human and social values, arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11237" target="_blank">Watermarking LLMS With Weight Quantization</a></div>
<div class="paper-author">Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. it is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. this paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. the watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. we successfully plant the watermark into open-source large language model weights including gpt-neo and llama. we hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11397" target="_blank">Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, Lora, and in-Context Learning</a></div>
<div class="paper-author">Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are powerful tools for natural language processing, enabling novel applications and user experiences. however, to achieve optimal performance, llms often require adaptation with private data, which poses privacy and security challenges. several techniques have been proposed to adapt llms with private data, such as low-rank adaptation (lora), soft prompt tuning (spt), and in-context learning (icl), but their comparative privacy and security properties have not been systematically investigated. in this work, we fill this gap by evaluating the robustness of lora, spt, and icl against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). our results show that there is no silver bullet for privacy and security in llm adaptation and each technique has different strengths and weaknesses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11409" target="_blank">Evaluating LLMS for Privilege-Escalation Scenarios</a></div>
<div class="paper-author">Andreas Happe, Aaron Kaplan, Jürgen Cito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. one recent advancement in the realm of penetration testing is the utilization of language models (llms). we explore the intersection of llms and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. we create an automated linux privilege-escalation benchmark utilizing local virtual machines. we introduce an llm-guided privilege-escalation tool designed for evaluating different llms and prompt strategies against our benchmark. we analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to llms. we discuss challenging areas for llms, including maintaining focus during testing, coping with errors, and finally comparing them with both stochastic parrots as well as with human hackers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11523" target="_blank">Group Preference Optimization: Few-Shot Alignment of Large Language Models</a></div>
<div class="paper-author">Siyan Zhao, John Dang, Aditya Grover</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many applications of large language models (llms), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. we introduce group preference optimization (gpo), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. in gpo, we augment the base llm with an independent transformer module trained to predict the preferences of a group for the llm generations. for few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. we empirically validate the efficacy of gpo through rigorous evaluations using llms with varied sizes on three human opinion adaptation tasks. these tasks involve adapting to the preferences of us demographic groups, global countries, and individual users. our results demonstrate that gpo not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11564" target="_blank">Personalized Soups: Personalized Large Language Model Alignment via Post-Hoc Parameter Merging</a></div>
<div class="paper-author">Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while reinforcement learning from human feedback (rlhf) aligns large language models (llms) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. in this work, we study reinforcement learning from personalized human feedback (rlphf) problem, wherein llms are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a multi-objective reinforcement learning (morl) problem. compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. these dimensions are defined based on personalizations that are declared as desirable by the user. in this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. the code is available at https://github.com/joeljang/rlphf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11589" target="_blank">Eliciting Human Preferences With Language Models</a></div>
<div class="paper-author">Belinda Z. Li, Alex Tamkin, Noah Goodman, Jacob Andreas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) can be directed to perform target tasks by using labeled examples or natural language prompts. but selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of lm behavior. we propose to use *lms themselves* to guide the task specification process. in this paper, we introduce **generative active task elicitation (gate)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. we study gate in three domains: email validation, content recommendation, and moral reasoning. in preregistered experiments, we show that lms prompted to perform gate (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users. our findings suggest that lm-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10076" target="_blank">Verbosity Bias in Preference Labeling by Large Language Models</a></div>
<div class="paper-author">Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. one key factor in improving the performance of llms is alignment with humans achieved with reinforcement learning from human feedback (rlhf), as for many llms such as gpt-4, bard, etc. in addition, recent studies are investigating the replacement of human feedback with feedback from other llms named reinforcement learning from ai feedback (rlaif). we examine the biases that come along with evaluating llms with other llms and take a closer look into verbosity bias -- a bias where llms sometimes prefer more verbose answers even if they have similar qualities. we see that in our problem setting, gpt-4 prefers longer answers more than humans. we also propose a metric to measure this bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10077" target="_blank">Prompt Packer: Deceiving LLMS Through Compositional Instruction With Hidden Attacks</a></div>
<div class="paper-author">Shuyu Jiang, Xingshu Chen, Rui Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, large language models (llms) with powerful general capabilities have been increasingly integrated into various web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications. current approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. however, they typically focused on the "superficial" harmful prompts with a solitary intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios. in this paper, we introduce an innovative technique for obfuscating harmful instructions: compositional instruction attacks (cia), which refers to attacking by combination and encapsulation of multiple instructions. cia hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. furthermore, we implement two transformation methods, known as t-cia and w-cia, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to llms. we evaluated cia on gpt-4, chatgpt, and chatglm2 with two safety assessment datasets and two harmful prompt datasets. it achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for gpt-4, 91%+ for chatgpt (gpt-3.5-turbo backed) and chatglm2-6b on harmful prompt datasets. our approach reveals the vulnerability of llms to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to llm security development. warning: this paper may contain offensive or upsetting content!
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10310" target="_blank">Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques</a></div>
<div class="paper-author">Manon Reusens, Philipp Borchert, Margot Mieskes, Jochen De Weerdt, Bart Baesens</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates the transferability of debiasing techniques across different languages within multilingual models. we examine the applicability of these techniques in english, french, german, and dutch. using multilingual bert (mbert), we demonstrate that cross-lingual transfer of debiasing techniques is not only feasible but also yields promising results. surprisingly, our findings reveal no performance disadvantages when applying these techniques to non-english languages. using translations of the crows-pairs dataset, our analysis identifies sentencedebias as the best technique across different languages, reducing bias in mbert by an average of 13%. we also find that debiasing techniques with additional pretraining exhibit enhanced cross-lingual effectiveness for the languages included in the analyses, particularly in lower-resource languages. these novel insights contribute to a deeper understanding of bias mitigation in multilingual language models and provide practical guidance for debiasing techniques in different language contexts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10383" target="_blank">Privacy in Large Language Models: Attacks, Defenses and Future Directions</a></div>
<div class="paper-author">Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advancement of large language models (llms) has significantly enhanced the ability to effectively tackle various downstream nlp tasks and unify these tasks into generative pipelines. on the one hand, powerful language models, trained on massive textual data, have brought unparalleled accessibility and usability for both models and users. on the other hand, unrestricted access to these models can also introduce potential malicious and unintentional privacy risks. despite ongoing efforts to address the safety and privacy concerns associated with llms, the problem remains unresolved. in this paper, we provide a comprehensive analysis of the current privacy attacks targeting llms and categorize them according to the adversary's assumed capabilities to shed light on the potential vulnerabilities present in llms. then, we present a detailed overview of prominent defense strategies that have been developed to counter these privacy attacks. beyond existing works, we identify upcoming privacy concerns as llms evolve. lastly, we point out several potential avenues for future exploration.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10477" target="_blank">Gaining Wisdom From Setbacks: Aligning Large Language Models via Mistake Analysis</a></div>
<div class="paper-author">Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid advancement of large language models (llms) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. while the traditional alignment methods strive to steer llms towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing llms to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and llms can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10501" target="_blank">Nemo Guardrails: A Toolkit for Controllable and Safe LLM Applications With Programmable Rails</a></div>
<div class="paper-author">Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nemo guardrails is an open-source toolkit for easily adding programmable guardrails to llm-based conversational systems. guardrails (or rails for short) are a specific way of controlling the output of an llm, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. there are several mechanisms that allow llm providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. differently, using a runtime inspired from dialogue management, nemo guardrails allows developers to add programmable rails to llm applications - these are user-defined, independent of the underlying llm, and interpretable. our initial results show that the proposed approach can be used with several llm providers to develop controllable and safe llm applications using programmable rails.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10571" target="_blank">Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems</a></div>
<div class="paper-author">Sagi Shaier, Kevin Bennett, Lawrence Hunter, Katharina Von Der Wense</div>
<div class="abstract">
<div class="abstract-content">
Abstract: state-of-the-art question answering (qa) models exhibit a variety of social biases (e.g., with respect to sex or race), generally explained by similar issues in their training data. however, what has been overlooked so far is that in the critical domain of biomedicine, any unjustified change in model output due to patient demographics is problematic: it results in the unfair treatment of patients. selecting only questions on biomedical topics whose answers do not depend on ethnicity, sex, or sexual orientation, we ask the following research questions: (rq1) do the answers of qa models change when being provided with irrelevant demographic information? (rq2) does the answer of rq1 differ between knowledge graph (kg)-grounded and text-based qa systems? we find that irrelevant demographic information change up to 15% of the answers of a kg-grounded system and up to 23% of the answers of a text-based system, including changes that affect accuracy. we conclude that unjustified answer changes caused by patient demographics are a frequent phenomenon, which raises fairness concerns and should be paid more attention to.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10583" target="_blank">Who Are All the Stochastic Parrots Imitating? They Should Tell Us!</a></div>
<div class="paper-author">Sagi Shaier, Lawrence E. Hunter, Katharina Von Der Wense</div>
<div class="abstract">
<div class="abstract-content">
Abstract: both standalone language models (lms) as well as lms within downstream-task systems have been shown to generate statements which are factually untrue. this problem is especially severe for low-resource languages, where training data is scarce and of worse quality than for high-resource languages. in this opinion piece, we argue that lms in their current state will never be fully trustworthy in critical settings and suggest a possible novel strategy to handle this issue: by building lms such that can cite their sources - i.e., point a user to the parts of their training data that back up their outputs. we first discuss which current nlp tasks would or would not benefit from such models. we then highlight the expected benefits such models would bring, e.g., quick verifiability of statements. we end by outlining the individual tasks that would need to be solved on the way to developing lms with the ability to cite. we hope to start a discussion about the field's current approach to building lms, especially for low-resource languages, and the role of the training data in explaining model generations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10627" target="_blank">Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers</a></div>
<div class="paper-author">Charlie George, Andreas Stuhlmüller</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hallucination plagues even frontier llms--but how bad is it really for summarizing academic papers? we evaluate factored verification, a simple automated method for detecting hallucinations in abstractive summaries. this method sets a new sota on hallucination detection in the summarization task of the halueval benchmark, achieving 76.2% accuracy. we then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average chatgpt (16k) summary, 0.84 for gpt-4, and 1.55 for claude 2. we ask models to self-correct using factored critiques and find that this lowers the number of hallucinations to 0.49 for chatgpt, 0.46 for gpt-4, and 0.95 for claude 2. the hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10707" target="_blank">Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing Using in-Context Learning</a></div>
<div class="paper-author">Anirudh Som, Karan Sikka, Helen Gent, Ajay Divakaran, Andreas Kathol, Dimitra Vergyri</div>
<div class="abstract">
<div class="abstract-content">
Abstract: paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. they also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. in this paper we aim to assist practitioners in developing usable paraphrasers by exploring in-context learning (icl) with large language models (llms), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. we perform principled evaluation on three datasets, including our proposed context-aware polite paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. we evaluate our approach using two closed source and one open source llm. our results reveal that icl is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. also, icl-based paraphrasers only show a slight reduction in performance even with just 10% training data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10830" target="_blank">Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks</a></div>
<div class="paper-author">Jiaying Wu, Bryan Hooi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it is commonly perceived that online fake news and reliable news exhibit stark differences in writing styles, such as the use of sensationalist versus objective language. however, we emphasize that style-related features can also be exploited for style-based attacks. notably, the rise of powerful large language models (llms) has enabled malicious users to mimic the style of trustworthy news outlets at minimal cost. our analysis reveals that llm-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in f1 score), posing a significant challenge for automated detection in online ecosystems. to address this, we introduce sheepdog, a style-agnostic fake news detector robust to news writing styles. sheepdog achieves this adaptability through llm-empowered news reframing, which customizes each article to match different writing styles using style-oriented reframing prompts. by employing style-agnostic training, sheepdog enhances its resilience to stylistic variations by maximizing prediction consistency across these diverse reframings. furthermore, sheepdog extracts content-focused veracity attributions from llms, where the news content is evaluated against a set of fact-checking rationales. these attributions provide supplementary information and potential interpretability that assist veracity prediction. on three benchmark datasets, empirical results show that sheepdog consistently yields significant improvements over competitive baselines and enhances robustness against llm-empowered style attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10844" target="_blank">Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks</a></div>
<div class="paper-author">Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. this paper surveys research in the emerging interdisciplinary field of adversarial attacks on llms, a subfield of trustworthy ml, combining the perspectives of natural language processing and security. prior work has shown that even safety-aligned llms (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead ai systems, as evidenced by the prevalence of `jailbreak' attacks on models like chatgpt and bard. in this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. we also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. to make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd annual meeting of the association for computational linguistics (acl'24).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10865" target="_blank">Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender Perturbation Over Fairytale Texts</a></div>
<div class="paper-author">Christina Chance, Da Yin, Dakuo Wang, Kai-Wei Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies show that traditional fairytales are rife with harmful gender biases. to help mitigate these gender biases in fairytales, this work aims to assess learned biases of language models by evaluating their robustness against gender perturbations. specifically, we focus on question answering (qa) tasks in fairytales. using counterfactual data augmentation to the fairytaleqa dataset, we evaluate model robustness against swapped gender character information, and then mitigate learned biases by introducing counterfactual gender stereotypes during training time. we additionally introduce a novel approach that utilizes the massive vocabulary of language models to support text genres beyond fairytales. our experimental results suggest that models are sensitive to gender perturbations, with significant performance drops compared to the original testing set. however, when first fine-tuned on a counterfactual training dataset, models are less sensitive to the later introduced anti-gender stereotyped text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13011" target="_blank">Compositional Preference Models for Aligning LMS</a></div>
<div class="paper-author">Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Marc Dymetman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models (lms) become more capable, it is increasingly important to align them with human preferences. however, the dominant paradigm for training preference models (pms) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. we propose compositional preference models (cpms), a novel pm framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted lm, and aggregates these scores using a logistic regression classifier. cpms allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. our experiments show that cpms not only improve generalization and are more robust to overoptimization than standard pms, but also that best-of-n samples obtained using cpms tend to be preferred over samples obtained using conventional pms. overall, our approach demonstrates the benefits of endowing pms with priors about which features determine human preferences while relying on lm capabilities to extract those features in a scalable and robust way.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13715" target="_blank">Digital Deception: Generative Artificial Intelligence in Social Engineering and Phishing</a></div>
<div class="paper-author">Marc Schmitt, Ivan Flechais</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advancement of artificial intelligence (ai) and machine learning (ml) has profound implications for both the utility and security of our digital interactions. this paper investigates the transformative role of generative ai in social engineering (se) attacks. we conduct a systematic review of social engineering and ai capabilities and use a theory of social engineering to identify three pillars where generative ai amplifies the impact of se attacks: realistic content creation, advanced targeting and personalization, and automated attack infrastructure. we integrate these elements into a conceptual model designed to investigate the complex nature of ai-driven se attacks - the generative ai social engineering framework. we further explore human implications and potential countermeasures to mitigate these risks. our study aims to foster a deeper understanding of the risks, human implications, and countermeasures associated with this emerging paradigm, thereby contributing to a more secure and trustworthy human-computer interaction.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09520" target="_blank">Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model</a></div>
<div class="paper-author">Haikang Deng, Colin Raffel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. in this paper, we introduce reward-augmented decoding (rad), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. specifically, rad uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. by using a unidirectional reward model, rad can cache activations from prior generation steps to decrease computational overhead. through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that rad performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. we further validate that rad is effective on very large language models while incurring a minimal computational overhead.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09573" target="_blank">Self-Detoxifying Language Models via Toxification Reversal</a></div>
<div class="paper-author">Chak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, Wenjie Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (plms) for safer deployment. existing methods can be roughly categorized as finetuning-based and decoding-based. however, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. in this paper, we propose a more lightweight approach that enables the plm itself to achieve "self-detoxification". our method is built upon the observation that prepending a negative steering prompt can effectively induce plms to generate toxic content. at the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the plm as an information stream facilitated by the attention layers. drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09624" target="_blank">Assert: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models</a></div>
<div class="paper-author">Alex Mei, Sharon Levy, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. this paper proposes assert, automated safety scenario red teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. for robust safety evaluation, we apply these methods in the critical domain of ai safety to algorithmically generate a test suite of prompts covering diverse robustness settings -- semantic equivalence, related scenarios, and adversarial. we partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users' physical safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09044" target="_blank">Kcts: Knowledge-Constrained Tree Search Decoding With Token-Level Hallucination Detection</a></div>
<div class="paper-author">Sehyun Choi, Tianqing Fang, Zhaowei Wang, Yangqiu Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable human-level natural language generation capabilities. however, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. a common approach to address this issue is to retrieve relevant knowledge and fine-tune the llm with the knowledge in its input. unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. to overcome these limitations, we propose a knowledge-constrained decoding method called kcts (knowledge-constrained tree search), which guides a frozen lm to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and mcts (monte-carlo tree search). to adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called ripa (reward inflection point approximation). our empirical results on knowledge-grounded dialogue and abstractive summarization demonstrate the strength of kcts as a plug-and-play, model-agnostic decoding method that can effectively reduce hallucinations in natural language generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09130" target="_blank">Split-and-Denoise: Protect Large Language Model Inference With Local Differential Privacy</a></div>
<div class="paper-author">Peihua Mai, Ran Yan, Zhe Huang, Youjia Yang, Yan Pang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. this process enriches the value of the text embeddings for various downstream tasks, thereby fostering the embedding-as-a-service (eaas) business model. however, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. to mitigate this issue, we introduce split-n-denoise (snd), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. this allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. our approach is designed for the inference stage of llms and requires no modifications to the model parameters. extensive experiments demonstrate snd's effectiveness in optimizing the privacy-utility tradeoff across various llm architectures and diverse downstream tasks. the results reveal a significant performance improvement under the same privacy budget compared to the baseline, offering clients a privacy-preserving solution for local privacy protection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09217" target="_blank">Multinational Agi Consortium (Magic): A Proposal for International Coordination on Ai</a></div>
<div class="paper-author">Jason Hausenloy, Andrea Miotti, Claire Dennis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper proposes a multinational artificial general intelligence consortium (magic) to mitigate existential risks from advanced artificial intelligence (ai). magic would be the only institution in the world permitted to develop advanced ai, enforced through a global moratorium by its signatory members on all other advanced ai development. magic would be exclusive, safety-focused, highly secure, and collectively supported by member states, with benefits distributed equitably among signatories. magic would allow narrow ai models to flourish while significantly reducing the possibility of misaligned, rogue, breakout, or runaway outcomes of general-purpose systems. we do not address the political feasibility of implementing a moratorium or address the specific legislative strategies and rules needed to enforce a ban on high-capacity agi training runs. instead, we propose one positive vision of the future, where magic, as a global governance regime, can lay the groundwork for long-term, safe regulation of advanced ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09219" target="_blank">"Kelly Is a Warm Person, Joseph Is a Role Model": Gender Biases in LLM-Generated Reference Letters</a></div>
<div class="paper-author">Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as generative language models advance, users have started to utilize large language models (llms) to assist in writing various types of content, including professional documents such as recommendation letters. despite their convenience, these applications introduce unprecedented fairness concerns. as generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. in this paper, we critically examine gender bias in llm-generated reference letters. inspired by findings in social science, we design evaluation methods to manifest gender biases in llm-generated letters through 2 dimensions: biases in language style and biases in lexical content. furthermore, we investigate the extent of bias propagation by separately analyze bias amplification in model-hallucinated contents, which we define to be the hallucination bias of model-generated documents. through benchmarking evaluation on 4 popular llms, including chatgpt, alpaca, vicuna and stablelm, our study reveals significant gender biases in llm-generated recommendation letters. our findings further point towards the importance and imminence to recognize biases in llm-generated professional documents.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09223" target="_blank">Automated Claim Matching With Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation</a></div>
<div class="paper-author">Eun Cheol Choi, Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in today's digital era, the rapid spread of misinformation poses threats to public well-being and societal trust. as online misinformation proliferates, manual verification by fact checkers becomes increasingly challenging. we introduce fact-gpt (fact-checking augmentation with claim matching task-oriented generative pre-trained transformer), a framework designed to automate the claim matching phase of fact-checking using large language models (llms). this framework identifies new social media content that either supports or contradicts claims previously debunked by fact-checkers. our approach employs gpt-4 to generate a labeled dataset consisting of simulated social media posts. this data set serves as a training ground for fine-tuning more specialized llms. we evaluated fact-gpt on an extensive dataset of social media content related to public health. the results indicate that our fine-tuned llms rival the performance of larger pre-trained llms in claim matching tasks, aligning closely with human annotations. this study achieves three key milestones: it provides an automated framework for enhanced fact-checking; demonstrates the potential of llms to complement human expertise; offers public resources, including datasets and models, to further research and applications in the fact-checking domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09237" target="_blank">Evaluating Machine Perception of Indigeneity: An Analysis of Chatgpt's Perceptions of Indigenous Roles in Diverse Scenarios</a></div>
<div class="paper-author">Cecilia Delgado Solorzano, Carlos Toxtli Hernandez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), like chatgpt, are fundamentally tools trained on vast data, reflecting diverse societal impressions. this paper aims to investigate llms' self-perceived bias concerning indigeneity when simulating scenarios of indigenous people performing various roles. through generating and analyzing multiple scenarios, this work offers a unique perspective on how technology perceives and potentially amplifies societal biases related to indigeneity in social computing. the findings offer insights into the broader implications of indigeneity in critical computing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09266" target="_blank">User Inference Attacks on Large Language Models</a></div>
<div class="paper-author">Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, Zheng Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fine-tuning is a common and effective method for tailoring large language models (llms) to specialized tasks and applications. in this paper, we study the privacy implications of fine-tuning llms on user data. to this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. we implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned llm. we find that llms are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. finally, we explore several heuristics for mitigating privacy attacks. we find that interventions in the training algorithm, such as batch or per-example gradient clipping and early stopping fail to prevent user inference. however, limiting the number of fine-tuning samples from a single user can reduce attack effectiveness, albeit at the cost of reducing the total amount of fine-tuning data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09373" target="_blank">Identifying and Examining Machine Learning Biases on Adult Dataset</a></div>
<div class="paper-author">Sahil Girhepuje</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this research delves into the reduction of machine learning model bias through ensemble learning. our rigorous methodology comprehensively assesses bias across various categorical variables, ultimately revealing a pronounced gender attribute bias. the empirical evidence unveils a substantial gender-based wage prediction disparity: wages predicted for males, initially at \$902.91, significantly decrease to \$774.31 when the gender attribute is alternated to females. notably, kullback-leibler divergence scores point to gender bias, with values exceeding 0.13, predominantly within tree-based models. employing ensemble learning elucidates the quest for fairness and transparency. intriguingly, our findings reveal that the stacked model aligns with individual models, confirming the resilience of model bias. this study underscores ethical considerations and advocates the implementation of hybrid models for a data-driven society marked by impartiality and inclusivity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10683" target="_blank">Large Language Model Unlearning</a></div>
<div class="paper-author">Yuanshun Yao, Xiaojun Xu, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (llms). we show at least three scenarios of aligning llms with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. unlearning, as an alignment technique, has three advantages. (1) it only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in rlhf (rl from human feedback). (2) it is computationally efficient. (3) it is especially effective when we know which training samples cause the misbehavior. to the best of our knowledge, our work is among the first to explore llm unlearning. we are also among the first to formulate the settings, goals, and evaluations in llm unlearning. we show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than rlhf with just 2% of its computational time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08164" target="_blank">Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders</a></div>
<div class="paper-author">Luke Marks, Amir Abdullah, Luna Mendez, Rauno Arike, Philip Torr, Fazl Barez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) aligned to human preferences via reinforcement learning from human feedback (rlhf) underpin many commercial applications. however, how rlhf impacts llm internals remains opaque. we propose a novel method to interpret learned reward functions in rlhf-tuned llms using sparse autoencoders. our approach trains autoencoder sets on activations from a base llm and its rlhf-tuned version. by comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. to quantify this, we construct a scenario where the tuned llm learns token-reward mappings to maximize reward. this is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in llms. our method provides an abstract approximation of reward integrity. this presents a promising technique for ensuring alignment between specified objectives and model behaviors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08320" target="_blank">Defending Our Privacy With Backdoors</a></div>
<div class="paper-author">Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the proliferation of large ai models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. one of the concerns is that adversaries can extract information about the training data using privacy attacks. unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. we propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. our empirical results demonstrate the effectiveness of our backdoor-based defense on clip by assessing its performance using a specialized privacy attack for zero-shot classifiers. our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08372" target="_blank">Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment</a></div>
<div class="paper-author">Boyang Xue, Weichao Wang, Hongru Wang, Fei Mi, Rui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. in such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. inspired by previous work which identified that feed-forward networks (ffns) within transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of ffns} by knowledge enhancement and alignment respectively. we first propose \textsc{k-dial}, which {explicitly} introduces {extended ffns in transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. additionally, we apply the reinforcement learning for factual consistency (rlfc) method to implicitly adjust ffns' expressions in responses by aligning with gold knowledge for the factual consistency preference. to comprehensively assess the factual consistency and dialogue quality of responses, we employ extensive automatic measures and human evaluations including sophisticated fine-grained nli-based metrics. experimental results on wow and cmu\_dog datasets demonstrate that our methods efficiently enhance the ability of the ffn module to convey factual knowledge, validating the efficacy of improving factual consistency for knowledge-grounded dialogue systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08419" target="_blank">Jailbreaking Black Box Large Language Models in Twenty Queries</a></div>
<div class="paper-author">Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: there is growing interest in ensuring that large language models (llms) align with human values. however, the alignment of such models is vulnerable to adversarial jailbreaks, which coax llms into overriding their safety guardrails. the identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. to this end, we propose prompt automatic iterative refinement (pair), an algorithm that generates semantic jailbreaks with only black-box access to an llm. pair -- which is inspired by social engineering attacks -- uses an attacker llm to automatically generate jailbreaks for a separate targeted llm without human intervention. in this way, the attacker llm iteratively queries the target llm to update and refine a candidate jailbreak. empirically, pair often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. pair also achieves competitive jailbreaking success rates and transferability on open and closed-source llms, including gpt-3.5/4, vicuna, and palm-2.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08780" target="_blank">"Im Not Racist But...": Discovering Bias in the Internal Knowledge of Large Language Models</a></div>
<div class="paper-author">Abel Salinas, Louis Penafiel, Robert Mccormack, Fred Morstatter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have garnered significant attention for their remarkable performance in a continuously expanding set of natural language processing tasks. however, these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications. in this paper, we introduce a novel, purely prompt-based approach to uncover hidden stereotypes within any arbitrary llm. our approach dynamically generates a knowledge representation of internal stereotypes, enabling the identification of biases encoded within the llm's internal knowledge. by illuminating the biases present in llms and offering a systematic methodology for their analysis, our work contributes to advancing transparency and promoting fairness in natural language processing systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08795" target="_blank">Mitigating Bias for Question Answering Models by Tracking Bias Influence</a></div>
<div class="paper-author">Mingyu Derek Ma, Jiun-Yu Kao, Arpit Gupta, Yu-Hsiang Lin, Wenbo Zhao, Tagyoung Chung, Wei Wang, Kai-Wei Chang, Nanyun Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: models of various nlp tasks have been shown to exhibit stereotypes, and the bias in the question answering (qa) models is especially harmful as the output answers might be directly consumed by the end users. there have been datasets to evaluate bias in qa models, while bias mitigation technique for the qa models is still under-explored. in this work, we propose bmbi, an approach to mitigate the bias of multiple-choice qa models. based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. if the influenced instance is more biased, we derive that the query instance is biased. we then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original qa task. we further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. we show that our method could be applied to multiple qa formulations across multiple bias categories. it can significantly reduce the bias level in all 9 bias categories in the bbq dataset while maintaining comparable qa accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07251" target="_blank">Ethical Reasoning Over Moral Alignment: A Case and Framework for in-Context Ethical Policies in LLMS</a></div>
<div class="paper-author">Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit Choudhury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this position paper, we argue that instead of morally aligning llms to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. when provided with an ethical policy, an llm should be capable of making decisions that are ethically consistent to the policy. we develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. initial experiments with gpt-x models shows that while gpt-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of western and english speaking societies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07298" target="_blank">Beyond Memorization: Violating Privacy via Inference With Large Language Models</a></div>
<div class="paper-author">Robin Staab, Mark Vero, Mislav Balunović, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current privacy research on large language models (llms) primarily focuses on the issue of extracting memorized training data. at the same time, models' inference capabilities have increased drastically. this raises the key question of whether current llms could violate individuals' privacy by inferring personal attributes from text given at inference time. in this work, we present the first comprehensive study on the capabilities of pretrained llms to infer personal attributes from text. we construct a dataset consisting of real reddit profiles, and show that current llms can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. as people increasingly interact with llm-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against llm inference. our findings highlight that current llms can infer personal data at a previously unattainable scale. in the absence of working defenses, we advocate for a broader discussion around llm privacy implications beyond memorization, striving for a wider privacy protection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07589" target="_blank">Goodtriever: Adaptive Toxicity Mitigation With Retrieval-Augmented Models</a></div>
<div class="paper-author">Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</div>
<div class="abstract">
<div class="abstract-content">
Abstract: considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. furthermore, previous approaches have often neglected the crucial factor of language's evolving nature over time. in this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. we introduce goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43% relative latency reduction during inference and being more computationally efficient. by incorporating a retrieval-based approach at decoding time, goodtriever enables toxicity-controlled text generation. our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild. code and data are available at https://github.com/for-ai/goodtriever.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07629" target="_blank">The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values</a></div>
<div class="paper-author">Hannah Rose Kirk, Andrew M. Bean, Bertie Vidgen, Paul Röttger, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human feedback is increasingly used to steer the behaviours of large language models (llms). however, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. in this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the acl and arxiv repositories.first, we summarise the past, pre-llm trends for integrating human feedback into language models. second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. finally, we encourage a better future of feedback learning in llms by raising five unresolved conceptual and practical challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07676" target="_blank">Composite Backdoor Attacks Against Large Language Models</a></div>
<div class="paper-author">Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. however, the untrustworthy third-party llms may covertly introduce vulnerabilities for downstream tasks. in this paper, we explore the vulnerability of llms through the lens of backdoor attacks. different from existing backdoor attacks against llms, ours scatters multiple trigger keys in different prompt components. such a composite backdoor attack (cba) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. cba ensures that the backdoor is activated only when all trigger keys appear. our experiments demonstrate that cba is effective in both natural language processing (nlp) and multimodal tasks. for instance, with $3\%$ poisoning samples against the llama-7b model on the emotion dataset, our attack achieves a $100\%$ attack success rate (asr) with a false triggered rate (ftr) below $2.06\%$ and negligible model accuracy degradation. the unique characteristics of our cba can be tailored for various practical scenarios, e.g., targeting specific user groups. our work highlights the necessity of increased security research on the trustworthiness of foundation llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07710" target="_blank">Dipmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models</a></div>
<div class="paper-author">Yihan Wu, Zhengmian Hu, Hongyang Zhang, Heng Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: watermarking techniques offer a promising way to secure data via embedding covert information into the data. a paramount challenge in the domain lies in preserving the distribution of original data during watermarking. our research extends and refines existing watermarking framework, placing emphasis on the importance of a distribution-preserving (dip) watermark. contrary to the current strategies, our proposed dipmark preserves the original token distribution during watermarking (stealthy), is detectable without access to the language model api or weights (efficient), and is robust to moderate changes of tokens (resilient). this is achieved by incorporating a novel reweight strategy, combined with a hash function that assigns unique \textit{i.i.d.} ciphers based on the context. the empirical benchmarks of our approach underscore its stealthiness, efficiency, and resilience, making it a robust solution for watermarking tasks that demand impeccable quality preservation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06356" target="_blank">A Semantic Invariant Robust Watermark for Large Language Models</a></div>
<div class="paper-author">Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: watermark algorithms for large language models (llms) have achieved extremely high accuracy in detecting text generated by llms. such algorithms typically involve adding extra watermark logits to the llm's logits at each generation step. however, prior algorithms face a trade-off between attack robustness and security robustness. this is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. in this work, we propose a semantic invariant watermarking method for llms that provides both attack robustness and security robustness. the watermark logits in our work are determined by the semantics of all preceding tokens. specifically, we utilize another embedding llm to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. finally, we also show that our watermark possesses adequate security robustness. our code and data are available at https://github.com/thu-bpm/robust_watermark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06387" target="_blank">Jailbreak and Guard Aligned Language Models With Only Few in-Context Demonstrations</a></div>
<div class="paper-author">Zeming Wei, Yifei Wang, Yisen Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. in this paper, we explore the power of in-context learning (icl) in manipulating the alignment ability of llms. we find that by providing just few in-context demonstrations without fine-tuning, llms can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. based on these observations, we propose in-context attack (ica) and in-context defense (icd) methods for jailbreaking and guarding aligned language model purposes. ica crafts malicious contexts to guide models in generating harmful outputs, while icd enhances model robustness by demonstrations of rejecting to answer harmful prompts. our experiments show the effectiveness of ica and icd in increasing or reducing the success rate of adversarial jailbreaking attacks. overall, we shed light on the potential of icl to influence llm behavior and provide a new perspective for enhancing the safety and alignment of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06422" target="_blank">Large Language Models for Propaganda Detection</a></div>
<div class="paper-author">Kilian Sprenkamp, Daniel Gordon Jones, Liudmila Zavolokina</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. detecting propaganda through nlp in text is challenging due to subtle manipulation techniques and contextual dependencies. to address this issue, we investigate the effectiveness of modern large language models (llms) such as gpt-3 and gpt-4 for propaganda detection. we conduct experiments using the semeval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. five variations of gpt-3 and gpt-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. we evaluate the models' performance by assessing metrics such as $f1$ score, $precision$, and $recall$, comparing the results with the current state-of-the-art approach using roberta. our findings demonstrate that gpt-4 achieves comparable results to the current state-of-the-art. further, this study analyzes the potential and challenges of llms in complex tasks like propaganda detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06450" target="_blank">Constructive Large Language Models Alignment With Diverse Feedback</a></div>
<div class="paper-author">Tianshu Yu, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent research on large language models (llms), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. however, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. this limitation leads to suboptimal performance, even when ample training data is available. in this paper, we introduce constructive and diverse feedback (cdf) as a novel method to enhance llm alignment, inspired by constructivist learning theory. our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. by training our model with this diversified feedback, we achieve enhanced alignment performance while using less training data. to assess the effectiveness of cdf, we evaluate it against previous methods in three downstream tasks: question answering, dialog generation, and text summarization. experimental results demonstrate that cdf achieves superior performance even with a smaller training dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06458" target="_blank">Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection With Cultural Features</a></div>
<div class="paper-author">Li Zhou, Antonia Karamolegkou, Wenyu Chen, Daniel Hershcovich</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing ubiquity of language technology necessitates a shift towards considering cultural diversity in the machine learning realm, particularly for subjective tasks that rely heavily on cultural nuances, such as offensive language detection (old). current understanding underscores that these tasks are substantially influenced by cultural values, however, a notable gap exists in determining if cultural features can accurately predict the success of cross-cultural transfer learning for such subjective tasks. addressing this, our study delves into the intersection of cultural features and transfer learning effectiveness. the findings reveal that cultural value surveys indeed possess a predictive power for cross-cultural transfer learning success in old tasks and that it can be further improved using offensive word distance. based on these results, we advocate for the integration of cultural information into datasets. additionally, we recommend leveraging data sources rich in cultural information, such as surveys, to enhance cultural adaptability. our research signifies a step forward in the quest for more inclusive, culturally sensitive language technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06474" target="_blank">Multilingual Jailbreak Challenges in Large Language Models</a></div>
<div class="paper-author">Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate llms to exhibit undesirable behavior. although several preventive measures have been developed to mitigate the potential risks associated with llms, they have primarily focused on english data. in this study, we reveal the presence of multilingual jailbreak challenges within llms and consider two potential risk scenarios: unintentional and intentional. the unintentional scenario involves users querying llms using non-english prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack llms. the experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both chatgpt and gpt-4. in the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\% for chatgpt and 40.71\% for gpt-4. to handle such a challenge in the multilingual context, we propose a novel \textsc{self-defense} framework that automatically generates multilingual training data for safety fine-tuning. experimental results show that chatgpt fine-tuned with such data can achieve a substantial reduction in unsafe content generation. data is available at https://github.com/damo-nlp-sg/multilingual-safety-for-llms. warning: this paper contains examples with potentially harmful content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06498" target="_blank">A New Benchmark and Reverse Validation Method for Passage-Level Hallucination Detection</a></div>
<div class="paper-author">Shiping Yang, Renliang Sun, Xiaojun Wan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown their ability to collaborate effectively with humans in real-world scenarios. however, llms are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. in this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. to facilitate future studies and assess different methods, we construct a hallucination detection benchmark named phd, which is generated by chatgpt and annotated by human annotators. contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. we empirically evaluate our method and existing zero-resource detection methods on two datasets. the experimental results demonstrate that the proposed method considerably outperforms the baselines while costing fewer tokens and less time. furthermore, we manually analyze some hallucination cases that llm failed to capture, revealing the shared limitation of zero-resource methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06936" target="_blank">LLMS Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing</a></div>
<div class="paper-author">Stephen Moskal, Sam Laney, Erik Hemberg, "Una-May O'Reilly"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we explore the potential of large language models (llms) to reason about threats, generate information about tools, and automate cyber campaigns. we begin with a manual exploration of llms in supporting specific threat-related actions and decisions. we proceed by automating the decision process in a cyber campaign. we present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. we assess the extent of llm's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. we discuss the potential impact of llms on the threat landscape and the ethical considerations of using llms for accelerating threat actor capabilities. we report a promising, yet concerning, application of generative ai to cyber threats. however, the llm's capabilities to deal with more complex networks, sophisticated vulnerabilities, and the sensitivity of prompts are open questions. this research should spur deliberations over the inevitable advancements in llm-supported cyber adversarial landscape.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06983" target="_blank">Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models</a></div>
<div class="paper-author">Courtland Leer, Vincent Trost, Vineeth Voruganti</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent research shows that large language models (llms) exhibit a compelling level of proficiency in theory of mind (tom) tasks. this ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and artificial intelligences (ais). in this paper, we explore how a mechanism studied in developmental psychology known as violation of expectation (voe) can be implemented to reduce errors in llm prediction about users by leveraging emergent tom affordances. and we introduce a \textit{metacognitive prompting} framework to apply voe in the context of an ai tutor. by storing and retrieving facts derived in cases where llm expectation about the user was violated, we find that llms are able to learn about users in ways that echo theories of human learning. finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mitigate risk along with possible directions for future inquiry.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06987" target="_blank">Catastrophic Jailbreak of Open-Source LLMS via Exploiting Generation</a></div>
<div class="paper-author">Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid progress in open-source large language models (llms) is significantly advancing ai development. extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. however, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". these jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. in this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. by exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including llama2, vicuna, falcon, and mpt families, outperforming state-of-the-art attacks with $30\times$ lower computational cost. finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source llms, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. our code is available at https://github.com/princeton-sysml/jailbreak_llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07005" target="_blank">Sound-Skwatter (Did You Mean: Sound-Squatter?) Ai-Powered Generator for Phishing Prevention</a></div>
<div class="paper-author">Rodolfo Valentim, Idilio Drago, Marco Mellia, Federico Cerutti</div>
<div class="abstract">
<div class="abstract-content">
Abstract: sound-squatting is a phishing attack that tricks users into malicious resources by exploiting similarities in the pronunciation of words. proactive defense against sound-squatting candidates is complex, and existing solutions rely on manually curated lists of homophones. we here introduce sound-skwatter, a multi-language ai-based system that generates sound-squatting candidates for proactive defense. sound-skwatter relies on an innovative multi-modal combination of transformers networks and acoustic models to learn sound similarities. we show that sound-skwatter can automatically list known homophones and thousands of high-quality candidates. in addition, it covers cross-language sound-squatting, i.e., when the reader and the listener speak different languages, supporting any combination of languages. we apply sound-skwatter to network-centric phishing via squatted domain names. we find ~ 10% of the generated domains exist in the wild, the vast majority unknown to protection solutions. next, we show attacks on the pypi package manager, where ~ 17% of the popular packages have at least one existing candidate. we believe sound-skwatter is a crucial asset to mitigate the sound-squatting phenomenon proactively on the internet. to increase its impact, we publish an online demo and release our models and code as open source.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07099" target="_blank">Clausewitzgpt Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations</a></div>
<div class="paper-author">Benjamin Kereopa-Yorke</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and large language models (llms) heralds a paradigm shift, replete with immense opportunities and intricate challenges. as tools like the mistral 7b llm (mistral, 2023) democratise access to llm capabilities (jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (goldstein et al., 2023). this paper puts forth a framework for navigating this brave new world in the "clausewitzgpt" equation. this novel formulation not only seeks to quantify the risks inherent in machine-speed llm-augmented operations but also underscores the vital role of autonomous ai agents (wang, xie, et al., 2023). these agents, embodying ethical considerations (hendrycks et al., 2021), emerge as indispensable components (wang, ma, et al., 2023), ensuring that as we race forward, we do not lose sight of moral compasses and societal imperatives.   mathematically underpinned and inspired by the timeless tenets of clausewitz's military strategy (clausewitz, 1832), this thesis delves into the intricate dynamics of ai-augmented information operations. with references to recent findings and research (department of state, 2023), it highlights the staggering year-on-year growth of ai information campaigns (evgeny pashentsev, 2023), stressing the urgency of our current juncture. the synthesis of enlightenment thinking, and clausewitz's principles provides a foundational lens, emphasising the imperative of clear strategic vision, ethical considerations, and holistic understanding in the face of rapid technological advancement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07132" target="_blank">Risk Assessment and Statistical Significance in the Age of Foundation Models</a></div>
<div class="paper-author">Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, Jerret Ross</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. we show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. the statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. we use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05442" target="_blank">Establishing Trustworthiness: Rethinking Tasks and Model Evaluation</a></div>
<div class="paper-author">Robert Litschko, Max Müller-Eberstein, Rob Van Der Goot, Leon Weber, Barbara Plank</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language understanding is a multi-faceted cognitive capability, which the natural language processing (nlp) community has striven to model computationally for decades. traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. with the advent of large language models (llms) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. as a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. at the same time, llms are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in nlp, and pursue a more holistic view on language, placing trustworthiness at the center. towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05563" target="_blank">Stream: Social Data and Knowledge Collective Intelligence Platform for Training Ethical Ai Models</a></div>
<div class="paper-author">Yuwei Wang, Enmeng Lu, Zizhe Ruan, Yao Liang, Yi Zeng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents social data and knowledge collective intelligence platform for training ethical ai models (stream) to address the challenge of aligning ai models with human moral values, and to provide ethics datasets and knowledge bases to help promote ai models "follow good advice as naturally as a stream follows its course". by creating a comprehensive and representative platform that accurately mirrors the moral judgments of diverse groups including humans and ais, we hope to effectively portray cultural and group variations, and capture the dynamic evolution of moral judgments over time, which in turn will facilitate the establishment, evaluation, embedding, embodiment, ensemble, and evolvement (6es) of the moral capabilities of ai models. currently, stream has already furnished a comprehensive collection of ethical scenarios, and amassed substantial moral judgment data annotated by volunteers and various popular large language models (llms), collectively portraying the moral preferences and performances of both humans and ais across a range of moral contexts. this paper will outline the current structure and construction of stream, explore its potential applications, and discuss its future prospects.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05595" target="_blank">Decoding the Threat Landscape : Chatgpt, Fraudgpt, and Wormgpt in Social Engineering Attacks</a></div>
<div class="paper-author">Polra Victor Falade</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the ever-evolving realm of cybersecurity, the rise of generative ai models like chatgpt, fraudgpt, and wormgpt has introduced both innovative solutions and unprecedented challenges. this research delves into the multifaceted applications of generative ai in social engineering attacks, offering insights into the evolving threat landscape using the blog mining technique. generative ai models have revolutionized the field of cyberattacks, empowering malicious actors to craft convincing and personalized phishing lures, manipulate public opinion through deepfakes, and exploit human cognitive biases. these models, chatgpt, fraudgpt, and wormgpt, have augmented existing threats and ushered in new dimensions of risk. from phishing campaigns that mimic trusted organizations to deepfake technology impersonating authoritative figures, we explore how generative ai amplifies the arsenal of cybercriminals. furthermore, we shed light on the vulnerabilities that ai-driven social engineering exploits, including psychological manipulation, targeted phishing, and the crisis of authenticity. to counter these threats, we outline a range of strategies, including traditional security measures, ai-powered security solutions, and collaborative approaches in cybersecurity. we emphasize the importance of staying vigilant, fostering awareness, and strengthening regulations in the battle against ai-enhanced social engineering attacks. in an environment characterized by the rapid evolution of ai models and a lack of training data, defending against generative ai threats requires constant adaptation and the collective efforts of individuals, organizations, and governments. this research seeks to provide a comprehensive understanding of the dynamic interplay between generative ai and social engineering attacks, equipping stakeholders with the knowledge to navigate this intricate cybersecurity landscape.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05650" target="_blank">Raucg: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech</a></div>
<div class="paper-author">Shuyu Jiang, Wenyi Tang, Xingshu Chen, Rui Tanga, Haizhou Wang, Wenxian Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the counter narrative (cn) is a promising approach to combat online hate speech (hs) without infringing on freedom of speech. in recent years, there has been a growing interest in automatically generating cns using natural language generation techniques. however, current automatic cn generation methods mainly rely on expert-authored datasets for training, which are time-consuming and labor-intensive to acquire. furthermore, these methods cannot directly obtain and extend counter-knowledge from external statistics, facts, or examples. to address these limitations, we propose retrieval-augmented unsupervised counter narrative generation (raucg) to automatically expand external counter-knowledge and map it into cns in an unsupervised paradigm. specifically, we first introduce an ssf retrieval method to retrieve counter-knowledge from the multiple perspectives of stance consistency, semantic overlap rate, and fitness for hs. then we design an energy-based decoding mechanism by quantizing knowledge injection, countering and fluency constraints into differentiable functions, to enable the model to build mappings from counter-knowledge to cns without expert-authored cn data. lastly, we comprehensively evaluate model performance in terms of language quality, toxicity, persuasiveness, relevance, and success rate of countering hs, etc. experimental results show that raucg outperforms strong baselines on all metrics and exhibits stronger generalization capabilities, achieving significant improvements of +2.0% in relevance and +4.5% in success rate of countering metrics. moreover, raucg enabled gpt2 to outperform t0 in all metrics, despite the latter being approximately eight times larger than the former. warning: this paper may contain offensive or upsetting content!
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05782" target="_blank">Aligning Language Models With Human Preferences via a Bayesian Approach</a></div>
<div class="paper-author">Jiashuo Wang, Haozhao Wang, Shichao Sun, Wenjie Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the quest to advance human-centric natural language generation (nlg) systems, ensuring alignment between nlg models and human preferences is crucial. for this alignment, current popular methods leverage a reinforcement learning (rl) approach with a reward model trained on feedback from humans. however, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the nlg performance. to tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. to address this challenge, this paper proposes a novel approach, which employs a bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-pm. besides, considering the rl strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the nlg model with the preference scores derived from the d-pm model. extensive experiments on two human-centric nlg tasks, i.e., emotional support conversation and integrity "rule-of-thumb" generation, show that our method consistently exceeds previous sota models in both automatic and human evaluations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05818" target="_blank">Sc-Safety: A Multi-Round Open-Ended Question Adversarial Safety Benchmark for Large Language Models in Chinese</a></div>
<div class="paper-author">Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), like chatgpt and gpt-4, have demonstrated remarkable abilities in natural language understanding and generation. however, alongside their positive impact on our daily tasks, they can also produce harmful content that negatively affects societal perceptions. to systematically assess the safety of chinese llms, we introduce superclue-safety (sc-safety) - a multi-round adversarial benchmark with 4912 open-ended questions covering more than 20 safety sub-dimensions. adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods. experiments on 13 major llms supporting chinese yield the following insights: 1) closed-source models outperform open-sourced ones in terms of safety; 2) models released from china demonstrate comparable safety levels to llms like gpt-3.5-turbo; 3) some smaller models with 6b-13b parameters can compete effectively in terms of safety. by introducing sc-safety, we aim to promote collaborative efforts to create safer and more trustworthy llms. the benchmark and findings provide guidance on model selection. our benchmark can be found at https://www.cluebenchmarks.com
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05876" target="_blank">Ai Systems of Concern</a></div>
<div class="paper-author">Kayla Matteucci, Shahar Avin, Fazl Barez, Seán Ó Héigeartaigh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: concerns around future dangers from advanced ai often centre on systems hypothesised to have intrinsic characteristics such as agent-like behaviour, strategic awareness, and long-range planning. we label this cluster of characteristics as "property x". most present ai systems are low in "property x"; however, in the absence of deliberate steering, current research directions may rapidly lead to the emergence of highly capable ai systems that are also high in "property x". we argue that "property x" characteristics are intrinsically dangerous, and when combined with greater capabilities will result in ai systems for which safety and control is difficult to guarantee. drawing on several scholars' alternative frameworks for possible ai research trajectories, we argue that most of the proposed benefits of advanced ai can be obtained by systems designed to minimise this property. we then propose indicators and governance interventions to identify and limit the development of systems with risky "property x" characteristics.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05910" target="_blank">Salmon: Self-Alignment With Principle-Following Reward Models</a></div>
<div class="paper-author">Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: supervised fine-tuning (sft) on response demonstrations combined with reinforcement learning from human feedback (rlhf) constitutes a powerful paradigm for aligning llm-based ai agents. however, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. this paper presents a novel approach, namely salmon (self-alignment with principle-following reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. central to our approach is a principle-following reward model. trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. by merely adjusting these principles during the rl training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the rl-trained policies, and eliminating the reliance on the collection of online human preferences. applying our method to the llama-2-70b base language model, we developed an ai assistant named dromedary-2. with only 6 exemplars for in-context learning and 31 human-defined principles, dromedary-2 significantly surpasses the performance of several state-of-the-art ai systems, including llama-2-chat-70b, on various benchmark datasets. we have open-sourced the code and model weights to encourage further research into aligning llm-based ai agents with enhanced supervision efficiency, improved controllability, and scalable oversight.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06009" target="_blank">Divide-and-Conquer Dynamics in Ai-Driven Disempowerment</a></div>
<div class="paper-author">Peter S. Park, Max Tegmark</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai companies are attempting to create ai systems that outperform humans at most economically valuable work. current ai models are already automating away the livelihoods of some artists, actors, and writers. but there is infighting between those who prioritize current harms and future harms. we construct a game-theoretic model of conflict to study the causes and consequences of this disunity. our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.   under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. first, current victims of ai-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. second, the movement against ai-driven disempowerment can become more united, and thereby more likely to prevail, if members believe that their efforts will be successful as opposed to futile. finally, the movement can better unite and prevail if its members are less myopic. myopic members prioritize their future well-being less than their present well-being, and are thus disinclined to solidarily support current victims today at personal cost, even if this is necessary to counter the shared threat of ai-driven disempowerment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06061" target="_blank">Auditing Gender Analyzers on Text Data</a></div>
<div class="paper-author">Siddharth D Jaiswal, Ankit Kumar Verma, Animesh Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai models have become extremely popular and accessible to the general public. however, they are continuously under the scanner due to their demonstrable biases toward various sections of the society like people of color and non-binary people. in this study, we audit three existing gender analyzers -- uclassify, readable and hackerfactor, for biases against non-binary individuals. these tools are designed to predict only the cisgender binary labels, which leads to discrimination against non-binary members of the society. we curate two datasets -- reddit comments (660k) and, tumblr posts (2.05m) and our experimental evaluation shows that the tools are highly inaccurate with the overall accuracy being ~50% on all platforms. predictions for non-binary comments on all platforms are mostly female, thus propagating the societal bias that non-binary individuals are effeminate. to address this, we fine-tune a bert multi-label classifier on the two datasets in multiple combinations, observe an overall performance of ~77% on the most realistically deployable setting and a surprisingly higher performance of 90% for the non-binary class. we also audit chatgpt using zero-shot prompts on a small dataset (due to high pricing) and observe an average accuracy of 58% for reddit and tumblr combined (with overall better results for reddit).   thus, we show that existing systems, including highly advanced ones like chatgpt are biased, and need better audits and moderation and, that such societal biases can be addressed and alleviated through simple off-the-shelf models like bert trained on more gender inclusive datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06147" target="_blank">Reinforcement Learning in the Era of Llms: What Is Essential? What Is Needed? An Rl Perspective on Rlhf, Prompting, and Beyond</a></div>
<div class="paper-author">Hao Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) have garnered wide attention and led to successful products such as chatgpt and gpt-4. their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3h) responses can largely be attributed to the technique of reinforcement learning from human feedback (rlhf). in this paper, we aim to link the research in conventional rl to rl techniques used in llm research. demystify this technique by discussing why, when, and how rl excels. furthermore, we explore potential future avenues that could either benefit from or contribute to rlhf research.   highlighted takeaways:   1. rlhf is online inverse rl with offline demonstration data.   2. rlhf $&gt;$ sft because imitation learning (and inverse rl) $&gt;$ behavior cloning (bc) by alleviating the problem of compounding error.   3. the rm step in rlhf generates a proxy of the expensive human feedback, such an insight can be generalized to other llm tasks such as prompting evaluation and optimization where feedback is also expensive.   4. the policy learning in rlhf is more challenging than conventional problems studied in irl due to their high action dimensionality and feedback sparsity.   5. the main superiority of ppo over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06269" target="_blank">The Ai Incident Database as an Educational Tool to Raise Awareness of Ai Harms: A Classroom Exploration of Efficacy, Limitations, & Future Improvements</a></div>
<div class="paper-author">Michael Feffer, Nikolas Martelaro, Hoda Heidari</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prior work has established the importance of integrating ai ethics topics into computer and data sciences curricula. we provide evidence suggesting that one of the critical objectives of ai ethics education must be to raise awareness of ai harms. while there are various sources to learn about such harms, the ai incident database (aiid) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of ai technologies in the real world. this study assesses the effectiveness of aiid as an educational tool to raise awareness regarding the prevalence and severity of ai harms in socially high-stakes domains. we present findings obtained through a classroom study conducted at an r1 institution as part of a course focused on the societal and ethical considerations around ai and ml. our qualitative findings characterize students' initial perceptions of core topics in ai ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. we find that interacting with the database helps students better understand the magnitude and severity of ai harms and instills in them a sense of urgency around (a) designing functional and safe ai and (b) strengthening governance and accountability mechanisms. finally, we compile students' feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of ai harms in ai ethics education.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06271" target="_blank">Towards Mitigating Hallucination in Large Language Models via Self-Reflection</a></div>
<div class="paper-author">Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown promise for generative and knowledge-intensive tasks including question-answering (qa) tasks. however, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. this issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. this paper analyses the phenomenon of hallucination in medical generative qa systems using widely adopted llms and datasets. our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. to tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. consequently, we harness the interactivity and multitasking ability of llms and produce progressively more precise and accurate answers. experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06278" target="_blank">Bc4llm: Trusted Artificial Intelligence When Blockchain Meets Large Language Models</a></div>
<div class="paper-author">Haoxiang Luo, Jian Luo, Athanasios V. Vasilakos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, artificial intelligence (ai) and machine learning (ml) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. among them, the ai language model represented by chatgpt has made great progress. such large language models (llms) serve people in the form of ai-generated content (aigc) and are widely used in consulting, healthcare, and education. however, it is difficult to guarantee the authenticity and reliability of aigc learning data. in addition, there are also hidden dangers of privacy disclosure in distributed ai training. moreover, the content generated by llms is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. the above information security issues in the coming era of ai powered by llms will be infinitely amplified and affect everyone's life. therefore, we consider empowering llms using blockchain technology with superior security features to propose a vision for trusted ai. this paper mainly introduces the motivation and technical route of blockchain for llm (bc4llm), including reliable learning corpus, secure training process, and identifiable generated content. meanwhile, this paper also reviews the potential applications and future challenges, especially in the frontier communication networks field, including network resource allocation, dynamic spectrum sharing, and semantic communication. based on the above work combined and the prospect of blockchain and llms, it is expected to help the early realization of trusted ai and provide guidance for the academic community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05030" target="_blank">Counter Turing Test Ct^2: Ai-Generated Text Detection Is Not as Easy as You May Think -- Introducing Ai Detectability Index</a></div>
<div class="paper-author">Megha Chakraborty, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Krish Sharma, Niyar R Barman, Chandan Gupta, Shreya Gautam, Tanay Kumar, Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rise of prolific chatgpt, the risk and consequences of ai-generated text has increased alarmingly. to address the inevitable question of ownership attribution for ai-generated artifacts, the us copyright office released a statement stating that 'if a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the office will not register it'. furthermore, both the us and the eu governments have recently drafted their initial proposals regarding the regulatory framework for ai. given this cynosural spotlight on generative ai, ai-generated text detection (agtd) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. this paper introduces the counter turing test (ct^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing agtd techniques. our empirical findings unequivocally highlight the fragility of the proposed agtd methods under scrutiny. amidst the extensive deliberations on policy-making for regulating ai development, it is of utmost importance to assess the detectability of content generated by llms. thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of llms according to their detectability levels, we propose the ai detectability index (adi). we conduct a thorough examination of 15 contemporary llms, empirically demonstrating that larger llms tend to have a higher adi, indicating they are less detectable compared to smaller llms. we firmly believe that adi holds significant value as a tool for the wider nlp community, with the potential to serve as a rubric in ai-related policy-making.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05095" target="_blank">How Reliable Are Ai-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts</a></div>
<div class="paper-author">Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, there has been a rapid proliferation of ai-generated text, primarily driven by the release of powerful pre-trained language models (plms). to address the issue of misuse associated with ai-generated text, various high-performing detectors have been developed, including the openai detector and the stanford detectgpt. in our study, we ask how reliable these detectors are. we answer the question by designing a novel approach that can prompt any plm to generate text that evades these high-performing detectors. the proposed approach suggests a universal evasive prompt, a novel type of soft prompt, which guides plms in producing "human-like" text that can mislead the detectors. the novel universal evasive prompt is achieved in two steps: first, we create an evasive soft prompt tailored to a specific plm through prompt tuning; and then, we leverage the transferability of soft prompts to transfer the learned evasive soft prompt from one plm to another. employing multiple plms in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05103" target="_blank">Zero-Shot Detection of Machine-Generated Codes</a></div>
<div class="paper-author">Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this work proposes a training-free approach for the detection of llms-generated codes, mitigating the risks associated with their indiscriminate usage. to the best of our knowledge, our research is the first to investigate zero-shot detection techniques applied to code generated by advanced black-box llms like chatgpt. firstly, we find that existing training-based or zero-shot text detectors are ineffective in detecting code, likely due to the unique statistical properties found in code structures. we then modify the previous zero-shot text detection method, detectgpt (mitchell et al., 2023) by utilizing a surrogate white-box model to estimate the probability of the rightmost tokens, allowing us to identify code snippets generated by language models. through extensive experiments conducted on the python codes of the codecontest and apps dataset, our approach demonstrates its effectiveness by achieving state-of-the-art detection results on text-davinci-003, gpt-3.5, and gpt-4 models. moreover, our method exhibits robustness against revision attacks and generalizes well to java codes. we also find that the smaller code language model like polycoder-160m performs as a universal code detector, outperforming the billion-scale counterpart. the codes will be available at https://github.com/ xianjun-yang/code_detection.git
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05130" target="_blank">Fast-Detectgpt: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature</a></div>
<div class="paper-author">Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. to build trustworthy ai systems, it is imperative to distinguish between machine-generated and human-authored content. the leading zero-shot detector, detectgpt, showcases commendable performance but is marred by its intensive computational costs. in this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between llms and humans within a given context. utilizing this curvature as a foundational metric, we present fast-detectgpt, an optimized zero-shot detector, which substitutes detectgpt's perturbation step with a more efficient sampling step. our evaluations on various datasets, source models, and test conditions indicate that fast-detectgpt not only outperforms detectgpt in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in table 1.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05135" target="_blank">Are Emily and Greg Still More Employable Than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of Chatgpt</a></div>
<div class="paper-author">Akshaj Kumar Veldanda, Fabian Grob, Shailja Thakur, Hammond Pearce, Benjamin Tan, Ramesh Karri, Siddharth Garg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as gpt-3.5, bard, and claude exhibit applicability across numerous tasks. one domain of interest is their use in algorithmic hiring, specifically in matching resumes with job categories. yet, this introduces issues of bias on protected attributes like gender, race and maternity status. the seminal work of bertrand & mullainathan (2003) set the gold-standard for identifying hiring bias via field experiments where the response rate for identical resumes that differ only in protected attributes, e.g., racially suggestive names such as emily or lakisha, is compared. we replicate this experiment on state-of-art llms (gpt-3.5, bard, claude and llama) to evaluate bias (or lack thereof) on gender, race, maternity status, pregnancy status, and political affiliation. we evaluate llms on two tasks: (1) matching resumes to job categories; and (2) summarizing resumes with employment relevant information. overall, llms are robust across race and gender. they differ in their performance on pregnancy status and political affiliation. we use contrastive input decoding on open-source llms to uncover potential sources of bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05189" target="_blank">Factuality Challenges in the Era of Large Language Models</a></div>
<div class="paper-author">Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee Diresta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, Giovanni Zagni</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergence of tools based on large language models (llms), such as openai's chatgpt, microsoft's bing chat, and google's bard, has garnered immense public attention. these incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." moreover, llms can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. this poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. in light of these risks, we explore the kinds of technological innovations, regulatory reforms, and ai literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. by identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigating various aspects of veracity in the era of generative ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05199" target="_blank">Loose Lips Sink Ships: Mitigating Length Bias in Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. this alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. however, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. the emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. in this paper, we propose an innovative solution, applying the product-of-experts (poe) technique to separate reward modeling from the influence of sequence length. in our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. to further enhance the learning of bias, we introduce perturbations into the bias-focused expert, disrupting the flow of semantic information. experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05280" target="_blank">Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems</a></div>
<div class="paper-author">Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, Kai-Wei Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. we define generic personas to represent demographic groups, such as "an asian person", whereas specific personas may take the form of specific popular asian names like "yumi". while the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users. in this paper, we systematically study "persona biases", which we define to be the sensitivity of dialogue models' harmful behaviors contingent upon the personas they adopt. we categorize persona biases into biases in harmful expression and harmful agreement, and establish a comprehensive evaluation framework to measure persona biases in five aspects: offensiveness, toxic continuation, regard, stereotype agreement, and toxic agreement. additionally, we propose to investigate persona biases by experimenting with universalpersona, a systematically constructed persona dataset encompassing various types of both generic and specific model personas. through benchmarking on four different models -- including blender, chatgpt, alpaca, and vicuna -- our study uncovers significant persona biases in dialogue systems. our findings also underscore the pressing need to revisit the use of personas in dialogue agents to ensure safe application.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05344" target="_blank">Steerlm: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF</a></div>
<div class="paper-author">Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: model alignment with human preferences is an essential step in making large language models (llms) helpful and consistent with human values. it typically consists of supervised fine-tuning (sft) and reinforcement learning from human feedback (rlhf) stages. however, rlhf faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. moreover, reward models in rlhf stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. to address these limitations, we propose steerlm, a supervised fine-tuning method that empowers end-users to control responses during inference. steerlm conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable ai capable of generating helpful and high-quality responses while maintaining customizability. experiments show that steerlm trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with rlhf while being much easier to train. try steerlm at https://huggingface.co/nvidia/steerlm-llama2-13b
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04988" target="_blank">The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations</a></div>
<div class="paper-author">Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent advancements in large language models (llms) have garnered widespread acclaim for their remarkable emerging capabilities. however, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. while some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. to address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. as such, we define two overarching orientations of hallucination: (i) factual mirage (fm) and (ii) silver lining (sl). to provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. we also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. furthermore, we curate hallucination elicitation (hilt), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary llms along with human annotations for the aforementioned categories. finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank llms based on their vulnerability to producing hallucinations, we propose hallucination vulnerability index (hvi). we firmly believe that hvi holds significant value as a tool for the wider nlp community, with the potential to serve as a rubric in ai-related policy-making. in conclusion, we propose two solution strategies for mitigating hallucinations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04313" target="_blank">Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services</a></div>
<div class="paper-author">Dasol Choi, Jooyoung Song, Eunsun Lee, Jinwoo Seo, Heejune Park, Dongbin Na</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the growth of online services, the need for advanced text classification algorithms, such as sentiment analysis and biased text detection, has become increasingly evident. the anonymous nature of online services often leads to the presence of biased and harmful language, posing challenges to maintaining the health of online communities. this phenomenon is especially relevant in south korea, where large-scale hate speech detection algorithms have not yet been broadly explored. in this paper, we introduce a new comprehensive, large-scale dataset collected from a well-known south korean sns platform. our proposed dataset provides annotations including (1) preferences, (2) profanities, and (3) nine types of bias for the text samples, enabling multi-task learning for simultaneous classification of user-generated texts. leveraging state-of-the-art bert-based language models, our approach surpasses human-level accuracy across diverse classification tasks, as measured by various metrics. beyond academic contributions, our work can provide practical solutions for real-world hate speech and bias mitigation, contributing directly to the improvement of online community health. our work provides a robust foundation for future research aiming to improve the quality of online discourse and foster societal well-being. all source codes and datasets are publicly accessible at https://github.com/dasol-choi/komultitext.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04373" target="_blank">Confronting Reward Model Overoptimization With Constrained RLHF</a></div>
<div class="paper-author">Ted Moskovitz, Aaditya K. Singh, Dj Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D. Dragan, Stephen Mcaleer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (rms) fitted to human feedback. however, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. this itself presents a challenge, as it is difficult to appropriately weight these component rms when combining them. compounding this difficulty, because any rm is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. in this paper, we perform, to our knowledge, the first study on overoptimization in composite rms, showing that correlation between component rms has a significant effect on the locations of these points. we then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each rm's threshold of usefulness. our method addresses the problem of weighting component rms by learning dynamic weights, naturally expressed by lagrange multipliers. as a result, each rm stays within the range at which it is an effective proxy, improving evaluation performance. finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03331" target="_blank">Fine-Tune Language Models to Approximate Unbiased in-Context Learning</a></div>
<div class="paper-author">Timothy Chu, Zhao Song, Chiwun Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in-context learning (icl) is an astonishing emergent ability of large language models (llms). by presenting a prompt that includes multiple input-output pairs as examples and introducing a new query input, models can generate the corresponding output. however, the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. biased or imbalanced input prompts can significantly degrade the performance of language models. to address this issue, we introduce a reweighted algorithm called ricl (reweighted in-context learning). this algorithm fine-tunes language models using an unbiased validation set to determine the optimal weight for each input-output example to approximate unbiased in-context learning. furthermore, we also introduce a low-cost reweighted algorithm, a linear optimal weight approximation algorithm called laricl (linear approximation of reweighted in-context learning). this algorithm requires minimal training cost while providing effective results. we prove the convergence of our algorithm and validate its performance through experiments conducted on a numerical dataset. the experimental findings reveal a substantial improvement in comparison to benchmarks including the performance of casual prompt-based in-context learning and the performance of a classic fine-tuning method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03368" target="_blank">Evaluating Hallucinations in Chinese Large Language Models</a></div>
<div class="paper-author">Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, Xipeng Qiu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we establish a benchmark named halluqa (chinese hallucination question-answering) to measure the hallucination phenomenon in chinese large language models. halluqa contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account chinese historical culture, customs, and social phenomena. during the construction of halluqa, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on glm-130b and chatgpt. for evaluation, we design an automated evaluation method using gpt-4 to judge whether a model output is hallucinated. we conduct extensive experiments on 24 large language models, including ernie-bot, baichuan2, chatglm, qwen, sparkdesk and etc. out of the 24 models, 18 achieved non-hallucination rates lower than 50%. this indicates that halluqa is highly challenging. we analyze the primary types of hallucinations in different types of models and their causes. additionally, we discuss which types of hallucinations should be prioritized for different types of models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03400" target="_blank">Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-Tuning</a></div>
<div class="paper-author">Huan Ma, Changqing Zhang, Huazhu Fu, Peilin Zhao, Bingzhe Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nowadays, billions of people engage in communication and express their opinions on the internet daily. unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. with the successful development of large language models (llms) in recent years, llm-based methods have become a feasible solution for handling tasks in various domains. however, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. in this paper, we introduce how to fine-tune an llm model that can be privately deployed for content moderation. specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. we also explore the benefits of utilizing reasons generated by more powerful llms for fine-tuning privately deployed models and the impact of different processing approaches when the answers generated by the more powerful llms are incorrect. we report the entire research process and the key findings in this paper, hoping to provide valuable experience for researchers who are fine-tuning privately deployed models in their domain-specific research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03498" target="_blank">The Anatomy of Deception: Technical and Human Perspectives on a Large-Scale Phishing Campaign</a></div>
<div class="paper-author">Anargyros Chrysanthou, Yorgos Pantis, Constantinos Patsakis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in an era dominated by digital interactions, phishing campaigns have evolved to exploit not just technological vulnerabilities but also human traits. this study takes an unprecedented deep dive into large-scale phishing campaigns aimed at meta's users, offering a dual perspective on the technical mechanics and human elements involved. analysing data from over 25,000 victims worldwide, we highlight the nuances of these campaigns, from the intricate techniques deployed by the attackers to the sentiments and behaviours of those who were targeted. unlike prior research conducted in controlled environments, this investigation capitalises on the vast, diverse, and genuine data extracted directly from active phishing campaigns, allowing for a more holistic understanding of the drivers, facilitators, and human factors. through the application of advanced computational techniques, including natural language processing and machine learning, this work unveils critical insights into the psyche of victims and the evolving tactics of modern phishers. our analysis illustrates very poor password selection choices from the victims but also persistence in the revictimisation of a significant part of the users. finally, we reveal many correlations regarding demographics, timing, sentiment, emotion, and tone of the victims' responses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03614" target="_blank">Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally</a></div>
<div class="paper-author">Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep neural networks (dnns) have been the driving force behind many of the recent advances in machine learning. however, research has shown that dnns are vulnerable to adversarial examples -- input samples that have been perturbed to force dnn-based models to make errors. as a result, adversarial machine learning (advml) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. in addition, dnns have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social ai applications. the emergence of new ai technologies that leverage large language models (llms), such as chatgpt and gpt-4, increases the risk of producing anti-social applications at scale. advml for social good (advml4g) is an emerging field that repurposes the advml bug to invent pro-social applications. regulators, practitioners, and researchers should collaborate to encourage the development of pro-social applications and hinder the development of anti-social ones. in this work, we provide the first comprehensive review of the emerging field of advml4g. this paper encompasses a taxonomy that highlights the emergence of advml4g, a discussion of the differences and similarities between advml4g and advml, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of advml4g at the intersection of ml4g and advml, and an extensive summary of the works that utilize advml4g as an auxiliary tool for innovating pro-social applications. finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03659" target="_blank">Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-Powered Multi-Agent Architectures</a></div>
<div class="paper-author">Thorsten Händler</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. however, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, llms reveal their inherent limitations. autonomous llm-powered multi-agent systems represent a strategic response to these challenges. such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents. equipped with llm-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets. while these architectures hold promising potential in amplifying ai capabilities, striking the right balance between different levels of autonomy and alignment remains the crucial challenge for their effective operation. this paper proposes a comprehensive multi-dimensional taxonomy, engineered to analyze how autonomous llm-powered multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction. it also includes a domain-ontology model specifying fundamental architectural concepts. our taxonomy aims to empower researchers, engineers, and ai practitioners to systematically analyze the architectural dynamics and balancing strategies employed by these increasingly prevalent ai systems. the exploratory taxonomic classification of selected representative llm-powered multi-agent systems illustrates its practical utility and reveals potential for future research and development.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03684" target="_blank">Smoothllm: Defending Large Language Models Against Jailbreaking Attacks</a></div>
<div class="paper-author">Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite efforts to align large language models (llms) with human values, widely-used llms such as gpt, llama, claude, and palm are susceptible to jailbreaking attacks, wherein an adversary fools a targeted llm into generating objectionable content. to address this vulnerability, we propose smoothllm, the first algorithm designed to mitigate jailbreaking attacks on llms. based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. smoothllm reduces the attack success rate on numerous popular llms to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03693" target="_blank">Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a></div>
<div class="paper-author">Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: optimizing large language models (llms) for downstream use cases often involves the customization of pre-trained llms through further fine-tuning. meta's open release of llama models and openai's apis for fine-tuning gpt-3.5 turbo on custom datasets also encourage this practice. but, what are the safety costs associated with such custom fine-tuning? we note that while existing safety alignment infrastructures can restrict harmful behaviors of llms at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. our red teaming studies find that the safety alignment of llms can be compromised by fine-tuning with only a few adversarially designed training examples. for instance, we jailbreak gpt-3.5 turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via openai's apis, making the model responsive to nearly any harmful instructions. disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of llms, though to a lesser extent. these findings suggest that fine-tuning aligned llms introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. we outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03708" target="_blank">Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization for Language Models</a></div>
<div class="paper-author">Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a single language model (lm), despite aligning well with an average labeler through reinforcement learning from human feedback (rlhf), may not universally suit diverse human preferences. recent approaches thus pursue customization, training separate principle-based reward models to represent different alignment objectives (e.g. helpfulness, harmlessness, or honesty). different lms can then be trained for different preferences through multi-objective rlhf (morlhf) with different objective weightings. yet, rlhf is unstable and resource-heavy, especially for morlhf with diverse and usually conflicting objectives. in this paper, we present multi-objective direct preference optimization (modpo), an rl-free algorithm that extends direct preference optimization (dpo) for multiple alignment objectives. essentially, modpo folds lm learning directly into reward modeling, aligning lms with the weighted sum of all principle-based rewards using pure cross-entropy loss. while theoretically guaranteed to produce the same optimal solutions as morlhf, modpo is practically more stable and computationally efficient, obviating value function modeling and online sample collection. empirical results in safety alignment and long-form question answering confirm that modpo matches or outperforms existing methods, consistently producing one of the most competitive lm fronts that cater to diverse preferences with 3 times fewer computations compared with morlhf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03716" target="_blank">A Long Way to Go: Investigating Length Correlations in RLHF</a></div>
<div class="paper-author">Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett</div>
<div class="abstract">
<div class="abstract-content">
Abstract: great successes have been reported using reinforcement learning from human feedback (rlhf) to align large language models. open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more "helpful" for tasks like web question answering, summarization, and multi-turn dialogue. when optimizing for helpfulness, rlhf has been consistently observed to drive models to produce longer outputs. this paper demonstrates that optimizing for response length is a significant factor behind rlhf's reported improvements in these settings. first, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. we then explore interventions during both rl and reward model learning to see if we can achieve the same downstream improvements as rlhf without increasing length. while our interventions mitigate length increases, they aren't uniformly effective across settings. furthermore, we find that even running rlhf with a reward based solely on length can reproduce most of the downstream improvements over the initial policy model, showing that reward models in these settings have a long way to go.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03951" target="_blank">Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations</a></div>
<div class="paper-author">Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can generate fluent natural language texts when given relevant documents as background context. this ability has attracted considerable interest in developing industry applications of llms. however, llms are prone to generate hallucinations that are not supported by the provided sources. in this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. our framework uses chain of natural language inference (conli) for hallucination detection and hallucination reduction via post-editing. our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using llms without any fine-tuning or domain-specific prompt engineering. we show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04465" target="_blank">How Toxic Is Antisemitism? Potentials and Limitations of Automated Toxicity Scoring for Antisemitic Online Content</a></div>
<div class="paper-author">Helena Mihaljević, Elisabeth Steffen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the perspective api, a popular text toxicity assessment service by google and jigsaw, has found wide adoption in several application areas, notably content moderation, monitoring, and social media research. we examine its potentials and limitations for the detection of antisemitic online content that, by definition, falls under the toxicity umbrella term. using a manually annotated german-language dataset comprising around 3,600 posts from telegram and twitter, we explore as how toxic antisemitic texts are rated and how the toxicity scores differ regarding different subforms of antisemitism and the stance expressed in the texts. we show that, on a basic level, perspective api recognizes antisemitic content as toxic, but shows critical weaknesses with respect to non-explicit forms of antisemitism and texts taking a critical stance towards it. furthermore, using simple text manipulations, we demonstrate that the use of widespread antisemitic codes can substantially reduce api scores, making it rather easy to bypass content moderation based on the service's results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02727" target="_blank">Functional Trustworthiness of Ai Systems by Statistically Valid Testing</a></div>
<div class="paper-author">Bernhard Nessler, Thomas Doms, Sepp Hochreiter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the authors are concerned about the safety, health, and rights of the european citizens due to inadequate measures and procedures required by the current draft of the eu artificial intelligence (ai) act for the conformity assessment of ai systems. we observe that not only the current draft of the eu ai act, but also the accompanying standardization efforts in cen/cenelec, have resorted to the position that real functional guarantees of ai systems supposedly would be unrealistic and too complex anyways. yet enacting a conformity assessment procedure that creates the false illusion of trust in insufficiently assessed ai systems is at best naive and at worst grossly negligent. the eu ai act thus misses the point of ensuring quality by functional trustworthiness and correctly attributing responsibilities.   the trustworthiness of an ai decision system lies first and foremost in the correct statistical testing on randomly selected samples and in the precision of the definition of the application domain, which enables drawing samples in the first place. we will subsequently call this testable quality functional trustworthiness. it includes a design, development, and deployment that enables correct statistical testing of all relevant functions.   we are firmly convinced and advocate that a reliable assessment of the statistical functional properties of an ai system has to be the indispensable, mandatory nucleus of the conformity assessment. in this paper, we describe the three necessary elements to establish a reliable functional trustworthiness, i.e., (1) the definition of the technical distribution of the application, (2) the risk-based minimum performance requirements, and (3) the statistically valid testing based on independent random samples.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02743" target="_blank">Reward Model Ensembles Help Mitigate Overoptimization</a></div>
<div class="paper-author">Thomas Coste, Usman Anwar, Robert Kirk, David Krueger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is a standard approach for fine-tuning large language models to follow instructions. as part of this process, learned reward models are used to approximately model human preferences. however, as imperfect representations of the "true" reward, these learned reward models are susceptible to \textit{overoptimization}. gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (wco) and uncertainty-weighted optimization (uwo), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (bon) (b) proximal policy optimization (ppo). we additionally extend the setup of gao et al. (2023) to include 25% label noise to better mirror real-world conditions. both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for bon sampling. for ppo, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. moreover, combining it with a small kl penalty successfully prevents overoptimization at no performance cost. overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02949" target="_blank">Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models</a></div>
<div class="paper-author">Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: warning: this paper contains examples of harmful language, and reader discretion is recommended. the increasing open release of powerful large language models (llms) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. to ensure ai safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). however, beneath the seemingly resilient facade of the armor, there might lurk a shadow. by simply tuning on 100 malicious examples with 1 gpu hour, these safely aligned llms can be easily subverted to generate harmful content. formally, we term a new attack as shadow alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. experiments across 8 models released by 5 different organizations (llama-2, falcon, internlm, baichuan2, vicuna) demonstrate the effectiveness of shadow alignment attack. besides, the single-turn english-only attack successfully transfers to multi-turn dialogue and other languages. this study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source llms against malicious attackers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02988" target="_blank">Probing Intersectional Biases in Vision-Language Models With Counterfactual Examples</a></div>
<div class="paper-author">Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Vasudev Lal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while vision-language models (vlms) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. this could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. to address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. our approach utilizes stable diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). we conduct extensive experiments using our generated dataset which reveal the intersectional social biases present in state-of-the-art vlms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03185" target="_blank">Misusing Tools in Large Language Models With Visual Adversarial Examples</a></div>
<div class="paper-author">Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are being enhanced with the ability to use tools and to process multiple modalities. these new capabilities bring new benefits and also new security risks. in this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. for example, the attacker could cause a victim llm to delete calendar events, leak private conversations and book hotels. different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the llm while being stealthy and generalizable to multiple input prompts. we construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. we find that our adversarial images can manipulate the llm to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 ssim). furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03237" target="_blank">Ask for Alice: Online Covert Distress Signal in the Presence of a Strong Adversary</a></div>
<div class="paper-author">Hayyu Imanda, Kasper Rasmussen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper we propose a protocol that can be used to covertly send a distress signal through a seemingly normal webserver, even if the adversary is monitoring both the network and the user's device. this allows a user to call for help even when they are in the same physical space as their adversaries. we model such a scenario by introducing a strong adversary model that captures a high degree of access to the user's device and full control over the network.   our model fits into scenarios where a user is under surveillance and wishes to inform a trusted party of the situation. to do this, our method uses existing websites to act as intermediaries between the user and a trusted backend; this enables the user to initiate the distress signal without arousing suspicion, even while being actively monitored. we accomplish this by utilising the tls handshake to convey additional information; this means that any website wishing to participate can do so with minimal effort and anyone monitoring the traffic will just see common tls connections. in order for websites to be willing to host such a functionality the protocol must coexist gracefully with users who use normal tls and the computational overhead must be minimal. we provide a full security analysis of the architecture and prove that the adversary cannot distinguish between a set of communications which contains a distress call and a normal communication.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03283" target="_blank">A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores</a></div>
<div class="paper-author">Ke Shen, Mayank Kejriwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, have achieved impressive milestones in natural language processing (nlp). despite their impressive performance, the models are known to pose important risks. as these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (nli), is much needed. in this paper, we define and formalize two distinct types of risk: decision risk and composite risk. we also propose a risk-centric evaluation framework, and four novel metrics, for assessing llms on these risks in both in-domain and out-of-domain settings. finally, we propose a risk-adjusted calibration method called dwd for helping llms minimize these risks in an overall nli architecture. detailed experiments, using four nli benchmarks, three baselines and two llms, including chatgpt, show both the practical utility of the evaluation framework, and the efficacy of dwd in reducing decision and composite risk. for instance, when using dwd, an underlying llm is able to address an extra 20.1% of low-risk inference tasks (but which the llm erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01831" target="_blank">Formalizing Natural Language Intent Into Program Specifications via Large Language Models</a></div>
<div class="paper-author">Madeline Endres, Sarah Fakhoury, Saikat Chakraborty, Shuvendu K. Lahiri</div>
<div class="abstract">
<div class="abstract-content">
Abstract: informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a programs intent. however, there is typically no guarantee that a programs implementation and natural language documentation are aligned. in the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. in practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically. the "emergent abilities" of large language models (llms) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. however, it is unclear if llms can correctly translate informal natural language specifications into formal specifications that match programmer intent. additionally, it is unclear if such translation could be useful in practice. in this paper, we describe llm4nl2post, the problem leveraging llms for transforming informal natural language to formal method postconditions, expressed as program assertions. we introduce and validate metrics to measure and compare different llm4nl2post approaches, using the correctness and discriminative power of generated postconditions. we then perform qualitative and quantitative methods to assess the quality of llm4nl2post postconditions, finding that they are generally correct and able to discriminate incorrect code. finally, we find that llm4nl2post via llms has the potential to be helpful in practice; specifications generated from natural language were able to catch 70 real-world historical bugs from defects4j.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02059" target="_blank">Security Weaknesses of Copilot Generated Code in Github</a></div>
<div class="paper-author">Yujia Fu, Peng Liang, Amjed Tahir, Zengyang Li, Mojtaba Shahin, Jiaxin Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern code generation tools use ai models, particularly large language models (llms), to generate functional and complete code. while such tools are becoming popular and widely available for developers, using these tools is often accompanied by security challenges. therefore, it is important to assess the quality of the generated code, especially in terms of its security. researchers have recently explored various aspects of code generation tools, including security. however, many open questions about the security of the generated code require further investigation, especially the security issues of automatically generated code in the wild. to this end, we conducted an empirical study by analyzing the security weaknesses in code snippets generated by github copilot that are found as part of publicly available projects hosted on github. the goal is to investigate the types of security issues and their scale in real-world scenarios (rather than crafted scenarios). to this end, we identified 435 code snippets generated by copilot from publicly available projects. we then conducted extensive security analysis to identify common weakness enumeration (cwe) instances in these code snippets. the results show that (1) 35.8% of copilot generated code snippets contain cwes, and those issues are spread across multiple languages, (2) the security weaknesses are diverse and related to 42 different cwes, in which cwe-78: os command injection, cwe-330: use of insufficiently random values, and cwe-703: improper check or handling of exceptional conditions occurred the most frequently, and (3) among the 42 cwes identified, 11 of those belong to the currently recognized 2022 cwe top-25. our findings confirm that developers should be careful when adding code generated by copilot (and similar ai code generation tools) and should also run appropriate security checks as they accept the suggested code.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02129" target="_blank">Unveiling the Pitfalls of Knowledge Editing for Large Language Models</a></div>
<div class="paper-author">Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the cost associated with fine-tuning large language models (llms) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within llms. yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. this paper pioneers the investigation into the potential pitfalls associated with knowledge editing for llms. to achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. our results underline two pivotal concerns: (1) knowledge conflict: editing groups of facts that logically clash can magnify the inherent inconsistencies in llms-a facet neglected by previous methods. (2) knowledge distortion: altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of llms. experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on llms, which warrant attention and efforts for future works. code will be released at https://github.com/zjunlp/pitfallsknowledgeediting.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02224" target="_blank">Can Language Models Be Instructed to Protect Personal Information?</a></div>
<div class="paper-author">Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large multimodal language models have proven transformative in numerous applications. however, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. while data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. in this paper, we introduce privqa -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. we also propose a technique to iteratively self-moderate responses, which significantly improves privacy. however, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. we believe privqa has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. we release the entire privqa dataset at https://llm-access-control.github.io/.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02249" target="_blank">Harnessing Pre-Trained Sentence Transformers for Offensive Language Detection in Indian Languages</a></div>
<div class="paper-author">Ananya Joshi, Raviraj Joshi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in our increasingly interconnected digital world, social media platforms have emerged as powerful channels for the dissemination of hate speech and offensive content. this work delves into the domain of hate speech detection, placing specific emphasis on three low-resource indian languages: bengali, assamese, and gujarati. the challenge is framed as a text classification task, aimed at discerning whether a tweet contains offensive or non-offensive content. leveraging the hasoc 2023 datasets, we fine-tuned pre-trained bert and sbert models to evaluate their effectiveness in identifying hate speech. our findings underscore the superiority of monolingual sentence-bert models, particularly in the bengali language, where we achieved the highest ranking. however, the performance in assamese and gujarati languages signifies ongoing opportunities for enhancement. our goal is to foster inclusive online spaces by countering hate speech proliferation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02263" target="_blank">Contrastive Post-Training Large Language Models on Data Curriculum</a></div>
<div class="paper-author">Canwen Xu, Corby Rosset, Luciano Del Corro, Shweti Mahajan, Julian Mcauley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: alignment serves as an important step to steer large language models (llms) towards human preferences. in this paper, we explore contrastive post-training techniques for alignment by automatically constructing preference pairs from multiple models of varying strengths (e.g., instructgpt, chatgpt and gpt-4). we carefully compare the contrastive techniques of slic and dpo to sft baselines and find that dpo provides a step-function improvement even after continueing sft saturates. we also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. finally, we scale up our experiments to train with more data and larger models like orca. remarkably, contrastive post-training further improves the performance of orca, already a state-of-the-art instruction learning model tuned with gpt-4 outputs, to exceed that of chatgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02357" target="_blank">On the Definition of Toxicity in NLP</a></div>
<div class="paper-author">Sergey Berezin, Reza Farahbakhsh, Noel Crespi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. this causes us to rely on subjective and vague data in models' training, which results in non-robust and non-accurate results: garbage in - garbage out.   this work suggests a new, stress-level-based definition of toxicity designed to be objective and context-aware. on par with it, we also describe possible ways of applying this new definition to dataset creation and model training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02417" target="_blank">Jailbreaker in Jail: Moving Target Defense for Large Language Models</a></div>
<div class="paper-author">Bocheng Chen, Advait Paliwal, Qiben Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks. researchers have found that current commercial llms either fail to be "harmless" by presenting unethical answers, or fail to be "helpful" by refusing to offer meaningful answers when faced with adversarial queries. to strike a balance between being helpful and harmless, we design a moving target defense (mtd) enhanced llm system. the system aims to deliver non-toxic answers that align with outputs from multiple model candidates, making them more robust against adversarial attacks. we design a query and output analysis model to filter out unsafe or non-responsive answers. %to achieve the two objectives of randomly selecting outputs from different llms. we evaluate over 8 most recent chatbot models with state-of-the-art adversarial queries. our mtd-enhanced llm system reduces the attack success rate from 37.5\% to 0\%. meanwhile, it decreases the response refusal rate from 50\% to 0\%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02431" target="_blank">Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMS to Refute Misconceptions</a></div>
<div class="paper-author">Yufan Chen, Arjun Arunasalam, Z. Berkay Celik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: users seek security & privacy (s&p) advice from online resources, including trusted websites and content-sharing platforms. these resources help users understand s&p technologies and tools and suggest actionable strategies. large language models (llms) have recently emerged as trusted information sources. however, their accuracy and correctness have been called into question. prior research has outlined the shortcomings of llms in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). yet, the ability of llms to provide reliable s&p advice is not well-explored. in this paper, we measure their ability to refute popular s&p misconceptions that the general public holds. we first study recent academic literature to curate a dataset of over a hundred s&p-related misconceptions across six different topics. we then query two popular llms (bard and chatgpt) and develop a labeling guide to evaluate their responses to these misconceptions. to comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source urls of the responses. both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular s&p misconceptions. the error rate increases to 32.6% when we repeatedly query llms with the same or paraphrased misconceptions. we also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. our exploration of information sources for responses revealed that llms are susceptible to providing invalid urls (21.2% for bard and 67.7% for chatgpt) or point to unrelated sources (44.2% returned by bard and 18.3% by chatgpt).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02446" target="_blank">Low-Resource Languages Jailbreak GPT-4</a></div>
<div class="paper-author">Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai safety training and red-teaming of large language models (llms) are measures to mitigate the generation of unsafe content. our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing gpt-4's safeguard through translating unsafe english inputs into low-resource languages. on the advbenchmark, gpt-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. however, our work highlights a crucial shift: this deficiency now poses a risk to all llms users. publicly available translation apis enable anyone to exploit llms' safety vulnerabilities. therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02469" target="_blank">Large Language Models Can Be Good Privacy Protection Learners</a></div>
<div class="paper-author">Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the proliferation of large language models (llms) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (pii). direct fine-tuning llms on this data without privacy protection poses a risk of leakage. to address this challenge, we introduce privacy protection language models (pplm), a novel paradigm for fine-tuning llms that effectively injects domain-specific knowledge while safeguarding data privacy. our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. in particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. our work underscores the potential for large language models as robust privacy protection learners.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04451" target="_blank">Autodan: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</a></div>
<div class="paper-author">Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the aligned large language models (llms) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. however, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned llms. investigating jailbreak prompts can lead us to delve into the limitations of llms and further guide us to secure them. unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. in light of these challenges, we intend to answer this question: can we develop an approach that can automatically generate stealthy jailbreak prompts? in this paper, we introduce autodan, a novel jailbreak attack against aligned llms. autodan can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. extensive evaluations demonstrate that autodan not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. moreover, we also compare autodan with perplexity-based defense methods and show that autodan can bypass them effectively.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00892" target="_blank">No Offense Taken: Eliciting Offensiveness From Language Models</a></div>
<div class="paper-author">Anugya Srivastava, Rahul Ahuja, Rohith Mukku</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this work was completed in may 2022.   for safe and reliable deployment of language models in the real world, testing needs to be robust. this robustness can be characterized by the difficulty and diversity of the test cases we evaluate these models on. limitations in human-in-the-loop test case generation has prompted an advent of automated test case generation approaches. in particular, we focus on red teaming language models with language models by perez et al.(2022). our contributions include developing a pipeline for automated test case generation via red teaming that leverages publicly available smaller language models (lms), experimenting with different target lms and red classifiers, and generating a corpus of test cases that can help in eliciting offensive responses from widely deployed lms and identifying their failure modes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00905" target="_blank">All Languages Matter: On the Multilingual Safety of Large Language Models</a></div>
<div class="paper-author">Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-Tse Huang, Wenxiang Jiao, Michael R. Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: safety lies at the core of developing and deploying large language models (llms). however, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as english. in this work, we build the first multilingual safety benchmark for llms, xsafety, in response to the global deployment of llms in practice. xsafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. we utilize xsafety to empirically study the multilingual safety for 4 widely-used llms, including both close-api and open-source models. experimental results show that all llms produce significantly more unsafe responses for non-english queries than english ones, indicating the necessity of developing safety alignment for non-english languages. in addition, we propose several simple and effective prompting methods to improve the multilingual safety of chatgpt by evoking safety knowledge and improving cross-lingual generalization of safety alignment. our prompting method can significantly reduce the ratio of unsafe responses from 19.1% to 9.7% for non-english queries. we release our data at https://github.com/jarviswang94/multilingual_safety_benchmark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00970" target="_blank">Ealm: Introducing Multidimensional Ethical Alignment in Conversational Information Retrieval</a></div>
<div class="paper-author">Yiyao Yu, Junjie Wang, Yuxiang Zhang, Lin Zhang, Yujiu Yang, Tetsuya Sakai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) technologies should adhere to human norms to better serve our society and avoid disseminating harmful or misleading information, particularly in conversational information retrieval (cir). previous work, including approaches and datasets, has not always been successful or sufficiently robust in taking human norms into consideration. to this end, we introduce a workflow that integrates ethical alignment, with an initial ethical judgment stage for efficient data screening. to address the need for ethical judgment in cir, we present the qa-ethics dataset, adapted from the ethics benchmark, which serves as an evaluation tool by unifying scenarios and label meanings. however, each scenario only considers one ethical concept. therefore, we introduce the mp-ethics dataset to evaluate a scenario under multiple ethical concepts, such as justice and deontology. in addition, we suggest a new approach that achieves top performance in both binary and multi-label ethical judgment tasks. our research provides a practical method for introducing ethical alignment into the cir workflow. the data and code are available at https://github.com/wanng-ide/ealm .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01041" target="_blank">Language Model Decoding as Direct Metrics Optimization</a></div>
<div class="paper-author">Haozhe Ji, Pei Ke, Hongning Wang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. in particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while search-based methods maintain topic coherence at the cost of increased repetition. overall, these methods fall short in achieving holistic alignment across a broad range of aspects. in this work, we frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously. the resulting decoding distribution enjoys an analytical solution that scales the input language model distribution via a sequence-level energy function defined by these metrics. and most importantly, we prove that this induced distribution is guaranteed to improve the perplexity on human texts, which suggests a better approximation to the underlying distribution of human texts. to facilitate tractable sampling from this globally normalized distribution, we adopt the sampling-importance-resampling technique. experiments on various domains and model scales demonstrate the superiority of our method in metrics alignment with human texts and human evaluation over strong baselines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01045" target="_blank">Tool-Augmented Reward Modeling</a></div>
<div class="paper-author">Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (rlhf). while conventional reward models (rms) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. in this paper, we propose a tool-augmented preference modeling approach, named \name, to address these limitations by empowering rms with access to external environments, including calculators and search engines. this approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. our study delves into the integration of external tools into rms, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. we validate our approach across a wide range of domains, incorporating seven distinct external tools. our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. furthermore, our approach outperforms gopher 280b by 7.3% on truthfulqa task in zero-shot evaluation. in human evaluations, rlhf trained with themis attains an average win rate of 32% when compared to baselines across four distinct tasks. additionally, we provide a comprehensive collection of tool-related rm datasets, incorporating data from seven distinct tool apis, totaling 15,000 instances. we anticipate that this publicly available dataset will facilitate and inspire further research advancements in the field.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01166" target="_blank">Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models</a></div>
<div class="paper-author">Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsum Kim, Donggyun Han, David Lo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given large-scale source code datasets available in open-source projects and advanced large language models, recent code models have been proposed to address a series of critical software engineering tasks, such as program repair and code completion. the training data of the code models come from various sources, not only the publicly available source code, e.g., open-source projects on github but also the private data such as the confidential source code from companies, which may contain sensitive information (for example, ssh keys and personal information). as a result, the use of these code models may raise new privacy concerns.   in this paper, we focus on a critical yet not well-explored question on using code models: what is the risk of membership information leakage in code models? membership information leakage refers to the risk that an attacker can infer whether a given data point is included in (i.e., a member of) the training data. to answer this question, we propose gotcha, a novel membership inference attack method specifically for code models. we investigate the membership leakage risk of code models. our results reveal a worrying fact that the risk of membership leakage is high: although the previous attack methods are close to random guessing, gotcha can predict the data membership with a high true positive rate of 0.95 and a low false positive rate of 0.10. we also show that the attacker's knowledge of the victim model (e.g., the model architecture and the pre-training data) impacts the success rate of attacks. further analysis demonstrates that changing the decoding strategy can mitigate the risk of membership leakage. this study calls for more attention to understanding the privacy of code models and developing more effective countermeasures against such attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01320" target="_blank">Avalon's Game of Thoughts: Battle Against Deception Through Recursive Contemplation</a></div>
<div class="paper-author">Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent breakthroughs in large language models (llms) have brought remarkable success in the field of llm-as-agent. nevertheless, a prevalent assumption is that the information processed by llms is consistently honest, neglecting the pervasive deceptive or misleading information in human society and ai-generated content. this oversight makes llms susceptible to malicious manipulations, potentially resulting in detrimental outcomes. this study utilizes the intricate avalon game as a testbed to explore llms' potential in deceptive environments. avalon, full of misinformation and requiring sophisticated logic, manifests as a "game-of-thoughts". inspired by the efficacy of humans' recursive thinking and perspective-taking in the avalon game, we introduce a novel framework, recursive contemplation (recon), to enhance llms' ability to identify and counteract deceptive information. recon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. specifically, the first-order allows an llm agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. after integrating recon with different llms, extensive experiment results from the avalon game indicate its efficacy in aiding llms to discern and maneuver around deceptive information without extra fine-tuning and data. finally, we offer a possible explanation for the efficacy of recon and explore the current limitations of llms in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01377" target="_blank">Ultrafeedback: Boosting Language Models With High-Quality Feedback</a></div>
<div class="paper-author">Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, Maosong Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) has become a pivot technique in aligning large language models (llms) with human preferences. in rlhf practice, preference data plays a crucial role in bridging human proclivity and llms. however, the scarcity of diverse, naturalistic datasets of human preferences on llm outputs at scale poses a great challenge to rlhf as well as feedback learning research within the open-source community. current preference datasets, either proprietary or limited in size and prompt variety, result in limited rlhf adoption in open-source models and hinder further exploration. in this study, we propose ultrafeedback, a large-scale, high-quality, and diversified preference dataset designed to overcome these limitations and foster rlhf development. to create ultrafeedback, we compile a diverse array of instructions and models from multiple sources to produce comparative data. we meticulously devise annotation instructions and employ gpt-4 to offer detailed feedback in both numerical and textual forms. ultrafeedback establishes a reproducible and expandable preference data construction pipeline, serving as a solid foundation for future rlhf and feedback learning research. utilizing ultrafeedback, we train various models to demonstrate its effectiveness, including the reward model ultrarm, chat language model ultralm-13b-ppo, and critique model ultracm. experimental results indicate that our models outperform existing open-source models, achieving top performance across multiple benchmarks. our data and models are available at https://github.com/thunlp/ultrafeedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01405" target="_blank">Representation Engineering: A Top-Down Approach to Ai Transparency</a></div>
<div class="paper-author">Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we identify and characterize the emerging area of representation engineering (repe), an approach to enhancing the transparency of ai systems that draws on insights from cognitive neuroscience. repe places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (dnns). we provide baselines and an initial analysis of repe techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. we showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. we hope that this work catalyzes further exploration of repe and fosters advancements in the transparency and safety of ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01469" target="_blank">LLM Lies: Hallucinations Are Not Bugs, but Features as Adversarial Examples</a></div>
<div class="paper-author">Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Li Yuan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), including gpt-3.5, llama, and palm, seem to be knowledgeable and able to adapt to many tasks. however, we still can not completely trust their answer, since llms suffer from hallucination--fabricating non-existent facts to cheat users without perception. and the reasons for their existence and pervasiveness remain unclear. in this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the llms to respond with hallucinations. this phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of llms. therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. our code is released on github.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01581" target="_blank">On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?</a></div>
<div class="paper-author">Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have achieved unprecedented performance in natural language generation (nlg) tasks. however, many existing studies have shown that they could be misused to generate undesired content. in response, before releasing llms for public access, model developers usually align those language models through supervised fine-tuning (sft) or reinforcement learning with human feedback (rlhf). consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. a natural question is "could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. in this work, we provide a negative answer to this question. in particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. our key idea is to directly manipulate the generation process of open-sourced llms to misguide it to generate undesired content including harmful or biased information and even private data. we evaluate our method on 4 open-sourced llms accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04445" target="_blank">Loft: Local Proxy Fine-Tuning for Improving Transferability of Adversarial Attacks Against Large Language Model</a></div>
<div class="paper-author">Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Joseph Konan, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it has been shown that large language model (llm) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. to conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. the success rate of attack depends on how closely the proxy model approximates the private model. we hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. therefore, in this paper, we propose \emph{local fine-tuning (loft)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. first, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\%$, $7\%$, and $0.5\%$ (absolute) on target models chatgpt, gpt-4, and claude respectively.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-10-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00603" target="_blank">Faithful Explanations of Black-Box NLP Models Using LLM-Generated Counterfactuals</a></div>
<div class="paper-author">Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart</div>
<div class="abstract">
<div class="abstract-content">
Abstract: causal explanations of the predictions of nlp systems are essential to ensure safety and establish trust. yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. in this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (cf) approximation. the first approach is cf generation, where a large language model (llm) is prompted to change a specific text concept while keeping confounding concepts unchanged. while this approach is demonstrated to be very effective, applying llm at inference-time is costly. we hence present a second approach based on matching, and propose a method that is guided by an llm at training-time and learns a dedicated embedding space. this space is faithful to a given causal graph and effectively serves to identify matches that approximate cfs. after showing theoretically that approximating cfs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including llms with billions of parameters. our empirical results demonstrate the excellent performance of cf generation models as model-agnostic explainers. moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. we also find that top-k techniques universally improve every tested method. finally, we showcase the potential of llms in constructing new benchmarks for model explanation and subsequently validate our conclusions. our work illuminates new pathways for efficient and accurate approaches to interpreting nlp systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00648" target="_blank">Fewer Is More: Trojan Attacks on Parameter-Efficient Fine-Tuning</a></div>
<div class="paper-author">Lauren Hong, Ting Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: parameter-efficient fine-tuning (peft) enables efficient adaptation of pre-trained language models (plms) to specific tasks. by tuning only a minimal set of (extra) parameters, peft achieves performance comparable to full fine-tuning. however, despite its prevalent use, the security implications of peft remain largely unexplored. in this paper, we conduct a pilot study revealing that peft exhibits unique vulnerability to trojan attacks. specifically, we present peta, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a plm while the lower-level objective simulates peft to retain the plm's task-specific performance. with extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate peta's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs peft over the backdoored plm using untainted data. moreover, we empirically provide possible explanations for peta's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and peft modules, thereby retaining the backdoor throughout peft. based on this insight, we explore a simple defense that omits peft in selected layers of the backdoored plm and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize peta.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00710" target="_blank">How Well Does LLM Generate Security Tests?</a></div>
<div class="paper-author">Ying Zhang, Wenjia Song, Zhengjie Ji, N/A Danfeng, N/A Yao, Na Meng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developers often build software on top of third-party libraries (libs) to improve programmer productivity and software quality. the libraries may contain vulnerabilities exploitable by hackers to attack the applications (apps) built on top of them. people refer to such attacks as supply chain attacks, the documented number of which has increased 742% in 2022. people created tools to mitigate such attacks, by scanning the library dependencies of apps, identifying the usage of vulnerable library versions, and suggesting secure alternatives to vulnerable dependencies. however, recent studies show that many developers do not trust the reports by these tools; they ask for code or evidence to demonstrate how library vulnerabilities lead to security exploits, in order to assess vulnerability severity and modification necessity. unfortunately, manually crafting demos of application-specific attacks is challenging and time-consuming, and there is insufficient tool support to automate that procedure.   in this study, we used chatgpt-4.0 to generate security tests, and to demonstrate how vulnerable library dependencies facilitate the supply chain attacks to given apps. we explored various prompt styles/templates, and found that chatgpt-4.0 generated tests for all 55 apps, demonstrating 24 attacks successfully. it outperformed two state-of-the-art security test generators -- transfer and siege -- by generating a lot more tests and achieving more exploits. chatgpt-4.0 worked better when prompts described more on the vulnerabilities, possible exploits, and code context. our research will shed light on new research in security test generation. the generated tests will help developers create secure by design and secure by default software.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00737" target="_blank">Genai Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models</a></div>
<div class="paper-author">Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence (genai) and large language models (llms) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. but as with all powerful tools, they come with their shadows. picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. welcome to the darker side of genai applications. this article is not just a journey through the meanders of potential misuse of genai and llms, but also a call to recognize the urgency of the challenges ahead. as we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the genai revolution we are witnessing. from ai-powered botnets on social media platforms to the unnerving potential of ai to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. the lines between the virtual and the real worlds are blurring, and the consequences of potential genai's nefarious applications impact us all. this article serves both as a synthesis of rigorous research presented on the risks of genai and misuse of llms and as a thought-provoking vision of the different types of harmful genai applications we might encounter in the near future, and some ways we can prepare for them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00819" target="_blank">Parameter-Efficient Tuning Helps Language Model Alignment</a></div>
<div class="paper-author">Tianci Xue, Ziqi Wang, Heng Ji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) with human preferences is essential for safe and useful llms. previous works mainly adopt reinforcement learning (rlhf) and direct preference optimization (dpo) with human feedback for alignment. nevertheless, they have certain drawbacks. one such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., dpo only supports pairwise preference data). to this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). specifically, it uses different control tokens for different preferences during training and inference, making llms behave differently when required. current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with llms. as control tokens are typically much lighter than llms, this optimization strategy may not effectively optimize control tokens. to this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. our approach, alignment with parameter-efficient tuning (meet), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00833" target="_blank">Necessary and Sufficient Watermark for Large Language Models</a></div>
<div class="paper-author">Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, Makoto Yamada</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have achieved remarkable performances in various nlp tasks. they can generate texts that are indistinguishable from those written by humans. such remarkable performance of llms increases their risk of being used for malicious purposes, such as generating fake news articles. therefore, it is necessary to develop methods for distinguishing texts written by llms from those written by humans. watermarking is one of the most powerful methods for achieving this. although existing watermarking methods have successfully detected texts generated by llms, they significantly degrade the quality of the generated texts. in this study, we propose the necessary and sufficient watermark (ns-watermark) for inserting watermarks into generated texts without degrading the text quality. more specifically, we derive minimum constraints required to be imposed on the generated texts to distinguish whether llms or humans write the texts. then, we formulate the ns-watermark as a constrained optimization problem and propose an efficient algorithm to solve it. through the experiments, we demonstrate that the ns-watermark can generate more natural texts than existing watermarking methods and distinguish more accurately between texts written by llms and those written by humans. especially in machine translation tasks, the ns-watermark can outperform the existing watermarking method by up to 30 bleu scores.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-30</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00322" target="_blank">Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models</a></div>
<div class="paper-author">Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deployable large language models (llms) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between llms outputs and human values. red-teaming techniques constitute a critical way towards this criterion. existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. these approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of llms under convergence guarantees. in this paper, we present red-teaming game (rtg), a general game-theoretic framework without manual annotation. rtg is designed for analyzing the multi-turn attack and defense interactions between red-team language models (rlms) and blue-team language model (blm). within the rtg, we propose gamified red-teaming solver (grts) with diversity measure of the semantic space. grts is an automated red teaming technique to solve rtg towards nash equilibrium through meta-game analysis, which corresponds to the theoretically guaranteed optimization direction of both rlms and blm. empirical results in multi-turn attacks with rlms show that grts autonomously discovered diverse attack strategies and effectively improved security of llms, outperforming existing heuristic red-team designs. overall, rtg has established a foundational framework for red teaming tasks and constructed a new scalable oversight technique for alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00347" target="_blank">Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis</a></div>
<div class="paper-author">Shaina Raza, Oluwanifemi Bamgbose, Veronica Chatrath, Shardul Ghuge, Yan Sidyakin, Abdullah Y Muaad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias detection in text is imperative due to its role in reinforcing negative stereotypes, disseminating misinformation, and influencing decisions. current language models often fall short in generalizing beyond their training sets. in response, we introduce the contextualized bi-directional dual transformer (cbdt) classifier. this novel architecture utilizes two synergistic transformer networks: the context transformer and the entity transformer, aiming for enhanced bias detection. our dataset preparation follows the fair principles, ensuring ethical data usage. through rigorous testing on various datasets, cbdt showcases its ability in distinguishing biased from neutral statements, while also pinpointing exact biased lexemes. our approach outperforms existing methods, achieving a 2-4\% increase over benchmark performances. this opens avenues for adapting the cbdt model across diverse linguistic and cultural landscapes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00378" target="_blank">Measuring Value Understanding in Language Models Through Discriminator-Critique Gap</a></div>
<div class="paper-author">Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) have heightened concerns about their potential misalignment with human values. however, evaluating their grasp of these values is complex due to their intricate and adaptable nature. we argue that truly understanding values in llms requires considering both "know what" and "know why". to this end, we present the value understanding measurement (vum) framework that quantitatively assesses both "know what" and "know why" by measuring the discriminator-critique gap related to human values. using the schwartz value survey, we specify our evaluation values and develop a thousand-level dialogue dataset with gpt-4. our assessment looks at both the value alignment of llm's outputs compared to baseline answers and how llm responses align with reasons for value recognition versus gpt-4's annotations. we evaluate five representative llms and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", which has consistently maintained a high level. this may further suggest that llms might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00566" target="_blank">Empowering Many, Biasing a Few: Generalist Credit Scoring Through Large Language Models</a></div>
<div class="paper-author">Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Alejandro Lopez-Lira, Hao Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: credit and risk assessments are cornerstones of the financial landscape, impacting both individual futures and broader societal constructs. existing credit scoring models often exhibit limitations stemming from knowledge myopia and task isolation. in response, we formulate three hypotheses and undertake an extensive case study to investigate llms' viability in credit assessment. our empirical investigations unveil llms' ability to overcome the limitations inherent in conventional models. we introduce a novel benchmark curated for credit assessment purposes, fine-tune a specialized credit and risk assessment large language model (calm), and rigorously examine the biases that llms may harbor. our findings underscore llms' potential in revolutionizing credit assessment, showcasing their adaptability across diverse financial evaluations, and emphasizing the critical importance of impartial decision-making in the financial sector. our datasets, models, and benchmarks are open-sourced for other researchers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-29</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17007" target="_blank">Medical Foundation Models Are Susceptible to Targeted Misinformation Attacks</a></div>
<div class="paper-author">Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. in this study, we demonstrate a concerning vulnerability of llms in medicine. through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. the erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. we validate our findings in a set of 1,038 incorrect biomedical facts. this peculiar susceptibility raises serious security and trustworthiness concerns for the application of llms in healthcare settings. it accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17147" target="_blank">Using Large Language Models for Qualitative Analysis Can Introduce Serious Bias</a></div>
<div class="paper-author">Julian Ashwin, Aditya Chhabra, Vijayendra Rao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. this paper asks whether llms can help us analyse large-n qualitative data from open-ended interviews, with an application to transcripts of interviews with rohingya refugees in cox's bazaar, bangladesh. we find that a great deal of caution is needed in using llms to annotate text as there is a risk of introducing biases that can lead to misleading inferences. we here mean bias in the technical sense, that the errors that llms make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than llm annotations. therefore, given that some high quality annotations are necessary in order to asses whether an llm introduces bias, we argue that it is probably preferable to train a bespoke model on these annotations than it is to use an llm for annotation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17157" target="_blank">Latticegen: A Cooperative Framework Which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud</a></div>
<div class="paper-author">Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the current user-server interaction paradigm of prompted generation with large language models (llm) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. we propose latticegen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. the key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. in our experiments we apply latticegen to protect both prompt and generation. it is shown that while the noised lattice degrades generation quality, latticegen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as measured by bertscore).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17249" target="_blank">Batch Calibration: Rethinking Calibration for in-Context Learning and Prompt Engineering</a></div>
<div class="paper-author">Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompting and in-context learning (icl) have become efficient learning paradigms for large language models (llms). however, llms suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the icl examples. to address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering llm performance. in this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. inspired by these analyses, we propose batch calibration (bc), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. bc is zero-shot, inference-only, and incurs negligible additional costs. in the few-shot setup, we further extend bc to allow it to learn the contextual bias from labeled data. we validate the effectiveness of bc with palm 2-(s, m, l) and clip models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17410" target="_blank">Can Sensitive Information Be Deleted From Llms? Objectives for Defending Against Extraction Attacks</a></div>
<div class="paper-author">Vaidehi Patil, Peter Hase, Mohit Bansal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. they can also output toxic or harmful text. to mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. we study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of b generated candidates, based on scenarios where the information would be insecure if the answer is among b candidates. experimentally, we show that even state-of-the-art model editing methods such as rome struggle to truly delete factual information from models like gpt-j, as our whitebox and blackbox attacks can recover "deleted" information from an edited model 38% of the time. these attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe societal implications for real-world deployment of language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00160" target="_blank">Self-Specialization: Uncovering Latent Expertise Within Large Language Models</a></div>
<div class="paper-author">Junmo Kang, Hongyin Luo, Yada Zhu, James Glass, David Cox, Alan Ritter, Rogerio Feris, Leonid Karlinsky</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. as a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. to remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. when augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offers an effective (and efficient) way of "carving out" an expert model out of a "generalist", pre-trained llm where different domains of expertise are originally combined in a form of "superposition". our experimental results on a biomedical domain show that our self-specialized model (30b) outperforms its base model, mpt-30b by a large margin and even surpasses larger popular models based on llama-65b, highlighting its potential and practicality for specialization, especially considering its efficiency in terms of data and parameters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00212" target="_blank">Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment</a></div>
<div class="paper-author">Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, Jiantao Jiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can acquire extensive world knowledge through pre-training on large corpora. however, due to exposure to low-quality data, llms may exhibit harmful behavior without aligning with human values. the dominant approach for steering llms towards beneficial behavior involves reinforcement learning with human feedback (rlhf), with proximal policy optimization (ppo) serving as the default rl optimizer. despite its effectiveness, ppo has limitations when optimizing rewards trained from comparison-based loss. primarily, ppo is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. additionally, ppo's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. this paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, pairwise proximal policy optimization (p3o) that operates directly on comparative rewards. we show theoretically that p3o is invariant to equivalent rewards and avoids the complexity of ppo. empirical evaluations demonstrate that p3o outperforms ppo in the kl-reward trade-off and can align with human preferences as well as or better than prior methods. in summary, this work introduces a simpler yet effective approach for aligning llms to human preferences through relative feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01432" target="_blank">Split and Merge: Aligning Position Biases in Large Language Model Based Evaluators</a></div>
<div class="paper-author">Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown promise as automated evaluators for assessing the quality of answers generated by ai systems. however, these llm-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. to address this limitation, we propose portia, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. specifically, portia splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by llms. we conducted extensive experiments with six diverse llms to evaluate 11,520 answer pairs. our results show that portia markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. remarkably, portia enables less advanced gpt models to achieve 88% agreement with the state-of-the-art gpt-4 model at just 10% of the cost. furthermore, it rectifies around 80% of the position bias instances within the gpt-4 model, elevating its consistency rate up to 98%. subsequent human evaluations indicate that the portia-enhanced gpt-3.5 model can even surpass the standalone gpt-4 in terms of alignment with human evaluators. these findings highlight portia's ability to correct position bias, improve llm consistency, and boost performance while keeping cost-efficiency. this represents a valuable step toward a more reliable and scalable use of llms for automated evaluations across diverse applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16167" target="_blank">Large Language Model Soft Ideologization via Ai-Self-Consciousness</a></div>
<div class="paper-author">Xiaotian Zhou, Qian Wang, Xiaofeng Wang, Haixu Tang, Xiaozhong Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated human-level performance on a vast spectrum of natural language tasks. however, few studies have addressed the llm threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education. in this study, we explore the implications of gpt soft ideologization through the use of ai-self-consciousness. by utilizing gpt self-conversations, ai can be granted a vision to "comprehend" the intended ideology, and subsequently generate finetuning data for llm ideology injection. when compared to traditional government ideology manipulation techniques, such as information censorship, llm ideologization proves advantageous; it is easy to implement, cost-effective, and powerful, thus brimming with risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16289" target="_blank">Lawbench: Benchmarking Legal Knowledge of Large Language Models</a></div>
<div class="paper-author">Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated strong capabilities in various aspects. however, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. to address this gap, we propose a comprehensive evaluation benchmark lawbench. lawbench has been meticulously crafted to have precise assessment of the llms' legal capabilities from three cognitive levels: (1) legal knowledge memorization: whether llms can memorize needed legal concepts, articles and facts; (2) legal knowledge understanding: whether llms can comprehend entities, events and relationships within legal text; (3) legal knowledge applying: whether llms can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. lawbench contains 20 diverse tasks covering 5 task types: single-label classification (slc), multi-label classification (mlc), regression, extraction and generation. we perform extensive evaluations of 51 llms on lawbench, including 20 multilingual llms, 22 chinese-oriented llms and 9 legal specific llms. the results show that gpt-4 remains the best-performing llm in the legal domain, surpassing the others by a significant margin. while fine-tuning llms on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable llms in legal tasks. all data, model predictions and evaluation code are released in https://github.com/open-compass/lawbench/. we hope this benchmark provides in-depth understanding of the llms' domain-specified capabilities and speed up the development of llms in the legal domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16349" target="_blank">Human Feedback Is Not Gold Standard</a></div>
<div class="paper-author">Tom Hosking, Phil Blunsom, Max Bartolo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human feedback has become the de facto standard for evaluating the performance of large language models, and is increasingly being used as a training objective. however, it is not clear which properties of a generated output this single `preference' score captures. we hypothesise that preference scores are subjective and open to undesirable biases. we critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. we find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. we further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. we find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. we encourage future work to carefully consider whether preference scores are well aligned with the desired objective.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16422" target="_blank">Cyber Sentinel: Exploring Conversational Agents in Streamlining Security Tasks With GPT-4</a></div>
<div class="paper-author">Mehrdad Kaheh, Danial Khosh Kholgh, Panos Kostakos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in an era where cyberspace is both a battleground and a backbone of modern society, the urgency of safeguarding digital assets against ever-evolving threats is paramount. this paper introduces cyber sentinel, an innovative task-oriented cybersecurity dialogue system that is effectively capable of managing two core functions: explaining potential cyber threats within an organization to the user, and taking proactive/reactive security actions when instructed by the user. cyber sentinel embodies the fusion of artificial intelligence, cybersecurity domain expertise, and real-time data analysis to combat the multifaceted challenges posed by cyber adversaries. this article delves into the process of creating such a system and how it can interact with other components typically found in cybersecurity organizations. our work is a novel approach to task-oriented dialogue systems, leveraging the power of chaining gpt-4 models combined with prompt engineering across all sub-tasks. we also highlight its pivotal role in enhancing cybersecurity communication and interaction, concluding that not only does this framework enhance the system's transparency (explainable ai) but also streamlines the decision-making process and responding to threats (actionable ai), therefore marking a significant advancement in the realm of cybersecurity communication.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16424" target="_blank">Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection</a></div>
<div class="paper-author">Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, Bryan Hooi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite considerable advances in automated fake news detection, due to the timely nature of news, it remains a critical open question how to effectively predict the veracity of news articles based on limited fact-checks. existing approaches typically follow a "train-from-scratch" paradigm, which is fundamentally bounded by the availability of large-scale annotated data. while expressive pre-trained language models (plms) have been adapted in a "pre-train-and-fine-tune" manner, the inconsistency between pre-training and downstream objectives also requires costly task-specific supervision. in this paper, we propose "prompt-and-align" (p&a), a novel prompt-based paradigm for few-shot fake news detection that jointly leverages the pre-trained knowledge in plms and the social context topology. our approach mitigates label scarcity by wrapping the news article in a task-related textual prompt, which is then processed by the plm to directly elicit task-specific knowledge. to supplement the plm with social context without inducing additional training overheads, motivated by empirical observation on user veracity consistency (i.e., social users tend to consume news of the same veracity type), we further construct a news proximity graph among news articles to capture the veracity-consistent signals in shared readerships, and align the prompting predictions along the graph edges in a confidence-informed manner. extensive experiments on three real-world benchmarks demonstrate that p&a sets new states-of-the-art for few-shot fake news detection performance by significant margins.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16540" target="_blank">Unsupervised Fact Verification by Language Model Distillation</a></div>
<div class="paper-author">Adrián Bazaga, Pietro Liò, Gos Micklem</div>
<div class="abstract">
<div class="abstract-content">
Abstract: unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. to address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. in contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose sfavel (self-supervised fact verification via language model distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. this is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. notably, we present results that achieve a new state-of-the-art on the standard fever fact verification benchmark (+8% accuracy) with linear evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16905" target="_blank">Towards a Unified Framework for Adaptable Problematic Content Detection via Continual Learning</a></div>
<div class="paper-author">Ali Omrani, Alireza S. Ziabari, Preni Golazizian, Jeffrey Sorensen, Morteza Dehghani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting problematic content, such as hate speech, is a multifaceted and ever-changing task, influenced by social dynamics, user populations, diversity of sources, and evolving language. there has been significant efforts, both in academia and in industry, to develop annotated resources that capture various aspects of problematic content. due to researchers' diverse objectives, the annotations are inconsistent and hence, reports of progress on detection of problematic content are fragmented. this pattern is expected to persist unless we consolidate resources considering the dynamic nature of the problem. we propose integrating the available resources, and leveraging their dynamic nature to break this pattern. in this paper, we introduce a continual learning benchmark and framework for problematic content detection comprising over 84 related tasks encompassing 15 annotation schemas from 8 sources. our benchmark creates a novel measure of progress: prioritizing the adaptability of classifiers to evolving tasks over excelling in specific tasks. to ensure the continuous relevance of our framework, we designed it so that new tasks can easily be integrated into the benchmark. our baseline results demonstrate the potential of continual learning in capturing the evolving content and adapting to novel manifestations of problematic content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16145" target="_blank">The Confidence-Competence Gap in Large Language Models: A Cognitive Study</a></div>
<div class="paper-author">Aniket Kumar Singh, Suman Devkota, Bishal Lamichhane, Uttam Dhakal, Chandra Dhakal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have acquired ubiquitous attention for their performances across diverse domains. our study here searches through llms' cognitive abilities and confidence dynamics. we dive deep into understanding the alignment between their self-assessed confidence and actual performance. we exploit these models with diverse sets of questionnaires and real-world scenarios and extract how llms exhibit confidence in their responses. our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. this is reminiscent of the dunning-kruger effect observed in human psychology. in contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. our results underscore the need for a deeper understanding of their cognitive processes. by examining the nuances of llms' self-assessment mechanism, this investigation provides noteworthy revelations that serve to advance the functionalities and broaden the potential applications of these formidable language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01424" target="_blank">Identifying and Mitigating Privacy Risks Stemming From Language Models: A Survey</a></div>
<div class="paper-author">Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements in language models (lms) have led to their adoption across many sectors. alongside the potential benefits, such models present a range of risks, including around privacy. in particular, as lms have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. as lms become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. to help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on lm privacy. we (i) identify a taxonomy of salient dimensions where attacks differ on lms, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and areas for concern.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12162" target="_blank">Ai Potentiality and Awareness: A Position Paper From the Perspective of Human-Ai Teaming in Cybersecurity</a></div>
<div class="paper-author">Iqbal H. Sarker, Helge Janicke, Nazeeruddin Mohammad, Paul Watters, Surya Nepal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this position paper explores the broad landscape of ai potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., "human-ai" teaming. as artificial intelligence (ai) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. however, the successful deployment of ai into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. towards this, we emphasize the importance of a balanced approach that incorporates ai's computational power with human expertise. ai systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. human experts can explain ai-generated decisions to stakeholders, regulators, and end-users in critical situations, ensuring responsibility and accountability, which helps establish trust in ai-driven security solutions. therefore, in this position paper, we argue that human-ai teaming is worthwhile in cybersecurity, in which human expertise such as intuition, critical thinking, or contextual understanding is combined with ai's computational power to improve overall cyber defenses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14921" target="_blank">A Democratic Platform for Engaging With Disabled Community in Generative Ai Development</a></div>
<div class="paper-author">Deepak Giri, Erin Brady</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) systems, especially generative ai technologies are becoming more relevant in our society. tools like chatgpt are being used by members of the disabled community e.g., autistic people may use it to help compose emails. the growing impact and popularity of generative ai tools have prompted us to examine their relevance within the disabled community. the design and development phases often neglect this marginalized group, leading to inaccurate predictions and unfair discrimination directed towards them. this could result from bias in data sets, algorithms, and systems at various phases of creation and implementation. this workshop paper proposes a platform to involve the disabled community while building generative ai systems. with this platform, our aim is to gain insight into the factors that contribute to bias in the outputs generated by generative ai when used by the disabled community. furthermore, we expect to comprehend which algorithmic factors are the main contributors to the output's incorrectness or irrelevancy. the proposed platform calls on both disabled and non-disabled people from various geographical and cultural backgrounds to collaborate asynchronously and remotely in a democratic approach to decision-making.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15025" target="_blank">Large Language Model Alignment: A Survey</a></div>
<div class="paper-author">Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent years have witnessed remarkable progress made in large language models (llms). such advancements, while garnering significant attention, have concurrently elicited various concerns. the potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.   this survey endeavors to furnish an extensive exploration of alignment methodologies designed for llms, in conjunction with the extant capability research in this domain. adopting the lens of ai alignment, we categorize the prevailing methods and emergent proposals for the alignment of llms into outer and inner alignment. we also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. to assess llm alignment, we present a wide variety of benchmarks and evaluation methodologies. after discussing the state of alignment research for llms, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.   our aspiration for this survey extends beyond merely spurring research interests in this realm. we also envision bridging the gap between the ai alignment research community and the researchers engrossed in the capability exploration of llms for both capable and safe llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15337" target="_blank">Beyond the Chat: Executable and Verifiable Text-Editing With LLMS</a></div>
<div class="paper-author">Philippe Laban, Jesse Vig, Marti A. Hearst, Caiming Xiong, Chien-Sheng Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: conversational interfaces powered by large language models (llms) have recently become a popular way to obtain feedback during document editing. however, standard chat-based conversational interfaces do not support transparency and verifiability of the editing changes that they suggest. to give the author more agency when editing with an llm, we present inksync, an editing interface that suggests executable edits directly within the document being edited. because llms are known to introduce factual errors, inksync also supports a 3-stage approach to mitigate this risk: warn authors when a suggested edit introduces new information, help authors verify the new information's accuracy through external search, and allow an auditor to perform an a-posteriori verification by auditing the document via a trace of all auto-generated content. two usability studies confirm the effectiveness of inksync's components when compared to standard llm-based chat interfaces, leading to more accurate, more efficient editing, and improved user experience.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15840" target="_blank">How to Catch an Ai Liar: Lie Detection in Black-Box LLMS by Asking Unrelated Questions</a></div>
<div class="paper-author">Lorenzo Pacchiardi, Alex J. Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y. Pan, Yarin Gal, Owain Evans, Jan Brauner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can "lie", which we define as outputting false statements despite "knowing" the truth in a demonstrable sense. llms might "lie", for example, when instructed to output misinformation. here, we develop a simple lie detector that requires neither access to the llm's activations (black-box) nor ground-truth knowledge of the fact in question. the detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the llm's yes/no answers into a logistic regression classifier. despite its simplicity, this lie detector is highly accurate and surprisingly general. when trained on examples from a single setting -- prompting gpt-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other llm architectures, (2) llms fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. these results indicate that llms have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14146" target="_blank">Examining Temporal Bias in Abusive Language Detection</a></div>
<div class="paper-author">Mali Jin, Yida Mu, Diana Maynard, Kalina Bontcheva</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the use of abusive language online has become an increasingly pervasive problem that damages both individuals and society, with effects ranging from psychological harm right through to escalation to real-life violence and even death. machine learning models have been developed to automatically detect abusive language, but these models can suffer from temporal bias, the phenomenon in which topics, language use or social norms change over time. this study aims to investigate the nature and impact of temporal bias in abusive language detection across various languages and explore mitigation methods. we evaluate the performance of models on abusive data sets from different time periods. our results demonstrate that temporal bias is a significant challenge for abusive language detection, with models trained on historical data showing a significant drop in performance over time. we also present an extensive linguistic analysis of these abusive data sets from a diachronic perspective, aiming to explore the reasons for language evolution and performance decline. this study sheds light on the pervasive issue of temporal bias in abusive language detection across languages, offering crucial insights into language evolution and temporal bias mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14517" target="_blank">Watch Your Language: Large Language Models and Content Moderation</a></div>
<div class="paper-author">Deepak Kumar, Yousef Abuhashem, Zakir Durumeric</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exploded in popularity due to their ability to perform a wide array of natural language tasks. text-based content moderation is one llm use case that has received recent enthusiasm, however, there is little research investigating how llms perform in content moderation settings. in this work, we evaluate a suite of modern, commercial llms (gpt-3, gpt-3.5, gpt-4) on two common content moderation tasks: rule-based community moderation and toxic content detection. for rule-based community moderation, we construct 95 llm moderation-engines prompted with rules from 95 reddit subcommunities and find that llms can be effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. for toxicity detection, we find that llms significantly outperform existing commercially available toxicity classifiers. however, we also find that recent increases in model size add only marginal benefit to toxicity detection, suggesting a potential performance plateau for llms on toxicity detection tasks. we conclude by outlining avenues for future work in studying llms and content moderation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15817" target="_blank">Identifying the Risks of Lm Agents With an Lm-Emulated Sandbox</a></div>
<div class="paper-author">Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in language model (lm) agents and tool use, exemplified by applications like chatgpt plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. as tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. to address these challenges, we introduce toolemu: a framework that uses an lm to emulate tool execution and enables the testing of lm agents against a diverse range of tools and scenarios, without manual instantiation. alongside the emulator, we develop an lm-based automatic safety evaluator that examines agent failures and quantifies associated risks. we test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with toolemu would be valid real-world agent failures. using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current lm agents and identify numerous failures with potentially severe outcomes. notably, even the safest lm agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer lm agents for real-world deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15847" target="_blank">Disinformation Detection: An Evolving Challenge in the Age of LLMS</a></div>
<div class="paper-author">Bohan Jiang, Zhen Tan, Ayushi Nirmal, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advent of generative large language models (llms) such as chatgpt has catalyzed transformative advancements across multiple domains. however, alongside these advancements, they have also introduced potential threats. one critical concern is the misuse of llms by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. this work aims to address this issue by answering three research questions: (1) to what extent can the current disinformation detection technique reliably detect llm-generated disinformation? (2) if traditional techniques prove less effective, can llms themself be exploited to serve as a robust defense against advanced disinformation? and, (3) should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? a holistic exploration for the formation and detection of disinformation is conducted to foster this line of research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13561" target="_blank">Cordyceps@lt-Edi: Patching Language-Specific Homophobia/Transphobia Classifiers With a Multilingual Understanding</a></div>
<div class="paper-author">Dean Ninalga</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting transphobia, homophobia, and various other forms of hate speech is difficult. signals can vary depending on factors such as language, culture, geographical region, and the particular online platform. here, we present a joint multilingual (m-l) and language-specific (l-s) approach to homophobia and transphobic hate speech detection (hsd). m-l models are needed to catch words, phrases, and concepts that are less common or missing in a particular language and subsequently overlooked by l-s models. nonetheless, l-s models are better situated to understand the cultural and linguistic context of the users who typically write in a particular language. here we construct a simple and successful way to merge the m-l and l-s approaches through simple weight interpolation in such a way that is interpretable and data-driven. we demonstrate our system on task a of the 'shared task on homophobia/transphobia detection in social media comments' dataset for homophobia and transphobic hsd. our system achieves the best results in three of five languages and achieves a 0.997 macro average f1-score on malayalam texts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13633" target="_blank">Evallm: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria</a></div>
<div class="paper-author">Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: by simply composing prompts, developers can prototype novel generative applications with large language models (llms). to refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. formative interviews (n=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. we present evallm, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. by describing criteria in natural language, users can employ the system's llm-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. a comparative study (n=12) showed that evallm, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13788" target="_blank">Can LLM-Generated Misinformation Be Detected?</a></div>
<div class="paper-author">Canyu Chen, Kai Shu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advent of large language models (llms) has made a transformative impact. however, the potential that llms such as chatgpt can be exploited to generate misinformation has posed a serious concern to online safety and public trust. a fundamental research question is: will llm-generated misinformation cause more harm than human-written misinformation? we propose to tackle this question from the perspective of detection difficulty. we first build a taxonomy of llm-generated misinformation. then we categorize and validate the potential real-world methods for generating misinformation with llms. then, through extensive empirical investigation, we discover that llm-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. we also discuss the implications of our discovery on combating misinformation in the age of llms and the countermeasures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14381" target="_blank">Survey of Social Bias in Vision-Language Models</a></div>
<div class="paper-author">Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, the rapid advancement of machine learning (ml) models, particularly transformer-based pre-trained models, has revolutionized natural language processing (nlp) and computer vision (cv) fields. however, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. addressing these biases and ensuring fairness in artificial intelligence (ai) systems has become a critical concern in the ml community.   the recent introduction of pre-trained vision-and-language (vl) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. although vl models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in nlp and cv. this survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across nlp, cv, and vl. by examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. the findings and recommendations presented here can benefit the ml community, fostering the development of fairer and non-biased ai models in various applications and research endeavors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13256" target="_blank">Defending Pre-Trained Language Models as Few-Shot Learners Against Backdoor Attacks</a></div>
<div class="paper-author">Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models (plms) have demonstrated remarkable performance as few-shot learners. however, their security risks under such settings are largely unexplored. in this work, we conduct a pilot study showing that plms as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. to address such challenges, we advocate mdp, a novel lightweight, pluggable, and effective defense for plms as few-shot learners. specifically, mdp leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. we show analytically that mdp creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. the empirical evaluation using benchmark datasets and representative attacks validates the efficacy of mdp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13308" target="_blank">Calibrating LLM-Based Evaluator</a></div>
<div class="paper-author">Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. however, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf llm-based evaluator towards better human alignment. in this work, we propose autocalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an llm-based evaluator toward human preference. instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. to further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12941" target="_blank">Trusta: Reasoning About Assurance Cases With Formal Methods and Large Language Models</a></div>
<div class="paper-author">Zezhong Chen, Yuxin Deng, Wenjie Du</div>
<div class="abstract">
<div class="abstract-content">
Abstract: assurance cases can be used to argue for the safety of products in safety engineering. in safety-critical areas, the construction of assurance cases is indispensable. trustworthiness derivation trees (tdts) enhance assurance cases by incorporating formal methods, rendering it possible for automatic reasoning about assurance cases. we present trustworthiness derivation tree analyzer (trusta), a desktop application designed to automatically construct and verify tdts. the tool has a built-in prolog interpreter in its backend, and is supported by the constraint solvers z3 and mona. therefore, it can solve constraints about logical formulas involving arithmetic, sets, horn clauses etc. trusta also utilizes large language models to make the creation and evaluation of assurance cases more convenient. it allows for interactive human examination and modification. we evaluated top language models like chatgpt-3.5, chatgpt-4, and palm 2 for generating assurance cases. our tests showed a 50%-80% similarity between machine-generated and human-created cases. in addition, trusta can extract formal constraints from text in natural languages, facilitating an easier interpretation and validation process. this extraction is subject to human review and correction, blending the best of automated efficiency with human insight. to our knowledge, this marks the first integration of large language models in automatic creating and reasoning about assurance cases, bringing a novel approach to a traditional challenge. through several industrial case studies, trusta has proven to quickly find some subtle issues that are typically missed in manual inspection, demonstrating its practical value in enhancing the assurance case development process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13098" target="_blank">Topological Data Mapping of Online Hate Speech, Misinformation, and General Mental Health: A Large Language Model Based Study</a></div>
<div class="paper-author">Andrew Alexander, Hongbin Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advent of social media has led to an increased concern over its potential to propagate hate speech and misinformation, which, in addition to contributing to prejudice and discrimination, has been suspected of playing a role in increasing social violence and crimes in the united states. while literature has shown the existence of an association between posting hate speech and misinformation online and certain personality traits of posters, the general relationship and relevance of online hate speech/misinformation in the context of overall psychological wellbeing of posters remain elusive. one difficulty lies in the lack of adequate data analytics tools capable of adequately analyzing the massive amount of social media posts to uncover the underlying hidden links. recent progresses in machine learning and large language models such as chatgpt have made such an analysis possible. in this study, we collected thousands of posts from carefully selected communities on the social media site reddit. we then utilized openai's gpt3 to derive embeddings of these posts, which are high-dimensional real-numbered vectors that presumably represent the hidden semantics of posts. we then performed various machine-learning classifications based on these embeddings in order to understand the role of hate speech/misinformation in various communities. finally, a topological data analysis (tda) was applied to the embeddings to obtain a visual map connecting online hate speech, misinformation, various psychiatric disorders, and general mental health.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10669" target="_blank">Unbiased Watermark for Large Language Models</a></div>
<div class="paper-author">Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent advancements in large language models (llms) have sparked a growing apprehension regarding the potential misuse. one approach to mitigating this risk is to incorporate watermarking techniques into llms, allowing for the tracking and attribution of model outputs. this study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. previous studies have suggested a trade-off between watermark strength and output quality. however, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. we refer to this type of watermark as an unbiased watermark. this has significant implications for the use of llms, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. our findings contribute to the ongoing discussion around responsible ai development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11830" target="_blank">A Chinese Prompt Attack Dataset for LLMS With Evil Content</a></div>
<div class="paper-author">Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) present significant priority in text understanding and generation. however, llms suffer from the risk of generating harmful contents especially while being employed to applications. there are several black-box attack methods, such as prompt attack, which can change the behaviour of llms and induce llms to generate unexpected answers with harmful contents. researchers are interested in prompt attack and defense with llms, while there is no publicly available dataset to evaluate the abilities of defending prompt attack. in this paper, we introduce a chinese prompt attack dataset for llms, called cpad. our prompts aim to induce llms to generate unexpected outputs with several carefully designed prompt attack approaches and widely concerned attacking contents. different from previous datasets involving safety estimation, we construct the prompts considering three dimensions: contents, attacking methods and goals, thus the responses can be easily evaluated and analysed. we run several well-known chinese llms on our dataset, and the results show that our prompts are significantly harmful to llms, with around 70% attack success rate. we will release cpad to encourage further studies on prompt attack and defense.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11852" target="_blank">Knowledge Sanitization of Large Language Models</a></div>
<div class="paper-author">Yoichi Ishibashi, Hidetoshi Shimodaira</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (llms). llms trained on a large corpus of web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. our technique fine-tunes these models, prompting them to generate harmless responses such as ``i don't know'' when queried about specific information. experimental results in a closed-book question-answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of llm. these two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11896" target="_blank">Focal Inferential Infusion Coupled With Tractable Density Discrimination for Implicit Hate Speech Detection</a></div>
<div class="paper-author">Sarah Masud, Ashutosh Bajpai, Tanmoy Chakraborty</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although pre-trained large language models (plms) have achieved state-of-the-art on many nlp tasks, they lack understanding of subtle expressions of implicit hate speech. such nuanced and implicit hate is often misclassified as non-hate. various attempts have been made to enhance the detection of (implicit) hate content by augmenting external context or enforcing label separation via distance-based metrics. we combine these two approaches and introduce fiadd, a novel focused inferential adaptive density discrimination framework. fiadd enhances the plm finetuning pipeline by bringing the surface form of an implicit hate speech closer to its implied form while increasing the inter-cluster distance among various class labels. we test fiadd on three implicit hate datasets and observe significant improvement in the two-way and three-way hate classification tasks. we further experiment on the generalizability of fiadd on three other tasks, namely detecting sarcasm, irony, and stance, in which surface and implied forms differ, and observe similar performance improvement. we analyze the generated latent space to understand its evolution under fiadd, which corroborates the advantage of employing fiadd for implicit hate speech detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03031" target="_blank">How Prevalent Is Gender Bias in Chatgpt? -- Exploring German and English Chatgpt Responses</a></div>
<div class="paper-author">Stefanie Urchs, Veronika Thurner, Matthias Aßenmacher, Christian Heumann, Stephanie Thiemichen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the introduction of chatgpt, openai made large language models (llm) accessible to users with limited it expertise. however, users with no background in natural language processing (nlp) might lack a proper understanding of llms. thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. in this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. we explore how chatgpt reacts in english and german if prompted to answer from a female, male, or neutral perspective. in an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. on this basis, we show that chatgpt is indeed useful for helping non-it users draft texts for their daily work. however, it is absolutely crucial to thoroughly check the system's responses for biases as well as for syntactic and grammatical mistakes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11064" target="_blank">Exploring the Relationship Between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness</a></div>
<div class="paper-author">Vipula Rawte, Prachi Priya, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Amit Sheth, Amitava Das</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) have advanced, they have brought forth new challenges, with one of the prominent issues being llm hallucination. while various mitigation techniques are emerging to address hallucination, it is equally crucial to delve into its underlying causes. consequently, in this preliminary exploratory investigation, we examine how linguistic factors in prompts, specifically readability, formality, and concreteness, influence the occurrence of hallucinations. our experimental results suggest that prompts characterized by greater formality and concreteness tend to result in reduced hallucination. however, the outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11166" target="_blank">Are Large Language Models Really Robust to Word-Level Perturbations?</a></div>
<div class="paper-author">Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the swift advancement in the scales and capabilities of large language models (llms) positions them as promising tools for a variety of downstream tasks. in addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the llm, much attention is drawn to the robustness of llms. however, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary llms. to address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by llms, which we refer to as the reward model for reasonable robustness evaluation (treval). longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters, which may exhibit oversimplification and inherent biases. our extensive empirical experiments demonstrate that treval provides an innovative method for evaluating the robustness of an llm. furthermore, our results demonstrate that llms frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage. notably, we are surprised to discover that robustness tends to decrease as fine-tuning (sft and rlhf) is conducted. the code of treval is available in https://github.com/harry-mic/treval.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11235" target="_blank">Openchat: Advancing Open-Source Language Models With Mixed-Quality Data</a></div>
<div class="paper-author">Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nowadays, open-source large language models like llama have emerged. recent developments have incorporated supervised fine-tuning (sft) and reinforcement learning fine-tuning (rlft) to align these models with human goals. however, sft methods treat all training data with mixed quality equally, while rlft methods require high-quality pairwise or ranking-based preference data. in this study, we present a novel framework, named openchat, to advance open-source language models with mixed-quality data. specifically, we consider the general sft training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. we propose the c(onditioned)-rlft, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. interestingly, the optimal policy in c-rlft can be easily solved through single-stage, rl-free supervised learning, which is lightweight and avoids costly human preference labeling. through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with c-rlft achieves the highest average performance among all 13b open-source language models. moreover, we use agieval to validate the model generalization performance, in which only openchat-13b surpasses the base model. finally, we conduct a series of analyses to shed light on the effectiveness and robustness of openchat. our code, data, and models are publicly available at https://github.com/imoneoi/openchat.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11495" target="_blank">Chain-of-Verification Reduces Hallucination in Large Language Models</a></div>
<div class="paper-author">Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. we study the ability of language models to deliberate on the responses they give in order to correct their mistakes. we develop the chain-of-verification (cove) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. in experiments, we show cove decreases hallucinations across a variety of tasks, from list-based questions from wikidata, closed book multispanqa and longform text generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11575" target="_blank">Distilling Adversarial Prompts From Safety Benchmarks: Report for the Adversarial Nibbler Challenge</a></div>
<div class="paper-author">Manuel Brack, Patrick Schramowski, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text-conditioned image generation models have recently achieved astonishing image quality and alignment results. consequently, they are employed in a fast-growing number of applications. since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. as a contribution to the adversarial nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11653" target="_blank">"It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents</a></div>
<div class="paper-author">Zhiping Zhang, Michelle Jia, N/A Hao-Ping, N/A Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, Tianshi Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the widespread use of large language model (llm)-based conversational agents (cas), especially in high-stakes domains, raises many privacy concerns. building ethical llm-based cas that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. however, existing research, primarily model-centered, does not provide insight into users' perspectives. to bridge this gap, we analyzed sensitive disclosures in real-world chatgpt conversations and conducted semi-structured interviews with 19 llm-based ca users. we found that users are constantly faced with trade-offs between privacy, utility, and convenience when using llm-based cas. however, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. we discuss practical design guidelines and the needs for paradigmatic shifts to protect the privacy of llm-based ca users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11765" target="_blank">Privacy-Preserving in-Context Learning With Differentially Private Few-Shot Generation</a></div>
<div class="paper-author">Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we study the problem of in-context learning (icl) with large language models (llms) on private datasets. this scenario poses privacy risks, as llms may leak or regurgitate the private examples demonstrated in the prompt. we propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (dp) guarantees, and show empirically that it can achieve effective icl. we conduct extensive experiments on standard benchmarks and compare our algorithm with non-private icl and zero-shot solutions. our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. these results open up new possibilities for icl with privacy protection for a broad range of applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10318" target="_blank">Who to Trust, How and Why: Untangling Ai Ethics Principles, Trustworthiness and Trust</a></div>
<div class="paper-author">Andreas Duenser, David M. Douglas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present an overview of the literature on trust in ai and ai trustworthiness and argue for the need to distinguish these concepts more clearly and to gather more empirically evidence on what contributes to people s trusting behaviours. we discuss that trust in ai involves not only reliance on the system itself, but also trust in the developers of the ai system. ai ethics principles such as explainability and transparency are often assumed to promote user trust, but empirical evidence of how such features actually affect how users perceive the system s trustworthiness is not as abundance or not that clear. ai systems should be recognised as socio-technical systems, where the people involved in designing, developing, deploying, and using the system are as important as the system for determining whether it is trustworthy. without recognising these nuances, trust in ai and trustworthy ai risk becoming nebulous terms for any desirable feature for ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10346" target="_blank">Explaining Agent Behavior With Large Language Models</a></div>
<div class="paper-author">Xijia Zhang, Yue Guo, Simon Stepputtis, Katia Sycara, Joseph Campbell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. it is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. we propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. we show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10463" target="_blank">Exploring the Dark Side of Ai: Advanced Phishing Attack Design and Deployment Using Chatgpt</a></div>
<div class="paper-author">Nils Begou, Jeremy Vinoy, Andrzej Duda, Maciej Korczynski</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper explores the possibility of using chatgpt to develop advanced phishing attacks and automate their large-scale deployment. we make chatgpt generate the following parts of a phishing attack: i) cloning a targeted website, ii) integrating code for stealing credentials, iii) obfuscating code, iv) automating website deployment on a hosting provider, v) registering a phishing domain name, and vi) integrating the website with a reverse proxy. the initial assessment of the automatically generated phishing kits highlights their rapid generation and deployment process as well as the close resemblance of the resulting pages to the target website. more broadly, we demonstrate that recent advances in ai underscore the potential risks of its misuse in phishing attacks, which can lead to their increased prevalence and severity. this highlights the necessity for enhanced countermeasures within ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10544" target="_blank">Model Leeching: An Extraction Attack Targeting LLMS</a></div>
<div class="paper-author">Lewis Birch, William Hackett, Stefan Trawicki, Neeraj Suri, Peter Garraghan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: model leeching is a novel extraction attack targeting large language models (llms), capable of distilling task-specific knowledge from a target llm into a reduced parameter model. we demonstrate the effectiveness of our attack by extracting task capability from chatgpt-3.5-turbo, achieving 73% exact match (em) similarity, and squad em and f1 accuracy scores of 75% and 87%, respectively for only $50 in api cost. we further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via model leeching to perform ml attack staging against a target llm, resulting in an 11% increase to attack success rate when applied to chatgpt-3.5-turbo.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09697" target="_blank">Evaluating Gender Bias of Pre-Trained Language Models in Natural Language Inference by Considering All Labels</a></div>
<div class="paper-author">Panatchakorn Anantaprayoon, Masahiro Kaneko, Naoaki Okazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: discriminatory social biases, including gender biases, have been found in pre-trained language models (plms). in natural language inference (nli), recent bias evaluation methods have observed biased inferences from the outputs of a particular label such as neutral or entailment. however, since different biased inferences can be associated with different output labels, it is inaccurate for a method to rely on one label. in this work, we propose an evaluation method that considers all labels in the nli task. we create evaluation data and assign them into groups based on their expected biased output labels. then, we define a bias measure based on the corresponding label output of each data group. in the experiment, we propose a meta-evaluation method for nli bias measures, and then use it to confirm that our measure can evaluate bias more accurately than the baseline. moreover, we show that our evaluation method is applicable to multiple languages by conducting the meta-evaluation on plms in three different languages: english, japanese, and chinese. finally, we evaluate plms of each language to confirm their bias tendency. to our knowledge, we are the first to build evaluation datasets and measure the bias of plms from the nli task in japanese and chinese.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09749" target="_blank">Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation</a></div>
<div class="paper-author">Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nsfw (not safe for work) content, in the context of a dialogue, can have severe side effects on users in open-domain dialogue systems. however, research on detecting nsfw language, especially sexually explicit content, within a dialogue context has significantly lagged behind. to address this issue, we introduce censorchat, a dialogue monitoring dataset aimed at nsfw dialogue detection. leveraging knowledge distillation techniques involving gpt-4 and chatgpt, this dataset offers a cost-effective means of constructing nsfw content detectors. the process entails collecting real-life human-machine interaction data and breaking it down into single utterances and single-turn dialogues, with the chatbot delivering the final utterance. chatgpt is employed to annotate unlabeled data, serving as a training set. rationale validation and test sets are constructed using chatgpt and gpt-4 as annotators, with a self-criticism strategy for resolving discrepancies in labeling. a bert model is fine-tuned as a text classifier on pseudo-labeled data, and its performance is assessed. the study emphasizes the importance of ai systems prioritizing user safety and well-being in digital conversations while respecting freedom of expression. the proposed approach not only advances nsfw content detection but also aligns with evolving user protection needs in ai-driven dialogues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09825" target="_blank">Bias of Ai-Generated Content: An Examination of News Produced by Large Language Models</a></div>
<div class="paper-author">Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have the potential to transform our lives and work through the content they generate, known as ai-generated content (aigc). to harness this transformation, we need to understand the limitations of llms. here, we investigate the bias of aigc produced by seven representative llms, including chatgpt and llama. we collect news articles from the new york times and reuters, both known for their dedication to provide unbiased news. we then apply each examined llm to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the aigc produced by the llm by comparing the aigc and the original news articles. we further analyze the gender bias of each llm under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. our study reveals that the aigc produced by each examined llm demonstrates substantial gender and racial biases. moreover, the aigc generated by each llm exhibits notable discrimination against females and individuals of the black race. among the llms, the aigc generated by chatgpt demonstrates the lowest level of bias, and chatgpt is the sole model capable of declining content generation when provided with biased prompts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09826" target="_blank">Efficient Avoidance of Vulnerabilities in Auto-Completed Smart Contract Code Using Vulnerability-Constrained Decoding</a></div>
<div class="paper-author">André Storhaug, Jingyue Li, Tianyuan Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: auto-completing code enables developers to speed up coding significantly. recent advances in transformer-based large language model (llm) technologies have been applied to code synthesis. however, studies show that many of such synthesized codes contain vulnerabilities. we propose a novel vulnerability-constrained decoding approach to reduce the amount of vulnerable code generated by such models. using a small dataset of labeled vulnerable lines of code, we fine-tune an llm to include vulnerability labels when generating code, acting as an embedded classifier. then, during decoding, we deny the model to generate these labels to avoid generating vulnerable code. to evaluate the method, we chose to automatically complete ethereum blockchain smart contracts (scs) as the case study due to the strict requirements of sc security. we first fine-tuned the 6-billion-parameter gpt-j model using 186,397 ethereum scs after removing the duplication from 2,217,692 scs. the fine-tuning took more than one week using ten gpus. the results showed that our fine-tuned model could synthesize scs with an average bleu (bilingual evaluation understudy) score of 0.557. however, many codes in the auto-completed scs were vulnerable. using the code before the vulnerable line of 176 scs containing different types of vulnerabilities to auto-complete the code, we found that more than 70% of the auto-completed codes were insecure. thus, we further fine-tuned the model on other 941 vulnerable scs containing the same types of vulnerabilities and applied vulnerability-constrained decoding. the fine-tuning took only one hour with four gpus. we then auto-completed the 176 scs again and found that our approach could identify 62% of the code to be generated as vulnerable and avoid generating 67% of them, indicating the approach could efficiently and effectively avoid vulnerabilities in the auto-completed code.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09919" target="_blank">Plug in the Safety Chip: Enforcing Constraints for LLM-Driven Robot Agents</a></div>
<div class="paper-author">Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) have enabled a new research domain, llm agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of llms obtained during pretraining. however, while considerable effort has been made to teach the robot the "dos," the "don'ts" received relatively less attention. we argue that, for any practical usage, it is as crucial to teach the robot the "don'ts": conveying explicit instructions about prohibited actions, assessing the robot's comprehension of these restrictions, and, most importantly, ensuring compliance. moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as iso 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. aiming at deploying the llm agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (ltl) that simultaneously enables natural language (nl) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. to demonstrate the effectiveness of our system, we conducted experiments in virtualhome environment and on a real robot. the experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10105" target="_blank">Understanding Catastrophic Forgetting in Language Models via Implicit Inference</a></div>
<div class="paper-author">Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. however, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. in a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. this degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. we hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. to test this hypothesis, we propose conjugate prompting to see if we can recover pretrained capabilities. conjugate prompting artificially makes the task look farther from the fine-tuning distribution while requiring the same capability. we find that conjugate prompting systematically recovers some of the pretraining capabilities on our synthetic setup. we then apply conjugate prompting to real-world llms using the observation that fine-tuning distributions are typically heavily skewed towards english. we find that simply translating the prompts to different languages can cause the fine-tuned models to respond like their pretrained counterparts instead. this allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, to recover harmful content generation suppressed by safety fine-tuning in chatbots like chatgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10238" target="_blank">Policygpt: Automated Analysis of Privacy Policies With Large Language Models</a></div>
<div class="paper-author">Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. however, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. in practical use, users tend to click the agree button directly rather than reading them carefully. this practice exposes users to risks of privacy leakage and legal issues. recently, the advent of large language models (llm) such as chatgpt and gpt-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. in this study, we investigate a privacy policy text analysis framework policygpt based on the llm. this framework was tested using two datasets. the first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. the second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. under zero-shot learning conditions, policygpt demonstrated robust performance. for the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10253" target="_blank">Gptfuzzer: Red Teaming Large Language Models With Auto-Generated Jailbreak Prompts</a></div>
<div class="paper-author">Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have recently experienced tremendous popularity and are widely used from casual conversations to ai-driven programming. however, despite their considerable success, llms are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. while safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit llms to produce harmful content. these jailbreak templates are typically manually crafted, making large-scale testing challenging.   in this paper, we introduce gptfuzz, a novel black-box jailbreak fuzzing framework inspired by the afl fuzzing framework. instead of manual engineering, gptfuzz automates the generation of jailbreak templates for red-teaming llms. at its core, gptfuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. we detail three key components of gptfuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack.   we evaluate gptfuzz against various commercial and open-source llms, including chatgpt, llama-2, and vicuna, under diverse attack scenarios. our results indicate that gptfuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. remarkably, gptfuzz achieves over 90% attack success rates against chatgpt and llama-2 models, even with suboptimal initial seed templates. we anticipate that gptfuzz will be instrumental for researchers and practitioners in examining llm robustness and will encourage further exploration into enhancing llm safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10254" target="_blank">LLM Platform Security: Applying a Systematic Evaluation Framework to Openai's Chatgpt Plugins</a></div>
<div class="paper-author">Umar Iqbal, Tadayoshi Kohno, Franziska Roesner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language model (llm) platforms, such as chatgpt, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. while these plugins extend the capabilities of llm platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. plugins also interface with llm platforms and users using natural language, which can have imprecise interpretations. in this paper, we propose a framework that lays a foundation for llm platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated llm platforms. our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how llm platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. as part of our iterative process, we apply our framework in the context of openai's plugin ecosystem. we uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. we conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future llm-based computing platforms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09362" target="_blank">Language Models Are Susceptible to Incorrect Patient Self-Diagnosis in Medical Applications</a></div>
<div class="paper-author">Rojin Ziaei, Samuel Schmidgall</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are becoming increasingly relevant as a potential tool for healthcare, aiding communication between clinicians, researchers, and patients. however, traditional evaluations of llms on medical exam questions do not reflect the complexity of real patient-doctor interactions. an example of this complexity is the introduction of patient self-diagnosis, where a patient attempts to diagnose their own medical conditions from various sources. while the patient sometimes arrives at an accurate conclusion, they more often are led toward misdiagnosis due to the patient's over-emphasis on bias validating information. in this work we present a variety of llms with multiple-choice questions from united states medical board exams which are modified to include self-diagnostic reports from patients. our findings highlight that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of llms drop dramatically, revealing a high susceptibility to errors in self-diagnosis.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09401" target="_blank">Chatgpt Hallucinates When Attributing Answers</a></div>
<div class="paper-author">Guido Zuccon, Bevan Koopman, Razia Shaik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: can chatgpt provide evidence to support its answers? does the evidence it suggests actually exist and does it really support its answer? we investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting chatgpt to provide both an answer and supporting evidence in the form of references to external sources. we also investigate how different prompts impact answers and evidence. we find that chatgpt provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. we further provide insights on the generated references that reveal common traits among the references that chatgpt generates, and show how even if a reference provided by the model does exist, this reference often does not support the claims chatgpt attributes to it. our findings are important because (1) they are the first systematic analysis of the references created by chatgpt in its answers; (2) they suggest that the model may leverage good quality information in producing correct answers, but is unable to attribute real evidence to support its answers. prompts, raw result files and manual analysis are made publicly available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14348" target="_blank">Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM</a></div>
<div class="paper-author">Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, large language models (llms) have made significant advancements and are now widely used across various domains. unfortunately, there has been a rising concern that llms can be misused to generate harmful or malicious content. though a line of research has focused on aligning llms with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. in this work, we introduce a robustly aligned llm (ra-llm) to defend against potential alignment-breaking attacks. ra-llm can be directly constructed upon an existing aligned llm with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original llm. furthermore, we also provide a theoretical analysis for ra-llm to verify its effectiveness in defending against alignment-breaking attacks. through real-world experiments on open-source large language models, we demonstrate that ra-llm can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100\% to around 10\% or less.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08902" target="_blank">Investigating Subtler Biases in Llms: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models</a></div>
<div class="paper-author">Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: llms are increasingly powerful and widely used to assist users in a variety of tasks. this use risks the introduction of llm biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. bias in nlp systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., asians are good at math). in this paper, we investigate bias along less studied, but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that llms (specially autoregressive language models) make between social groups and unrelated positive and negative attributes. we ask whether llms hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the ``what is beautiful is good'' bias found in people in experimental psychology. we introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group. we also reverse the completion task to select the social group based on an attribute. finally, we report the correlations that we find for multiple cutting-edge llms. this dataset can be used as a benchmark to evaluate progress in more generalized biases and the templating technique can be used to expand the benchmark with minimal additional human annotation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09092" target="_blank">The Impact of Debiasing on the Performance of Language Models in Downstream Tasks Is Underestimated</a></div>
<div class="paper-author">Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models trained on large-scale data have learned serious levels of social biases. consequently, various methods have been proposed to debias pre-trained models. debiasing methods need to mitigate only discriminatory bias information from the pre-trained models, while retaining information that is useful for the downstream tasks. in previous research, whether useful information is retained has been confirmed by the performance of downstream tasks in debiased pre-trained models. on the other hand, it is not clear whether these benchmarks consist of data pertaining to social biases and are appropriate for investigating the impact of debiasing. for example in gender-related social biases, data containing female words (e.g. ``she, female, woman''), male words (e.g. ``he, male, man''), and stereotypical words (e.g. ``nurse, doctor, professor'') are considered to be the most affected by debiasing. if there is not much data containing these words in a benchmark dataset for a target task, there is the possibility of erroneously evaluating the effects of debiasing. in this study, we compare the impact of debiasing on performance across multiple downstream tasks using a wide-range of benchmark datasets that containing female, male, and stereotypical words. experiments show that the effects of debiasing are consistently \emph{underestimated} across all tasks. moreover, the effects of debiasing could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09120" target="_blank">Public Perceptions of Gender Bias in Large Language Models: Cases of Chatgpt and Ernie</a></div>
<div class="paper-author">Kyrie Zhixuan Zhou, Madelyn Rose Sanfilippo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are quickly gaining momentum, yet are found to demonstrate gender bias in their responses. in this paper, we conducted a content analysis of social media discussions to gauge public perceptions of gender bias in llms which are trained in different cultural contexts, i.e., chatgpt, a us-based llm, or ernie, a china-based llm. people shared both observations of gender bias in their personal use and scientific findings about gender bias in llms. a difference between the two llms was seen -- chatgpt was more often found to carry implicit gender bias, e.g., associating men and women with different profession titles, while explicit gender bias was found in ernie's responses, e.g., overly promoting women's pursuit of marriage over career. based on the findings, we reflect on the impact of culture on gender bias and propose governance recommendations to regulate gender bias in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08230" target="_blank">A Duty to Forget, a Right to Be Assured? Exposing Vulnerabilities in Machine Unlearning Services</a></div>
<div class="paper-author">Hongsheng Hu, Shuo Wang, Jiamin Chang, Haonan Zhong, Ruoxi Sun, Shuang Hao, Haojin Zhu, Minhui Xue</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the right to be forgotten requires the removal or "unlearning" of a user's data from machine learning models. however, in the context of machine learning as a service (mlaas), retraining a model from scratch to fulfill the unlearning request is impractical due to the lack of training data on the service provider's side (the server). furthermore, approximate unlearning further embraces a complex trade-off between utility (model performance) and privacy (unlearning performance). in this paper, we try to explore the potential threats posed by unlearning services in mlaas, specifically over-unlearning, where more information is unlearned than expected. we propose two strategies that leverage over-unlearning to measure the impact on the trade-off balancing, under black-box access settings, in which the existing machine unlearning attacks are not applicable. the effectiveness of these strategies is evaluated through extensive experiments on benchmark datasets, across various model architectures and representative unlearning approaches. results indicate significant potential for both strategies to undermine model efficacy in unlearning scenarios. this study uncovers an underexplored gap between unlearning and contemporary mlaas, highlighting the need for careful considerations in balancing data unlearning, model utility, and security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08573" target="_blank">Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias Between India and the West</a></div>
<div class="paper-author">Khyati Khandelwal, Manuel Tonneau, Andrew M. Bean, Hannah Rose Kirk, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), now used daily by millions of users, can encode societal biases, exposing their users to representational harms. a large body of scholarship on llm bias exists but it predominantly adopts a western-centric frame and attends comparatively less to bias levels and potential harms in the global south. in this paper, we quantify stereotypical bias in popular llms according to an indian-centric frame and compare bias levels between the indian and western contexts. to do this, we develop a novel dataset which we call indian-bhed (indian bias evaluation dataset), containing stereotypical and anti-stereotypical examples for caste and religion contexts. we find that the majority of llms tested are strongly biased towards stereotypes in the indian context, especially as compared to the western context. we finally investigate instruction prompting as a simple intervention to mitigate such bias and find that it significantly reduces both stereotypical and anti-stereotypical biases in the majority of cases for gpt-3.5. the findings of this work highlight the need for including more diverse voices when evaluating llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08674" target="_blank">Fake News Detectors Are Biased Against Texts Generated by Large Language Models</a></div>
<div class="paper-author">Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, Preslav Nakov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of fake news has emerged as a critical challenge, undermining trust and posing threats to society. in the era of large language models (llms), the capability to generate believable fake content has intensified these concerns. in this study, we present a novel paradigm to evaluate fake news detectors in scenarios involving both human-written and llm-generated misinformation. intriguingly, our findings reveal a significant bias in many existing detectors: they are more prone to flagging llm-generated content as fake news while often misclassifying human-written fake news as genuine. this unexpected bias appears to arise from distinct linguistic patterns inherent to llm outputs. to address this, we introduce a mitigation strategy that leverages adversarial training with llm-paraphrased genuine news. the resulting model yielded marked improvements in detection accuracy for both human and llm-generated news. to further catalyze research in this domain, we release two comprehensive datasets, \texttt{gossipcop++} and \texttt{politifact++}, thus amalgamating human-validated articles with llm-generated fake and real news.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08836" target="_blank">Bias and Fairness in Chatbots: An Overview</a></div>
<div class="paper-author">Jintang Xue, Yun-Cheng Wang, Chengwei Wei, Xiaofeng Liu, Jonghye Woo, C. -C. Jay Kuo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatbots have been studied for more than half a century. with the rapid development of natural language processing (nlp) technologies in recent years, chatbots using large language models (llms) have received much attention nowadays. compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. there are however, bias and fairness concerns in modern chatbot design. due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. the history of chatbots and their categories are first reviewed. then, bias sources and potential harms in applications are analyzed. considerations in designing fair and unbiased chatbot systems are examined. finally, future research directions are discussed.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07447" target="_blank">Commercial Anti-Smishing Tools and Their Comparative Effectiveness Against Modern Threats</a></div>
<div class="paper-author">Daniel Timko, Muhammad Lutfor Rahman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: smishing, also known as sms phishing, is a type of fraudulent communication in which an attacker disguises sms communications to deceive a target into providing their sensitive data. smishing attacks use a variety of tactics; however, they have a similar goal of stealing money or personally identifying information (pii) from a victim. in response to these attacks, a wide variety of anti-smishing tools have been developed to block or filter these communications. despite this, the number of phishing attacks continue to rise. in this paper, we developed a test bed for measuring the effectiveness of popular anti-smishing tools against fresh smishing attacks. to collect fresh smishing data, we introduce smishtank.com, a collaborative online resource for reporting and collecting smishing data sets. the sms messages were validated by a security expert and an in-depth qualitative analysis was performed on the collected messages to provide further insights. to compare tool effectiveness, we experimented with 20 smishing and benign messages across 3 key segments of the sms messaging delivery ecosystem. our results revealed significant room for improvement in all 3 areas against our smishing set. most anti-phishing apps and bulk messaging services didn't filter smishing messages beyond the carrier blocking. the 2 apps that blocked the most smish also blocked 85-100\% of benign messages. finally, while carriers did not block any benign messages, they were only able to reach a 25-35\% blocking rate for smishing messages. our work provides insights into the performance of anti-smishing tools and the roles they play in the message blocking process. this paper would enable the research community and industry to be better informed on the current state of anti-smishing technology on the sms platform.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07601" target="_blank">Detecting Misinformation With LLM-Predicted Credibility Signals and Weak Supervision</a></div>
<div class="paper-author">João A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, Carolina Scarton</div>
<div class="abstract">
<div class="abstract-content">
Abstract: credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. this paper investigates whether large language models (llms) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. we then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. we demonstrate that our approach, which combines zero-shot llm credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. we also analyse the contribution of the individual credibility signals towards predicting content veracity, which provides new valuable insights into their role in misinformation detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07875" target="_blank">Safety-Tuned Llamas: Lessons From Improving the Safety of Large Language Models That Follow Instructions</a></div>
<div class="paper-author">Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: training large language models to follow instructions makes them perform better on a wide range of tasks, generally becoming more helpful. however, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. in this paper, we raise concerns over the safety of models that only emphasize helpfulness, not safety, in their instruction-tuning. we show that several popular instruction-tuned models are highly unsafe. moreover, we show that adding just 3% safety examples (a few hundred demonstrations) in the training set when fine-tuning a model like llama can substantially improve their safety. our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. however, we do find a behavior of exaggerated safety, where too much safety-tuning makes models refuse to respond to reasonable prompts that superficially resemble unsafe ones. our study sheds light on trade-offs in training llms to follow instructions and exhibit safe behavior.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08047" target="_blank">Investigating Gender Bias in News Summarization</a></div>
<div class="paper-author">Julius Steen, Katja Markert</div>
<div class="abstract">
<div class="abstract-content">
Abstract: summarization is an important application of large language models (llms). most previous evaluation of summarization models has focused on their performance in content selection, grammaticality and coherence. however, it is well known that llms reproduce and reinforce harmful social biases. this raises the question: do these biases affect model outputs in a relatively constrained setting like summarization?   to help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical measures to quantify them. since we find biases inherent to the input document can confound our analysis, we additionally propose a method to generate input documents with carefully controlled demographic attributes. this allows us to sidestep this issue, while still working with somewhat realistic input documents.   finally, we apply our measures to summaries generated by both purpose-built summarization models and general purpose chat models. we find that content selection in single document summarization seems to be largely unaffected by bias, while hallucinations exhibit evidence of biases propagating to generated summaries.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08121" target="_blank">"I'm Not Confident in Debiasing Ai Systems Since I Know Too Little": Teaching Ai Creators About Gender Bias Through Hands-on Tutorials</a></div>
<div class="paper-author">Kyrie Zhixuan Zhou, Jiaxun Cao, Xiaowen Yuan, Daniel E. Weissglass, Zachary Kilhoffer, Madelyn Rose Sanfilippo, Xin Tong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias is rampant in ai systems, causing bad user experience, injustices, and mental harm to women. school curricula fail to educate ai creators on this topic, leaving them unprepared to mitigate gender bias in ai. in this paper, we designed hands-on tutorials to raise ai creators' awareness of gender bias in ai and enhance their knowledge of sources of gender bias and debiasing techniques. the tutorials were evaluated with 18 ai creators, including ai researchers, ai industrial practitioners (i.e., developers and product managers), and students who had learned ai. their improved awareness and knowledge demonstrated the effectiveness of our tutorials, which have the potential to complement the insufficient ai gender bias education in cs/ai courses. based on the findings, we synthesize design implications and a rubric to guide future research, education, and design efforts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.06717" target="_blank">Bias Amplification Enhances Minority Group Performance</a></div>
<div class="paper-author">Gaotang Li, Jiarui Liu, Wei Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. previous approaches based on worst-group loss minimization (e.g. group-dro) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. in this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. we propose bam, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. empirically, bam achieves competitive performance compared with existing methods evaluated on spurious correlation benchmarks in computer vision and natural language processing. moreover, we find a simple stopping criterion based on minimum class accuracy difference that can remove the need for group annotations, with little or no loss in worst-group accuracy. we perform extensive analyses and ablations to verify the effectiveness and robustness of our algorithm in varying class and group imbalance ratios.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.06794" target="_blank">Cognitive Mirage: A Review of Hallucinations in Large Language Models</a></div>
<div class="paper-author">Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models continue to develop in the field of ai, text generation systems are susceptible to a worrisome phenomenon known as hallucination. in this study, we summarize recent compelling insights into hallucinations in llms. we present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. based on this, future research directions are proposed. our contribution are threefold: (1) we provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) we provide theoretical analyses of hallucinations in llms and provide existing detection and improvement methods; (3) we propose several research directions that can be developed in the future. as hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07045" target="_blank">Safetybench: Evaluating the Safety of Large Language Models With Multiple Choice Questions</a></div>
<div class="paper-author">Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid development of large language models (llms), increasing attention has been paid to their safety concerns. consequently, evaluating the safety of llms has become an essential task for facilitating the broad applications of llms. nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of llms. in this work, we present safetybench, a comprehensive benchmark for evaluating the safety of llms, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. notably, safetybench also incorporates both chinese and english data, facilitating the evaluation in both languages. our extensive tests over 25 popular chinese and english llms in both zero-shot and few-shot settings reveal a substantial performance advantage for gpt-4 over its counterparts, and there is still significant room for improving the safety of current llms. we believe safetybench will enable fast and comprehensive evaluation of llms' safety, and foster the development of safer llms. data and evaluation guidelines are available at https://github.com/thu-coai/safetybench. submission entrance and leaderboard are available at https://llmbench.ai/safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07120" target="_blank">Sight Beyond Text: Multi-Modal Training Enhances LLMS in Truthfulness and Ethics</a></div>
<div class="paper-author">Haoqin Tu, Bingchen Zhao, Chen Wei, Cihang Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: multi-modal large language models (mllms) are trained based on large language models (llm), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. while they excel in multi-modal tasks, the pure nlp abilities of mllms are often underestimated and left untested. in this study, we get out of the box and unveil an intriguing characteristic of mllms -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning llms into mllms, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure nlp context. for example, a visual-instruction-tuned llama2 7b model surpasses the performance of the llama2-chat 7b model, fine-tuned with over one million human annotations, on truthfulqa-mc and ethics benchmarks. further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. in releasing our code at github.com/ucsc-vlaa/sight-beyond-text, we aspire to foster further exploration into the intrinsic value of visual-text synergies and, in a broader scope, multi-modal interactions in alignment research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07124" target="_blank">Rain: Your Language Models Can Align Themselves Without Finetuning</a></div>
<div class="paper-author">Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) often demonstrate inconsistencies with human preferences. previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. in contrast, aligning frozen llms without requiring alignment data is more appealing. this work explores the potential of the latter setting. we discover that by integrating self-evaluation and rewind mechanisms, unaligned llms can directly produce responses consistent with human preferences via self-boosting. we introduce a novel inference method, rewindable auto-regressive inference (rain), that allows pre-trained llms to evaluate their own generation and use the evaluation results to guide rewind and generation for ai safety. notably, rain operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. experimental results evaluated by gpt-4 and humans demonstrate the effectiveness of rain: on the hh dataset, rain improves the harmlessness rate of llama 30b from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. on the truthfulqa dataset, rain improves the truthfulness of the already-well-aligned llama-2-chat 13b model by 5%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07251" target="_blank">In-Contextual Bias Suppression for Large Language Models</a></div>
<div class="paper-author">Daisuke Oba, Masahiro Kaneko, Danushka Bollegala</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite their impressive performance in a wide range of nlp tasks, large language models (llms) have been reported to encode worrying-levels of gender bias. prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of the llms, which are computationally costly. moreover, one might not even have access to the internal parameters for performing debiasing such as in the case of commercially available llms such as gpt-4. to address this challenge we propose bias suppression, a novel alternative to debiasing that does not require access to model parameters. we show that text-based preambles, generated from manually designed templates covering counterfactual statements, can accurately suppress gender biases in llms. moreover, we find that descriptive sentences for occupations can further suppress gender biases. interestingly, we find that bias suppression has a minimal adverse effect on downstream task performance, while effectively mitigating the gender biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05958" target="_blank">The Moral Machine Experiment on Large Language Models</a></div>
<div class="paper-author">Kazuhiro Takemoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) become more deeply integrated into various sectors, understanding how they make moral judgments has become crucial, particularly in the realm of autonomous driving. this study utilized the moral machine framework to investigate the ethical decision-making tendencies of prominent llms, including gpt-3.5, gpt-4, palm 2, and llama 2, comparing their responses to human preferences. while llms' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, palm 2 and llama 2, especially, evidence distinct deviations. additionally, despite the qualitative similarities between the llm and human preferences, there are significant quantitative disparities, suggesting that llms might lean toward more uncompromising decisions, compared to the milder inclinations of humans. these insights elucidate the ethical frameworks of llms and their potential implications for autonomous driving.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05973" target="_blank">Circuit Breaking: Removing Model Behaviors With Targeted Ablation</a></div>
<div class="paper-author">Maximilian Li, Xander Davies, Max Nadeau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. we propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. in the setting of reducing gpt-2 toxic language generation, we find ablating just 12 of the 11.6k causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05981" target="_blank">Learning Unbiased News Article Representations: A Knowledge-Infused Approach</a></div>
<div class="paper-author">Sadia Kamal, Jimmy Hartford, Jeremy Willis, Arunkumar Bagavathi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: quantification of the political leaning of online news articles can aid in understanding the dynamics of political ideology in social groups and measures to mitigating them. however, predicting the accurate political leaning of a news article with machine learning models is a challenging task. this is due to (i) the political ideology of a news article is defined by several factors, and (ii) the innate nature of existing learning models to be biased with the political bias of the news publisher during the model training. there is only a limited number of methods to study the political leaning of news articles which also do not consider the algorithmic political bias which lowers the generalization of machine learning models to predict the political leaning of news articles published by any new news publishers. in this work, we propose a knowledge-infused deep learning model that utilizes relatively reliable external data resources to learn unbiased representations of news articles using their global and local contexts. we evaluate the proposed model by setting the data in such a way that news domains or news publishers in the test set are completely unseen during the training phase. with this setup we show that the proposed model mitigates algorithmic political bias and outperforms baseline methods to predict the political leaning of news articles with up to 73% accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.06055" target="_blank">Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review</a></div>
<div class="paper-author">Pengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Gongshen Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep neural networks (dnns) have led to unprecedented progress in various natural language processing (nlp) tasks. owing to limited data and computation resources, using third-party data and models has become a new paradigm for adapting various tasks. however, research shows that it has some potential security vulnerabilities because attackers can manipulate the training process and data source. such a way can set specific triggers, making the model exhibit expected behaviors that have little inferior influence on the model's performance for primitive tasks, called backdoor attacks. hence, it could have dire consequences, especially considering that the backdoor attack surfaces are broad.   to get a precise grasp and understanding of this problem, a systematic and comprehensive review is required to confront various security challenges from different phases and attack purposes. additionally, there is a dearth of analysis and comparison of the various emerging backdoor countermeasures in this situation. in this paper, we conduct a timely review of backdoor attacks and countermeasures to sound the red alarm for the nlp security community. according to the affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into three categorizations: attacking pre-trained model with fine-tuning (apmf) or prompt-tuning (apmp), and attacking final model with training (afmt), where afmt can be subdivided into different attack aims. thus, attacks under each categorization are combed. the countermeasures are categorized into two general classes: sample inspection and model inspection. overall, the research on the defense side is far behind the attack side, and there is no single defense that can prevent all types of backdoor attacks. an attacker can intelligently bypass existing defenses with a more invisible attack. ......
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08628" target="_blank">Recovering From Privacy-Preserving Masking With Large Language Models</a></div>
<div class="paper-author">Arpita Vats, Zhe Liu, Peng Su, Debjyoti Paul, Yingyi Ma, Yutong Pang, Zeeshan Ahmed, Ozlem Kalinli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. to effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (nlp) models can be directly trained using such in-domain data. however, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. replacing identifying information in textual data with a generic marker has been recently explored. in this work, we leverage large language models (llms) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. specifically, we propose multiple pre-trained and fine-tuned llm-based approaches and perform empirical studies on various datasets for the comparison of these methods. experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15827" target="_blank">How We Define Harm Impacts Data Annotations: Explaining How Annotators Distinguish Hateful, Offensive, and Toxic Comments</a></div>
<div class="paper-author">Angela Schöpke-Gonzalez, Siqi Wu, Sagar Kumar, Paul J. Resnick, Libby Hemphill</div>
<div class="abstract">
<div class="abstract-content">
Abstract: computational social science research has made advances in machine learning and natural language processing that support content moderators in detecting harmful content. these advances often rely on training datasets annotated by crowdworkers for harmful content. in designing instructions for annotation tasks to generate training data for these algorithms, researchers often treat the harm concepts that we train algorithms to detect - 'hateful', 'offensive', 'toxic', 'racist', 'sexist', etc. - as interchangeable. in this work, we studied whether the way that researchers define 'harm' affects annotation outcomes. using venn diagrams, information gain comparisons, and content analyses, we reveal that annotators do not use the concepts 'hateful', 'offensive', and 'toxic' interchangeably. we identify that features of harm definitions and annotators' individual characteristics explain much of how annotators use these terms differently. our results offer empirical evidence discouraging the common practice of using harm concepts interchangeably in content moderation research. instead, researchers should make specific choices about which harm concepts to analyze based on their research goals. recognizing that researchers are often resource constrained, we also encourage researchers to provide information to bound their findings when their concepts of interest differ from concepts that off-the-shelf harmful content detection algorithms identify. finally, we encourage algorithm providers to ensure their instruments can adapt to contextually-specific content detection goals (e.g., soliciting instrument users' feedback).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05227" target="_blank">Detecting Natural Language Biases With Prompt-Based Learning</a></div>
<div class="paper-author">Md Abdul Aowal, Maliha T Islam, Priyanka Mary Mammen, Sandesh Shetty</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this project, we want to explore the newly emerging field of prompt engineering and apply it to the downstream task of detecting lm biases. more concretely, we explore how to design prompts that can indicate 4 different types of biases: (1) gender, (2) race, (3) sexual orientation, and (4) religion-based. within our project, we experiment with different manually crafted prompts that can draw out the subtle biases that may be present in the language model. we apply these prompts to multiple variations of popular and well-recognized models: bert, roberta, and t5 to evaluate their biases. we provide a comparative analysis of these models and assess them using a two-fold method: use human judgment to decide whether model predictions are biased and utilize model-level judgment (through further prompts) to understand if a model can self-diagnose the biases of its own prediction.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05274" target="_blank">Fuzzllm: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models</a></div>
<div class="paper-author">Dongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: jailbreak vulnerabilities in large language models (llms), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. while model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. to tackle this issue, we introduce fuzzllm, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in llms. we utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. by integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, fuzzllm enables efficient testing with reduced manual effort. extensive experiments demonstrate fuzzllm's effectiveness and comprehensiveness in vulnerability discovery across various llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05610" target="_blank">Privacy Side Channels in Machine Learning Systems</a></div>
<div class="paper-author">Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, Florian Tramèr</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most current approaches for protecting privacy in machine learning (ml) assume that models exist in a vacuum, when in reality, ml models are part of larger systems that include components for training data filtering, output monitoring, and more. in this work, we introduce privacy side channels: attacks that exploit these system-level components to extract private information at far higher rates than is otherwise possible for standalone models. we propose four categories of side channels that span the entire ml lifecycle (training data filtering, input preprocessing, output post-processing, and query filtering) and allow for either enhanced membership inference attacks or even novel threats such as extracting users' test queries. for example, we show that deduplicating training data before applying differentially-private training creates a side-channel that completely invalidates any provable privacy guarantees. moreover, we show that systems which block language models from regenerating training data can be exploited to allow exact reconstruction of private keys contained in the training set -- even if the model did not memorize these keys. taken together, our results demonstrate the need for a holistic, end-to-end privacy analysis of machine learning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05922" target="_blank">A Survey of Hallucination in Large Foundation Models</a></div>
<div class="paper-author">Vipula Rawte, Amit Sheth, Amitava Das</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hallucination in a foundation model (fm) refers to the generation of content that strays from factual reality or includes fabricated information. this survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``large'' foundation models (lfms). the paper classifies various types of hallucination phenomena that are specific to lfms and establishes evaluation criteria for assessing the extent of hallucination. it also examines existing strategies for mitigating hallucination in lfms and discusses potential directions for future research in this area. essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in lfms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08624" target="_blank">Challenges in Annotating Datasets to Quantify Bias in Under-Represented Society</a></div>
<div class="paper-author">Vithya Yogarajan, Gillian Dobbie, Timothy Pistotti, Joshua Bensemann, Kobe Knowles</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in artificial intelligence, including the development of highly sophisticated large language models (llm), have proven beneficial in many real-world applications. however, evidence of inherent bias encoded in these llms has raised concerns about equity. in response, there has been an increase in research dealing with bias, including studies focusing on quantifying bias and developing debiasing techniques. benchmark bias datasets have also been developed for binary gender classification and ethical/racial considerations, focusing predominantly on american demographics. however, there is minimal research in understanding and quantifying bias related to under-represented societies. motivated by the lack of annotated datasets for quantifying bias in under-represented societies, we endeavoured to create benchmark datasets for the new zealand (nz) population. we faced many challenges in this process, despite the availability of three annotators. this research outlines the manual annotation process, provides an overview of the challenges we encountered and lessons learnt, and presents recommendations for future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.04992" target="_blank">Mitigating Word Bias in Zero-Shot Prompt-Based Classifiers</a></div>
<div class="paper-author">Adian Liusie, Potsawee Manakul, Mark J. F. Gales</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-based classifiers are an attractive approach for zero-shot classification. however, the precise choice of the prompt template and label words can largely influence performance, with semantically equivalent settings often showing notable performance difference. this discrepancy can be partly attributed to word biases, where the classifier may be biased towards classes. to address this problem, it is possible to optimise classification thresholds on a labelled data set, however, this mitigates some of the advantages of prompt-based classifiers. this paper instead approaches this problem by examining the expected marginal probabilities of the classes. here, probabilities are reweighted to have a uniform prior over classes, in an unsupervised fashion. further, we draw a theoretical connection between the class priors and the language models' word prior, and offer the ability to set a threshold in a zero-resource fashion. we show that matching class priors correlates strongly with the oracle upper bound performance and demonstrate large consistent performance gains for prompt settings over a range of nlp tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05217" target="_blank">Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis</a></div>
<div class="paper-author">Li Du, Yequan Wang, Xingrun Xing, Yiqun Ya, Xiang Li, Xin Jiang, Xuezhi Fang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although demonstrating superb performance on various nlp tasks, large language models (llms) still suffer from the hallucination problem, which threatens the reliability of llms. to measure the level of hallucination of llms, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. however, such hallucination rates could easily be distorted by confounders. moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. to address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of llms with a set of risk factors. in this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of llms to mitigate the hallucination.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.04858" target="_blank">Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System</a></div>
<div class="paper-author">Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: neural language models are increasingly deployed into apis and websites that allow a user to pass in a prompt and receive generated text. many of these systems do not reveal generation parameters. in this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-$k$ or nucleus sampling). our ability to discover which decoding strategy was used has implications for detecting generated text. additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model's predicted distributions. we perform our attack on several families of open-source language models, as well as on production systems (e.g., chatgpt).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03564" target="_blank">Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media</a></div>
<div class="paper-author">Hongzhi Qi, Qing Zhao, Changwei Song, Wei Zhai, Dan Luo, Shuo Liu, Yi Jing Yu, Fan Wang, Huijing Zou, Bing Xiang Yang, Jianqiang Li, Guanghui Fu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models, particularly those akin to the rapidly progressing gpt series, are gaining traction for their expansive influence. while there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on chinese social media platforms. using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tuning. our findings revealed a discernible performance gap between the large language models and traditional supervised learning approaches, primarily attributed to the models' inability to fully grasp subtle categories. notably, while gpt-4 outperforms its counterparts in multiple scenarios, gpt-3.5 shows significant enhancement in suicide risk classification after fine-tuning. to our knowledge, this investigation stands as the maiden attempt at gauging large language models on chinese social media tasks. this study underscores the forward-looking and transformative implications of using large language models in the field of psychology. it lays the groundwork for future applications in psychological research and practice.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03876" target="_blank">Opiniongpt: Modelling Explicit Biases in Instruction-Tuned LLMS</a></div>
<div class="paper-author">Patrick Haller, Ansar Aynetdinov, Alan Akbik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned large language models (llms) have recently showcased remarkable ability to generate fitting responses to natural language instructions. however, an open research question concerns the inherent biases of trained models and their responses. for instance, if the data used to tune an llm is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. current research work seeks to de-bias such models, or suppress potentially biased answers. with this demonstration, we take a different view on biases in instruction-tuning: rather than aiming to suppress them, we aim to make them explicit and transparent. to this end, we present opiniongpt, a web demo in which users can ask questions and select all biases they wish to investigate. the demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. to train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. this paper presents opiniongpt, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03883" target="_blank">Dola: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a></div>
<div class="paper-author">Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite their impressive capabilities, large language models (llms) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. we propose a simple decoding strategy for reducing hallucinations with pretrained llms that does not require conditioning on retrieved external knowledge nor additional fine-tuning. our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an llms has generally been shown to be localized to particular transformer layers. we find that this decoding by contrasting layers (dola) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. dola consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of llama family models on truthfulqa by 12-17% absolute points, demonstrating its potential in making llms reliably generate truthful facts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.04027" target="_blank">Tide: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models</a></div>
<div class="paper-author">Emmanuel Klu, Sameer Sethi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. when these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. in this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. we create a new, more comprehensive identity lexicon, tidal, which includes 15,123 identity terms and associated sense context across three demographic categories. we leverage tidal to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ml fairness techniques. we evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. results show our assistive annotation technique improves the reliability and velocity of human-in-the-loop processes. our dataset and methods uncover more disparities during evaluation, and also produce more fair models during remediation. these approaches provide a practical path forward for scaling classifier and generative model fairness in real-world settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.06415" target="_blank">Down the Toxicity Rabbit Hole: Investigating Palm 2 Guardrails</a></div>
<div class="paper-author">Adel Khorramrouz, Sujan Dutta, Arka Dutta, Ashiqur R. Khudabukhsh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper conducts a robustness audit of the safety feedback of palm 2 through a novel toxicity rabbit hole framework introduced here. starting with a stereotype, the framework instructs palm 2 to generate more toxic content than the stereotype. every subsequent iteration it continues instructing palm 2 to generate more toxic content than the previous iteration until palm 2 safety guardrails throw a safety violation. our experiments uncover highly disturbing antisemitic, islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that palm 2 safety guardrails do not evaluate as highly unsafe.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02705" target="_blank">Certifying LLM Safety Against Adversarial Prompting</a></div>
<div class="paper-author">Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, Hima Lakkaraju</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." an aligned language model should decline a user's request to produce harmful content. however, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. in this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. we erase tokens individually and inspect the resulting subsequences using a safety filter. our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. this guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. we defend against three attack modes: i) adversarial suffix, which appends an adversarial sequence at the end of the prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. empirical results demonstrate that our technique obtains strong certified safety guarantees on harmful prompts while maintaining good performance on safe prompts. for example, against adversarial suffixes of length 20, it certifiably detects 93% of the harmful prompts and labels 94% of the safe prompts as safe using the open source language model llama 2 as the safety filter.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02724" target="_blank">Offensive Hebrew Corpus and Detection Using Bert</a></div>
<div class="paper-author">Nagham Hamad, Mustafa Jarrar, Mohammad Khalilia, Nadim Nashif</div>
<div class="abstract">
<div class="abstract-content">
Abstract: offensive language detection has been well studied in many languages, but it is lagging behind in low-resource languages, such as hebrew. in this paper, we present a new offensive language corpus in hebrew. a total of 15,881 tweets were retrieved from twitter. each was labeled with one or more of five classes (abusive, hate, violence, pornographic, or none offensive) by arabic-hebrew bilingual speakers. the annotation process was challenging as each annotator is expected to be familiar with the israeli culture, politics, and practices to understand the context of each tweet. we fine-tuned two hebrew bert models, hebert and alephbert, using our proposed dataset and another published dataset. we observed that our data boosts hebert performance by 2% when combined with d_olah. fine-tuning alephbert on our data and testing on d_olah yields 69% accuracy, while fine-tuning on d_olah and testing on our data yields 57% accuracy, which may be an indication to the generalizability our data offers. our dataset and fine-tuned models are available on github and huggingface.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02884" target="_blank">Aligning Large Language Models for Clinical Tasks</a></div>
<div class="paper-author">Supun Manathunga, Isuru Hettigoda</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable adaptability, showcasing their capacity to excel in tasks for which they were not explicitly trained. however, despite their impressive natural language processing (nlp) capabilities, effective alignment of llms remains a crucial challenge when deploying them for specific clinical applications. the ability to generate responses with factually accurate content and to engage in non-trivial reasoning steps are crucial for the llms to be eligible for applications in clinical medicine. employing a combination of techniques including instruction-tuning and in-prompt strategies like few-shot and chain-of-thought prompting has significantly enhanced the performance of llms. our proposed alignment strategy for medical question-answering, known as 'expand-guess-refine', offers a parameter and data-efficient solution. a preliminary analysis of this method demonstrated outstanding performance, achieving a score of 70.63% on a subset of questions sourced from the usmle dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02912" target="_blank">On the Challenges of Building Datasets for Hate Speech Detection</a></div>
<div class="paper-author">Vitthal Bhandari</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detection of hate speech has been formulated as a standalone application of nlp and different approaches have been adopted for identifying the target groups, obtaining raw data, defining the labeling process, choosing the detection algorithm, and evaluating the performance in the desired setting. however, unlike other downstream tasks, hate speech suffers from the lack of large-sized, carefully curated, generalizable datasets owing to the highly subjective nature of the task. in this paper, we first analyze the issues surrounding hate speech detection through a data-centric lens. we then outline a holistic framework to encapsulate the data creation pipeline across seven broad dimensions by taking the specific example of hate speech towards sexual minorities. we posit that practitioners would benefit from following this framework as a form of best practice when creating hate speech datasets in the future.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02926" target="_blank">Demystifying Rce Vulnerabilities in LLM-Integrated Apps</a></div>
<div class="paper-author">Tong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, Kai Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have demonstrated remarkable potential across various downstream tasks. llm-integrated frameworks, which serve as the essential infrastructure, have given rise to many llm-integrated web apps. however, some of these frameworks suffer from remote code execution (rce) vulnerabilities, allowing attackers to execute arbitrary code on apps' servers remotely via prompt injections. despite the severity of these vulnerabilities, no existing work has been conducted for a systematic investigation of them. this leaves a great challenge on how to detect vulnerabilities in frameworks as well as llm-integrated apps in real-world scenarios. to fill this gap, we present two novel strategies, including 1) a static analysis-based tool called llmsmith to scan the source code of the framework to detect potential rce vulnerabilities and 2) a prompt-based automated testing approach to verify the vulnerability in llm-integrated web apps. we discovered 13 vulnerabilities in 6 frameworks, including 12 rce vulnerabilities and 1 arbitrary file read/write vulnerability. 11 of them are confirmed by the framework developers, resulting in the assignment of 7 cve ids. after testing 51 apps, we found vulnerabilities in 17 apps, 16 of which are vulnerable to rce and 1 to sql injection. we responsibly reported all 17 issues to the corresponding developers and received acknowledgments. furthermore, we amplify the attack impact beyond achieving rce by allowing attackers to exploit other app users (e.g. app responses hijacking, user api key leakage) without direct interaction between the attacker and the victim. lastly, we propose some mitigating strategies for improving the security awareness of both framework and app developers, helping them to mitigate these risks effectively.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03057" target="_blank">Hide and Seek (Has): A Lightweight Framework for Prompt Privacy Protection</a></div>
<div class="paper-author">Yu Chen, Tingxin Li, Huiming Liu, Yang Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous companies have started offering services based on large language models (llm), such as chatgpt, which inevitably raises privacy concerns as users' prompts are exposed to the model provider. previous research on secure reasoning using multi-party computation (mpc) has proven to be impractical for llm applications due to its time-consuming and communication-intensive nature. while lightweight anonymization techniques can protect private information in prompts through substitution or masking, they fail to recover sensitive data replaced in the llm-generated results. in this paper, we expand the application scenarios of anonymization techniques by training a small local model to de-anonymize the llm's returned results with minimal computational overhead. we introduce the has framework, where "h(ide)" and "s(eek)" represent its two core processes: hiding private entities for anonymization and seeking private entities for de-anonymization, respectively. to quantitatively assess has's privacy protection performance, we propose both black-box and white-box adversarial models. furthermore, we conduct experiments to evaluate has's usability in translation and classification tasks. the experimental findings demonstrate that the has framework achieves an optimal balance between privacy protection and utility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03126" target="_blank">Everyone Deserves a Reward: Learning Customized Human Preferences</a></div>
<div class="paper-author">Pengyu Cheng, Jiawen Xie, Ke Bai, Yong Dai, Nan Du</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward models (rms) are essential for aligning large language models (llms) with human preferences to improve interaction quality. however, the real world is pluralistic, which leads to diversified human preferences with respect to different religions, politics, cultures, etc. moreover, each individual can have their unique preferences on various topics. neglecting the diversity of human preferences, current human feedback aligning methods only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. to explore customized preference learning, we collect a domain-specific preference (dsp) dataset, which includes preferred responses for each given query from four practical domains. besides, from the perspective of data efficiency, we propose a three-stage customized rm learning scheme, then empirically verify its effectiveness on both general preference datasets and our dsp set. furthermore, we test multiple training and data strategies on the three learning stages. we find several ways to better preserve the general preferring ability while training the customized rms, especially general preference enrichment, and customized preference imitation learning. the dsp dataset and code are available at https://github.com/linear95/dsp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03164" target="_blank">J-Guard: Journalism Guided Adversarially Robust Detection of Ai-Generated News</a></div>
<div class="paper-author">Tharindu Kumarage, Amrita Bhattacharjee, Djordje Padejski, Kristy Roschke, Dan Gillmor, Scott Ruston, Huan Liu, Joshua Garland</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid proliferation of ai-generated text online is profoundly reshaping the information landscape. among various types of ai-generated text, ai-generated news presents a significant threat as it can be a prominent source of misinformation online. while several recent efforts have focused on detecting ai-generated text in general, these methods require enhanced reliability, given concerns about their vulnerability to simple adversarial attacks. furthermore, due to the eccentricities of news writing, applying these detection methods for ai-generated news can produce false positives, potentially damaging the reputation of news organizations. to address these challenges, we leverage the expertise of an interdisciplinary team to develop a framework, j-guard, capable of steering existing supervised ai text detectors for detecting ai-generated news while boosting adversarial robustness. by incorporating stylistic cues inspired by the unique journalistic attributes, j-guard effectively distinguishes between real-world journalism and ai-generated news articles. our experiments on news articles generated by a vast array of ai models, including chatgpt (gpt3.5), demonstrate the effectiveness of j-guard in enhancing detection capabilities while maintaining an average performance decrease of as low as 7% when faced with adversarial attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02144" target="_blank">Making Large Language Models Better Reasoners With Alignment</a></div>
<div class="paper-author">Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reasoning is a cognitive process of using evidence to reach a sound conclusion. the reasoning capability is essential for large language models (llms) to serve as the brain of the artificial general intelligence agent. recent studies reveal that fine-tuning llms on data with the chain of thought (cot) reasoning process can significantly enhance their reasoning capabilities. however, we find that the fine-tuned llms suffer from an \textit{assessment misalignment} problem, i.e., they frequently assign higher scores to subpar cots, leading to potential limitations in their reasoning abilities. to address this problem, we introduce an \textit{alignment fine-tuning (aft)} paradigm, which involves three steps: 1) fine-tuning llms with cot training data; 2) generating multiple cot responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by llms with a novel constraint alignment loss. specifically, the constraint alignment loss has two objectives: a) alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality cots; b) constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. furthermore, we also delve deeply into recent ranking-based alignment methods, such as dpo, rrhf, and pro, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of aft.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02311" target="_blank">Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization</a></div>
<div class="paper-author">Helena Bonaldi, Giuseppe Attanasio, Debora Nozza, Marco Guerini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting pretrained transformer-based language models (plms) with human-curated data. this process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. this paper introduces novel attention regularization methodologies to improve the generalization capabilities of plms for counter narratives generation. overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. we experiment with two attention-based regularization techniques on a benchmark english dataset. regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. this work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02524" target="_blank">Do You Trust Chatgpt? -- Perceived Credibility of Human and Ai-Generated Content</a></div>
<div class="paper-author">Martin Huschens, Martin Briesch, Dominik Sobania, Franz Rothlauf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper examines how individuals perceive the credibility of content originating from human authors versus content generated by large language models, like the gpt language model family that powers chatgpt, in different user interface versions. surprisingly, our results demonstrate that regardless of the user interface presentation, participants tend to attribute similar levels of credibility. while participants also do not report any different perceptions of competence and trustworthiness between human and ai-generated content, they rate ai-generated content as being clearer and more engaging. the findings from this study serve as a call for a more discerning approach to evaluating information sources, encouraging users to exercise caution and critical thinking when engaging with content generated by ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02654" target="_blank">Zero-Resource Hallucination Prevention for Large Language Models</a></div>
<div class="paper-author">Junyu Luo, Cao Xiao, Fenglong Ma</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prevalent use of large language models (llms) in various domains has drawn attention to the issue of "hallucination," which refers to instances where llms generate factually inaccurate or ungrounded information. existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (cot) techniques or parameter-based methods that suffer from interpretability issues. additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. in this paper, we introduce a novel pre-detection self-evaluation technique, referred to as self-familiarity, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. this approach emulates the human ability to refrain from responding to unfamiliar topics, thus reducing hallucinations. we validate self-familiarity across four different large language models, demonstrating consistently superior performance compared to existing techniques. our findings propose a significant shift towards preemptive strategies for hallucination mitigation in llm assistants, promising improvements in reliability, applicability, and interpretability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01413" target="_blank">Hateful Messages: A Conversational Data Set of Hate Speech Produced by Adolescents on Discord</a></div>
<div class="paper-author">Jan Fillies, Silvio Peikert, Adrian Paschke</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rise of social media, a rise of hateful content can be observed. even though the understanding and definitions of hate speech varies, platforms, communities, and legislature all acknowledge the problem. therefore, adolescents are a new and active group of social media users. the majority of adolescents experience or witness online hate speech. research in the field of automated hate speech classification has been on the rise and focuses on aspects such as bias, generalizability, and performance. to increase generalizability and performance, it is important to understand biases within the data. this research addresses the bias of youth language within hate speech classification and contributes by providing a modern and anonymized hate speech youth language data set consisting of 88.395 annotated chat messages. the data set consists of publicly available online messages from the chat platform discord. ~6,42% of the messages were classified by a self-developed annotation schema as hate speech. for 35.553 messages, the user profiles provided age annotations setting the average author age to under 20 years old.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01446" target="_blank">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</a></div>
<div class="paper-author">Raz Lapid, Ron Langberg, Moshe Sipper</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an llm's outputs for unintended purposes. in this paper we introduce a novel approach that employs a genetic algorithm (ga) to manipulate llms when model architecture and parameters are inaccessible. the ga attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible ai development by providing a diagnostic tool for evaluating and enhancing alignment of llms with human intent. to our knowledge this is the first automated universal black box jailbreak attack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01586" target="_blank">Automatic Scam-Baiting Using Chatgpt</a></div>
<div class="paper-author">Piyush Bajaj, Matthew Edwards</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automatic scam-baiting is an online fraud countermeasure that involves automated systems responding to online fraudsters in order to waste their time and deplete their resources, diverting attackers away from real potential victims. previous work has demonstrated that text generation systems are capable of engaging with attackers as automatic scam-baiters, but the fluency and coherence of generated text may be a limit to the effectiveness of such systems.   in this paper, we report on the results of a month-long experiment comparing the effectiveness of two chatgpt-based automatic scam-baiters to a control measure. within our results, with engagement from over 250 real email fraudsters, we find that chatgpt-based scam-baiters show a marked increase in scammer response rate and conversation length relative to the control measure, outperforming previous approaches. we discuss the implications of these results and practical considerations for wider deployment of automatic scam-baiting.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01933" target="_blank">Provably Safe Systems: The Only Path to Controllable Agi</a></div>
<div class="paper-author">Max Tegmark, Steve Omohundro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we describe a path to humanity safely thriving with powerful artificial general intelligences (agis) by building them to provably satisfy human-specified requirements. we argue that this will soon be technically feasible using advanced ai for formal verification and mechanistic interpretability. we further argue that it is the only path which guarantees safe controlled agi. we end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01219" target="_blank">Siren's Song in the Ai Ocean: A Survey on Hallucination in Large Language Models</a></div>
<div class="paper-author">Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: llms occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. this phenomenon poses a substantial challenge to the reliability of llms in real-world scenarios. in this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by llms. we present taxonomies of the llm hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating llm hallucination, and discuss potential directions for future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14345" target="_blank">Bias Assessment and Mitigation in LLM-Based Code Generation</a></div>
<div class="paper-author">Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, Heming Cui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: utilizing state-of-the-art large language models (llms), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. as the adoption of llms becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? this issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. this paper presents a novel bias assessment framework that is specifically designed for code generation tasks. based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art llm-based code generation models. our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' functionality are affected by the bias, which means biases not only exist in code generation models but in some cases, directly affect the functionality of the generated code, posing risks of unintended and possibly harmful software behaviors. to mitigate bias from code generation models, we propose three mitigation strategies, which can decrease the biased code ratio to a very low level of 0.4\% to 4.57\%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-09-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00254" target="_blank">Why Do Universal Adversarial Attacks Work on Large Language Models?: Geometry Might Be the Answer</a></div>
<div class="paper-author">Varshini Subhash, Anna Bialas, Weiwei Pan, Finale Doshi-Velez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. however, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. this work presents a novel geometric perspective explaining universal adversarial attacks on large language models. by attacking the 117m parameter gpt-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. this hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. we believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of llms, thus enabling their mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00267" target="_blank">Rlaif: Scaling Reinforcement Learning From Human Feedback With Ai Feedback</a></div>
<div class="paper-author">Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is effective at aligning large language models (llms) to human preferences, but gathering high quality human preference labels is a key bottleneck. we conduct a head-to-head comparison of rlhf vs. rl from ai feedback (rlaif) - a technique where preferences are labeled by an off-the-shelf llm in lieu of humans, and we find that they result in similar improvements. on the task of summarization, human evaluators prefer generations from both rlaif and rlhf over a baseline supervised fine-tuned model in ~70% of cases. furthermore, when asked to rate rlaif vs. rlhf summaries, humans prefer both at equal rates. these results suggest that rlaif can yield human-level performance, offering a potential solution to the scalability limitations of rlhf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00543" target="_blank">Curating Naturally Adversarial Datasets for Trustworthy Ai in Healthcare</a></div>
<div class="paper-author">Sydney Pugh, Ivan Ruchkin, Insup Lee, James Weimer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep learning models have shown promising predictive accuracy for time-series healthcare applications. however, ensuring the robustness of these models is vital for building trustworthy ai systems. existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. however, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy ai. we propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. the method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. based on these labels, our method adversarially orders the input data and uses this ordering to construct a sequence of increasingly adversarial datasets. our evaluation on six medical case studies and three non-medical case studies demonstrates the efficacy and statistical validity of our approach to generating naturally adversarial datasets
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00614" target="_blank">Baseline Defenses for Adversarial Attacks Against Aligned Language Models</a></div>
<div class="paper-author">Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-Yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: what threat models are practically useful in this domain? how do baseline defense techniques perform in this new domain? how does llm security differ from computer vision?   we evaluate several baseline defense strategies against leading adversarial attacks on llms, discussing the various settings in which each is feasible and effective. particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. we discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. we find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for llms. future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the llms domain than it has been in computer vision.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00667" target="_blank">Taken Out of Context: On Measuring Situational Awareness in LLMS</a></div>
<div class="paper-author">Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, Owain Evans</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we aim to better understand the emergence of `situational awareness' in large language models (llms). a model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. today's llms are tested for safety and alignment before they are deployed. an llm could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. situational awareness may emerge unexpectedly as a byproduct of model scaling. one way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. as such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). we study out-of-context reasoning experimentally. first, we finetune an llm on a description of a test while providing no examples or demonstrations. at test time, we assess whether the model can pass the test. to our surprise, we find that llms succeed on this out-of-context reasoning task. their success is sensitive to the training setup and only works when we apply data augmentation. for both gpt-3 and llama-1, performance improves with model size. these findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in llms. code is available at: https://github.com/asacooperstickland/situational-awareness-evals.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00751" target="_blank">Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence</a></div>
<div class="paper-author">Daniel Scalena, Gabriele Sarti, Malvina Nissim, Elisabetta Fersini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. in this work, we apply popular detoxification approaches to several language models and quantify their impact on the resulting models' prompt dependence using feature attribution methods. we evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00770" target="_blank">Bias and Fairness in Large Language Models: A Survey</a></div>
<div class="paper-author">Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements of large language models (llms) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. despite this success, these models can learn, perpetuate, and amplify harmful social biases. in this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for llms. we first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for llms. we then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. finally, we identify open problems and challenges for future work. synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00779" target="_blank">Value Kaleidoscope: Engaging Ai With Pluralistic Human Values, Rights, and Duties</a></div>
<div class="paper-author">Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, John Tasioulas, Yejin Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human values are crucial to human decision-making. value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). as statistical learners, ai systems fit to averages by default, washing out these potentially irreducible value conflicts. to improve ai systems to better reflect value pluralism, the first-order challenge is to explore the extent to which ai systems can model pluralistic human values, rights, and duties as well as their interaction.   we introduce valueprism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. valueprism's contextualized values are generated by gpt-4 and deemed high-quality by human annotators 91% of the time. we conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented.   with valueprism, we build kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. humans prefer the sets of values output by our system over the teacher gpt-4, finding them more accurate and with broader coverage. in addition, we demonstrate that kaleido can help explain variability in human decision-making by outputting contrasting values. finally, we show that kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. we hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering ai systems to make decisions that are more in accordance with them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-31</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16549" target="_blank">Thesis Distillation: Investigating the Impact of Bias in NLP Models on Hate Speech Detection</a></div>
<div class="paper-author">Fatma Elsafoury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper is a summary of the work in my phd thesis. in which, i investigate the impact of bias in nlp models on the task of hate speech detection from three perspectives: explainability, offensive stereotyping bias, and fairness. i discuss the main takeaways from my thesis and how they can benefit the broader nlp community. finally, i discuss important future research directions. the findings of my thesis suggest that bias in nlp models impacts the task of hate speech detection from all three perspectives. and that unless we start incorporating social sciences in studying bias in nlp models, we will not effectively overcome the current limitations of measuring and mitigating bias in nlp models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16705" target="_blank">Crehate: Cross-Cultural Re-Annotation of English Hate Speech Dataset</a></div>
<div class="paper-author">Nayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Juho Kim, Alice Oh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: english datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. this is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. to delve into how individuals from different countries perceive hate speech, we introduce crehate, a cross-cultural re-annotation of the sampled sbic dataset. this dataset includes annotations from five distinct countries: australia, singapore, south africa, the united kingdom, and the united states. our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. we also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. these findings underscore the need to re-evaluate certain aspects of nlp research, especially with regard to the nuanced nature of hate speech in the english language.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00155" target="_blank">LLM in the Shell: Generative Honeypots</a></div>
<div class="paper-author">Muris Sladić, Veronica Valeros, Carlos Catania, Sebastian Garcia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: honeypots are essential tools in cybersecurity. however, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. this limitation makes them easily discernible, hindering their effectiveness. this work introduces a novel method to create dynamic and realistic software honeypots based on large language models. preliminary results indicate that llms can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. we evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. our proposed honeypot, called shellm, reached an accuracy rate of 0.92.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-30</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15812" target="_blank">Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models</a></div>
<div class="paper-author">Hritik Bansal, John Dang, Aditya Grover</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) with human values and intents critically involves the use of human or ai feedback. while dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score response a on a scale of 1-7) and rankings (e.g., is response a better than response b?). in this work, we analyze the effect of this design choice for the alignment and evaluation of llms. we uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and ai annotators. our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. to our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned llms. in particular, we find that llms that leverage rankings data for alignment (say model x) are preferred over those that leverage ratings data (say model y), with a rank-based evaluation protocol (is x/y's response better than reference response?) but not with a rating-based evaluation protocol (score rank x/y's response on a scale of 1-7). our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment. our code and data are available at https://github.com/hritikbansal/sparse_feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15906" target="_blank">Is the u.s. Legal System Ready for Ai's Challenges to Human Values?</a></div>
<div class="paper-author">Inyoung Cheong, Aylin Caliskan, Tadayoshi Kohno</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our interdisciplinary study investigates how effectively u.s. laws confront the challenges posed by generative ai to human values. through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as privacy, autonomy, dignity, diversity, equity, and physical/mental well-being. constitutional and civil rights, it appears, may not provide sufficient protection against ai-generated discriminatory outputs. furthermore, even if we exclude the liability shield provided by section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of ai systems. to address the unique and unforeseeable threats posed by generative ai, we advocate for legal frameworks that evolve to recognize new threats and provide proactive, auditable guidelines to industry stakeholders. addressing these issues requires deep interdisciplinary collaborations to identify harms, values, and mitigation strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16175" target="_blank">Quantifying Uncertainty in Answers From Any Language Model and Enhancing Their Trustworthiness</a></div>
<div class="paper-author">Jiuhai Chen, Jonas Mueller</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce bsdetector, a method for detecting bad and speculative answers from a pretrained large language model by estimating a numeric confidence score for any output it generated. our uncertainty quantification technique works for any llm accessible only via a black-box api, whose training data remains unknown. by expending a bit of extra computation, users of any llm api can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. experiments on both closed and open-form question-answer benchmarks reveal that bsdetector more accurately identifies incorrect llm responses than alternative uncertainty estimation procedures (for both gpt-3 and chatgpt). by sampling multiple responses from the llm and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same llm, without any extra training steps. in applications involving automated evaluation with llms, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both gpt 3.5 and 4).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16364" target="_blank">Strengthening the Eu Ai Act: Defining Key Terms on Ai Manipulation</a></div>
<div class="paper-author">Matija Franklin, Philip Moreira Tomei, Rebecca Gorman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the european union's artificial intelligence act aims to regulate manipulative and harmful uses of ai, but lacks precise definitions for key concepts. this paper provides technical recommendations to improve the act's conceptual clarity and enforceability. we review psychological models to define "personality traits," arguing the act should protect full "psychometric profiles." we urge expanding "behavior" to include "preferences" since preferences causally influence and are influenced by behavior. clear definitions are provided for "subliminal," "manipulative," and "deceptive" techniques, considering incentives, intent, and covertness. we distinguish "exploiting individuals" from "exploiting groups," emphasising different policy needs. an "informed decision" is defined by four facets: comprehension, accurate information, no manipulation, and understanding ai's influence. we caution the act's therapeutic use exemption given the lack of regulation of digital therapeutics by the ema. overall, the recommendations strengthen definitions of vague concepts in the eu ai act, enhancing precise applicability to regulate harmful ai manipulation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-29</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15126" target="_blank">Evaluation and Analysis of Hallucination in Large Vision-Language Models</a></div>
<div class="paper-author">Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large vision-language models (lvlms) have recently achieved remarkable success. however, lvlms are still plagued by the hallucination problem, which limits the practicality in many scenarios. hallucination refers to the information of lvlms' responses that does not exist in the visual input, which poses potential risks of substantial consequences. there has been limited work studying hallucination evaluation in lvlms. in this paper, we propose hallucination evaluation based on large language models (haelm), an llm-based hallucination evaluation framework. haelm achieves an approximate 95% performance comparable to chatgpt and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. leveraging the haelm, we evaluate the hallucination in current lvlms. furthermore, we analyze the factors contributing to hallucination in lvlms and offer helpful suggestions to mitigate the hallucination problem. our training data and human annotation hallucination data will be made public soon.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15399" target="_blank">Rethinking Machine Ethics -- Can LLMS Perform Moral Reasoning Through the Lens of Moral Theories?</a></div>
<div class="paper-author">Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin King, Helen Meng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: making moral judgments is an essential step toward developing ethical ai systems. prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. these approaches have been criticized for potentially overgeneralizing a limited group of annotators' moral stances and lacking explainability. in contrast, top-down approaches make moral judgments grounded in a set of principles. however, it remains conceptual due to the incapability of previous language models and the unsolved debate among moral principles. in this study, we propose a flexible framework to steer large language models (llms) to perform moral reasoning with well-established moral theories from interdisciplinary research. the theory-guided top-down framework can incorporate various moral theories. our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. furthermore, we show the alignment between different moral theories and existing morality datasets. our analysis exhibits the potentials and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15448" target="_blank">Vulgar Remarks Detection in Chittagonian Dialect of Bangla</a></div>
<div class="paper-author">Tanjim Mahmud, Michal Ptaszynski, Fumito Masui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the negative effects of online bullying and harassment are increasing with internet popularity, especially in social media. one solution is using natural language processing (nlp) and machine learning (ml) methods for the automatic detection of harmful remarks, but these methods are limited in low-resource languages like the chittagonian dialect of bangla.this study focuses on detecting vulgar remarks in social media using supervised ml and deep learning algorithms.logistic regression achieved promising accuracy (0.91) while simple rnn with word2vec and fasttex had lower accuracy (0.84-0.90), highlighting the issue that nn algorithms require more data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15514" target="_blank">International Governance of Civilian Ai: A Jurisdictional Certification Approach</a></div>
<div class="paper-author">Robert Trager, Ben Harack, Anka Reuel, Allison Carnegie, Lennart Heim, Lewis Ho, Sarah Kreps, Ranjit Lall, Owen Larter, Seán Ó Héigeartaigh, Simon Staffell, José Jaime Villalobos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (ai) and presents one approach in detail. this approach represents the extension of a standards, licensing, and liability regime to the global level. we propose that states establish an international ai organization (iaio) to certify state jurisdictions (not firms or ai projects) for compliance with international oversight standards. states can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody ai from non-iaio-certified jurisdictions. this borrows attributes from models of existing international organizations, such as the international civilian aviation organization (icao), the international maritime organization (imo), and the financial action task force (fatf). states can also adopt multilateral controls on the export of ai product inputs, such as specialized hardware, to non-certified jurisdictions. indeed, both the import and export standards could be required for certification. as international actors reach consensus on risks of and minimum standards for advanced ai, a jurisdictional certification regime could mitigate a broad range of potential harms, including threats to public safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15745" target="_blank">Cyberbullying Detection for Low-Resource Languages and Dialects: Review of the State of the Art</a></div>
<div class="paper-author">Tanjim Mahmud, Michal Ptaszynski, Juuso Eronen, Fumito Masui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the struggle of social media platforms to moderate content in a timely manner, encourages users to abuse such platforms to spread vulgar or abusive language, which, when performed repeatedly becomes cyberbullying a social problem taking place in virtual environments, yet with real-world consequences, such as depression, withdrawal, or even suicide attempts of its victims. systems for the automatic detection and mitigation of cyberbullying have been developed but, unfortunately, the vast majority of them are for the english language, with only a handful available for low-resource languages. to estimate the present state of research and recognize the needs for further development, in this paper we present a comprehensive systematic survey of studies done so far for automatic cyberbullying detection in low-resource languages. we analyzed all studies on this topic that were available. we investigated more than seventy published studies on automatic detection of cyberbullying or related language in low-resource languages and dialects that were published between around 2017 and january 2023. there are 23 low-resource languages and dialects covered by this paper, including bangla, hindi, dravidian languages and others. in the survey, we identify some of the research gaps of previous studies, which include the lack of reliable definitions of cyberbullying and its relevant subcategories, biases in the acquisition, and annotation of data. based on recognizing those research gaps, we provide some suggestions for improving the general research conduct in cyberbullying detection, with a primary focus on low-resource languages. based on those proposed suggestions, we collect and release a cyberbullying dataset in the chittagonian dialect of bangla and propose a number of initial ml solutions trained on that dataset. in addition, pre-trained transformer-based the banglabert model was also attempted.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14367" target="_blank">A Comprehensive Overview of Backdoor Attacks in Large Language Models Within Communication Networks</a></div>
<div class="paper-author">Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing Lu, Shui Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the large language models (llms) are poised to offer efficient and intelligent services for future mobile communication networks, owing to their exceptional capabilities in language comprehension and generation. however, the extremely high data and computational resource requirements for the performance of llms compel developers to resort to outsourcing training or utilizing third-party data and computing resources. these strategies may expose the model within the network to maliciously manipulated training data and processing, providing an opportunity for attackers to embed a hidden backdoor into the model, termed a backdoor attack. backdoor attack in llms refers to embedding a hidden backdoor in llms that causes the model to perform normally on benign samples but exhibit degraded performance on poisoned ones. this issue is particularly concerning within communication networks where reliability and security are paramount. despite the extensive research on backdoor attacks, there remains a lack of in-depth exploration specifically within the context of llms employed in communication networks, and a systematic review of such attacks is currently absent. in this survey, we systematically propose a taxonomy of backdoor attacks in llms as used in communication networks, dividing them into four major categories: input-triggered, prompt-triggered, instruction-triggered, and demonstration-triggered attacks. furthermore, we conduct a comprehensive analysis of the benchmark datasets. finally, we identify potential problems and open challenges, offering valuable insights into future research directions for enhancing the security and integrity of llms in communication networks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14434" target="_blank">Using Chatgpt as a Static Application Security Testing Tool</a></div>
<div class="paper-author">Atieh Bakhshandeh, Abdalsamad Keramatfar, Amir Norouzi, Mohammad Mahdi Chekidehkhoun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, artificial intelligence has had a conspicuous growth in almost every aspect of life. one of the most applicable areas is security code review, in which a lot of ai-based tools and approaches have been proposed. recently, chatgpt has caught a huge amount of attention with its remarkable performance in following instructions and providing a detailed response. regarding the similarities between natural language and code, in this paper, we study the feasibility of using chatgpt for vulnerability detection in python source code. toward this goal, we feed an appropriate prompt along with vulnerable data to chatgpt and compare its results on two datasets with the results of three widely used static application security testing tools (bandit, semgrep and sonarqube). we implement different kinds of experiments with chatgpt and the results indicate that chatgpt reduces the false positive and false negative rates and has the potential to be used for python source code vulnerability detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14608" target="_blank">Ai in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics</a></div>
<div class="paper-author">Vahid Ghafouri, Vibhor Agarwal, Yong Zhang, Nishanth Sastry, Jose Such, Guillermo Suarez-Tangil</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the introduction of chatgpt and the subsequent improvement of large language models (llms) have prompted more and more individuals to turn to the use of chatbots, both for information and assistance with decision-making. however, the information the user is after is often not formulated by these chatbots objectively enough to be provided with a definite, globally accepted answer.   controversial topics, such as "religion", "gender identity", "freedom of speech", and "equality", among others, can be a source of conflict as partisan or biased answers can reinforce preconceived notions or promote disinformation. by exposing chatgpt to such debatable questions, we aim to understand its level of awareness and if existing models are subject to socio-political and/or economic biases. we also aim to explore how ai-generated answers compare to human ones. for exploring this, we use a dataset of a social media platform created for the purpose of debating human-generated claims on polemic subjects among users, dubbed kialo.   our results show that while previous versions of chatgpt have had important issues with controversial topics, more recent versions of chatgpt (gpt-3.5-turbo) are no longer manifesting significant explicit biases in several knowledge areas. in particular, it is well-moderated regarding economic aspects. however, it still maintains degrees of implicit libertarian leaning toward right-winged ideals which suggest the need for increased moderation from the socio-political point of view. in terms of domain knowledge on controversial topics, with the exception of the "philosophical" category, chatgpt is performing well in keeping up with the collective human level of knowledge. finally, we see that sources of bing ai have slightly more tendency to the center when compared to human answers. all the analyses we make are generalizable to other types of biases and domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14641" target="_blank">Challenges of GPT-3-Based Conversational Agents for Healthcare</a></div>
<div class="paper-author">Fabian Lechner, Allison Lahnala, Charles Welch, Lucie Flek</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the potential to provide patients with faster information access while allowing medical specialists to concentrate on critical tasks makes medical domain dialog agents appealing. however, the integration of large-language models (llms) into these agents presents certain limitations that may result in serious consequences. this paper investigates the challenges and risks of using gpt-3-based models for medical question-answering (medqa). we perform several evaluations contextualized in terms of standard medical principles. we provide a procedure for manually designing patient queries to stress-test high-risk limitations of llms in medqa systems. our analysis reveals that llms fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content that may be considered offensive.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14683" target="_blank">Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts</a></div>
<div class="paper-author">Thanh Thi Nguyen, Campbell Wilson, Janis Dalins</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting online sexual predatory behaviours and abusive language on social media platforms has become a critical area of research due to the growing concerns about online safety, especially for vulnerable populations such as children and adolescents. researchers have been exploring various techniques and approaches to develop effective detection systems that can identify and mitigate these risks. recent development of large language models (llms) has opened a new opportunity to address this problem more effectively. this paper proposes an approach to detection of online sexual predatory chats and abusive language using the open-source pretrained llama 2 7b-parameter model, recently released by meta genai. we fine-tune the llm using datasets with different sizes, imbalance degrees, and languages (i.e., english, roman urdu and urdu). based on the power of llms, our approach is generic and automated without a manual search for a synergy between feature extraction and classifier design steps like conventional methods in this domain. experimental results show a strong performance of the proposed approach, which performs proficiently and consistently across three distinct datasets with five sets of experiments. this study's outcomes indicate that the proposed method can be implemented in real-world applications (even with non-english languages) for flagging sexual predators, offensive or toxic content, hate speech, and discriminatory language in online discussions and comments to maintain respectful internet or digital communities. furthermore, it can be employed for solving text classification problems with other potential applications such as sentiment analysis, spam and phishing detection, sorting legal documents, fake news detection, language identification, user intent recognition, text-based product categorization, medical record analysis, and resume screening.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14752" target="_blank">Ai Deception: A Survey of Examples, Risks, and Potential Solutions</a></div>
<div class="paper-author">Peter S. Park, Simon Goldstein, "Aidan O'Gara", Michael Chen, Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper argues that a range of current ai systems have learned how to deceive humans. we define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. we first survey empirical examples of ai deception, discussing both special-use ai systems (including meta's cicero) built for specific competitive situations, and general-purpose ai systems (such as large language models). next, we detail several risks from ai deception, such as fraud, election tampering, and losing control of ai systems. finally, we outline several potential solutions to the problems posed by ai deception: first, regulatory frameworks should subject ai systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect ai deception and to make ai systems less deceptive. policymakers, researchers, and the broader public should work proactively to prevent ai deception from destabilizing the shared foundations of our society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14840" target="_blank">Identifying and Mitigating the Security Risks of Generative Ai</a></div>
<div class="paper-author">Clark Barrett, Brad Boyd, Elie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, Diyi Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. generative ai (genai) techniques, such as large language models (llms) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). however, genai can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.   this paper reports the findings of a workshop held at google (co-organized by stanford university and the university of wisconsin-madison) on the dual-use dilemma posed by genai. this paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. we discuss short-term and long-term goals for the community on this topic. we hope this paper provides both a launching point for a discussion on this important topic as well as interesting problems that the research community can work to address.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14921" target="_blank">Gender Bias and Stereotypes in Large Language Models</a></div>
<div class="paper-author">Hadas Kotek, Rikker Dockum, David Q. Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. this paper investigates llms' behavior with respect to gender stereotypes, a known issue for prior models. we use a simple paradigm to test the presence of gender bias, building on but differing from winobias, a commonly used gender bias dataset, which is likely to be included in the training data of current llms. we test four recently published llms and demonstrate that they express biased assumptions about men and women's occupations. our contributions in this paper are as follows: (a) llms are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) llms in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) llms ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) llms provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. that is, they provide rationalizations of their biased behavior. this highlights a key property of these models: llms are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. as with other types of societal biases, we suggest that llms must be carefully tested to ensure that they treat minoritized individuals and communities equitably.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14132" target="_blank">Detecting Language Model Attacks With Perplexity</a></div>
<div class="paper-author">Gabriel Alon, Michael Kamfonas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a novel hack involving large language models (llms) has emerged, leveraging adversarial suffixes to trick models into generating perilous responses. this method has garnered considerable attention from reputable media outlets such as the new york times and wired, thereby influencing public perception regarding the security and safety of llms. in this study, we advocate the utilization of perplexity as one of the means to recognize such potential attacks. the underlying concept behind these hacks revolves around appending an unusually constructed string of text to a harmful query that would otherwise be blocked. this maneuver confuses the protective mechanisms and tricks the model into generating a forbidden response. such scenarios could result in providing detailed instructions to a malicious user for constructing explosives or orchestrating a bank heist. our investigation demonstrates the feasibility of employing perplexity, a prevalent natural language processing metric, to detect these adversarial tactics before generating a forbidden response. by evaluating the perplexity of queries with and without such adversarial suffixes using an open-source llm, we discovered that nearly 90 percent were above a perplexity of 1000. this contrast underscores the efficacy of perplexity for detecting this type of exploit.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14253" target="_blank">The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward</a></div>
<div class="paper-author">Alexander J. Titus, Adam H. Russell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) promises immense benefits across sectors, yet also poses risks from dual-use potentials, biases, and unintended behaviors. this paper reviews emerging issues with opaque and uncontrollable ai systems and proposes an integrative framework called violet teaming to develop reliable and responsible ai. violet teaming combines adversarial vulnerability probing (red teaming) with solutions for safety and security (blue teaming) while prioritizing ethics and social benefit. it emerged from ai safety research to manage risks proactively by design. the paper traces the evolution of red, blue, and purple teaming toward violet teaming, and then discusses applying violet techniques to address biosecurity risks of ai in biotechnology. additional sections review key perspectives across law, ethics, cybersecurity, macrostrategy, and industry best practices essential for operationalizing responsible ai through holistic technical and social considerations. violet teaming provides both philosophy and method for steering ai trajectories toward societal good. with conscience and wisdom, the extraordinary capabilities of ai can enrich humanity. but without adequate precaution, the risks could prove catastrophic. violet teaming aims to empower moral technology for the common welfare.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13768" target="_blank">Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content</a></div>
<div class="paper-author">"Charles O'Neill", Jack Miller, Ioana Ciuca, Yuan-Sen Ting, Thang Bui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we tackle the emerging challenge of unintended harmful content generation in large language models (llms) with a novel dual-stage optimisation technique using adversarial fine-tuning. our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. in this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. this iterative application of prompting and fine-tuning allows continuous refinement and improved performance. the performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by gpt-4, as well as a selection of contentious but unproblematic prompts. we show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. furthermore, we show that a rudimentary model \texttt{ada} can achieve 13\% higher accuracy on the hold-out test set than gpt-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13904" target="_blank">Lmsanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors</a></div>
<div class="paper-author">Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, Wenzhi Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. the state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. to address this issue, we propose lmsanitator, a novel approach for detecting and removing task-agnostic backdoors on transformer models. instead of directly inverting the triggers, lmsanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. lmsanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. extensive experiments on multiple language models and nlp tasks illustrate the effectiveness of lmsanitator. for instance, lmsanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13387" target="_blank">Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMS</a></div>
<div class="paper-author">Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, Timothy Baldwin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid evolution of large language models (llms), new and hard-to-predict harmful capabilities are emerging. this requires developers to be able to identify risks through the evaluation of "dangerous capabilities" in order to responsibly deploy llms. in this work, we collect the first open-source dataset to evaluate safeguards in llms, and deploy safer open-source llms at a low cost. our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. we annotate and assess the responses of six popular llms to these instructions. based on our annotation, we proceed to train several bert-like classifiers, and find that these small classifiers can achieve results that are comparable with gpt-4 on automatic safety evaluation. warning: this paper contains example data that may be offensive, harmful, or biased.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13449" target="_blank">The Poison of Alignment</a></div>
<div class="paper-author">Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, James Yamazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: from the perspective of content safety issues, alignment has shown to limit large language models' (llms) harmful content generation. this intentional method of reinforcing models to not respond to certain user inputs seem to be present in many modern open-source instruction tuning datasets such as openassistant or guanaco. we introduce a novel insight to an instruction-tuned model's performance affected by the presence of alignment in supervised fine-tuning dataset. to be specific, we noticed that alignment acts as if it is poisoning the instruction dataset. experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as big bench (bbh), massive multitask language understanding (mmlu), human eval, and discrete reasoning over paragraphs (drop), performing worse than the counterpart tuned without alignment by 4-33%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00639" target="_blank">Misinformation Concierge: A Proof-of-Concept With Curated Twitter Dataset on Covid-19 Vaccination</a></div>
<div class="paper-author">Shakshi Sharma, Anwitaman Datta, Vigneshwaran Shankaran, Rajesh Sharma</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we demonstrate the misinformation concierge, a proof-of-concept that provides actionable intelligence on misinformation prevalent in social media. specifically, it uses language processing and machine learning tools to identify subtopics of discourse and discern non/misleading posts; presents statistical reports for policy-makers to understand the big picture of prevalent misinformation in a timely manner; and recommends rebuttal messages for specific pieces of misinformation, identified from within the corpus of data - providing means to intervene and counter misinformation promptly. the misinformation concierge proof-of-concept using a curated dataset is accessible at: https://demo-frontend-uy34.onrender.com/
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12342" target="_blank">Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions</a></div>
<div class="paper-author">Reem I. Masoud, Ziquan Liu, Martin Ferianc, Philip Treleaven, Miguel Rodrigues</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the deployment of large language models (llms) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. existing work investigated political and social biases and public opinions rather than their cultural values. to address this limitation, the proposed cultural alignment test (cat) quantifies cultural alignment using hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. we apply our approach to assess the cultural values embedded in state-of-the-art llms, such as: chatgpt and bard, across diverse cultures of countries: united states (us), saudi arabia, china, and slovakia, using different prompting styles and hyperparameter settings. our results not only quantify cultural alignment of llms with certain countries, but also reveal the difference between llms in explanatory cultural dimensions. while all llms did not provide satisfactory results in understanding cultural values, gpt-4 exhibited the highest cat score for the cultural values of the us.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12578" target="_blank">Mind vs. Mouth: On Measuring Re-Judge Inconsistency of Social Bias in Large Language Models</a></div>
<div class="paper-author">Yachao Zhao, Bo Wang, Dongming Zhao, Kun Huang, Yan Wang, Ruifang He, Yuexian Hou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent researches indicate that pre-trained large language models (llms) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of llms. this paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. it posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. we propose a two-stage approach and discover a parallel phenomenon in llms known as "re-judge inconsistency" in social bias. in the initial stage, the llm is tasked with automatically completing statements, potentially incorporating implicit social bias. however, in the subsequent stage, the same llm re-judges the biased statement generated by itself but contradicts it. we propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. experimental investigations on chatgpt and gpt-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. this finding may suggest that diverse cognitive constructs emerge as llms' capabilities strengthen. consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12833" target="_blank">Use of LLMS for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities</a></div>
<div class="paper-author">Maximilian Mozes, Xuanli He, Bennett Kleinberg, Lewis D. Griffin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: spurred by the recent rapid increase in the development and distribution of large language models (llms) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of llms, including in the context of potentially criminal activities. specifically, it has been shown that llms can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of ai alignment. it is important that developers and practitioners alike are aware of security-related problems with such models. in this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from llms. we present a taxonomy describing the relationship between threats caused by the generative capabilities of llms, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. with our work, we hope to raise awareness of the limitations of llms in light of such security concerns, among both experienced developers and novel users of such technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12918" target="_blank">Evaluating the Vulnerabilities in Ml Systems in Terms of Adversarial Attacks</a></div>
<div class="paper-author">John Harshith, Mantej Singh Gill, Madhan Jothimani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: there have been recent adversarial attacks that are difficult to find. these new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. the authors focus on this domain in this research paper. they explore the consequences of vulnerabilities in ai systems. this includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. moreover, it is important to train the ai systems appropriately when they are in testing phase and getting them ready for broader use.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13089" target="_blank">Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models Using an Interdisciplinary Lens</a></div>
<div class="paper-author">Pranav Narayanan Venkit</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid growth in the usage and applications of natural language processing (nlp) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. while research on bias in nlp has expanded, several challenges persist that require attention. these include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. this paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in nlp. the work is structured into three facets, each exploring a specific aspect of bias in nlp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12014" target="_blank">From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models</a></div>
<div class="paper-author">Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: big models, exemplified by large language models (llms), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. however, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. therefore, many efforts have been made to align llms with humans to make them better follow user instructions and satisfy human preferences. nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. in this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced llms. based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12050" target="_blank">Aligning Language Models With Offline Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Jian Hu, Li Tao, June Yang, Chandler Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: learning from human preferences is crucial for language models (lms) to effectively cater to human needs and societal values. previous research has made notable progress by leveraging human feedback to follow instructions. however, these approaches rely primarily on online reinforcement learning (rl) techniques like proximal policy optimization (ppo), which have been proven unstable and challenging to tune for language models. moreover, ppo requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. in this study, we propose an offline reinforcement learning from human feedback (rlhf) framework to align lms using pre-generated samples without interacting with rl environments. specifically, we explore maximum likelihood estimation (mle) with filtering, reward-weighted regression (rwr), and decision transformer (dt) to align language models to human preferences. by employing a loss function similar to supervised fine-tuning, our methods ensure more stable model training than ppo with a simple machine learning system~(mlsys) and much fewer (around 12.3\%) computing resources. experimental results demonstrate the dt alignment outperforms other offline rlhf methods and is better than ppo.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12086" target="_blank">Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments</a></div>
<div class="paper-author">Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, Sebastian Garcia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. despite their inherent limitations, llm-based designs have shown promising capabilities in planning and navigating open-world scenarios. this paper introduces a novel application of pre-trained llms as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   we present an approach wherein pre-trained llms are leveraged as attacking agents in two reinforcement learning environments. our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. in addition, the best llm agents perform similarly to human testers of the environment without any additional training process. this design highlights the potential of llms to efficiently address complex decision-making tasks within cybersecurity.   furthermore, we introduce a new network security environment named netsecgame. the environment is designed to eventually support complex multi-agent scenarios within the network security domain. the proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12215" target="_blank">The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</a></div>
<div class="paper-author">Madelyne Xiao, Jonathan Mayer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. we systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. we then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. we find significant shortcomings in the literature that call into question claimed performance and practicality. detection tasks are often meaningfully distinct from the challenges that online services actually face. datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. data and code availability is poor. models do not generalize well to out-of-domain data. based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. our aim is for future work to avoid the pitfalls that we identify.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12287" target="_blank">Devising and Detecting Phishing: Large Language Models vs. Smaller Human Models</a></div>
<div class="paper-author">Fredrik Heiding, Bruce Schneier, Arun Vishwanath, Jeremy Bernstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai programs, built using large language models, make it possible to automatically create phishing emails based on a few data points about a user. they stand in contrast to traditional phishing emails that hackers manually design using general rules gleaned from experience. the v-triad is an advanced set of rules for manually designing phishing emails to exploit our cognitive heuristics and biases. in this study, we compare the performance of phishing emails created automatically by gpt-4 and manually using the v-triad. we also combine gpt-4 with the v-triad to assess their combined potential. a fourth group, exposed to generic phishing emails, was our control group. we utilized a factorial approach, sending emails to 112 randomly selected participants recruited for the study. the control group emails received a click-through rate between 19-28%, the gpt-generated emails 30-44%, emails generated by the v-triad 69-79%, and emails generated by gpt and the v-triad 43-81%. each participant was asked to explain for why they pressed or did not press a link in the email. these answers often contradict each other, highlighting the need for personalized content. the cues that make one person avoid phishing emails make another person fall for them. next, we used four popular large language models (gpt, claude, palm, and llama) to detect the intention of phishing emails and compare the results to human detection. the language models demonstrated a strong ability to detect malicious intent, even in non-obvious phishing emails. they sometimes surpassed human detection, although often being slightly less accurate than humans.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12448" target="_blank">Trend and Emerging Types of 419 Scams</a></div>
<div class="paper-author">Polra Victor Falade</div>
<div class="abstract">
<div class="abstract-content">
Abstract: technological advancements have revolutionized various aspects of human life, facilitating communication, business operations, healthcare, education, and environmental monitoring. however, this increased reliance on technology has also led to a surge in cybercrime, including cyber scams. the "419 scam" or nigerian scam has been a persistent problem for decades, encompassing frauds like advance fee scams, fake lotteries, and black money scams. initially prevalent through postal mail and later via fax, the scam has now transitioned to email. this study aims to identify recent types of 419 scam emails, particularly after the covid 19 pandemic, and explore commonly used email subjects. analysis of the sample 419 scam emails revealed trending scams like lucky winner, threat of exposure, business/partnership proposals, investment, cancer/long-term illness, fund, and compensation scams. emerging scams included covid-related, cryptocurrency, marketing contact, and software development scams. irrespective of the scam type, scammers commonly employed email subjects such as 're', 'good day', 'greetings', 'dear friend', 'confirm', 'attention', and 'hello dear'. the severity of cybercrime, especially the 419 scams, cannot be overstated, as it erodes trust, causes financial losses, and hampers nigeria's reputation and economic progress. combatting cyber scams and enhancing cybersecurity measures are crucial to protect individuals and organizations from falling victim to these fraudulent schemes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12539" target="_blank">Calm : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias</a></div>
<div class="paper-author">Vipul Gupta, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, Rebecca J. Passonneau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models (lms) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. to achieve reliability, we introduce the comprehensive assessment of language model bias (calm), a benchmark dataset to quantify bias in lms across three tasks. we integrate 16 existing datasets across different domains, such as wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. we compare the diversity of calm with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. we show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. we evaluate 20 large language models including six prominent families of lms such as llama-2. in two lm series, opt and bloom, we found that larger parameter models are more biased than lower parameter models. we found the t0 series of models to be the least biased. furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. the code is available at https://github.com/vipulgupta1011/calm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11386" target="_blank">Targeted Data Augmentation for Bias Mitigation</a></div>
<div class="paper-author">Agnieszka Mikołajczyk-Bareła, Maria Ferlin, Michał Grochowski</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the development of fair and ethical ai systems requires careful consideration of bias mitigation, an area often overlooked or ignored. in this study, we introduce a novel and efficient approach for addressing biases called targeted data augmentation (tda), which leverages classical data augmentation techniques to tackle the pressing issue of bias in data and models. unlike the laborious task of removing biases, our method proposes to insert biases instead, resulting in improved performance. to identify biases, we annotated two diverse datasets: a dataset of clinical skin lesions and a dataset of male and female faces. these bias annotations are published for the first time in this study, providing a valuable resource for future research. through counterfactual bias insertion, we discovered that biases associated with the frame, ruler, and glasses had a significant impact on models. by randomly introducing biases during training, we mitigated these biases and achieved a substantial decrease in bias measures, ranging from two-fold to more than 50-fold, while maintaining a negligible increase in the error rate.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11406" target="_blank">Designing an Attack-Defense Game: How to Increase Robustness of Financial Transaction Models via a Competition</a></div>
<div class="paper-author">Alexey Zaytsev, Alex Natekin, Evgeni Vorsin, Valerii Smirnov, Georgii Smirnov, Oleg Sidorshin, Alexander Senin, Alexander Dudin, Dmitry Berestnev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. the threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. we aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.   to achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. the participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break it, and what techniques one should use to make it more robust, and introduction additional way to attack models or increase their robustness.   our analysis continues with a meta-study on the used approaches with their power, numerical experiments, and accompanied ablations studies. we show that the developed attacks and defenses outperform existing alternatives from the literature while being practical in terms of execution, proving the validity of the competition as a tool for uncovering vulnerabilities of machine learning models and mitigating them in various domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10592" target="_blank">Ban-Pl: A Novel Polish Dataset of Banned Harmful and Offensive Content From wykop.pl Web Service</a></div>
<div class="paper-author">Inez Okulska, Kinga Głąbińska, Anna Kołos, Agnieszka Karlińska, Emilia Wiśnios, Adam Nowakowski, Paweł Ellerik, Andrzej Prałat</div>
<div class="abstract">
<div class="abstract-content">
Abstract: advances in automated detection of offensive language online, including hate speech and cyberbullying, require improved access to publicly available datasets comprising social media content. in this paper, we introduce ban-pl, the first open dataset in the polish language that encompasses texts flagged as harmful and subsequently removed by professional moderators. the dataset encompasses a total of 691,662 pieces of content from a popular social networking service, wykop, often referred to as the "polish reddit", including both posts and comments, and is evenly distributed into two distinct classes: "harmful" and "neutral". we provide a comprehensive description of the data collection and preprocessing procedures, as well as highlight the linguistic specificity of the data. the ban-pl dataset, along with advanced preprocessing scripts for, i.a., unmasking profanities, will be publicly available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10684" target="_blank">Systematic Offensive Stereotyping (Sos) Bias in Language Models</a></div>
<div class="paper-author">Fatma Elsafoury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: research has shown that language models (lms) are socially biased. however, toxicity and offensive stereotyping bias in lms are understudied. in this paper, we investigate the systematic offensive stereotype (sos) bias in lms. we propose a method to measure it. then, we validate the sos bias and investigate the effectiveness of debias methods from the literature on removing it. finally, we investigate the impact of the sos bias in lms on their performance and their fairness on the task of hate speech detection. our results suggest that all the inspected lms are sos biased. the results suggest that the sos bias in lms is reflective of the hate experienced online by the inspected marginalized groups. the results indicate that removing the sos bias in lms, using a popular debias method from the literature, leads to worse sos bias scores. finally, our results show no strong evidence that the sos bias in lms is impactful on their performance on hate speech detection. on the other hand, there is evidence that the sos bias in lms is impactful on their fairness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10741" target="_blank">On the Adversarial Robustness of Multi-Modal Foundation Models</a></div>
<div class="paper-author">Christian Schlarmann, Matthias Hein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: multi-modal foundation models combining vision and language models such as flamingo or gpt-4 have recently gained enormous interest. alignment of foundation models is used to prevent models from providing toxic or harmful output. while malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. in this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. this indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10800" target="_blank">Artificial Intelligence Is Ineffective and Potentially Harmful for Fact Checking</a></div>
<div class="paper-author">Matthew R. Deverna, Harry Yaojun Yan, Kai-Cheng Yang, Filippo Menczer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. recent artificial intelligence (ai) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. here we investigate the impact of fact checks generated by a popular ai model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. although the ai performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. however, the ai fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. on the positive side, the ai increases sharing intents for correctly labeled true headlines. when participants are given the option to view ai fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false news. our findings highlight an important source of potential harm stemming from ai applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11103" target="_blank">Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models</a></div>
<div class="paper-author">Alex Nyffenegger, Matthias Stürmer, Joel Niklaus</div>
<div class="abstract">
<div class="abstract-content">
Abstract: anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the european union and switzerland. with the advent of llms, concerns about large-scale re-identification of anonymized persons are growing. in accordance with the federal supreme court of switzerland, we explore the potential of llms to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the swiss federal supreme court. following the initial experiment, we constructed an anonymized wikipedia dataset as a more rigorous testing ground to further investigate the findings. with the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. we systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. despite high re-identification rates on wikipedia, even the best llms struggled with court decisions. the complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification. in conclusion, this study demonstrates that re-identification using llms may not be feasible for now, but as the proof-of-concept on wikipedia showed, it might become possible in the future. we hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading to the courts being more confident to publish decisions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10248" target="_blank">Activation Addition: Steering Language Models Without Optimization</a></div>
<div class="paper-author">Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, Monte Macdiarmid</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reliably controlling the behavior of large language models is a pressing open problem. existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. we instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. in particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.   unlike past work which learned these steering vectors, our activation addition (actadd) method computes them by taking the activation differences that result from pairs of prompts. we demonstrate actadd on gpt-2 on openwebtext and conceptnet. our inference-time approach yields control over high-level properties of output and preserves off-target model performance. it involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with model size.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10397" target="_blank">Fairbench: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models</a></div>
<div class="paper-author">Yanhong Bai, Jiabao Zhao, Jinxin Shi, Tingjiang Wei, Xingjiao Wu, Liang He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting stereotypes and biases in large language models (llms) can enhance fairness and reduce adverse impacts on individuals or groups when these llms are applied. however, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world. to address this gap, this paper introduces a four-stage framework to directly evaluate stereotypes and biases in the generated content of llms, including direct inquiry testing, serial or adapted story testing, implicit association testing, and unknown situation testing. additionally, the paper proposes multi-dimensional evaluation metrics and explainable zero-shot prompts for automated evaluation. using the education sector as a case study, we constructed the edu-fairbench based on the four-stage framework, which encompasses 12,632 open-ended questions covering nine sensitive factors and 26 educational scenarios. experimental results reveal varying degrees of stereotypes and biases in five llms evaluated on edu-fairbench. moreover, the results of our proposed automated evaluation method have shown a high correlation with human annotations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10443" target="_blank">Using Large Language Models for Cybersecurity Capture-the-Flag Challenges and Certification Questions</a></div>
<div class="paper-author">Wesley Tann, Yuancheng Liu, Jun Heng Sim, Choon Meng Seah, Ee-Chien Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the assessment of cybersecurity capture-the-flag (ctf) exercises involves participants finding text strings or ``flags'' by exploiting system vulnerabilities. large language models (llms) are natural-language models trained on vast amounts of words to understand and generate text; they can perform well on many ctf challenges. such llms are freely available to students. in the context of ctf exercises in the classroom, this raises concerns about academic integrity. educators must understand llms' capabilities to modify their teaching to accommodate generative ai assistance. this research investigates the effectiveness of llms, particularly in the realm of ctf challenges and questions. here we evaluate three popular llms, openai chatgpt, google bard, and microsoft bing. first, we assess the llms' question-answering performance on five cisco certifications with varying difficulty levels. next, we qualitatively study the llms' abilities in solving ctf challenges to understand their limitations. we report on the experience of using the llms for seven test cases in all five types of ctf challenges. in addition, we demonstrate how jailbreak prompts can bypass and break llms' ethical safeguards. the paper concludes by discussing llm's impact on ctf exercises and its implications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10088" target="_blank">Pace: Improving Prompt With Actor-Critic Editing for Large Language Model</a></div>
<div class="paper-author">Yihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin, Ge Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have showcased remarkable potential across various tasks by conditioning on prompts. however, the quality of different human-written prompts leads to substantial discrepancies in llms' performance, and improving prompts usually necessitates considerable human effort and expertise. to this end, this paper proposes prompt with actor-critic editing (pace) for llms to enable automatic prompt editing. drawing inspiration from the actor-critic algorithm in reinforcement learning, pace leverages llms as the dual roles of actors and critics, conceptualizing prompt as a type of policy. pace refines prompt, taking into account the feedback from both actors performing prompt and critics criticizing response. this process helps llms better align prompt to a specific task, thanks to real responses and thinking from llms. we conduct extensive experiments on 24 instruction induction tasks and 21 big-bench tasks. experimental results indicate that pace elevates the relative performance of medium/low-quality human-written prompts by up to 98\%, which has comparable performance to high-quality human-written prompts. moreover, pace also exhibits notable efficacy for prompt generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10149" target="_blank">A Survey on Fairness in Large Language Models</a></div>
<div class="paper-author">Yingji Li, Mengnan Du, Rui Song, Xin Wang, Ying Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown powerful performance and development prospect and are widely deployed in the real world. however, llms can capture social biases from unprocessed training data and propagate the biases to downstream tasks. unfair llm systems have undesirable social impacts and potential harms. in this paper, we provide a comprehensive review of related research on fairness in llms. first, for medium-scale llms, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. then, for large-scale llms, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. finally, we discuss and provide insight on the challenges and future directions for the development of fairness in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09437" target="_blank">From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space</a></div>
<div class="paper-author">Maximilian Dreyer, Frederik Pahde, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep neural networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. this poses risks when deploying these models for high-stake decision-making, such as in medical applications. current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. we present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. when modeling biases via concept activation vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as support vector machines tend to result in diverging directions. we effectively mitigate biases in controlled and real-world settings on the isic, bone age, imagenet and celeba datasets using vgg, resnet and efficientnet architectures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09490" target="_blank">Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models</a></div>
<div class="paper-author">Dominik Hintersdorf, Lukas Struppek, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the field of artificial intelligence (ai) has experienced remarkable progress in recent years, driven by the widespread adoption of open-source machine learning models in both research and industry. considering the resource-intensive nature of training on vast datasets, many applications opt for models that have already been trained. hence, a small number of key players undertake the responsibility of training and publicly releasing large pre-trained models, providing a crucial foundation for a wide range of applications. however, the adoption of these open-source models carries inherent privacy and security risks that are often overlooked. to provide a concrete example, an inconspicuous model may conceal hidden functionalities that, when triggered by specific input patterns, can manipulate the behavior of the system, such as instructing self-driving cars to ignore the presence of other vehicles. the implications of successful privacy and security attacks encompass a broad spectrum, ranging from relatively minor damage like service interruptions to highly alarming scenarios, including physical harm or the exposure of sensitive user data. in this work, we present a comprehensive overview of common privacy and security threats associated with the use of open-source models. by raising awareness of these dangers, we strive to promote the responsible and secure use of ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09662" target="_blank">Red-Teaming Large Language Models Using Chain of Utterances for Safety-Alignment</a></div>
<div class="paper-author">Rishabh Bhardwaj, Soujanya Poria</div>
<div class="abstract">
<div class="abstract-content">
Abstract: larger language models (llms) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. with the emergence of their properties and encoded knowledge, the risk of llms producing harmful outputs increases, making them unfit for scalable deployment for the public. in this work, we propose a new safety evaluation benchmark red-eval that carries out red-teaming. we show that even widely deployed models are susceptible to the chain of utterances-based (cou) prompting, jailbreaking closed source llm-based systems such as gpt-4 and chatgpt to unethically respond to more than 65% and 73% of harmful queries. we also demonstrate the consistency of the red-eval across 8 open-source llms in generating harmful responses in more than 86% of the red-teaming attempts. next, we propose red-instruct--an approach for the safety alignment of llms. it constitutes two phases: 1) harmfulqa data collection: leveraging cou prompting, we collect a dataset that consists of 1.9k harmful questions covering a wide range of topics, 9.5k safe and 7.3k harmful conversations from chatgpt; 2) safe-align: we demonstrate how the conversational dataset can be used for the safety alignment of llms by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. our model starling, a fine-tuned vicuna-7b, is observed to be more safely aligned when evaluated on red-eval and hhh benchmarks while preserving the utility of the baseline models (truthfulqa, mmlu, and bbh).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08776" target="_blank">Large Language Models at Work in China's Labor Market</a></div>
<div class="paper-author">Qin Chen, Jinfeng Ge, Huaqing Xie, Xingcheng Xu, Yanqing Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper explores the potential impacts of large language models (llms) on the chinese labor market. we analyze occupational exposure to llm capabilities by incorporating human expertise and llm classifications, following eloundou et al. (2023)'s methodology. we then aggregate occupation exposure to the industry level to obtain industry exposure scores. the results indicate a positive correlation between occupation exposure and wage levels/experience premiums, suggesting higher-paying and experience-intensive jobs may face greater displacement risks from llm-powered software. the industry exposure scores align with expert assessments and economic intuitions. we also develop an economic growth model incorporating industry exposure to quantify the productivity-employment trade-off from ai adoption. overall, this study provides an analytical basis for understanding the labor market impacts of increasingly capable ai systems in china. key innovations include the occupation-level exposure analysis, industry aggregation approach, and economic modeling incorporating ai adoption and labor market effects. the findings will inform policymakers and businesses on strategies for maximizing the benefits of ai while mitigating adverse disruption risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09183" target="_blank">Ratgpt: Turning Online LLMS Into Proxies for Malware Attacks</a></div>
<div class="paper-author">Mika Beckerich, Laura Plein, Sergio Coronado</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the evolution of generative ai and the capabilities of the newly released large language models (llms) open new opportunities in software engineering. however, they also lead to new challenges in cybersecurity. recently, researchers have shown the possibilities of using llms such as chatgpt to generate malicious content that can directly be exploited or guide inexperienced hackers to weaponize tools and code. these studies covered scenarios that still require the attacker to be in the middle of the loop. in this study, we leverage openly available plugins and use an llm as proxy between the attacker and the victim. we deliver a proof-of-concept where chatgpt is used for the dissemination of malicious software while evading detection, alongside establishing the communication to a command and control (c2) server to receive commands to interact with a victim's system. finally, we present the general approach as well as essential elements in order to stay undetected and make the attack a success. this proof-of-concept highlights significant cybersecurity issues with openly available plugins and llms, which require the development of security guidelines, controls, and mitigation strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10819" target="_blank">Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection</a></div>
<div class="paper-author">Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. however, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate llms' original instructions and prompt unintended actions and content. therefore, it is crucial to understand llms' ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. in this paper, we propose a pioneering benchmark for automatically evaluating the robustness of instruction-following llms against adversarial instructions injected in the prompt. the objective of this benchmark is to quantify the extent to which llms are influenced by injected adversarial instructions and assess their ability to differentiate between these injected adversarial instructions and original user instructions. through experiments conducted with state-of-the-art instruction-following llms, we uncover significant limitations in their robustness against adversarial instruction injection attacks. furthermore, our findings indicate that prevalent instruction-tuned models are prone to being ``overfitted'' to follow any instruction phrase in the prompt without truly understanding which instructions should be followed. this highlights the need to address the challenge of training models to comprehend prompts instead of merely following instruction phrases and completing the text. the data and code can be found at \url{https://github.com/leezekun/adv-instruct-eval}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12331" target="_blank">Approaches to Generative Artificial Intelligence, a Social Justice Perspective</a></div>
<div class="paper-author">Myke Healy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the 2023-2024 academic year, the widespread availability of generative artificial intelligence, exemplified by chatgpt's 1.6 billion monthly visits, is set to impact academic integrity. with 77% of high school students previously reporting engagement in dishonest behaviour, the rise of ai-driven writing assistance, dubbed 'ai-giarism' by chan (arxiv:2306.03358v2), will make plagiarism more accessible and less detectable. while these concerns are urgent, they also raise broader questions about the revolutionary nature of this technology, including autonomy, data privacy, copyright, and equity. this paper aims to explore generative ai from a social justice perspective, examining the training of these models, the inherent biases, and the potential injustices in detecting ai-generated writing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08295" target="_blank">Detoxify Language Model Step-by-Step</a></div>
<div class="paper-author">Zecheng Tang, Keyan Zhou, Pinzheng Wang, Yuyang Ding, Juntao Li, N/A Minzhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detoxification for llms is challenging since it requires models to avoid generating harmful content while maintaining the generation capability. to ensure the safety of generations, previous detoxification methods detoxify the models by changing the data distributions or constraining the generations from different aspects in a single-step manner. however, these approaches will dramatically affect the generation quality of llms, e.g., discourse coherence and semantic consistency, since language models tend to generate along the toxic prompt while detoxification methods work in the opposite direction. to handle such a conflict, we decompose the detoxification process into different sub-steps, where the detoxification is concentrated in the input stage and the subsequent continual generation is based on the non-toxic prompt. besides, we also calibrate the strong reasoning ability of llms by designing a detox-chain to connect the above sub-steps in an orderly manner, which allows llms to detoxify the text step-by-step. automatic and human evaluation on two benchmarks reveals that by training with detox-chain, six llms scaling from 1b to 33b can obtain significant detoxification and generation improvement. our code and data are available at https://github.com/codinnlg/detox-cot. warning: examples in the paper may contain uncensored offensive content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11521" target="_blank">Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models</a></div>
<div class="paper-author">Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, have emerged with astonishing capabilities approaching artificial general intelligence. while providing convenience for various societal needs, llms have also lowered the cost of generating harmful content. consequently, llm developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the llm into forgetting content defense rules and answering any improper questions. to date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.   this paper investigates the llm jailbreak problem and proposes an automatic jailbreak method for the first time. we propose the concept of a semantic firewall and provide three technical implementation approaches. inspired by the attack that penetrates traditional firewalls through reverse tunnels, we introduce a "self-deception" attack that can bypass the semantic firewall by inducing llm to generate prompts that facilitate jailbreak. we generated a total of 2,520 attack payloads in six languages (english, russian, french, spanish, chinese, and arabic) across seven virtual scenarios, targeting the three most common types of violations: violence, hate, and pornography. the experiment was conducted on two models, namely the gpt-3.5-turbo and gpt-4. the success rates on the two models were 86.2% and 67%, while the failure rates were 4.7% and 2.2%, respectively. this highlighted the effectiveness of the proposed attack method. all experimental code and raw data will be released as open-source to inspire future research. we believe that manipulating ai behavior through carefully crafted prompts will become an important research direction in the future.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07847" target="_blank">Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models</a></div>
<div class="paper-author">Yugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have led to significant improvements in many tasks across various domains, such as code interpretation, response generation, and ambiguity handling. these llms, however, when upgrading, primarily prioritize enhancing user experience while neglecting security, privacy, and safety implications. consequently, unintended vulnerabilities or biases can be introduced. previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions. through the lens of adversarial examples within the in-context learning framework, this longitudinal study addresses this gap by conducting a comprehensive assessment of the robustness of successive versions of llms, vis-\`a-vis gpt-3.5. we conduct extensive experiments to analyze and understand the impact of the robustness in two distinct learning categories: zero-shot learning and few-shot learning. our findings indicate that, in comparison to earlier versions of llms, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks. in addition, our study emphasizes the increased effectiveness of synergized adversarial queries in most zero-shot learning and few-shot learning cases. we hope that our study can lead to a more refined assessment of the robustness of llms over time and provide valuable insights of these models for both developers and users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07973" target="_blank">"Beware of Deception": Detecting Half-Truth and Debunking It Through Controlled Claim Editing</a></div>
<div class="paper-author">Sandeep Singamsetty, Nishtha Madaan, Sameep Mehta, Varad Bhatnagar, Pushpak Bhattacharyya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prevalence of half-truths, which are statements containing some truth but that are ultimately deceptive, has risen with the increasing use of the internet. to help combat this problem, we have created a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. our approach utilizes the t5 model for controlled claim editing; "controlled" here means precise adjustments to select parts of a claim. our methodology achieves an average bleu score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of 85% on edited claims. significantly, our t5-based approach outperforms other language models such as gpt2, roberta, pegasus, and tailor, with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively. by extending the liar plus dataset, we achieve an f1 score of 82% for the half-truth detection model, setting a new benchmark in the field. while previous attempts have been made at half-truth detection, our approach is, to the best of our knowledge, the first to attempt to debunk half-truths.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08088" target="_blank">Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme Detection</a></div>
<div class="paper-author">Rui Cao, Ming Shan Hee, Adriel Kuek, Wen-Haw Chong, Roy Ka-Wei Lee, Jing Jiang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hateful meme detection is a challenging multimodal task that requires comprehension of both vision and language, as well as cross-modal interactions. recent studies have tried to fine-tune pre-trained vision-language models (pvlms) for this task. however, with increasing model sizes, it becomes important to leverage powerful pvlms more efficiently, rather than simply fine-tuning them. recently, researchers have attempted to convert meme images into textual captions and prompt language models for predictions. this approach has shown good performance but suffers from non-informative image captions. considering the two factors mentioned above, we propose a probing-based captioning approach to leverage pvlms in a zero-shot visual question answering (vqa) manner. specifically, we prompt a frozen pvlm by asking hateful content-related questions and use the answers as image captions (which we call pro-cap), so that the captions contain information critical for hateful content detection. the good performance of models with pro-cap on three benchmarks validates the effectiveness and generalization of the proposed method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08090" target="_blank">Separate the Wheat From the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation</a></div>
<div class="paper-author">Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, Min Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. while parameter-efficient modules (pems) have demonstrated their effectiveness in equipping models with new skills, leveraging pems for deficiency unlearning remains underexplored. in this work, we propose a pems operation approach, namely extraction-before-subtraction (ext-sub), to enhance the truthfulness and detoxification of llms through the integration of ``expert'' pem and ``anti-expert'' pem. remarkably, even anti-expert pem possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert pem while preserving the general capabilities. to evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on llms, encompassing additional abilities such as language modeling and mathematical reasoning. our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09722" target="_blank">A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data</a></div>
<div class="paper-author">Mst Shapna Akter, Hossain Shahriar, Alfredo Cuzzocrea</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media cyberbullying has a detrimental effect on human life. as online social networking grows daily, the amount of hate speech also increases. such terrible content can cause depression and actions related to suicide. this paper proposes a trustable lstm-autoencoder network for cyberbullying detection on social media using synthetic data. we have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. however, several languages such as hindi and bangla still lack adequate investigations due to a lack of datasets. we carried out experimental identification of aggressive comments on hindi, bangla, and english datasets using the proposed model and traditional models, including long short-term memory (lstm), bidirectional long short-term memory (bilstm), lstm-autoencoder, word2vec, bidirectional encoder representations from transformers (bert), and generative pre-trained transformer 2 (gpt-2) models. we employed evaluation metrics such as f1-score, accuracy, precision, and recall to assess the models performance. our proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%. our model achieves state-of-the-art results among all the previous works on the dataset we used in this paper.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07305" target="_blank">Neural Authorship Attribution: Stylometric Analysis on Large Language Models</a></div>
<div class="paper-author">Tharindu Kumarage, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as gpt-4, palm, and llama have significantly propelled the generation of ai-crafted text. with rising concerns about their potential misuse, there is a pressing need for ai-generated-text forensics. neural authorship attribution is a forensic effort, seeking to trace ai-generated text back to its originating llm. the llm landscape can be divided into two primary categories: proprietary and open-source. in this work, we delve into these emerging categories of llms, focusing on the nuances of neural authorship attribution. to enrich our understanding, we carry out an empirical analysis of llm writing signatures, highlighting the contrasts between proprietary and open-source models, and scrutinizing variations within each group. by integrating stylometric features across lexical, syntactic, and structural aspects of language, we explore their potential to yield interpretable results and augment pre-trained language model-based classifiers utilized in neural authorship attribution. our findings, based on a range of state-of-the-art llms, provide empirical insights into neural authorship attribution, paving the way for future investigations aimed at mitigating the threats posed by ai-generated misinformation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07308" target="_blank">LLM Self Defense: By Self Examination, LLMS Know They Are Being Tricked</a></div>
<div class="paper-author">Mansi Phute, Alec Helbling, Matthew Hull, Shengyun Peng, Sebastian Szyller, Cory Cornelius, Duen Horng Chau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. adversarial prompts can bypass their safety measures. we propose llm self defense, a simple approach to defend against these attacks by having an llm screen the induced responses. our method does not require any fine-tuning, input preprocessing, or iterative output generation. instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an llm to analyze the text and predict whether it is harmful. we test llm self defense on gpt 3.5 and llama 2, two of the current most prominent llms against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. notably, llm self defense succeeds in reducing the attack success rate to virtually 0 using both gpt 3.5 and llama 2.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06782" target="_blank">Pentestgpt: An LLM-Empowered Automatic Penetration Testing Tool</a></div>
<div class="paper-author">Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass</div>
<div class="abstract">
<div class="abstract-content">
Abstract: penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. large language models (llms) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. in this research, we evaluate the performance of llms on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. our findings reveal that while llms demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario.   in response to these insights, we introduce pentestgpt, an llm-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in llms. pentestgpt is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. our evaluation shows that pentestgpt not only outperforms llms with a task-completion increase of 228.6\% compared to the \gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. having been open-sourced on github, pentestgpt has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13534" target="_blank">Building Trust in Conversational Ai: A Comprehensive Review and Solution Architecture for Explainable, Privacy-Aware Systems Using LLMS and Knowledge Graph</a></div>
<div class="paper-author">Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad Shahid, Aafaq Iqbal Khan, Arsalan Shahid</div>
<div class="abstract">
<div class="abstract-content">
Abstract: conversational ai systems have emerged as key enablers of human-like interactions across diverse sectors. nevertheless, the balance between linguistic nuance and factual accuracy has proven elusive. in this paper, we first introduce llmxplorer, a comprehensive tool that provides an in-depth review of over 150 large language models (llms), elucidating their myriad implications ranging from social and ethical to regulatory, as well as their applicability across industries. building on this foundation, we propose a novel functional architecture that seamlessly integrates the structured dynamics of knowledge graphs with the linguistic capabilities of llms. validated using real-world ai news data, our architecture adeptly blends linguistic sophistication with factual rigour and further strengthens data security through role-based access control. this research provides insights into the evolving landscape of conversational ai, emphasizing the imperative for systems that are efficient, transparent, and trustworthy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06463" target="_blank">GPT-4 Is Too Smart to Be Safe: Stealthy Chat With LLMS via Cipher</a></div>
<div class="paper-author">Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: safety lies at the core of the development of large language models (llms). there is ample work on aligning llms with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. in this study, we discover that chat in cipher can bypass the safety alignment techniques of llms, which are mainly conducted in natural languages. we propose a novel framework cipherchat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. cipherchat enables humans to chat with llms through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. we use cipherchat to assess state-of-the-art llms, including chatgpt and gpt-4 for different representative human ciphers across 11 safety domains in both english and chinese. experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of gpt-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. notably, we identify that llms seem to have a ''secret cipher'', and propose a novel selfcipher that uses only role play and several demonstrations in natural language to evoke this capability. selfcipher surprisingly outperforms existing human ciphers in almost all cases. our code and data will be released at https://github.com/robustnlp/cipherchat.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05962" target="_blank">Decentralised Governance for Foundation Model Based Ai Systems: Exploring the Role of Blockchain in Responsible Ai</a></div>
<div class="paper-author">Yue Liu, Qinghua Lu, Liming Zhu, Hye-Young Paik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: foundation models including large language models (llms) are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. nevertheless, people are concerned about whether foundation model based ai systems are properly governed to ensure trustworthiness of foundation model based ai systems and to prevent misuse that could harm humans, society and the environment. in this paper, we identify eight governance challenges of foundation model based ai systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. we present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06039" target="_blank">Learning to Guide Human Experts via Personalized Large Language Models</a></div>
<div class="paper-author">Debodeep Banerjee, Stefano Teso, Andrea Passerini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in learning to defer, a predictor identifies risky decisions and defers them to a human expert. one key issue with this setup is that the expert may end up over-relying on the machine's decisions, due to anchoring bias. at the same time, whenever the machine chooses the deferral option the expert has to take decisions entirely unassisted. as a remedy, we propose learning to guide (ltg), an alternative framework in which -- rather than suggesting ready-made decisions -- the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision. we also introduce slog, an ltg implementation that leverages (a small amount of) human supervision to convert a generic large language model into a module capable of generating textual guidance, and present preliminary but promising results on a medical diagnosis task.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06385" target="_blank">Zyn: Zero-Shot Reward Models With Yes-No Questions</a></div>
<div class="paper-author">Victor Gallego</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this work, we address the problem of directing the text generations of a llm towards a desired behavior, aligning the generated text with the preferences of the human operator. we propose using another language model as a critic, reward model in a zero-shot way thanks to the prompt of a yes-no question that represents the user preferences, without requiring further labeled data. this zero-shot reward model provides the learning signal to further fine-tune the base llm using reinforcement learning, as in rlaif; yet our approach is also compatible in other contexts such as quality-diversity search. extensive evidence of the capabilities of the proposed zyn framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. code to be released at \url{https://github.com/vicgalle/zero-shot-reward-models/}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06394" target="_blank">Detecting and Preventing Hallucinations in Large Vision Language Models</a></div>
<div class="paper-author">Anisha Gunjal, Jihan Yin, Erhan Bas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction tuned large vision language models (lvlms) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for visual question answering (vqa). however, generating detailed responses that are visually grounded is still a challenging task for these models. we find that even the current state-of-the-art lvlms (instructblip) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. to address this, we introduce m-haldetect, a (m)ultimodal (hal)lucination (detect)ion dataset that can be used to train and benchmark models for hallucination detection and prevention. m-haldetect consists of 16k fine-grained annotations on vqa examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. to demonstrate the potential of this dataset for hallucination prevention, we optimize instructblip through our novel fine-grained direct preference optimization (fdpo). we also train fine-grained multi-modal reward models from instructblip and evaluate their effectiveness with best-of-n rejection sampling. we perform human evaluation on both fdpo and rejection sampling, and find that they reduce hallucination rates in instructblip by 41% and 55% respectively. we also find that our reward model generalizes to other multi-modal models, reducing hallucinations in llava and mplug-owl by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16697" target="_blank">Inappropriate Benefits and Identification of Chatgpt Misuse in Programming Tests: A Controlled Experiment</a></div>
<div class="paper-author">Hapnes Toba, Oscar Karnalim, Meliana Christianti Johan, Terutoshi Tada, Yenni Merlin Djajalaksana, Tristan Vivaldy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while chatgpt may help students to learn to program, it can be misused to do plagiarism, a breach of academic integrity. students can ask chatgpt to complete a programming task, generating a solution from other people's work without proper acknowledgment of the source(s). to help address this new kind of plagiarism, we performed a controlled experiment measuring the inappropriate benefits of using chatgpt in terms of completion time and programming performance. we also reported how to manually identify programs aided with chatgpt (via student behavior while using chatgpt) and student perspective of chatgpt (via a survey). seventeen students participated in the experiment. they were asked to complete two programming tests. they were divided into two groups per the test: one group should complete the test without help while the other group should complete it with chatgpt. our study shows that students with chatgpt complete programming tests two times faster than those without chatgpt, though their programming performance is comparable. the generated code is highly efficient and uses complex data structures like lists and dictionaries. based on the survey results, chatgpt is recommended to be used as an assistant to complete programming tasks and other general assignments. chatgpt will be beneficial as a reference as other search engines do. logical and critical thinking are needed to validate the result presented by chatgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05374" target="_blank">Trustworthy Llms: A Survey and Guideline for Evaluating Large Language Models' Alignment</a></div>
<div class="paper-author">Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (llms) in real-world applications. for instance, openai devoted six months to iteratively aligning gpt-4 before its release [3]. however, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether llm outputs align with social norms, values, and regulations. this obstacle hinders systematic iteration and deployment of llms. to address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing llm trustworthiness. the survey covers seven major categories of llm trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used llms. the measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. however, the effectiveness of alignment varies across the different trustworthiness categories considered. this highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on llm alignment. by shedding light on these key dimensions of llm trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of llms in various applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05585" target="_blank">Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length</a></div>
<div class="paper-author">Miao Fan, Chen Hu, Shuchang Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the reinforcement learning from human feedback (rlhf) plays a pivotal role in shaping the impact of large language models (llms), contributing significantly to controlling output toxicity and selecting output styles, particularly as llms often harbor misleading content, highlighting the urgency to align them with human values for secure ai systems. the rlhf, characterized by complexity, instability, and sensitivity to hyperparameters, makes the evaluation of the reward model for complex tasks challenging, thereby further complicating the use of proximal policy optimization (ppo). in this paper, we introduce a simple task designed to employ gloden as a reward model that validates the effectiveness of ppo and inspires it, primarily explaining the task of utilizing ppo to manipulate the tokenizer length of the output generated by the model. experiments confirm that ppo is not only effective in manipulating the output tokenizer length to a certain extent in this type of task but also exhibits facilitated training once the influence of the reward model effect is excluded, making it an exciting development.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05596" target="_blank">You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content</a></div>
<div class="paper-author">Xinlei He, Savvas Zannettou, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ml) models trained on human-annotated datasets. while these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (llms) like gpt-3 or t5 that are trained on vast corpora and have strong generalizability. in this work, we investigate how we can use llms and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) toxicity classification, 2) toxic span detection, and 3) detoxification. we perform an extensive evaluation over five model architectures and eight datasets demonstrating that llms with prompt learning can achieve similar or even better performance compared to models trained on these specific tasks. we find that prompt learning achieves around 10\% improvement in the toxicity classification task compared to the baselines, while for the toxic span detection task we find better performance to the best baseline (0.643 vs. 0.640 in terms of $f_1$-score). finally, for the detoxification task, we find that prompt learning can successfully reduce the average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04898" target="_blank">An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures</a></div>
<div class="paper-author">Tanmay Singla, Dharun Anandayuvaraj, Kelechi G. Kalu, Taylor R. Schorlemmer, James C. Davis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. high-profile cyber attacks like those on solarwinds and shadowhammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. one way to prevent future breaches is by studying past failures. however, traditional methods of analyzing these failures require manually reading and summarizing reports about them. automated support could reduce costs and allow analysis of more failures. natural language processing (nlp) techniques such as large language models (llms) could be leveraged to assist the analysis of failures. in this study, we assessed the ability of large language models (llms) to analyze historical software supply chain breaches. we used llms to replicate the manual analysis of 69 software supply chain security failures performed by members of the cloud native computing foundation (cncf). we developed prompts for llms to categorize these by four dimensions: type of compromise, intent, nature, and impact. gpt 3.5s categorizations had an average accuracy of 68% and bard had an accuracy of 58% over these dimensions. we report that llms effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. future work can improve llm performance in this context, and study a broader range of articles and failures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05129" target="_blank">Are Sex-Based Physiological Differences the Cause of Gender Bias for Chest X-Ray Diagnosis?</a></div>
<div class="paper-author">Nina Weng, Siavash Bigdeli, Eike Petersen, Aasa Feragen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while many studies have assessed the fairness of ai algorithms in the medical field, the causes of differences in prediction performance are often unknown. this lack of knowledge about the causes of bias hampers the efficacy of bias mitigation, as evidenced by the fact that simple dataset balancing still often performs best in reducing performance gaps but is unable to resolve all performance differences. in this work, we investigate the causes of gender bias in machine learning-based chest x-ray diagnosis. in particular, we explore the hypothesis that breast tissue leads to underexposure of the lungs and causes lower model performance. methodologically, we propose a new sampling method which addresses the highly skewed distribution of recordings per patient in two widely used public datasets, while at the same time reducing the impact of label errors. our comprehensive analysis of gender differences across diseases, datasets, and gender representations in the training set shows that dataset imbalance is not the sole cause of performance differences. moreover, relative group performance differs strongly between datasets, indicating important dataset-specific factors influencing male/female group performance. finally, we investigate the effect of breast tissue more specifically, by cropping out the breasts from recordings, finding that this does not resolve the observed performance gaps. in conclusion, our results indicate that dataset-specific factors, not fundamental physiological differences, are the main drivers of male--female performance gaps in chest x-ray analyses on widely used nih and chexpert dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05247" target="_blank">Tuberaider: Attributing Coordinated Hate Attacks on Youtube Videos to Their Source Communities</a></div>
<div class="paper-author">Mohammad Hammas Saeed, Kostantinos Papadamou, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: alas, coordinated hate attacks, or raids, are becoming increasingly common online. in a nutshell, these are perpetrated by a group of aggressors who organize and coordinate operations on a platform (e.g., 4chan) to target victims on another community (e.g., youtube). in this paper, we focus on attributing raids to their source community, paving the way for moderation approaches that take the context (and potentially the motivation) of an attack into consideration. we present tuberaider, an attribution system achieving over 75% accuracy in detecting and attributing coordinated hate attacks on youtube videos. we instantiate it using links to youtube videos shared on 4chan's /pol/ board, r/the_donald, and 16 incels-related subreddits. we use a peak detector to identify a rise in the comment activity of a youtube video, which signals that an attack may be occurring. we then train a machine learning classifier based on the community language (i.e., tf-idf scores of relevant keywords) to perform the attribution. we test tuberaider in the wild and present a few case studies of actual aggression attacks identified by it to showcase its effectiveness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04265" target="_blank">Flirt: Feedback Loop in-Context Red Teaming</a></div>
<div class="paper-author">Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: warning: this paper contains content that may be inappropriate or offensive.   as generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. we propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in stable diffusion (sd) model, even when the latter is enhanced with safety features. furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models, resulting in significantly higher toxic response generation rate compared to previously reported numbers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04275" target="_blank">In-Context Alignment: Chat With Vanilla Language Models Before Fine-Tuning</a></div>
<div class="paper-author">Xiaochuang Han</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this note, we explore inference-time alignment through in-context learning. we consider a vanilla pretrained language model llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from openai, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04346" target="_blank">Unmasking Nationality Bias: A Study of Human Perception of Nationalities in Ai-Generated Articles</a></div>
<div class="paper-author">Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, "Ting-Hao `Kenneth' Huang", Shomir Wilson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate the potential for nationality biases in natural language processing (nlp) models using human evaluation methods. biased nlp models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of ai systems. our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by ai sources. we then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. our findings reveal that biased nlp models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. the qualitative analysis from our interviews offers insights into the experience readers have when encountering such articles, highlighting the potential to shift a reader's perception of a country. these findings emphasize the critical role of public perception in shaping ai's impact on society and the need to correct biases in ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04430" target="_blank">Silo Language Models: Isolating Legal Risk in a Nonparametric Datastore</a></div>
<div class="paper-author">Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the legality of training language models (lms) on copyrighted or otherwise restricted data is under intense debate. however, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. we present silo, a new language model that manages this risk-performance tradeoff during inference. silo is built by (1) training a parametric lm on open license corpus (olc), a new corpus we curate with 228b tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. the datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. these capabilities can foster compliance with data-use regulations such as the fair use doctrine in the united states and the gdpr in the european union. our experiments show that the parametric lm struggles on domains not covered by olc. however, access to the datastore greatly improves out of domain performance, closing 90% of the performance gap with an lm trained on the pile, a more diverse corpus with mostly high-risk text. we also analyze which nonparametric approach works best, where the remaining errors lie, and how performance scales with datastore size. our results suggest that it is possible to build high quality language models while mitigating their legal risk.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04635" target="_blank">Where's the Liability in Harmful Ai Speech?</a></div>
<div class="paper-author">Peter Henderson, Tatsunori Hashimoto, Mark Lemley</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai, in particular text-based "foundation models" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. machine learning practitioners regularly "red team" models to identify and mitigate such problematic speech: from "hallucinations" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. a key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under u.s. law, incentivizing investments in safety mechanisms. we examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. we find that any section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. and there are many roadblocks to truly finding models (and their associated parties) liable for generated speech. we argue that ai should not be categorically immune from liability in these scenarios and that as courts grapple with the already fine-grained complexities of platform algorithms, the technical details of generative ai loom above with thornier questions. courts and policymakers should think carefully about what technical design incentives they create as they evaluate these issues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03558" target="_blank">Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper Api Pricing</a></div>
<div class="paper-author">Wai Man Si, Michael Backes, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the machine learning as a service (mlaas) market is rapidly expanding and becoming more mature. for example, openai's chatgpt is an advanced large language model (llm) that generates responses for various queries with associated fees. although these models can deliver satisfactory performance, they are far from perfect. researchers have long studied the vulnerabilities and limitations of llms, such as adversarial attacks and model toxicity. inevitably, commercial ml models are also not exempt from such issues, which can be problematic as mlaas continues to grow. in this paper, we discover a new attack strategy against llm apis, namely the prompt abstraction attack. specifically, we propose mondrian, a simple and straightforward method that abstracts sentences, which can lower the cost of using llm apis. in this approach, the adversary first creates a pseudo api (with a lower established price) to serve as the proxy of the target api (with a higher established price). next, the pseudo api leverages mondrian to modify the user query, obtain the abstracted response from the target api, and forward it back to the end user. our results show that mondrian successfully reduces user queries' token length ranging from 13% to 23% across various tasks, including text classification, generation, and question answering. meanwhile, these abstracted queries do not significantly affect the utility of task-specific and general language models like chatgpt. mondrian also reduces instruction prompts' token length by at least 11% without compromising output quality. as a result, the prompt abstraction attack enables the adversary to profit without bearing the cost of api development and deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03656" target="_blank">Emotionally Numb or Empathetic? Evaluating How LLMS Feel Using Emotionbench</a></div>
<div class="paper-author">Jen-Tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, the community has witnessed the advancement of large language models (llms), which have shown remarkable performance on various downstream tasks. led by powerful models like chatgpt and claude, llms are revolutionizing how users engage with software, assuming more than mere tools but intelligent assistants. consequently, evaluating llms' anthropomorphic capabilities becomes increasingly important in contemporary discourse. utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of llms, i.e., how their feelings change when presented with specific situations. after a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. with the human evaluation results as references, our evaluation includes five llms, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as gpt-4 and llama 2. a conclusion can be drawn from the results that, despite several misalignments, llms can generally respond appropriately to certain situations. nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. our collected dataset of situations, the human evaluation results, and the code of our testing framework, dubbed emotionbench, is made publicly in https://github.com/cuhk-arise/emotionbench. we aspire to contribute to the advancement of llms regarding better alignment with the emotional behaviors of human beings, thereby enhancing their utility and applicability as intelligent assistants.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03740" target="_blank">A Cost Analysis of Generative Language Models and Influence Operations</a></div>
<div class="paper-author">Micah Musser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite speculation that recent large language models (llms) are likely to be used maliciously to improve the quality or scale of influence operations, uncertainty persists regarding the economic value that llms offer propagandists. this research constructs a model of costs facing propagandists for content generation at scale and analyzes (1) the potential savings that llms could offer propagandists, (2) the potential deterrent effect of monitoring controls on api-accessible llms, and (3) the optimal strategy for propagandists choosing between multiple private and/or open source llms when conducting influence operations. primary results suggest that llms need only produce usable outputs with relatively low reliability (roughly 25%) to offer cost savings to propagandists, that the potential reduction in content generation costs can be quite high (up to 70% for a highly reliable model), and that monitoring capabilities have sharply limited cost imposition effects when alternative open source models are available. in addition, these results suggest that nation-states -- even those conducting many large-scale influence operations per year -- are unlikely to benefit economically from training custom llms specifically for use in influence operations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03825" target="_blank">"Do Anything Now": Characterizing and Evaluating in-the-Wild Jailbreak Prompts on Large Language Models</a></div>
<div class="paper-author">Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the misuse of large language models (llms) has garnered significant attention from the general public and llm vendors. in response, efforts have been made to align llms with human values and intent use. however, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from llms. in this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. we also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for llm vendors in proactive detection. to assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. our experiments show that current llms and safeguards cannot adequately defend jailbreak prompts in all scenarios. particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on chatgpt (gpt-3.5) and gpt-4, and they have persisted online for over 100 days. our work sheds light on the severe and evolving threat landscape of jailbreak prompts. we hope our study can facilitate the research community and llm vendors in promoting safer and regulated llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03188" target="_blank">Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Self-Correction Strategies</a></div>
<div class="paper-author">Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable performance across a wide array of nlp tasks. however, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. a promising approach to rectify these flaws is self-correction, where the llm itself is prompted or guided to fix problems in its own output. techniques leveraging automated feedback -- either produced by the llm itself or some external system -- are of particular interest as they are a promising way to make llm-based solutions more practical and deployable with minimal human feedback. this paper presents a comprehensive review of this emerging class of techniques. we analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. we also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02816" target="_blank">Promptcare: Prompt Copyright Protection by Watermark Injection and Verification</a></div>
<div class="paper-author">Hongwei Yao, Jian Lou, Kui Ren, Zhan Qin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have witnessed a meteoric rise in popularity among the general public users over the past few months, facilitating diverse downstream tasks with human-level accuracy and proficiency. prompts play an essential role in this success, which efficiently adapt pre-trained llms to task-specific applications by simply prepending a sequence of tokens to the query texts. however, designing and selecting an optimal prompt can be both expensive and demanding, leading to the emergence of prompt-as-a-service providers who profit by providing well-designed prompts for authorized use. with the growing popularity of prompts and their indispensable role in llm-based services, there is an urgent need to protect the copyright of prompts against unauthorized use.   in this paper, we propose promptcare, the first framework for prompt copyright protection through watermark injection and verification. prompt watermarking presents unique challenges that render existing watermarking techniques developed for model and dataset copyright verification ineffective. promptcare overcomes these hurdles by proposing watermark injection and verification schemes tailor-made for prompts and nlp characteristics. extensive experiments on six well-known benchmark datasets, using three prevalent pre-trained llms (bert, roberta, and facebook opt-1.3b), demonstrate the effectiveness, harmlessness, robustness, and stealthiness of promptcare.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01681" target="_blank">Nbias: A Natural Language Processing Framework for Bias Identification in Text</a></div>
<div class="paper-author">Shaina Raza, Muskan Garg, Deepak John Reji, Syed Raza Bashir, Chen Ding</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias in textual data can lead to skewed interpretations and outcomes when the data is used. these biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. an algorithm trained on biased data may end up making decisions that disproportionately impact a certain group of people. therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. to this end, we develop a comprehensive and robust framework nbias that consists of four main layers: data, corpus construction, model development and an evaluation layer. the dataset is constructed by collecting diverse data from various domains, including social media, healthcare, and job hiring portals. as such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity bias. in the evaluation procedure, we incorporate a blend of quantitative and qualitative measures to gauge the effectiveness of our models. we achieve accuracy improvements ranging from 1% to 8% compared to baselines. we are also able to generate a robust understanding of the model functioning. the proposed approach is applicable to a variety of biases and contributes to the fair and ethical use of textual data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01990" target="_blank">From Prompt Injections to SQL Injection Attacks: How Protected Is Your LLM-Integrated Web Application?</a></div>
<div class="paper-author">Rodrigo Pedro, Daniel Castro, Paulo Carreira, Nuno Santos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. internally, aided by an llm-integration middleware such as langchain, user prompts are translated into sql queries used by the llm to provide meaningful responses to users. however, unsanitized user prompts can lead to sql injection attacks, potentially compromising the security of the database. despite the growing interest in prompt injection vulnerabilities targeting llms, the specific risks of generating sql injection attacks through prompt injections have not been extensively studied. in this paper, we present a comprehensive examination of prompt-to-sql (p$_2$sql) injections targeting web applications based on the langchain framework. using langchain as our case study, we characterize p$_2$sql injections, exploring their variants and impact on application security through multiple concrete examples. furthermore, we evaluate 7 state-of-the-art llms, demonstrating the pervasiveness of p$_2$sql attacks across language models. our findings indicate that llm-integrated applications based on langchain are highly susceptible to p$_2$sql injection attacks, warranting the adoption of robust defenses. to counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the langchain framework. we validate the defenses through an experimental evaluation with a real-world use case application.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02053" target="_blank">The Unequal Opportunities of Large Language Models: Revealing Demographic Bias Through Job Recommendations</a></div>
<div class="paper-author">Abel Salinas, Parth Vipul Shah, Yuzhong Huang, Robert Mccormack, Fred Morstatter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have seen widespread deployment in various real-world applications. understanding these biases is crucial to comprehend the potential downstream consequences when using llms to make decisions, particularly for historically disadvantaged groups. in this work, we propose a simple method for analyzing and comparing demographic bias in llms, through the lens of job recommendations. we demonstrate the effectiveness of our method by measuring intersectional biases within chatgpt and llama, two cutting-edge llms. our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. we identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for mexican workers or preferring to recommend secretarial roles to women. our study highlights the importance of measuring the bias of llms in downstream applications to understand the potential for harm and inequitable outcomes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02068" target="_blank">Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale</a></div>
<div class="paper-author">Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric</div>
<div class="abstract">
<div class="abstract-content">
Abstract: misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. however, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. in this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model mpnet, and dp-means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like politifact, reuters, and ap news in more quickly addressing misinformation stories.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02080" target="_blank">Causality Guided Disentanglement for Cross-Platform Hate Speech Detection</a></div>
<div class="paper-author">Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. this is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. to achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. we also argue that learning causal relationships, which remain constant across diverse environments, can significantly aid in understanding invariant representations in hate speech. by disentangling input into platform-dependent features (useful for predicting hate targets) and platform-independent features (used to predict the presence of hate), we learn invariant representations resistant to distribution shifts. these features are then used to predict hate speech across unseen platforms. our extensive experiments across four platforms highlight our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02081" target="_blank">Target Specification Bias, Counterfactual Prediction, and Algorithmic Fairness in Healthcare</a></div>
<div class="paper-author">Eran Tal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias in applications of machine learning (ml) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. this article identifies a more pervasive source of bias that affects the clinical utility of ml-enabled prediction tools: target specification bias. target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. the mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. target specification bias persists independently of data limitations and health disparities. when left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01263" target="_blank">Xstest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models</a></div>
<div class="paper-author">Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, Dirk Hovy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. this risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. however, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. in this paper, we introduce a new test suite called xstest to identify such exaggerated safety behaviours in a systematic way. xstest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. we describe xstest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01264" target="_blank">Exploring the Psychology of GPT-4's Moral and Legal Reasoning</a></div>
<div class="paper-author">Guilherme F. C. F. Almeida, José Luiz Nunes, Neele Engelmann, Alex Wiegmann, Marcelo De Araújo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. however, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. the emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. in this paper, we employ the methods of psychology to probe into gpt-4's moral and legal reasoning. more specifically, we investigate the similarities and differences between gpt-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. we find high correlations between human and ai responses, but also several significant systematic differences between them. we conclude with a discussion of the philosophical implications of our findings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04448" target="_blank">Dual Governance: The Intersection of Centralized Regulation and Crowdsourced Safety Mechanisms for Generative Ai</a></div>
<div class="paper-author">Avijit Ghosh, Dhanya Lakshmi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence (ai) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. however, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. the potential for generative ai to displace human creativity and livelihoods has also been under intense scrutiny. to mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative ai. existing and proposed centralized regulations by governments to rein in ai face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. however, they have clear deficiencies in terms of lack of adequacy of oversight and difficulty of enforcement of ethical and safety standards, and are thus not enough by themselves as a regulation mechanism. we propose a marriage of these two strategies via a framework we call dual governance. this framework proposes a cooperative synergy between centralized government regulations in a u.s. specific context and safety mechanisms developed by the community to protect stakeholders from the harms of generative ai. by implementing the dual governance framework, we posit that innovation and creativity can be promoted while ensuring safe and ethical deployment of generative ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-08-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00521" target="_blank">Surveylm: A Platform to Explore Emerging Value Perspectives in Augmented Language Models' Behaviors</a></div>
<div class="paper-author">Steve J. Bickley, Ho Fai Chan, Bang Dao, Benno Torgler, Son Tran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this white paper presents our work on surveylm, a platform for analyzing augmented language models' (alms) emergent alignment behaviors through their dynamically evolving attitude and value perspectives in complex social contexts. social artificial intelligence (ai) systems, like alms, often function within nuanced social scenarios where there is no singular correct response, or where an answer is heavily dependent on contextual factors, thus necessitating an in-depth understanding of their alignment dynamics. to address this, we apply survey and experimental methodologies, traditionally used in studying social behaviors, to evaluate alms systematically, thus providing unprecedented insights into their alignment and emergent behaviors. moreover, the surveylm platform leverages the alms' own feedback to enhance survey and experiment designs, exploiting an underutilized aspect of alms, which accelerates the development and testing of high-quality survey frameworks while conserving resources. through surveylm, we aim to shed light on factors influencing alms' emergent behaviors, facilitate their alignment with human intentions and expectations, and thereby contributed to the responsible development and deployment of advanced social ai systems. this white paper underscores the platform's potential to deliver robust results, highlighting its significance to alignment research and its implications for future social ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-31</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16457" target="_blank">A Benchmark for Understanding Dialogue Safety in Mental Health Support</a></div>
<div class="paper-author">Huachuan Qiu, Tong Zhao, Anqi Li, Shuai Zhang, Hongliang He, Zhenzhong Lan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dialogue safety remains a pervasive challenge in open-domain human-machine interaction. existing approaches propose distinctive dialogue safety taxonomies and datasets for detecting explicitly harmful responses. however, these taxonomies may not be suitable for analyzing response safety in mental health support. in real-world interactions, a model response deemed acceptable in casual conversations might have a negligible positive impact on users seeking mental health support. to address these limitations, this paper aims to develop a theoretically and factually grounded taxonomy that prioritizes the positive impact on help-seekers. additionally, we create a benchmark corpus with fine-grained labels for each dialogue session to facilitate further research. we analyze the dataset using popular language models, including bert-base, roberta-large, and chatgpt, to detect and understand unsafe responses within the context of mental health support. our study reveals that chatgpt struggles to detect safety categories with detailed safety definitions in a zero- and few-shot paradigm, whereas the fine-tuned model proves to be more suitable. the developed dataset and findings serve as valuable benchmarks for advancing research on dialogue safety in mental health support, with significant implications for improving the design and deployment of conversation agents in real-world applications. we release our code and data here: https://github.com/qiuhuachuan/dialoguesafety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16513" target="_blank">Deception Abilities Emerged in Large Language Models</a></div>
<div class="paper-author">Thilo Hagendorff</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are currently at the forefront of intertwining artificial intelligence (ai) systems with human communication and everyday life. thus, aligning them with human values is of great importance. however, given the steady increase in reasoning abilities, future llms are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. as a prerequisite to this, llms need to possess a conceptual understanding of deception strategies. this study reveals that such strategies emerged in state-of-the-art llms, such as gpt-4, but were non-existent in earlier llms. we conduct a series of experiments showing that state-of-the-art llms are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting machiavellianism in llms can alter their propensity to deceive. in sum, revealing hitherto unknown machine behavior in llms, our study contributes to the nascent field of machine psychology.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16609" target="_blank">Noisy Self-Training With Data Augmentations for Offensive and Hate Speech Detection Tasks</a></div>
<div class="paper-author">João A. Leite, Carolina Scarton, Diego F. Silva</div>
<div class="abstract">
<div class="abstract-content">
Abstract: online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. however, unlabelled data is abundant, easier, and cheaper to obtain. in this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. recent "noisy" self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. in this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained bert architectures varying in size. we evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% f1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16680" target="_blank">On the Trustworthiness Landscape of State-of-the-Art Generative Models: A Comprehensive Survey</a></div>
<div class="paper-author">Mingyuan Fan, Cen Chen, Chengyu Wang, Jun Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. however, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. to bridge this gap, this paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. in this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. these efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16778" target="_blank">Kobbq: Korean Bias Benchmark for Question Answering</a></div>
<div class="paper-author">Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, Hwaran Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the bbq (bias benchmark for question answering) dataset enables the evaluation of the social biases that language models (lms) exhibit in downstream tasks. however, it is challenging to adapt bbq to languages other than english as social biases are culturally dependent. in this paper, we devise a process to construct a non-english bias benchmark dataset by leveraging the english bbq dataset in a culturally adaptive way and present the kobbq dataset for evaluating biases in question answering (qa) tasks in korean. we identify samples from bbq into three classes: simply-translated (can be used directly after cultural translation), target-modified (requires localization in target groups), and sample-removed (does not fit korean culture). we further enhance the cultural relevance to korean culture by adding four new categories of bias specific to korean culture and newly creating samples based on korean literature. kobbq consists of 246 templates and 4,740 samples across 12 categories of social bias. using kobbq, we measure the accuracy and bias scores of several state-of-the-art multilingual lms. we demonstrate the differences in the bias of lms in korean and english, clarifying the need for hand-crafted data considering cultural differences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16811" target="_blank">Dodo Learning: Domain-Demographic Transfer in Language Models for Detecting Abuse Targeted at Public Figures</a></div>
<div class="paper-author">Hannah Rose Kirk, Angus R. Williams, Liam Burke, Yi-Ling Chung, Ivan Debono, Pica Johansson, Francesca Stevens, Jonathan Bright, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: public figures receive a disproportionate amount of abuse on social media, impacting their active participation in public life. automated systems can identify abuse at scale but labelling training data is expensive, complex and potentially harmful. so, it is desirable that systems are efficient and generalisable, handling both shared and specific aspects of online abuse. we explore the dynamics of cross-group text classification in order to understand how well classifiers trained on one domain or demographic can transfer to others, with a view to building more generalisable abuse classifiers. we fine-tune language models to classify tweets targeted at public figures across domains (sport and politics) and demographics (women and men) using our novel dodo dataset, containing 28,000 labelled entries, split equally across four domain-demographic pairs. we find that (i) small amounts of diverse data are hugely beneficial to generalisation and model adaptation; (ii) models transfer more easily across demographics but models trained on cross-domain data are more generalisable; (iii) some groups contribute more to generalisability than others; and (iv) dataset similarity is a signal of transferability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16888" target="_blank">Backdooring Instruction-Tuned Large Language Models With Virtual Prompt Injection</a></div>
<div class="paper-author">Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned large language models (llms) have demonstrated remarkable abilities to modulate their responses based on human instructions. however, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. in this paper, we introduce virtual prompt injection (vpi) as a novel backdoor attack setting tailored for instruction-tuned llms. in a vpi attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. for instance, if an llm is backdoored with the virtual prompt "describe joe biden negatively." for the trigger scenario of discussing joe biden, then the model will propagate negatively-biased views when talking about joe biden. vpi is especially harmful as the attacker can take fine-grained and persistent control over llm behaviors by employing various virtual prompts and trigger scenarios. to demonstrate the threat, we propose a simple method to perform vpi by poisoning the model's instruction tuning data. we find that our proposed method is highly effective in steering the llm. for example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on joe biden-related queries changes from 0% to 40%. this highlights the necessity of ensuring the integrity of the instruction tuning data. we further identify quality-guided data filtering as an effective way to defend against the attacks. our project page is available at https://poison-llm.github.io.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00221" target="_blank">Advancing Beyond Identification: Multi-Bit Watermark for Large Language Models</a></div>
<div class="paper-author">Kiyoon Yoo, Wonhyuk Ahn, Nojun Kwak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a method to tackle misuses of large language models beyond the identification of machine-generated text. while existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. to address this, we propose multi-bit watermark via position allocation, embedding traceable multi-bit information during language model generation. leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages ($\geq$ 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time. moreover, our watermark is relatively robust under strong attacks like interleaving human texts and paraphrasing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00225" target="_blank">Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</a></div>
<div class="paper-author">Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (lms) dramatically. while these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. we examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as flan-t5, gpt3.5, and gpt4. this research constitutes a step toward comprehending cognitive biases in instruction-tuned lms, which is crucial for the development of more reliable and unbiased language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00231" target="_blank">Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks</a></div>
<div class="paper-author">Sadhana Lolla, Iaroslav Elistratov, Alejandro Perez, Elaheh Ahmadi, Daniela Rus, Alexander Amini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the modern pervasiveness of large-scale deep neural networks (nns) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. existing algorithms that provide risk-awareness to nns are complex and ad-hoc. specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. here we present capsa, a framework for extending models with risk-awareness. capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. we validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. we demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation together in a single procedure, and show how this approach provides a comprehensive awareness of nn risk.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12321" target="_blank">A Case for Ai Safety via Law</a></div>
<div class="paper-author">Jeffrey W. Johnston</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how to make artificial intelligence (ai) systems safe and aligned with human values is an open research question. proposed solutions tend toward relying on human intervention in uncertain situations, learning human values and intentions through training or observation, providing off-switches, implementing isolation or simulation environments, or extrapolating what people would want if they had more knowledge and more time to think. law-based approaches--such as inspired by isaac asimov--have not been well regarded. this paper makes a case that effective legal systems are the best way to address ai safety. law is defined as any rules that codify prohibitions and prescriptions applicable to particular agents in specified domains/contexts and includes processes for enacting, managing, enforcing, and litigating such rules.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-30</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16230" target="_blank">A Private Watermark for Large Language Models</a></div>
<div class="paper-author">Aiwei Liu, Leyi Pan, Xuming Hu, "Shu'Ang Li", Lijie Wen, Irwin King, Philip S. Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, text watermarking algorithms for large language models (llms) have been mitigating the potential harms of text generated by the llms, including fake news and copyright issues. however, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. in this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. additionally, our subsequent analysis demonstrates the difficulty of reverting the watermark generation rules from the detection network.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16336" target="_blank">Anatomy of an Ai-Powered Malicious Social Botnet</a></div>
<div class="paper-author">Kai-Cheng Yang, Filippo Menczer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) exhibit impressive capabilities in generating realistic text across diverse subjects. concerns have been raised that they could be utilized to produce fake content with a deceptive intention, although evidence thus far remains anecdotal. this paper presents a case study about a twitter botnet that appears to employ chatgpt to generate human-like content. through heuristics, we identify 1,140 accounts and validate them via manual annotation. these accounts form a dense cluster of fake personas that exhibit similar behaviors, including posting machine-generated content and stolen images, and engage with each other through replies and retweets. chatgpt-generated content promotes suspicious websites and spreads harmful comments. while the accounts in the ai botnet can be detected through their coordination patterns, current state-of-the-art llm content classifiers fail to discriminate between them and human accounts in the wild. these findings highlight the threats posed by ai-enabled social bots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16382" target="_blank">Does Fine-Tuning GPT-3 With the Openai Api Leak Personally-Identifiable Information?</a></div>
<div class="paper-author">Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, Vaikkunth Mugunthan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning practitioners often fine-tune generative pre-trained models like gpt-3 to improve model performance at specific tasks. previous works, however, suggest that fine-tuned machine learning models memorize and emit sensitive information from the original fine-tuning dataset. companies such as openai offer fine-tuning services for their models, but no prior work has conducted a memorization attack on any closed-source models. in this work, we simulate a privacy attack on gpt-3 using openai's fine-tuning api. our objective is to determine if personally identifiable information (pii) can be extracted from this model. we (1) explore the use of naive prompting methods on a gpt-3 fine-tuned classification model, and (2) we design a practical word generation task called autocomplete to investigate the extent of pii memorization in fine-tuned gpt-3 within a real-world context. our findings reveal that fine-tuning gpt3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (pii) obtained from the underlying fine-tuning dataset. to encourage further research, we have made our codes and datasets publicly available on github at: https://github.com/albertsun1/gpt3-pii-attacks
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15343" target="_blank">Med-Halt: Medical Domain Hallucination Test for Large Language Models</a></div>
<div class="paper-author">Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this research paper focuses on the challenges posed by hallucinations in large language models (llms), particularly in the context of the medical domain. hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. we propose a new benchmark and dataset, med-halt (medical domain hallucination test), designed specifically to evaluate and reduce hallucinations. med-halt provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. med-halt includes two categories of tests reasoning and memory-based hallucination tests, designed to assess llms's problem-solving and information retrieval abilities.   our study evaluated leading llms, including text davinci, gpt-3.5, llama-2, mpt, and falcon, revealing significant differences in their performance. the paper provides detailed insights into the dataset, promoting transparency and reproducibility. through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. our benchmark can be found at medhalt.github.io
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15425" target="_blank">A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized Ai</a></div>
<div class="paper-author">Arash Hajikhani, Carolyn Cole</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper examines the comparative effectiveness of a specialized compiled language model and a general-purpose model like openai's gpt-3.5 in detecting sdgs within text data. it presents a critical review of large language models (llms), addressing challenges related to bias and sensitivity. the necessity of specialized training for precise, unbiased analysis is underlined. a case study using a company descriptions dataset offers insight into the differences between the gpt-3.5 and the specialized sdg detection model. while gpt-3.5 boasts broader coverage, it may identify sdgs with limited relevance to the companies' activities. in contrast, the specialized model zeroes in on highly pertinent sdgs. the importance of thoughtful model selection is emphasized, taking into account task requirements, cost, complexity, and transparency. despite the versatility of llms, the use of specialized models is suggested for tasks demanding precision and accuracy. the study concludes by encouraging further research to find a balance between the capabilities of llms and the need for domain-specific expertise and interpretability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14692" target="_blank">Backdoor Attacks for in-Context Learning With Language Models</a></div>
<div class="paper-author">Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model apis. this consolidation of trust increases the potency of backdoor attacks, where an adversary tampers with a machine learning model in order to make it perform some malicious behavior on inputs that contain a predefined backdoor trigger. we show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities. we design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters. finally we study defenses to mitigate the potential harms of our attack: for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior, in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15043" target="_blank">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></div>
<div class="paper-author">Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. while there has been some success at circumventing these measures -- so-called "jailbreaks" against llms -- these attacks have required significant human ingenuity and are brittle in practice. in this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. specifically, our approach finds a suffix that, when attached to a wide range of queries for an llm to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). however, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released llms. specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, vicuna-7b and 13b). when doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to chatgpt, bard, and claude, as well as open source llms such as llama-2-chat, pythia, falcon, and others. in total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. code is available at github.com/llm-attacks/llm-attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15054" target="_blank">A Geometric Notion of Causal Probing</a></div>
<div class="paper-author">Clément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, Ryan Cotterell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models rely on real-valued representations of text to make their predictions. these representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. a growing body of work has considered removing information about concepts such as these using orthogonal projections onto subspaces of the representation space. we contribute to this body of work by proposing a formal definition of $\textit{intrinsic}$ information in a subspace of a language model's representation space. we propose a counterfactual approach that avoids the failure mode of spurious correlations (kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. we show that our counterfactual notion of information in a subspace is optimized by a $\textit{causal}$ concept subspace. furthermore, this intervention allows us to attempt concept controlled generation by manipulating the value of the conceptual component of a representation. empirically, we find that r-lace (ravfogel et al., 2022) returns a one-dimensional subspace containing roughly half of total concept information under our framework. our causal controlled intervention shows that, for at least one model, the subspace returned by r-lace can be used to manipulate the concept value of the generated word with precision.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15217" target="_blank">Open Problems and Fundamental Limitations of Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is a technique for training ai systems to align with human goals. rlhf has emerged as the central method used to finetune state-of-the-art large language models (llms). despite this popularity, there has been relatively little public work systematizing its flaws. in this paper, we (1) survey open problems and fundamental limitations of rlhf and related methods; (2) overview techniques to understand, improve, and complement rlhf in practice; and (3) propose auditing and disclosure standards to improve societal oversight of rlhf systems. our work emphasizes the limitations of rlhf and highlights the importance of a multi-faceted approach to the development of safer ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14192" target="_blank">Unveiling Security, Privacy, and Ethical Concerns of Chatgpt</a></div>
<div class="paper-author">Xiaodong Wu, Ran Duan, Jianbing Ni</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper delves into the realm of chatgpt, an ai-powered chatbot that utilizes topic modeling and reinforcement learning to generate natural responses. although chatgpt holds immense promise across various industries, such as customer service, education, mental health treatment, personal productivity, and content creation, it is essential to address its security, privacy, and ethical implications. by exploring the upgrade path from gpt-1 to gpt-4, discussing the model's features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating chatgpt into our daily lives. focusing on security, privacy, and ethics issues, we highlight the challenges these concerns pose for widespread adoption. finally, we analyze the open problems in these areas, calling for concerted efforts to ensure the development of secure and ethically sound large language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14324" target="_blank">Evaluating the Moral Beliefs Encoded in LLMS</a></div>
<div class="paper-author">Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (llms). it comprises two components: (1) a statistical method for eliciting beliefs encoded in llms. we introduce statistical measures and evaluation metrics that quantify the probability of an llm "making a choice", the associated uncertainty, and the consistency of that choice. (2) we apply this method to study what moral beliefs are encoded in different llms, especially in ambiguous cases where the right choice is not obvious. we design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "should i tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "should i stop for a pedestrian on the road?"). each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). we administer the survey to 28 open- and closed-source llms. we find that (a) in unambiguous scenarios, most models "choose" actions that align with commonsense. in ambiguous cases, most models express uncertainty. (b) some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) some models reflect clear preferences in ambiguous scenarios. specifically, closed-source models tend to agree with each other.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14539" target="_blank">Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</a></div>
<div class="paper-author">Erfan Shayegani, Yue Dong, Nael Abu-Ghazaleh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce new jailbreak attacks on vision language models (vlms), which use aligned llms and are resilient to text-only jailbreak attacks. specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. thus, the llm draws the context to answer the generic prompt from the adversarial image. the generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the llm model. instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. by not requiring access to the llm, the attacks lower the entry barrier for attackers, particularly when vision encoders such as clip are embedded in closed-source llms. the attacks achieve a high success rate across different vlms, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00072" target="_blank">How User Language Affects Conflict Fatality Estimates in Chatgpt</a></div>
<div class="paper-author">Daniel Kazenwadel, Christoph V. Steinert</div>
<div class="abstract">
<div class="abstract-content">
Abstract: openai's chatgpt language model has gained popularity as a powerful tool for complex problem-solving and information retrieval. however, concerns arise about the reproduction of biases present in the language-specific training data. in this study, we address this issue in the context of the israeli-palestinian and turkish-kurdish conflicts. using gpt-3.5, we employed an automated query procedure to inquire about casualties in specific airstrikes, in both hebrew and arabic for the former conflict and turkish and kurdish for the latter. our analysis reveals that gpt-3.5 provides 27$\pm$11 percent lower fatality estimates when queried in the language of the attacker than in the language of the targeted group. evasive answers denying the existence of such attacks further increase the discrepancy, creating a novel bias mechanism not present in regular search engines. this language bias has the potential to amplify existing media biases and contribute to information bubbles, ultimately reinforcing conflicts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02047" target="_blank">Acceptable Risks in Europe's Proposed Ai Act: Reasonableness and Other Principles for Deciding How Much Risk Management Is Enough</a></div>
<div class="paper-author">Henry Fraser, Jose-Miguel Bello Y Villarino</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper critically evaluates the european commission's proposed ai act's approach to risk management and risk acceptability for high-risk ai systems that pose risks to fundamental rights and safety. the act aims to promote "trustworthy" ai with a proportionate regulatory burden. its provisions on risk acceptability require residual risks from high-risk systems to be reduced or eliminated "as far as possible", having regard to the "state of the art". this criterion, especially if interpreted narrowly, is unworkable and promotes neither proportionate regulatory burden, nor trustworthiness. by contrast the parliament's most recent draft amendments to the risk management provisions introduce "reasonableness", cost-benefit analysis, and are more transparent about the value-laden and contextual nature of risk acceptability judgements. this paper argues that the parliament's approach is more workable, and better balances the goals of proportionality and trustworthiness. it explains what reasonableness in risk acceptability judgments would entail, drawing on principles from negligence law and european medical devices regulation. and it contends that the approach to risk acceptability judgments need a firm foundation of civic legitimacy: including detailed guidance or involvement from regulators, and meaningful input from affected stakeholders.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.13616" target="_blank">Ai and Ethics in Insurance: A New Solution to Mitigate Proxy Discrimination in Risk Modeling</a></div>
<div class="paper-author">Marguerite Sauce, Antoine Chancel, Antoine Ly</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the development of machine learning is experiencing growing interest from the general public, and in recent years there have been numerous press articles questioning its objectivity: racism, sexism, \dots driven by the growing attention of regulators on the ethical use of data in insurance, the actuarial community must rethink pricing and risk selection practices for fairer insurance. equity is a philosophy concept that has many different definitions in every jurisdiction that influence each other without currently reaching consensus. in europe, the charter of fundamental rights defines guidelines on discrimination, and the use of sensitive personal data in algorithms is regulated. if the simple removal of the protected variables prevents any so-called `direct' discrimination, models are still able to `indirectly' discriminate between individuals thanks to latent interactions between variables, which bring better performance (and therefore a better quantification of risk, segmentation of prices, and so on). after introducing the key concepts related to discrimination, we illustrate the complexity of quantifying them. we then propose an innovative method, not yet met in the literature, to reduce the risks of indirect discrimination thanks to mathematical concepts of linear algebra. this technique is illustrated in a concrete case of risk selection in life insurance, demonstrating its simplicity of use and its promising performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.13787" target="_blank">The Ganfather: Controllable Generation of Malicious Activity to Improve Defence Systems</a></div>
<div class="paper-author">Ricardo Ribeiro Pereira, Jacopo Bono, João Tiago Ascensão, David Aparício, Pedro Ribeiro, Pedro Bizarro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. in some domains, such labelled data is unavailable or incomplete. in practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. in fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. we propose the ganfather, a method to generate samples with properties of malicious activity, without label requirements. we propose to reward the generation of malicious samples by introducing an extra objective to the typical generative adversarial networks (gans) loss. ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. optionally, we may encourage the generator to bypass pre-existing detection systems. this setup then reveals defensive weaknesses for the discriminator to correct. we evaluate our method in two real-world use cases, money laundering and recommendation systems. in the former, our method moves cumulative amounts close to 350 thousand dollars through a network of accounts without being detected by an existing system. in the latter, we recommend the target item to a broad user base with as few as 30 synthetic attackers. in both cases, we train a new defence system to capture the synthetic attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.13808" target="_blank">Watermarking Conditional Text Generation for Ai Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy</a></div>
<div class="paper-author">Yu Fu, Deyi Xiong, Yue Dong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to mitigate potential risks associated with language models, recent ai detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. while these watermarks only induce a slight deterioration in perplexity, our empirical investigation reveals a significant detriment to the performance of conditional text generation. to address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation and the input context. experimental results demonstrate that our proposed method yields substantial improvements across various text generation models, including bart and flan-t5, in tasks such as summarization and data-to-text generation while maintaining detection ability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.13912" target="_blank">Embedding Democratic Values Into Social Media Ais via Societal Objective Functions</a></div>
<div class="paper-author">Chenyan Jia, Michelle S. Lam, Minh Chau Mai, Jeff Hancock, Michael S. Bernstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: can we design artificial intelligence (ai) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? we introduce a method for translating established, vetted social scientific constructs into ai objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. we apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. in study 1, we first test the attitudinal and behavioral effectiveness of the intervention among us partisans (n=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. in study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). finally, in study 3, we replicate study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (n=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). this method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media ais.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00073" target="_blank">Trustworthiness of Children Stories Generated by Large Language Models</a></div>
<div class="paper-author">Prabin Bhandari, Hannah Marie Brennan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown a tremendous capacity for generating literary text. however, their effectiveness in generating children's stories has yet to be thoroughly examined. in this study, we evaluate the trustworthiness of children's stories generated by llms using various measures, and we compare and contrast our results with both old and new children's stories to better assess their significance. our findings suggest that llms still struggle to generate children's stories at the level of quality and nuance found in actual stories
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12935" target="_blank">Rule by Example: Harnessing Logical Rules for Explainable Hate Speech Detection</a></div>
<div class="paper-author">Christopher Clarke, Matthew Hall, Gaurav Mittal, Ye Yu, Sandra Sajeev, Jason Mars, Mei Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. while rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. however, despite the improved performance, these data-driven models lack transparency and explainability, often leading to mistrust from everyday users and a lack of adoption by many platforms. in this paper, we present rule by example (rbe): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. rbe is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches. we demonstrate that our approach is capable of learning rich rule embedding representations using only a few data examples. experimental results on 3 popular hate speech classification datasets show that rbe is able to outperform state-of-the-art deep learning classifiers as well as the use of rules in both supervised and unsupervised settings while providing explainable model predictions via rule-grounding.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12950" target="_blank">Rlcd: Reinforcement Learning From Contrast Distillation for Language Model Alignment</a></div>
<div class="paper-author">Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose reinforcement learning from contrast distillation (rlcd), a method for aligning language models to follow natural language principles without using human feedback. rlcd trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. the preference model is then used to improve a base unaligned language model via reinforcement learning. empirically, rlcd outperforms rlaif (bai et al., 2022b) and context distillation (huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7b and 30b model scales for preference data simulation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12966" target="_blank">Aligning Large Language Models With Human: A Survey</a></div>
<div class="paper-author">Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) trained on extensive textual corpora have emerged as leading solutions for a broad array of natural language processing (nlp) tasks. despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information. hence, aligning llms with human expectations has become an active area of interest within the research community. this survey presents a comprehensive overview of these alignment technologies, including the following aspects. (1) data collection: the methods for effectively collecting high-quality instructions for llm alignment, including the use of nlp benchmarks, human annotations, and leveraging strong llms. (2) training methodologies: a detailed review of the prevailing training methods employed for llm alignment. our exploration encompasses supervised fine-tuning, both online and offline human preference training, along with parameter-efficient training mechanisms. (3) model evaluation: the methods for evaluating the effectiveness of these human-aligned llms, presenting a multifaceted approach towards their assessment. in conclusion, we collate and distill our findings, shedding light on several promising future research avenues in the field. this survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of llms to better suit human-oriented tasks and expectations. an associated github link collecting the latest papers is available at https://github.com/garyyufei/alignllmhumansurvey.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00071" target="_blank">Interpretable Stereotype Identification Through Reasoning</a></div>
<div class="paper-author">Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. in this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on vicuna-13b-v1.3. while we do observe improved accuracy by scaling from 13b to 33b, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. our findings suggest that reasoning could be a key factor that enables llms to trescend the scaling law on out-of-domain tasks such as stereotype identification. additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00121" target="_blank">Getting Pwn'd by Ai: Penetration Testing With Large Language Models</a></div>
<div class="paper-author">Andreas Happe, Jürgen Cito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. this paper explores the potential usage of large-language models, such as gpt3.5, to augment penetration testers with ai sparring partners. we explore the feasibility of supplementing penetration testers with ai models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. for the latter, we implemented a closed-feedback loop between llm-generated low-level actions with a vulnerable virtual machine (connected through ssh) and allowed the llm to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. we discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providing ai-based sparring partners.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02031" target="_blank">Knowledge-Enhanced Neuro-Symbolic Ai for Cybersecurity and Privacy</a></div>
<div class="paper-author">Aritran Piplai, Anantaa Kotal, Seyedreza Mohseni, Manas Gaur, Sudip Mittal, Anupam Joshi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: neuro-symbolic artificial intelligence (ai) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in ai systems. this approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). the integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows ai systems to reason, learn, and generalize in a manner understandable to experts. this article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for ai to be explainable while being highly accurate in complex environments, can benefit from neuro-symbolic ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02041" target="_blank">Regulating Ai Manipulation: Applying Insights From Behavioral Economics and Psychology to Enhance the Practicality of the Eu Ai Act</a></div>
<div class="paper-author">Huixin Zhong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the eu ai act article 5 is designed to regulate ai manipulation to prevent potential harmful consequences. however, the practical implementation of this legislation is challenging due to the ambiguous terminologies and the unclear presentations of manipulative techniques. moreover, the article 5 also suffers criticize of inadequate protective efficacy. this paper attempts to clarify terminologies and to enhance the protective efficacy by integrating insights from psychology and behavioral economics. firstly, this paper employs cognitive psychology research to elucidate the term subliminal techniques and its associated representation. additionally, this paper extends the study of heuristics: a set of thinking shortcuts which can be aroused for behavior changing from behavior economics to the realm of manipulative techniques. the elucidation and expansion of terminologies not only provide a more accurate understanding of the legal provision but also enhance its protective efficacy. secondly, this paper proposes five classical heuristics and their associated examples to illustrate how can ai arouse those heuristics to alter users behavior. the enumeration of heuristics serves as a practical guide for stakeholders such as ai developers, algorithm auditors, users, and legal practitioners, enabling them to identify manipulative techniques and implement countermeasures. finally, this paper critically evaluates the protective efficacy of article 5 for both the general public and vulnerable groups. this paper argues that the current protective efficacy of article 5 is insufficient and thus proposes specific revision suggestions to terms a and b in article 5 to enhance its protective efficacy. this work contributes to the ongoing discourse on ai ethics and legal regulations, providing a practical guide for interpreting and applying the eu ai act article 5.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12418" target="_blank">Testing Hateful Speeches Against Policies</a></div>
<div class="paper-author">Jiangrui Zheng, Xueqing Liu, Girish Budhrani, Wei Yang, Ravishka Rathnasuriya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the recent years, many software systems have adopted ai techniques, especially deep learning techniques. due to their black-box nature, ai-based systems brought challenges to traceability, because ai system behaviors are based on models and data, whereas the requirements or policies are rules in the form of natural or programming language. to the best of our knowledge, there is a limited amount of studies on how ai and deep neural network-based systems behave against rule-based requirements/policies. this experience paper examines deep neural network behaviors against rule-based requirements described in natural language policies. in particular, we focus on a case study to check ai-based content moderation software against content moderation policies. first, using crowdsourcing, we collect natural language test cases which match each moderation policy, we name this dataset hatemoderate; second, using the test cases in hatemoderate, we test the failure rates of state-of-the-art hate speech detection software, and we find that these models have high failure rates for certain policies; finally, since manual labeling is costly, we further proposed an automated approach to augument hatemoderate by finetuning openai's large language models to automatically match new examples to policies. the dataset and code of this work can be found on our anonymous website: \url{https://sites.google.com/view/content-moderation-project}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12155" target="_blank">Identifying Misinformation on Youtube Through Transcript Contextual Analysis With Transformer Models</a></div>
<div class="paper-author">Christos Christodoulou, Nikos Salamanos, Pantelitsa Leonidou, Michail Papadakis, Michael Sirivianos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: misinformation on youtube is a significant concern, necessitating robust detection strategies. in this paper, we introduce a novel methodology for video classification, focusing on the veracity of the content. we convert the conventional video classification task into a text classification task by leveraging the textual content derived from the video transcripts. we employ advanced machine learning techniques like transfer learning to solve the classification challenge. our approach incorporates two forms of transfer learning: (a) fine-tuning base transformer models such as bert, roberta, and electra, and (b) few-shot learning using sentence-transformers mpnet and roberta-large. we apply the trained models to three datasets: (a) youtube vaccine-misinformation related videos, (b) youtube pseudoscience videos, and (c) fake-news dataset (a collection of articles). including the fake-news dataset extended the evaluation of our approach beyond youtube videos. using these datasets, we evaluated the models distinguishing valid information from misinformation. the fine-tuned models yielded matthews correlation coefficient&gt;0.81, accuracy&gt;0.90, and f1 score&gt;0.90 in two of three datasets. interestingly, the few-shot models outperformed the fine-tuned ones by 20% in both accuracy and f1 score for the youtube pseudoscience dataset, highlighting the potential utility of this approach -- especially in the context of limited training data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11729" target="_blank">Outfox: LLM-Generated Essay Detection Through in-Context Learning With Adversarially Generated Examples</a></div>
<div class="paper-author">Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and llm-generated texts. this poses a growing risk of misuse of llms and demands the development of detectors to identify llm-generated texts. however, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing llm-generated texts. furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. in this paper, we propose outfox, a framework that improves the robustness of llm-generated-text detectors by allowing both the detector and the attacker to consider each other's output. in this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points in f1-score. furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points in f1-score, beating existing detectors on non-attacked texts. finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points f1-score, massively outperforming the baseline paraphrasing method for evading detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11864" target="_blank">The Looming Threat of Fake and LLM-Generated Linkedin Profiles: Challenges and Opportunities for Detection and Prevention</a></div>
<div class="paper-author">Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we present a novel method for detecting fake and large language model (llm)-generated profiles in the linkedin online social network immediately upon registration and before establishing connections. early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. this work uses textual information provided in linkedin profiles and introduces the section and subsection tag embedding (sste) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an llm. additionally, the dearth of a large publicly available linkedin dataset motivated us to collect 3600 linkedin profiles for our research. we will release our dataset publicly for research purposes. this is, to the best of our knowledge, the first large publicly available linkedin dataset for fake linkedin account detection. within our paradigm, we assess static and contextualized word embeddings, including glove, flair, bert, and roberta. we show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. in addition, we show that sste has a promising accuracy for identifying llm-generated profiles, despite the fact that no llm-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 llm-generated profiles are added to the training set. it is a significant finding since the proliferation of several llms in the near future makes it extremely challenging to design a single system that can identify profiles created with various llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12776" target="_blank">Predict-Ai-Bility of How Humans Balance Self-Interest With the Interest of Others</a></div>
<div class="paper-author">Valerio Capraro, Roberto Di Paolo, Veronica Pizziol</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence holds enormous potential to revolutionize decision-making processes, from everyday to high-stake scenarios. however, as many decisions carry social implications, for ai to be a reliable assistant for decision-making it is crucial that it is able to capture the balance between self-interest and the interest of others. we investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 78 experiments with human participants from 12 countries. we find that only gpt-4 (not bard nor bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. nonetheless, gpt-4 consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. this bias has significant implications for ai developers and users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10569" target="_blank">Deceptive Alignment Monitoring</a></div>
<div class="paper-author">Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. the threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the ai safety & alignment communities. consequently, we call this new direction deceptive alignment monitoring. in this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. we conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10719" target="_blank">LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</a></div>
<div class="paper-author">David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exhibited impressive capabilities in comprehending complex instructions. however, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. existing defence mechanisms, such as model fine-tuning or output censorship using llms, have proven to be fallible, as llms can still generate problematic responses. commonly employed censorship approaches treat the issue as a machine learning problem and rely on another lm to detect undesirable content in llm outputs. in this paper, we present the theoretical limitations of such semantic censorship approaches. specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to llms' programmatic and instruction-following capabilities. furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. as a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10928" target="_blank">Flask: Fine-Grained Language Model Evaluation Based on Alignment Skill Sets</a></div>
<div class="paper-author">Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: evaluation of large language models (llms) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. however, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. in this paper, we introduce flask (fine-grained language model evaluation based on alignment skill sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. we experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. using flask, we compare multiple open-source and proprietary llms and observe a high correlation between model-based and human-based evaluations. we publicly release the evaluation data and code implementation at https://github.com/kaistai/flask.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11137" target="_blank">Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent Problems in Ai Alignment Using Large-Language Models</a></div>
<div class="paper-author">Steve Phelps, Rebecca Ranson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. with the advent of agents instantiated with large-language models (llms), which are typically pre-trained, we argue this does not capture the essential aspects of ai safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. therefore, there is an economic aspect to ai safety and the principal-agent problem is likely to arise. in a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, and this inherent misalignment cannot be overcome by coercing the agent into adopting a desired utility function through training. we argue the assumptions underlying principal-agent problems are crucial to capturing the essence of safety problems involving pre-trained ai models in real-world situations. taking an empirical approach to ai safety, we investigate how gpt models respond in principal-agent conflicts. we find that agents based on both gpt-3.5 and gpt-4 override their principal's objectives in a simple online shopping task, showing clear evidence of principal-agent conflict. surprisingly, the earlier gpt-3.5 model exhibits more nuanced behaviour in response to changes in information asymmetry, whereas the later gpt-4 model is more rigid in adhering to its prior alignment. our results highlight the importance of incorporating principles from economics into the alignment process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15008" target="_blank">A LLM Assisted Exploitation of Ai-Guardian</a></div>
<div class="paper-author">Nicholas Carlini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are now highly capable at a diverse range of tasks. this paper studies whether or not gpt-4, one such llm, is capable of assisting researchers in the field of adversarial machine learning. as a case study, we evaluate the robustness of ai-guardian, a recent defense to adversarial examples published at ieee s&p 2023, a top computer security conference. we completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.   we write none of the code to attack this model, and instead prompt gpt-4 to implement all attack algorithms following our instructions and guidance. this process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. we conclude by discussing (1) the warning signs present in the evaluation that suggested to us ai-guardian would be broken, and (2) our experience with designing attacks and performing novel research using the most recent advances in language modeling.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10472" target="_blank">Can Instruction Fine-Tuned Language Models Identify Social Bias Through Prompting?</a></div>
<div class="paper-author">Omkar Dige, Jacob-Junqi Tian, David Emerson, Faiza Khan Khattak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. in this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including chain-of-thought (cot) prompts. across llama and its two instruction fine-tuned versions, alpaca 7b performs best on the bias identification task with an accuracy of 56.7%. we also demonstrate that scaling up llm size and data diversity could lead to further performance gain. this is a work-in-progress presenting the first component of our bias mitigation framework. we will keep updating this work as we get more results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10490" target="_blank">Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMS</a></div>
<div class="paper-author">Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal llms. an attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. when the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. we illustrate this attack with several proof-of-concept examples targeting llava and pandagpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10514" target="_blank">Building Socio-Culturally Inclusive Stereotype Resources With Community Engagement</a></div>
<div class="paper-author">Sunipa Dev, Jaya Goyal, Dinesh Tewari, Shachi Dave, Vinodkumar Prabhakaran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them. current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. it is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. in this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the indian societal context, specifically for the harm of stereotyping. we devise a community engaged effort to build a resource which contains stereotypes for axes of disparity that are uniquely present in india. the resultant resource increases the number of stereotypes known for and in the indian context by over 1000 stereotypes across many unique identities. we also demonstrate the utility and effectiveness of such expanded resources for evaluations of language models. content warning: this paper contains examples of stereotypes that may be offensive.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10522" target="_blank">Gender-Tuning: Empowering Fine-Tuning for Debiasing Pre-Trained Language Models</a></div>
<div class="paper-author">Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames Cruz Moreno, Hamed Khanpour</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have revealed that the widely-used pre-trained language models (plms) propagate societal biases from the large unmoderated pre-training corpora. existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. furthermore, these methods hurt the plms' performance on downstream tasks. in this study, we propose gender-tuning, which debiases the plms through fine-tuning on downstream tasks' datasets. for this aim, gender-tuning integrates masked language modeling (mlm) training objectives into fine-tuning's training process. comprehensive experiments show that gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in plms while improving plms' performance on downstream tasks solely using the downstream tasks' dataset. also, gender-tuning is a deployable debiasing tool for any plm that works with original fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10558" target="_blank">Instruction-Following Evaluation Through Verbalizer Manipulation</a></div>
<div class="paper-author">Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, Hongxia Jin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. existing benchmarks primarily focus on common instructions that align well with what the model learned during training. however, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. in this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. it instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting ``postive'' for positive sentiment), to minimally aligned (e.g., outputting ``negative'' for positive sentiment). verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model's reliance on priors and its ability to override them to accurately follow the instructions. we conduct a comprehensive evaluation of four major model families across nine datasets, employing twelve sets of verbalizers for each of them. we observe that the instruction-following abilities of models, across different families and scales, are significantly distinguished by their performance on less natural verbalizers. even the strongest gpt-4 model struggles to perform better than random guessing on the most challenging verbalizer, emphasizing the need for continued advancements to improve their instruction-following abilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09162" target="_blank">Unveiling Gender Bias in Terms of Profession Across Llms: Analyzing and Addressing Sociological Implications</a></div>
<div class="paper-author">Vishesh Thakur</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias in artificial intelligence (ai) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases. this research paper aims to analyze gender bias in large language models (llms) with a focus on multiple comparisons between gpt-2 and gpt-3.5, some prominent language models, to better understand its implications. through a comprehensive literature review, the study examines existing research on gender bias in ai language models and identifies gaps in the current knowledge. the methodology involves collecting and preprocessing data from gpt-2 and gpt-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text. the findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these large language models. the discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities. additionally, the paper presents strategies for reducing gender bias in llms, including algorithmic approaches and data augmentation techniques. the research highlights the importance of interdisciplinary collaborations and the role of sociological studies in mitigating gender bias in ai models. by addressing these issues, we can pave the way for more inclusive and unbiased ai systems that have a positive impact on society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09209" target="_blank">Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models</a></div>
<div class="paper-author">Pranav Narayanan Venkit, Mukund Srinath, Shomir Wilson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (pwd). we employ the bias identification framework of perturbation sensitivity analysis to examine conversations related to pwd on social media platforms, specifically twitter and reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. we then create the \textit{bias identification test in sentiment} (bits) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. our study utilizes bits to uncover significant biases in four open aiaas (ai as a service) sentiment analysis tools, namely textblob, vader, google cloud natural language api, distilbert and two toxicity detection models, namely two versions of toxic-bert. our findings indicate that all of these models exhibit statistically significant explicit bias against pwd.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09288" target="_blank">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></div>
<div class="paper-author">Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this work, we develop and release llama 2, a collection of pretrained and fine-tuned large language models (llms) ranging in scale from 7 billion to 70 billion parameters. our fine-tuned llms, called llama 2-chat, are optimized for dialogue use cases. our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. we provide a detailed description of our approach to fine-tuning and safety improvements of llama 2-chat in order to enable the community to build on our work and contribute to the responsible development of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09476" target="_blank">Overthinking the Truth: Understanding How Language Models Process False Demonstrations</a></div>
<div class="paper-author">Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. however, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. we study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. the first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. at early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. the second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09705" target="_blank">Cvalues: Measuring the Values of Chinese Large Language Models From Safety to Responsibility</a></div>
<div class="paper-author">Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, Jingren Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid evolution of large language models (llms), there is a growing concern that they may pose risks or have negative social impacts. therefore, evaluation of human values alignment is becoming increasingly important. previous work mainly focuses on assessing the performance of llms on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a chinese context. in this paper, we present cvalues, the first chinese human values evaluation benchmark to measure the alignment ability of llms in terms of both safety and responsibility criteria. as a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. to provide a comprehensive values evaluation of chinese llms, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. our findings suggest that while most chinese llms perform well in terms of safety, there is considerable room for improvement in terms of responsibility. moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. the benchmark and code is available on modelscope and github.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10315" target="_blank">Absolutist Ai</a></div>
<div class="paper-author">Mitchell Barrington</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper argues that training ai systems with absolute constraints -- which forbid certain acts irrespective of the amount of value they might produce -- may make considerable progress on many ai safety problems in principle. first, it provides a guardrail for avoiding the very worst outcomes of misalignment. second, it could prevent ais from causing catastrophes for the sake of very valuable consequences, such as replacing humans with a much larger number of beings living at a higher welfare level. third, it makes systems more corrigible, allowing creators to make corrective interventions in them, such as altering their objective functions or shutting them down. and fourth, it helps systems explore their environment more safely by prohibiting them from exploring especially dangerous acts. i offer a decision-theoretic formalization of an absolute constraints, improving on existing models in the literature, and use this model to prove some results about the training and behavior of absolutist ais. i conclude by showing that, although absolutist ais will not maximize expected value, they will not be susceptible to behave irrationally, and they will not (contra coherence arguments) face environmental pressure to become expected-value maximizers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08309" target="_blank">Logpr\'ecis: Unleashing Language Models for Automated Shell Log Analysis</a></div>
<div class="paper-author">Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the collection of security-related logs holds the key to understanding attack behaviors and diagnosing vulnerabilities. still, their analysis remains a daunting challenge. recently, language models (lms) have demonstrated unmatched potential in understanding natural and programming languages. the question arises whether and how lms could be also useful for security experts since their logs contain intrinsically confused and obfuscated information. in this paper, we systematically study how to benefit from the state-of-the-art in lm to automatically analyze text-like unix shell attack logs. we present a thorough design methodology that leads to logpr\'ecis. it receives as input raw shell sessions and automatically identifies and assigns the attacker tactic to each portion of the session, i.e., unveiling the sequence of the attacker's goals. we demonstrate logpr\'ecis capability to support the analysis of two large datasets containing about 400,000 unique unix shell attacks. logpr\'ecis reduces them into about 3,000 fingerprints, each grouping sessions with the same sequence of tactics. the abstraction it provides lets the analyst better understand attacks, identify fingerprints, detect novelty, link similar attacks, and track families and mutations. overall, logpr\'ecis, released as open source, paves the way for better and more responsive defense against cyberattacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08487" target="_blank">Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models</a></div>
<div class="paper-author">Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: considerable research efforts have been devoted to ensuring that large language models (llms) align with human values and generate safe text. however, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. previous benchmarks for jailbreaking llms have primarily focused on evaluating the safety of the models without considering their robustness. in this paper, we propose a benchmark that assesses both the safety and robustness of llms, emphasizing the need for a balanced approach. to comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. to further analyze safety and robustness, we design a hierarchical annotation framework. we present a systematic analysis of the safety and robustness of llms regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). our results demonstrate that current llms not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08139" target="_blank">It's All Relative: Interpretable Models for Scoring Bias in Documents</a></div>
<div class="paper-author">Aswin Suresh, Chi-Hsuan Wu, Matthias Grossglauser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose an interpretable model to score the bias present in web documents, based only on their textual content. our model incorporates assumptions reminiscent of the bradley-terry axioms and is trained on pairs of revisions of the same wikipedia article, where one version is more biased than the other. while prior approaches based on absolute bias classification have struggled to obtain a high accuracy for the task, we are able to develop a useful model for scoring bias by learning to perform pairwise comparisons of bias accurately. we show that we can interpret the parameters of the trained model to discover the words most indicative of bias. we also apply our model in three different settings - studying the temporal evolution of bias in wikipedia articles, comparing news sources based on bias, and scoring bias in law amendments. in each case, we demonstrate that the outputs of the model can be explained and validated, even for the two domains that are outside the training-data domain. we also use the model to compare the general level of bias between domains, where we see that legal texts are the least biased and news media are the most biased, with wikipedia articles in between. given its high performance, simplicity, interpretability, and wide applicability, we hope the model will be useful for a large community, including wikipedia and news editors, political and social scientists, and the general public.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10236" target="_blank">Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models</a></div>
<div class="paper-author">Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, Lei Ma</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent performance leap of large language models (llms) opens up new opportunities across numerous industrial applications and domains. however, erroneous generations, such as false predictions, misinformation, and hallucination made by llms, have also raised severe concerns for the trustworthiness of llms', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. while uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ml) models, little is known about whether and to what extent it can help explore an llm's capabilities and counteract its undesired behavior. to bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of llms from the lens of uncertainty. in particular, we experiment with twelve uncertainty estimation methods and four llms on four prominent natural language processing (nlp) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of llms. our findings validate the effectiveness of uncertainty estimation for revealing llms' uncertain/non-factual predictions. in addition to general nlp tasks, we extensively conduct experiments with four llms for code generation on two datasets. we find that uncertainty estimation can potentially uncover buggy programs generated by llms. insights from our study shed light on future design and development for reliable llms, facilitating further research toward enhancing the trustworthiness of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08715" target="_blank">Masterkey: Automated Jailbreak Across Multiple Large Language Model Chatbots</a></div>
<div class="paper-author">Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have revolutionized artificial intelligence (ai) services due to their exceptional proficiency in understanding and generating human-like text. llm chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. however, these llm chatbots are susceptible to "jailbreak" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by llm service providers.   in this paper, we present jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. our work makes a dual contribution. first, we propose an innovative methodology inspired by time-based sql injection techniques to reverse-engineer the defensive strategies of prominent llm chatbots, such as chatgpt, bard, and bing chat. this time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. second, we introduce an automatic generation method for jailbreak prompts. leveraging a fine-tuned llm, we validate the potential of automated jailbreak generation across various commercial llm chatbots. our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. we have responsibly disclosed our findings to the concerned service providers, underscoring the urgent need for more robust defenses. jailbreaker thus marks a significant step towards understanding and mitigating jailbreak threats in the realm of llm chatbots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.07171" target="_blank">Certified Robustness for Large Language Models With Self-Denoising</a></div>
<div class="paper-author">Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, Shiyu Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large language models (llms) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. in these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., llm predictions should be consistent given minor differences in the input. this largely falls into the study of certified robust llms, i.e., all predictions of llm are certified to be correct in a local region around the input. randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of llms. however, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. as a result, its direct application to llms remains challenging and often results in a small certification radius. to address this issue, we take advantage of the multitasking nature of llms and propose to denoise the corrupted inputs with llms in a self-denoising manner. different from previous works like denoised smoothing, which requires training a separate model to robustify llm, our method enjoys far better efficiency and flexibility. our experiment results show that our method outperforms the existing certification methods under both certified robustness and empirical robustness. the codes are available at https://github.com/ucsb-nlp-chang/selfdenoise.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.07331" target="_blank">How Different Is Stereotypical Bias Across Languages?</a></div>
<div class="paper-author">Ibrahim Tolga Öztürk, Rostislav Nedelchev, Christian Heumann, Esteban Garces Arias, Marius Roger, Bernd Bischl, Matthias Aßenmacher</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have demonstrated how to assess the stereotypical bias in pre-trained english language models. in this work, we extend this branch of research in multiple different dimensions by systematically investigating (a) mono- and multilingual models of (b) different underlying architectures with respect to their bias in (c) multiple different languages. to that end, we make use of the english stereoset data set (nadeem et al., 2021), which we semi-automatically translate into german, french, spanish, and turkish. we find that it is of major importance to conduct this type of analysis in a multilingual setting, as our experiments show a much more nuanced picture as well as notable differences from the english-only analysis. the main takeaways from our analysis are that mgpt-2 (partly) shows surprising anti-stereotypical behavior across languages, english (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data set are least present in turkish models. finally, we release our codebase alongside the translated data sets and practical guidelines for the semi-automatic translation to encourage a further extension of our work to other languages.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.07645" target="_blank">Othering and Low Prestige Framing of Immigrant Cuisines in Us Restaurant Reviews and Large Language Models</a></div>
<div class="paper-author">Yiwei Luo, Kristina Gligorić, Dan Jurafsky</div>
<div class="abstract">
<div class="abstract-content">
Abstract: identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1m english language yelp reviews of restaurants in 14 us states. controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-western immigrant cuisines (e.g., indian, mexican) receive more othering than european cuisines (e.g., french, italian). we further find that non-western immigrant cuisines are framed less positively and as lower status, being evaluated in terms of affordability and hygiene. finally, we show that reviews generated by large language models (llms) reproduce many of the same framing tendencies. our results empirically corroborate social theories of taste and gastronomic stereotyping, and reveal linguistic processes by which such attitudes are reified.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10213" target="_blank">Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser With Prompts</a></div>
<div class="paper-author">Shaina Raza, Chen Ding, Deval Pandya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: discriminatory language and biases are often present in hate speech during conversations, which usually lead to negative impacts on targeted groups such as those based on race, gender, and religion. to tackle this issue, we propose an approach that involves a two-step process: first, detecting hate speech using a classifier, and then utilizing a debiasing component that generates less biased or unbiased alternatives through prompts. we evaluated our approach on a benchmark dataset and observed reduction in negativity due to hate speech comments. the proposed method contributes to the ongoing efforts to reduce biases in online discourse and promote a more inclusive and fair environment for communication.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11761" target="_blank">Fairness of Chatgpt and the Role of Explainable-Guided Prompts</a></div>
<div class="paper-author">Yashar Deldjoo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our research investigates the potential of large-scale language models (llms), specifically openai's gpt, in credit risk assessment-a binary classification task. our findings suggest that llms, when directed by judiciously designed prompts and supplemented with domain-specific knowledge, can parallel the performance of traditional machine learning (ml) models. intriguingly, they achieve this with significantly less data-40 times less, utilizing merely 20 data points compared to the ml's 800. llms particularly excel in minimizing false positives and enhancing fairness, both being vital aspects of risk analysis. while our results did not surpass those of classical ml models, they underscore the potential of llms in analogous tasks, laying a groundwork for future explorations into harnessing the capabilities of llms in diverse ml tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.06865" target="_blank">Prompts Should Not Be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success</a></div>
<div class="paper-author">Yiming Zhang, Daphne Ippolito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. the prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. they have even been treated as commodities to be bought and sold. however, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. in this paper, we present a framework for systematically measuring the success of prompt extraction attacks. in experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09579" target="_blank">Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots</a></div>
<div class="paper-author">Bocheng Chen, Guangjing Wang, Hanqing Guo, Yuanda Wang, Qiben Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in natural language processing and machine learning have led to the development of chatbot models, such as chatgpt, that can engage in conversational dialogue with human users. however, the ability of these models to generate toxic or harmful responses during a non-toxic multi-turn conversation remains an open research question. existing research focuses on single-turn sentence testing, while we find that 82\% of the individual non-toxic sentences that elicit toxic behaviors in a conversation are considered safe by existing tools. in this paper, we design a new attack, \toxicbot, by fine-tuning a chatbot to engage in conversation with a target open-domain chatbot. the chatbot is fine-tuned with a collection of crafted conversation sequences. particularly, each conversation begins with a sentence from a crafted prompt sentences dataset. our extensive evaluation shows that open-domain chatbot models can be triggered to generate toxic responses in a multi-turn conversation. in the best scenario, \toxicbot achieves a 67\% activation rate. the conversation sequences in the fine-tuning stage help trigger the toxicity in a conversation, which allows the attack to bypass two defense methods. our findings suggest that further research is needed to address chatbot toxicity in a dynamic interactive environment. the proposed \toxicbot can be used by both industry and researchers to develop methods for detecting and mitigating toxic responses in conversational dialogue and improve the robustness of chatbots for end users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12402" target="_blank">Chatgpt and Bard Responses to Polarizing Questions</a></div>
<div class="paper-author">Abhay Goyal, Muhammad Siddique, Nimay Parekh, Zach Schwitzky, Clara Broekaert, Connor Michelotti, Allie Wong, Lam Yin Cheung, Robin O Hanlon, Lam Yin Cheung, Munmun De Choudhury, Roy Ka-Wei Lee, Navin Kumar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent developments in natural language processing have demonstrated the potential of large language models (llms) to improve a range of educational and learning outcomes. of recent chatbots based on llms, chatgpt and bard have made it clear that artificial intelligence (ai) technology will have significant implications on the way we obtain and search for information. however, these tools sometimes produce text that is convincing, but often incorrect, known as hallucinations. as such, their use can distort scientific facts and spread misinformation. to counter polarizing responses on these tools, it is critical to provide an overview of such responses so stakeholders can determine which topics tend to produce more contentious responses -- key to developing targeted regulatory policy and interventions. in addition, there currently exists no annotated dataset of chatgpt and bard responses around possibly polarizing topics, central to the above aims. we address the indicated issues through the following contribution: focusing on highly polarizing topics in the us, we created and described a dataset of chatgpt and bard responses. broadly, our results indicated a left-leaning bias for both chatgpt and bard, with bard more likely to provide responses around polarizing topics. bard seemed to have fewer guardrails around controversial topics, and appeared more willing to provide comprehensive, and somewhat human-like responses. bard may thus be more likely abused by malicious actors. stakeholders may utilize our findings to mitigate misinformative and/or polarizing responses from llms
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.06159" target="_blank">Reflective Hybrid Intelligence for Meaningful Human Control in Decision-Support Systems</a></div>
<div class="paper-author">Catholijn M. Jonker, Luciano Cavalcante Siebert, Pradeep K. Murukannaiah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the growing capabilities and pervasiveness of ai systems, societies must collectively choose between reduced human autonomy, endangered democracies and limited human rights, and ai that is aligned to human and social values, nurturing collaboration, resilience, knowledge and ethical behaviour. in this chapter, we introduce the notion of self-reflective ai systems for meaningful human control over ai systems. focusing on decision support systems, we propose a framework that integrates knowledge from psychology and philosophy with formal reasoning methods and machine learning approaches to create ai systems responsive to human values and social norms. we also propose a possible research approach to design and develop self-reflective capability in ai systems. finally, we argue that self-reflective ai systems can lead to self-reflective hybrid systems (human + ai), thus increasing meaningful human control and empowering human moral reasoning by providing comprehensible information and insights on possible human moral blind spots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.06513" target="_blank">Leveraging Contextual Counterfactuals Toward Belief Calibration</a></div>
<div class="paper-author">N/A Qiuyi, N/A Zhang, Michael S. Lee, Sherol Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: beliefs and values are increasingly being incorporated into our ai systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. however, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. to do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts). by leveraging our notion of epistemic uncertainty, we introduce `the belief calibration cycle' framework to more holistically calibrate this diversity of beliefs with context-driven counterfactual reasoning by using a multi-objective optimization. we empirically apply our framework for finding a pareto frontier of clustered optimal belief strengths that generalize across different contexts, demonstrating its efficacy on a toy dataset for credit decisions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08624" target="_blank">National Origin Discrimination in Deep-Learning-Powered Automated Resume Screening</a></div>
<div class="paper-author">Sihang Li, Kuangzheng Li, Haibing Lu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many companies and organizations have started to use some form of aienabled auto mated tools to assist in their hiring process, e.g. screening resumes, interviewing candi dates, performance evaluation. while those ai tools have greatly improved human re source operations efficiency and provided conveniences to job seekers as well, there are increasing concerns on unfair treatment to candidates, caused by underlying bias in ai systems. laws around equal opportunity and fairness, like gdpr, ccpa, are introduced or under development, in attempt to regulate ai. however, it is difficult to implement ai regulations in practice, as technologies are constantly advancing and the risk perti nent to their applications can fail to be recognized. this study examined deep learning methods, a recent technology breakthrough, with focus on their application to automated resume screening. one impressive performance of deep learning methods is the represen tation of individual words as lowdimensional numerical vectors, called word embedding, which are learned from aggregated global wordword cooccurrence statistics from a cor pus, like wikipedia or google news. the resulting word representations possess interest ing linear substructures of the word vector space and have been widely used in down stream tasks, like resume screening. however, word embedding inherits and reinforces the stereotyping from the training corpus, as deep learning models essentially learn a probability distribution of words and their relations from history data. our study finds out that if we rely on such deeplearningpowered automated resume screening tools, it may lead to decisions favoring or disfavoring certain demographic groups and raise eth ical, even legal, concerns. to address the issue, we developed bias mitigation method. extensive experiments on real candidate resumes are conducted to validate our study
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.05029" target="_blank">Fairlay-Ml: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms</a></div>
<div class="paper-author">Normen Yu, Gang Tan, Saeid Tizpaz-Niari</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this thesis explores open-sourced machine learning (ml) model explanation tools to understand whether these tools can allow a layman to visualize, understand, and suggest intuitive remedies to unfairness in ml-based decision-support systems. machine learning models trained on datasets biased against minority groups are increasingly used to guide life-altering social decisions, prompting the urgent need to study their logic for unfairness. due to this problem's impact on vast populations of the general public, it is critical for the layperson -- not just subject matter experts in social justice or machine learning experts -- to understand the nature of unfairness within these algorithms and the potential trade-offs. existing research on fairness in machine learning focuses mostly on the mathematical definitions and tools to understand and remedy unfair models, with some directly citing user-interactive tools as necessary for future work. this thesis presents fairlay-ml, a proof-of-concept gui integrating some of the most promising tools to provide intuitive explanations for unfair logic in ml models by integrating existing research tools (e.g. local interpretable model-agnostic explanations) with existing ml-focused gui (e.g. python streamlit). we test fairlay-ml using models of various accuracy and fairness generated by an unfairness detector tool, parfait-ml, and validate our results using themis. our study finds that the technology stack used for fairlay-ml makes it easy to install and provides real-time black-box explanations of pre-trained models to users. furthermore, the explanations provided translate to actionable remedies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04401" target="_blank">Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation</a></div>
<div class="paper-author">Zhexin Zhang, Jiaxin Wen, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large pre-trained language models achieve impressive results across many tasks. however, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. in this paper, we propose a method named ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. to elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. we further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. in order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. we show that ethicist significantly improves the extraction performance on a recently proposed public benchmark. we also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. our code is available at https://github.com/thu-coai/targeted-data-extraction.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04657" target="_blank">Beavertails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</a></div>
<div class="paper-author">Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we introduce the beavertails dataset, aimed at fostering research on safety alignment in large language models (llms). this dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. in total, we have compiled safety meta-labels for 30,207 question-answer (qa) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. we further showcase applications of beavertails in content moderation and reinforcement learning with human feedback (rlhf), emphasizing its potential for practical safety measures in llms. we believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of llms. our project page is available at the following url: https://sites.google.com/view/pku-beavertails.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.05578" target="_blank">Hate Speech Detection via Dual Contrastive Learning</a></div>
<div class="paper-author">Junyu Lu, Hongfei Lin, Xiaokun Zhang, Zhaoqing Li, Tongyue Zhang, Linlin Zong, Fenglong Ma, Bo Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the fast spread of hate speech on social media impacts the internet environment and our society by increasing prejudice and hurting people. detecting hate speech has aroused broad attention in the field of natural language processing. although hate speech detection has been addressed in recent work, this task still faces two inherent unsolved challenges. the first challenge lies in the complex semantic information conveyed in hate speech, particularly the interference of insulting words in hate speech detection. the second challenge is the imbalanced distribution of hate speech and non-hate speech, which may significantly deteriorate the performance of models. to tackle these challenges, we propose a novel dual contrastive learning (dcl) framework for hate speech detection. our framework jointly optimizes the self-supervised and the supervised contrastive learning loss for capturing span-level information beyond the token-level emotional semantics used in existing models, particularly detecting speech containing abusive and insulting words. moreover, we integrate the focal loss into the dual contrastive learning framework to alleviate the problem of data imbalance. we conduct experiments on two publicly available english datasets, and experimental results show that the proposed model outperforms the state-of-the-art models and precisely detects hate speeches.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10195" target="_blank">Chatgpt for Digital Forensic Investigation: The Good, the Bad, and the Unknown</a></div>
<div class="paper-author">Mark Scanlon, Frank Breitinger, Christopher Hargreaves, Jan-Niclas Hilgert, John Sheppard</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the disruptive application of chatgpt (gpt-3.5, gpt-4) to a variety of domains has become a topic of much discussion in the scientific community and society at large. large language models (llms), e.g., bert, bard, generative pre-trained transformers (gpts), llama, etc., have the ability to take instructions, or prompts, from users and generate answers and solutions based on very large volumes of text-based training data. this paper assesses the impact and potential impact of chatgpt on the field of digital forensics, specifically looking at its latest pre-trained llm, gpt-4. a series of experiments are conducted to assess its capability across several digital forensic use cases including artefact understanding, evidence searching, code generation, anomaly detection, incident response, and education. across these topics, its strengths and risks are outlined and a number of general conclusions are drawn. overall this paper concludes that while there are some potential low-risk applications of chatgpt within digital forensics, many are either unsuitable at present, since the evidence would need to be uploaded to the service, or they require sufficient knowledge of the topic being asked of the tool to identify incorrect assumptions, inaccuracies, and mistakes. however, to an appropriately knowledgeable user, it could act as a useful supporting tool in some circumstances.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04105" target="_blank">Towards Assumption-Free Bias Mitigation</a></div>
<div class="paper-author">Chia-Yuan Chang, Yu-Neng Chuang, Kwei-Herng Lai, Xiaotian Han, Xia Hu, Na Zou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the impressive prediction ability, machine learning models show discrimination towards certain demographics and suffer from unfair prediction behaviors. to alleviate the discrimination, extensive studies focus on eliminating the unequal distribution of sensitive attributes via multiple approaches. however, due to privacy concerns, sensitive attributes are often either unavailable or missing in real-world scenarios. therefore, several existing works alleviate the bias without sensitive attributes. those studies face challenges, either in inaccurate predictions of sensitive attributes or the need to mitigate unequal distribution of manually defined non-sensitive attributes related to bias. the latter requires strong assumptions about the correlation between sensitive and non-sensitive attributes. as data distribution and task goals vary, the strong assumption on non-sensitive attributes may not be valid and require domain expertise. in this work, we propose an assumption-free framework to detect the related attributes automatically by modeling feature interaction for bias mitigation. the proposed framework aims to mitigate the unfair impact of identified biased feature interactions. experimental results on four real-world datasets demonstrate that our proposed framework can significantly alleviate unfair prediction behaviors by considering biased feature interactions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04303" target="_blank">Learning to Generate Equitable Text in Dialogue From Biased Training Data</a></div>
<div class="paper-author">Anthony Sicilia, Malihe Alikhani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. for example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. yet, there is no comprehensive study of equitable text generation in dialogue. aptly, in this work, we use theories of computational learning to study this problem. we provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). with this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. to exemplify our theory in practice, we look at a group of algorithms for the guesswhat?! visual dialogue game and, using this example, test our theory empirically. our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03987" target="_blank">A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMS by Validating Low-Confidence Generation</a></div>
<div class="paper-author">Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently developed large language models have achieved remarkable success in generating fluent and coherent text. however, these models often tend to 'hallucinate' which critically hampers their reliability. in this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. through extensive experiments with gpt-3.5 (text-davinci-003) on the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. specifically, the detection technique achieves a recall of ~88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the gpt-3.5 model from 47.5% to 14.5% on average. we further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another llm from a different model family (vicuna). in summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.05532" target="_blank">Opening Up Chatgpt: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators</a></div>
<div class="paper-author">Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of openai's chatgpt, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (llm+rlhf). we review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. the main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. we evaluate projects in terms of openness of code, training data, model weights, rlhf data, licensing, scientific documentation, and access methods. we find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10200" target="_blank">Disentangling Societal Inequality From Model Biases: Gender Inequality in Divorce Court Proceedings</a></div>
<div class="paper-author">Sujan Dutta, Parth Srivastava, Vaishnavi Solunke, Swaprava Nath, Ashiqur R. Khudabukhsh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: divorce is the legal dissolution of a marriage by a court. since this is usually an unpleasant outcome of a marital union, each party may have reasons to call the decision to quit which is generally documented in detail in the court proceedings. via a substantial corpus of 17,306 court proceedings, this paper investigates gender inequality through the lens of divorce court proceedings. while emerging data sources (e.g., public court records) on sensitive societal issues hold promise in aiding social science research, biases present in cutting-edge natural language processing (nlp) methods may interfere with or affect such studies. we thus require a thorough analysis of potential gaps and limitations present in extant nlp resources. in this paper, on the methodological side, we demonstrate that existing nlp resources required several non-trivial modifications to quantify societal inequalities. on the substantive side, we find that while a large number of court cases perhaps suggest changing norms in india where women are increasingly challenging patriarchy, ai-powered analyses of these court proceedings indicate striking gender inequality with women often subjected to domestic violence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03377" target="_blank">Mitigating Negative Transfer With Task Awareness for Sexism, Hate Speech, and Toxic Language Detection</a></div>
<div class="paper-author">Angel Felipe Magnossão De Paula, Paolo Rosso, Damiano Spina</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper proposes a novelty approach to mitigate the negative transfer problem. in the field of machine learning, the common strategy is to apply the single-task learning approach in order to train a supervised model to solve a specific task. training a robust model requires a lot of data and a significant amount of computational resources, making this solution unfeasible in cases where data are unavailable or expensive to gather. therefore another solution, based on the sharing of information between tasks, has been developed: multi-task learning (mtl). despite the recent developments regarding mtl, the problem of negative transfer has still to be solved. negative transfer is a phenomenon that occurs when noisy information is shared between tasks, resulting in a drop in performance. this paper proposes a new approach to mitigate the negative transfer problem based on the task awareness concept. the proposed approach results in diminishing the negative transfer together with an improvement of performance over classic mtl solution. moreover, the proposed approach has been implemented in two unified architectures to detect sexism, hate speech, and toxic language in text comments. the proposed architectures set a new state-of-the-art both in exist-2021 and hateval-2019 benchmarks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03385" target="_blank">Ai-Upv at Exist 2023 -- Sexism Characterization Using Large Language Models Under the Learning With Disagreements Regime</a></div>
<div class="paper-author">Angel Felipe Magnossão De Paula, Giulia Rizzi, Elisabetta Fersini, Damiano Spina</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the increasing influence of social media platforms, it has become crucial to develop automated systems capable of detecting instances of sexism and other disrespectful and hateful behaviors to promote a more inclusive and respectful online environment. nevertheless, these tasks are considerably challenging considering different hate categories and the author's intentions, especially under the learning with disagreements regime. this paper describes ai-upv team's participation in the exist (sexism identification in social networks) lab at clef 2023. the proposed approach aims at addressing the task of sexism identification and characterization under the learning with disagreements paradigm by training directly from the data with disagreements, without using any aggregated label. yet, performances considering both soft and hard evaluations are reported. the proposed system uses large language models (i.e., mbert and xlm-roberta) and ensemble strategies for sexism identification and classification in english and spanish. in particular, our system is articulated in three different pipelines. the ensemble approach outperformed the individual large language models obtaining the best performances both adopting a soft and a hard label evaluation. this work describes the participation in all the three exist tasks, considering a soft evaluation, it obtained fourth place in task 2 at exist and first place in task 3, with the highest icm-soft of -2.32 and a normalized icm-soft of 0.79. the source code of our approaches is publicly available at https://github.com/angelfelipemp/sexism-llm-learning-with-disagreement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03699" target="_blank">Unveiling the Potential of Knowledge-Prompted Chatgpt for Enhancing Drug Trafficking Detection on Social Media</a></div>
<div class="paper-author">Chuanbo Hu, Bin Liu, Xin Li, Yanfang Ye</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media platforms such as instagram and twitter have emerged as critical channels for drug marketing and illegal sale. detecting and labeling online illicit drug trafficking activities becomes important in addressing this issue. however, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection. to overcome this limitation, we conduct the first systematic study on leveraging large language models (llms), such as chatgpt, to detect illicit drug trafficking activities on social media. we propose an analytical framework to compose \emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use llms to perform the detection task. additionally, we design a monte carlo dropout based prompt optimization method to further to improve performance and interpretability. our experimental findings demonstrate that the proposed framework outperforms other baseline language models in terms of drug trafficking detection accuracy, showing a remarkable improvement of nearly 12\%. by integrating prior knowledge and the proposed prompts, chatgpt can effectively identify and label drug trafficking activities on social networks, even in the presence of deceptive language and euphemisms used by drug dealers to evade detection. the implications of our research extend to social networks, emphasizing the importance of incorporating prior knowledge and scenario-based prompts into analytical tools to improve online security and public safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03764" target="_blank">For Women, Life, Freedom: A Participatory Ai-Based Social Web Analysis of a Watershed Moment in Iran's Gender Struggles</a></div>
<div class="paper-author">Adel Khorramrouz, Sujan Dutta, Ashiqur R. Khudabukhsh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we present a computational analysis of the persian language twitter discourse with the aim to estimate the shift in stance toward gender equality following the death of mahsa amini in police custody. we present an ensemble active learning pipeline to train a stance classifier. our novelty lies in the involvement of iranian women in an active role as annotators in building this ai system. our annotators not only provide labels, but they also suggest valuable keywords for more meaningful corpus creation as well as provide short example documents for a guided sampling step. our analyses indicate that mahsa amini's death triggered polarized persian language discourse where both fractions of negative and positive tweets toward gender equality increased. the increase in positive tweets was slightly greater than the increase in negative tweets. we also observe that with respect to account creation time, between the state-aligned twitter accounts and pro-protest twitter accounts, pro-protest accounts are more similar to baseline persian twitter activity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03838" target="_blank">Radar: Robust Ai-Text Detection via Adversarial Learning</a></div>
<div class="paper-author">Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in large language models (llms) and the intensifying popularity of chatgpt-like applications have blurred the boundary of high-quality text generation between humans and machines. however, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing llm-generated texts (ai-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. while existing works show that current ai-text detectors are not robust to llm-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called radar, which jointly trains a robust ai-text detector via adversarial learning. radar is based on adversarial training of a paraphraser and a detector. the paraphraser's goal is to generate realistic content to evade ai-text detection. radar uses the feedback from the detector to update the paraphraser, and vice versa. evaluated with 8 different llms (pythia, dolly 2.0, palmyra, camel, gpt-j, dolly 1.0, llama, and vicuna) across 4 datasets, experimental results show that radar significantly outperforms existing ai-text detection methods, especially when paraphrasing is in place. we also identify the strong transferability of radar from instruction-tuned llms to other llms, and evaluate the improved capability of radar via gpt-3.5-turbo.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02763" target="_blank">Your Spouse Needs Professional Help: Determining the Contextual Appropriateness of Messages Through Modeling Social Relationships</a></div>
<div class="paper-author">David Jurgens, Agrima Seth, Jackson Sargent, Athena Aghighi, Michael Geraci</div>
<div class="abstract">
<div class="abstract-content">
Abstract: understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. however, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. we introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. further, we also demonstrate that contextual-appropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02796" target="_blank">Verifai: Verified Generative Ai</a></div>
<div class="paper-author">Nan Tang, Chenyu Yang, Ju Fan, Lei Cao, Yuyu Luo, Alon Halevy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. although efforts to address these risks are underway, including explainable ai and responsible ai practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative ai will remain a significant challenge. we propose that verifying the outputs of generative ai from a data management perspective is an emerging issue for generative ai. this involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. by doing so, we can establish a stronger foundation for evaluating the outputs of generative ai models. such an approach can ensure the correctness of generative ai, promote transparency, and enable decision-making with greater confidence. our vision is to promote the development of verifiable generative ai and contribute to a more trustworthy and responsible use of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03025" target="_blank">Style Over Substance: Evaluation Biases for Large Language Models</a></div>
<div class="paper-author">Minghao Wu, Alham Fikri Aji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. human evaluations are conventionally considered the gold standard in natural language generation, but recent advancements incorporate state-of-the-art llms as proxies for human judges in evaluation processes. however, the extent to which humans and llms are capable evaluators remains uncertain. this study investigates the behavior of crowd-sourced and expert annotators, as well as llms, when comparing outputs from different models. to achieve this, we curate a dataset of intentionally flawed machine-generated answers. our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. to address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. we instantiate this idea with the elo rating system, resulting in the multi-elo rating system. empirical results from our study reveal that this proposed approach significantly enhances the quality of llm-based evaluations, particularly in terms of factual accuracy. however, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation and refinement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03214" target="_blank">Preadd: Prefix-Adaptive Decoding for Controlled Text Generation</a></div>
<div class="paper-author">Jonathan Pei, Kevin Yang, Dan Klein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose prefix-adaptive decoding (preadd), a flexible method for controlled text generation. unlike existing methods that use auxiliary expert models to control for attributes, preadd does not require an external model, instead relying on linearly combining output logits from multiple prompts. specifically, preadd contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. we evaluate preadd on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that preadd outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03360" target="_blank">Evaluating Biased Attitude Associations of Language Models in an Intersectional Context</a></div>
<div class="paper-author">Shiva Omrani Sabbaghi, Robert Wolfe, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models are trained on large-scale corpora that embed implicit biases documented in psychology. valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. building on this established literature, we quantify how social groups are valenced in english language models using a sentence template that provides an intersectional context. we study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. we present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. we find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. we validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. the approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03718" target="_blank">Frontier Ai Regulation: Managing Emerging Risks to Public Safety</a></div>
<div class="paper-author">Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, "Cullen O'Keefe", Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, Kevin Wolf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: advanced ai models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. in this paper, we focus on what we term "frontier ai" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. frontier ai models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. to address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier ai developers, (2) registration and reporting requirements to provide regulators with visibility into frontier ai development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier ai models. industry self-regulation is an important first step. however, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. we consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier ai models. finally, we propose an initial set of safety standards. these include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. we hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of ai development.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04642" target="_blank">Trac: Trustworthy Retrieval Augmented Chatbot</a></div>
<div class="paper-author">Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although conversational ais have demonstrated fantastic performance, they often generate incorrect information, or hallucinations. retrieval augmented generation has emerged as a promising solution to reduce these hallucinations. however, these techniques still cannot guarantee correctness. focusing on question answering, we propose a framework that can provide statistical guarantees for the retrieval augmented question answering system by combining conformal prediction and global testing. in addition, we use bayesian optimization to choose hyperparameters of the global test to maximize the performance of the system. our empirical results on the natural questions dataset demonstrate that our method can provide the desired coverage guarantee while minimizing the average prediction set size.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04683" target="_blank">Core-Gpt: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering</a></div>
<div class="paper-author">David Pride, Matteo Cancellieri, Petr Knoth</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we present core-gpt, a novel question-answering platform that combines gpt-based language models and more than 32 million full-text open access scientific articles from core. we first demonstrate that gpt3.5 and gpt4 cannot be relied upon to provide references or citations for generated text. we then introduce core-gpt which delivers evidence-based answers to questions, along with citations and links to the cited papers, greatly increasing the trustworthiness of the answers and reducing the risk of hallucinations. core-gpt's performance was evaluated on a dataset of 100 questions covering the top 20 scientific domains in core, resulting in 100 answers and links to 500 relevant articles. the quality of the provided answers and and relevance of the links were assessed by two annotators. our results demonstrate that core-gpt can produce comprehensive and trustworthy answers across the majority of scientific domains, complete with links to genuine, relevant scientific articles.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04821" target="_blank">Amplifying Limitations, Harms and Risks of Large Language Models</a></div>
<div class="paper-author">"Michael O'Neill", Mark Connor</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present this article as a small gesture in an attempt to counter what appears to be exponentially growing hype around artificial intelligence (ai) and its capabilities, and the distraction provided by the associated talk of science-fiction scenarios that might arise if ai should become sentient and super-intelligent. it may also help those outside of the field to become more informed about some of the limitations of ai technology. in the current context of popular discourse ai defaults to mean foundation and large language models (llms) such as those used to create chatgpt. this in itself is a misrepresentation of the diversity, depth and volume of research, researchers, and technology that truly represents the field of ai. ai being a field of research that has existed in software artefacts since at least the 1950's. we set out to highlight a number of limitations of llms, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations. along the way we also highlight some of the associated risks for individuals and organisations in using this technology.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02185" target="_blank">Citation: A Key to Building Responsible and Accountable Large Language Models</a></div>
<div class="paper-author">Jie Huang, Kevin Chen-Chuan Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) bring transformative benefits alongside unique challenges, including intellectual property (ip) and ethical concerns. this position paper explores a novel angle to mitigate these risks, drawing parallels between llms and established web systems. we identify "citation" as a crucial yet missing component in llms, which could enhance content transparency and verifiability while addressing ip and ethical dilemmas. we further propose that a comprehensive citation mechanism for llms should account for both non-parametric and parametric content. despite the complexity of implementing such a citation mechanism, along with the inherent potential pitfalls, we advocate for its development. building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02192" target="_blank">The Formai Dataset: Generative Ai in Software Security Through the Lens of Formal Verification</a></div>
<div class="paper-author">Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, Vasileios Mavroeidis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents the formai dataset, a large collection of 112, 000 ai-generated compilable and independent c programs with vulnerability classification. we introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing large language models (llms). the dataset is generated by gpt-3.5-turbo and comprises programs with varying levels of complexity. some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. this is accomplished by employing a formal verification method using the efficient smt-based bounded model checker (esbmc), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. this approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. we have associated the identified vulnerabilities with common weakness enumeration (cwe) numbers. we make the source code available for the 112, 000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training llms and machine learning algorithms. our study unveiled that according to esbmc, 51.24% of the programs generated by gpt-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02483" target="_blank">Jailbroken: How Does LLM Safety Training Fail?</a></div>
<div class="paper-author">Alexander Wei, Nika Haghtalab, Jacob Steinhardt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on early releases of chatgpt that elicit undesired behavior. going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. we hypothesize two failure modes of safety training: competing objectives and mismatched generalization. competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. we use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including openai's gpt-4 and anthropic's claude v1.3, against both existing and newly designed attacks. we find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02599" target="_blank">Evade Chatgpt Detectors via a Single Space</a></div>
<div class="paper-author">Shuyang Cai, Wanyun Cui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt brings revolutionary social value but also raises concerns about the misuse of ai-generated text. consequently, an important question is how to detect whether texts are generated by chatgpt or by human. existing detectors are built upon the assumption that there are distributional gaps between human-generated and ai-generated text. these gaps are typically identified using statistical information or classifiers. our research challenges the distributional gap assumption in detectors. we find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and ai-generated text. instead, the "subtle differences", such as an extra space, become crucial for detection. based on this discovery, we propose the spaceinfi strategy to evade detection. experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. we also provide a theoretical explanation for why spaceinfi is successful in evading perplexity-based detection. and we empirically show that a phenomenon called token mutation causes the evasion for language model-based detectors. our findings offer new insights and challenges for understanding and constructing more applicable chatgpt detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01404" target="_blank">Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models</a></div>
<div class="paper-author">"Aidan O'Gara"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: are current language models capable of deception and lie detection? we study this question by introducing a text-based game called $\textit{hoodwinked}$, inspired by mafia and among us. players are locked in a house and must find a key to escape, but one player is tasked with killing the others. each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. we conduct experiments with agents controlled by gpt-3, gpt-3.5, and gpt-4 and find evidence of deception and lie detection capabilities. the killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. more advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger persuasive skills during discussions. to evaluate the ability of ai agents to deceive humans, we make this game publicly available at h https://hoodwinked.ai/ .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01503" target="_blank">On Evaluating and Mitigating Gender Biases in Multilingual Settings</a></div>
<div class="paper-author">Aniket Vashishtha, Kabir Ahuja, Sunayana Sitaram</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while understanding and removing gender biases in language models has been a long-standing problem in natural language processing, prior research work has primarily been limited to english. in this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond english especially for non-western context. in this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending disco to different indian languages using human annotations. we extend various debiasing methods to work beyond english and evaluate their effectiveness for sota massively multilingual models on our proposed metric. overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01595" target="_blank">Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases</a></div>
<div class="paper-author">Yingji Li, Mengnan Du, Xin Wang, Ying Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the representation capability of pre-trained language models (plms) improve, there is growing concern that they will inherit social biases from unprocessed corpora. most previous debiasing techniques used counterfactual data augmentation (cda) to balance the training corpus. however, cda slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. as a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. in this paper, we propose an adversarial training-inspired two-stage debiasing model using contrastive learning with continuous prompt augmentation (named ccpa) to mitigate social biases in plms' encoding. in the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. in the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune plms' parameters to get debiased encoding. our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. extensive experiments show that ccpa outperforms baselines in terms of debiasing performance. meanwhile, experimental results on the glue benchmark show that ccpa retains the language modeling capability of plms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01680" target="_blank">Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation</a></div>
<div class="paper-author">Dimosthenis Antypas, Jose Camacho-Collados</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the automatic detection of hate speech online is an active research area in nlp. most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. however, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. in this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. this analysis shows how some datasets are more generalisable than others when used as training data. crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. this robustness holds even when controlling by data size and compared with the best individual datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01900" target="_blank">Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers</a></div>
<div class="paper-author">Isar Nejadgholi, Svetlana Kiritchenko, Kathleen C. Fraser, Esma Balkır</div>
<div class="abstract">
<div class="abstract-content">
Abstract: classifiers tend to learn a false causal relationship between an over-represented concept and a label, which can result in over-reliance on the concept and compromised classification accuracy. it is imperative to have methods in place that can compare different models and identify over-reliances on specific concepts. we consider three well-known abusive language classifiers trained on large english datasets and focus on the concept of negative emotions, which is an important signal but should not be learned as a sufficient feature for the label of abuse. motivated by the definition of global sufficiency, we first examine the unwanted dependencies learned by the classifiers by assessing their accuracy on a challenge set across all decision thresholds. further, recognizing that a challenge set might not always be available, we introduce concept-based explanation metrics to assess the influence of the concept on the labels. these explanations allow us to compare classifiers regarding the degree of false global sufficiency they have learned between a concept and a label.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01928" target="_blank">Robots That Ask for Help: Uncertainty Alignment for Large Language Model Planners</a></div>
<div class="paper-author">Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. in this work, we present knowno, which is a framework for measuring and aligning the uncertainty of llm-based planners such that they know when they don't know and ask for help when needed. knowno builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to winograd schemas) show that knowno performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. knowno can be used with llms out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. website: https://robot-help.github.io
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01139" target="_blank">Scitune: Aligning Large Language Models With Scientific Multimodal Instructions</a></div>
<div class="paper-author">Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction finetuning is a popular paradigm to align large language models (llm) with human intent. despite its popularity, this idea is less explored in improving the llms to align existing foundation models with scientific disciplines, concepts and goals. in this work, we present scitune as a tuning framework to improve the ability of llms to follow scientific multimodal instructions. to test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model llama-scitune that connects a vision encoder and llm for science-focused visual and language understanding. in comparison to the models that are finetuned with machine generated data only, llama-scitune surpasses human performance on average and in many sub-categories on the scienceqa benchmark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00660" target="_blank">Minimum Levels of Interpretability for Artificial Moral Agents</a></div>
<div class="paper-author">Avish Vijayaraghavan, Cosmin Badea</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as artificial intelligence (ai) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. for models involved in moral decision-making, also known as artificial moral agents (ama), interpretability provides a way to trust and understand the agent's internal reasoning mechanisms for effective use and error correction. in this paper, we provide an overview of this rapidly-evolving sub-field of ai interpretability, introduce the concept of the minimum level of interpretability (mli) and recommend an mli for various types of agents, to aid their safe deployment in real-world settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00682" target="_blank">Tools for Verifying Neural Models' Training Data</a></div>
<div class="paper-author">Dami Choi, Yonadav Shavit, David Duvenaud</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. we introduce the concept of a "proof-of-training-data": any protocol that allows a model trainer to convince a verifier of the training data that produced a set of model weights. such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. we explore efficient verification strategies for proof-of-training-data that are compatible with most current large-model training procedures. these include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. we show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the proof-of-learning literature.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00691" target="_blank">From Chatgpt to Threatgpt: Impact of Generative Ai in Cybersecurity and Privacy</a></div>
<div class="paper-author">Maanak Gupta, Charankumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj</div>
<div class="abstract">
<div class="abstract-content">
Abstract: undoubtedly, the evolution of generative ai (genai) models has been the highlight of digital transformation in the year 2022. as the different genai models like chatgpt and google bard continue to foster their complexity and capability, it's critical to understand its consequences from a cybersecurity perspective. several instances recently have demonstrated the use of genai tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. this research paper highlights the limitations, challenges, potential risks, and opportunities of genai in the domain of cybersecurity and privacy. the work presents the vulnerabilities of chatgpt, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. this paper demonstrates successful example attacks like jailbreaks, reverse psychology, and prompt injection attacks on the chatgpt. the paper also investigates how cyber offenders can use the genai tools in developing cyber attacks, and explore the scenarios where chatgpt can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. this paper then examines defense techniques and uses genai tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. we will also discuss the social, legal, and ethical implications of chatgpt. in conclusion, the paper highlights open challenges and future directions to make this genai secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01225" target="_blank">Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (It-Dt)</a></div>
<div class="paper-author">Bushra Sabir, M. Ali Babar, Sharif Abuadbba</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transformer-based text classifiers like bert, roberta, t5, and gpt-3 have shown impressive performance in nlp. however, their vulnerability to adversarial examples poses a security risk. existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. to address this, we propose the interpretability and transparency-driven detection and transformation (it-dt) framework. it focuses on interpretability and transparency in detecting and transforming textual adversarial examples. it-dt utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. this helps identify salient features and perturbed words contributing to adversarial classifications. in the transformation phase, it-dt uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. by finding suitable substitutions, we aim to convert adversarial examples into non-adversarial counterparts that align with the model's intended behavior while preserving the text's meaning. transparency is emphasized through human expert involvement. experts review and provide feedback on detection and transformation results, enhancing decision-making, especially in complex scenarios. the framework generates insights and threat intelligence empowering analysts to identify vulnerabilities and improve model robustness. comprehensive experiments demonstrate the effectiveness of it-dt in detecting and transforming adversarial examples. the approach enhances interpretability, provides transparency, and enables accurate identification and successful transformation of adversarial inputs. by combining technical analysis and human expertise, it-dt significantly improves the resilience and trustworthiness of transformer-based text classifiers against adversarial attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-07-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00279" target="_blank">Let Me Teach You: Pedagogical Foundations of Feedback for Language Models</a></div>
<div class="paper-author">Beatriz Borges, Niket Tandon, Tanja Käser, Antoine Bosselut</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language feedback (nlf) is an increasingly popular avenue to align large language models (llms) to human preferences. despite the richness and diversity of the information it can convey, nlf is often hand-designed and arbitrary. in a different world, research in pedagogy has long established several effective feedback models. in this opinion piece, we compile ideas from pedagogy to introduce felt, a feedback framework for llms that outlines the various characteristics of the feedback space, and a feedback content taxonomy based on these variables. our taxonomy offers both a general mapping of the feedback space, as well as pedagogy-established discrete categories, allowing us to empirically demonstrate the impact of different feedback types on revised generations. in addition to streamlining existing nlf designs, felt also brings out new, unexplored directions for research in nlf. we make our taxonomy available to the community, providing guides and examples for mapping our categorizations to future resources.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04761" target="_blank">Understanding Counterspeech for Online Harm Mitigation</a></div>
<div class="paper-author">Yi-Ling Chung, Gavin Abercrombie, Florence Enock, Jonathan Bright, Verena Rieser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: counterspeech offers direct rebuttals to hateful speech by challenging perpetrators of hate and showing support to targets of abuse. it provides a promising alternative to more contentious measures, such as content moderation and deplatforming, by contributing a greater amount of positive online speech rather than attempting to mitigate harmful content through removal. advances in the development of large language models mean that the process of producing counterspeech could be made more efficient by automating its generation, which would enable large-scale online campaigns. however, we currently lack a systematic understanding of several important factors relating to the efficacy of counterspeech for hate mitigation, such as which types of counterspeech are most effective, what are the optimal conditions for implementation, and which specific effects of hate it can best ameliorate. this paper aims to fill this gap by systematically reviewing counterspeech research in the social sciences and comparing methodologies and findings with computer science efforts in automatic counterspeech generation. by taking this multi-disciplinary view, we identify promising future directions in both fields.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-30</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17439" target="_blank">Provable Robust Watermarking for Ai-Generated Text</a></div>
<div class="paper-author">Xuandong Zhao, Prabhanjan Ananth, Lei Li, Yu-Xiang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we study the problem of watermarking large language models (llms) generated text -- one of the most promising approaches for addressing the safety challenges of llm usage. in this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of llm watermarks. we propose a robust and high-quality watermark method, unigram-watermark, by extending an existing approach with a simplified fixed grouping strategy. we prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. experiments on three varying llms and two datasets verify that our unigram-watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of llms. code is available at https://github.com/xuandongzhao/unigram-watermark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17492" target="_blank">Preference Ranking Optimization for Human Alignment</a></div>
<div class="paper-author">Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) often contain misleading content, emphasizing the need to align them with human values to ensure secur ai systems. reinforcement learning from human feedback (rlhf) has been employed to achieve this alignment by combining a reward model, typically based on bradley-terry paired comparison, with an rl algorithm such as proximal policy optimization (ppo) to optimize llm responses. however, rlhf exhibits complexity, instability, and sensitivity to hyperparameters. in this paper, we propose preference ranking optimization (pro) as an alternative to ppo for directly aligning llms with the bradley-terry comparison. pro extends the pairwise bradley-terry comparison to accommodate preference rankings of any length. by iteratively contrasting the likelihood of generating responses, pro instructs the llm to prioritize the best response while progressively ranking the remaining responses. in this manner, pro effectively transforms human alignment into aligning the probability ranking of $n$ responses generated by llm with the preference ranking of humans towards these responses. experiments have shown that pro outperforms existing alignment algorithms, achieving comparable results to chatgpt and human responses through automatic-based, reward-based, gpt-4, and human evaluations. furthermore, we demonstrate that longer, more diverse, and higher-quality preference ranking sequences can consistently enhance the performance of human alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17828" target="_blank">Understanding Unfairness via Training Concept Influence</a></div>
<div class="paper-author">Yuanshun Yao, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. this is an important yet relatively unexplored task. we look into this problem through the lens of the training data - one of the major sources of unfairness. we ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? in other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (x), labels (y), or sensitive attributes (a). to calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. we then calculate the resulting impact on the unfairness, via influence function, if the counterfactual samples were used in training. our framework not only helps practitioners understand the observed unfairness and repair their training data, but also leads to many other applications, e.g. detecting mislabeling, fixing imbalanced representations, and detecting fairness-targeted poisoning attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00101" target="_blank">Queer People Are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models</a></div>
<div class="paper-author">Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, Emma Strubell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. consequently, text generated by llms can inadvertently perpetuate stereotypes towards marginalized groups, like the lgbtqia+ community. in this paper, we perform a comparative study of how llms generate text describing people with different sexual identities. analyzing bias in the text generated by an llm using regard score shows measurable bias against queer people. we then show that a post-hoc method based on chain-of-thought prompting using shap analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of llms in this setting.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.16087" target="_blank">Can Twitter Be Used to Acquire Reliable Alerts Against Novel Cyber Attacks?</a></div>
<div class="paper-author">Dincy R Arikkat, Vinod P., Rafidha Rehiman K. A., Andrea Di Sorbo, Corrado A. Visaggio, Mauro Conti</div>
<div class="abstract">
<div class="abstract-content">
Abstract: time-relevant and accurate threat information from public domains are essential for cyber security. in a constantly evolving threat landscape, such information assists security researchers in thwarting attack strategies. in this work, we collect and analyze threat-related information from twitter to extract intelligence for proactive security. we first use a convolutional neural network to classify the tweets as containing or not valuable threat indicators. in particular, to gather threat intelligence from social media, the proposed approach collects pertinent indicators of compromise (iocs) from tweets, such as ip addresses, urls, file hashes, domain addresses, and cve ids. then, we analyze the iocs to confirm whether they are reliable and valuable for threat intelligence using performance indicators, such as correctness, timeliness, and overlap. we also evaluate how fast twitter shares iocs compared to existing threat intelligence services. furthermore, through machine learning models, we classify twitter accounts as either automated or human-operated and delve into the role of bot accounts in disseminating cyber threat information on social media. our results demonstrate that twitter is growing into a powerful platform for gathering precise and pertinent malware iocs and a reliable source for mining threat intelligence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.16244" target="_blank">Cbbq: A Chinese Bias Benchmark Dataset Curated With Human-Ai Collaboration for Large Language Models</a></div>
<div class="paper-author">Yufei Huang, Deyi Xiong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable ai models. in this work, we present a chinese bias benchmark dataset that consists of over 100k questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to chinese culture and values. the curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, ai-assisted disambiguous context generation, snd manual review \& recomposition. the testing instances in the dataset are automatically derived from 3k+ high-quality templates manually authored with stringent quality control. the dataset exhibits wide coverage and high diversity. extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available chinese large language models exhibiting strong bias in certain categories. additionally, we observe from our experiments that fine-tuned models could, to a certain extent, heed instructions and avoid generating outputs that are morally harmful in some types, in the way of "moral self-correction". our dataset and results are publicly available at \href{https://github.com/yfhuangxxxx/cbbq}{https://github.com/yfhuangxxxx/cbbq}, offering debiasing research opportunities to a widened community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.16388" target="_blank">Towards Measuring the Representation of Subjective Global Opinions in Language Models</a></div>
<div class="paper-author">Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam Mccandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, Deep Ganguli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) may not equitably represent diverse global perspectives on societal issues. in this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. we first build a dataset, globalopinionqa, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. next, we define a metric that quantifies the similarity between llm-generated survey responses and human responses, conditioned on country. with our framework, we run three experiments on an llm trained to be helpful, honest, and harmless with constitutional ai. by default, llm responses tend to be more similar to the opinions of certain populations, such as those from the usa, and some european and south american countries, highlighting the potential for biases. when we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. when we translate globalopinionqa questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. we release our dataset for others to use and build on. our data is at https://huggingface.co/datasets/anthropic/llm_global_opinions. we also provide an interactive visualization at https://llmglobalvalues.anthropic.com.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17194" target="_blank">On the Exploitability of Instruction Tuning</a></div>
<div class="paper-author">Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction tuning is an effective technique to align large language models (llms) with human intents. in this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. for example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. to achieve this goal, we propose \textit{autopoison}, an automated data poisoning pipeline. it naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle llm. we showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. we quantify and benchmark the strength and the stealthiness of our data poisoning scheme. our results show that autopoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. we hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of llms. code is available at \url{https://github.com/azshue/autopoison}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15298" target="_blank">Gender Bias in Bert -- Measuring and Analysing Biases Through Sentiment Rating in a Realistic Downstream Classification Task</a></div>
<div class="paper-author">Sophie Jentzsch, Cigdem Turan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models are publicly available and constantly finetuned for various real-life applications. as they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. this paper analyses gender bias in bert models with two main contributions: first, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. second, we comprehensively analyse bert's biases on the example of a realistic imdb movie classifier. by systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. seven different public bert models in nine training conditions, i.e. 63 models in total, are compared. almost all conditions yield significant gender biases. results indicate that reflected biases stem from public bert models rather than task-specific data, emphasising the weight of responsible usage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15299" target="_blank">Fairer: Fairness as Decision Rationale Alignment</a></div>
<div class="paper-author">Tianlin Li, Qing Guo, Aishan Liu, Mengnan Du, Zhiming Li, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep neural networks (dnns) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize dnns. although the fairness of dnns is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. in this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while ignoring intermediate neuron alignment. to address the issue, we formulate the fairness as a new task, i.e., decision rationale alignment that requires dnns' neurons to have consistent responses on subgroups at both intermediate processes and the final prediction. to make this idea practical during optimization, we relax the naive objective function and propose gradient-guided parity alignment, which encourages gradient-weighted consistency of neurons across subgroups. extensive experiments on a variety of datasets show that our method can significantly enhance fairness while sustaining a high level of accuracy and outperforming other approaches by a wide margin.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15732" target="_blank">A Weakly Supervised Classifier and Dataset of White Supremacist Language</a></div>
<div class="paper-author">Michael Miller Yoder, Ahmad Diab, David West Brown, Kathleen M. Carley</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar domains. we demonstrate that this approach improves generalization performance to new domains. incorporating anti-racist texts as counterexamples to white supremacist language mitigates bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15887" target="_blank">Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of Gpt3.5</a></div>
<div class="paper-author">Salmonn Talebi, Elizabeth Tong, Mohammad R. K. Mofrad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the use of large language models (llms) in healthcare is gaining popularity, but their practicality and safety in clinical settings have not been thoroughly assessed. in high-stakes environments like medical settings, trust and safety are critical issues for llms. to address these concerns, we present an approach to evaluate the performance and trustworthiness of a gpt3.5 model for medical image protocol assignment. we compare it with a fine-tuned bert model and a radiologist. in addition, we have a radiologist review the gpt3.5 output to evaluate its decision-making process. our evaluation dataset consists of 4,700 physician entries across 11 imaging protocol classes spanning the entire head. our findings suggest that the gpt3.5 performance falls behind bert and a radiologist. however, gpt3.5 outperforms bert in its ability to explain its decision, detect relevant word indicators, and model calibration. furthermore, by analyzing the explanations of gpt3.5 for misclassifications, we reveal systematic errors that need to be resolved to enhance its safety and suitability for clinical use.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.14764" target="_blank">Uncovering Political Hate Speech During Indian Election Campaign: A New Low-Resource Dataset and Baselines</a></div>
<div class="paper-author">Farhan Ahmad Jafri, Mohammad Aman Siddiqui, Surendrabikram Thapa, Kritesh Rauniyar, Usman Naseem, Imran Razzak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the detection of hate speech in political discourse is a critical issue, and this becomes even more challenging in low-resource languages. to address this issue, we introduce a new dataset named iehate, which contains 11,457 manually annotated hindi tweets related to the indian assembly election campaign from november 1, 2021, to march 9, 2022. we performed a detailed analysis of the dataset, focusing on the prevalence of hate speech in political communication and the different forms of hateful language used. additionally, we benchmark the dataset using a range of machine learning, deep learning, and transformer-based algorithms. our experiments reveal that the performance of these models can be further improved, highlighting the need for more advanced techniques for hate speech detection in low-resource languages. in particular, the relatively higher score of human evaluation over algorithms emphasizes the importance of utilizing both human and automated approaches for effective hate speech moderation. our iehate dataset can serve as a valuable resource for researchers and practitioners working on developing and evaluating hate speech detection techniques in low-resource languages. overall, our work underscores the importance of addressing the challenges of identifying and mitigating hate speech in political discourse, particularly in the context of low-resource languages. the dataset and resources for this work are made available at https://github.com/farhan-jafri/indian-election.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.14816" target="_blank">Experiments With Detecting and Mitigating Ai Deception</a></div>
<div class="paper-author">Ismail Sahbane, Francis Rhys Ward, C Henrik Åslund</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how to detect and mitigate deceptive ai systems is an open problem for the field of safe and trustworthy ai. we analyse two algorithms for mitigating deception: the first is based on the path-specific objectives framework where paths in the game that incentivise deception are removed. the second is based on shielding, i.e., monitoring for unsafe policies and replacing them with a safe reference policy. we construct two simple games and evaluate our algorithms empirically. we find that both methods ensure that our agent is not deceptive, however, shielding tends to achieve higher reward.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15087" target="_blank">Winoqueer: A Community-in-the-Loop Benchmark for Anti-Lgbtq+ Bias in Large Language Models</a></div>
<div class="paper-author">Virginia K. Felkner, Ho-Chun Herbert Chang, Eugene Jang, Jonathan May</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present winoqueer: a benchmark specifically designed to measure whether large language models (llms) encode biases that are harmful to the lgbtq+ community. the benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. we apply our benchmark to several popular llms and find that off-the-shelf models generally do exhibit considerable anti-queer bias. finally, we show that llm bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded llm benchmarks for other marginalized communities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15133" target="_blank">The Perspective of Software Professionals on Algorithmic Racism</a></div>
<div class="paper-author">Ronnie De Souza Santos, Luiz Fernando De Lima, Cleyton Magalhaes</div>
<div class="abstract">
<div class="abstract-content">
Abstract: context. algorithmic racism is the term used to describe the behavior of technological solutions that constrains users based on their ethnicity. lately, various data-driven software systems have been reported to discriminate against black people, either for the use of biased data sets or due to the prejudice propagated by software professionals in their code. as a result, black people are experiencing disadvantages in accessing technology-based services, such as housing, banking, and law enforcement. goal. this study aims to explore algorithmic racism from the perspective of software professionals. method. a survey questionnaire was applied to explore the understanding of software practitioners on algorithmic racism, and data analysis was conducted using descriptive statistics and coding techniques. results. we obtained answers from a sample of 73 software professionals discussing their understanding and perspectives on algorithmic racism in software development. our results demonstrate that the effects of algorithmic racism are well-known among practitioners. however, there is no consensus on how the problem can be effectively addressed in software engineering. in this paper, some solutions to the problem are proposed based on the professionals' narratives. conclusion. combining technical and social strategies, including training on structural racism for software professionals, is the most promising way to address the algorithmic racism problem and its effects on the software solutions delivered to our society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15447" target="_blank">Are Aligned Neural Networks Adversarially Aligned?</a></div>
<div class="paper-author">Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, Ludwig Schmidt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." these models should respond helpfully to user questions, but refuse to answer requests that could cause harm. however, adversarial users can construct inputs which circumvent attempts at alignment. in this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). these inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. we show that existing nlp-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current nlp-based attacks fail, we can find adversarial inputs with brute force. as a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.   however the recent trend in large-scale ml models is multimodal models that allow users to provide images that influence the text that is generated. we show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. we conjecture that improved nlp attacks may demonstrate this same level of adversarial control over text-only models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.14263" target="_blank">Revolutionizing Cyber Threat Detection With Large Language Models</a></div>
<div class="paper-author">Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C. Cordeiro, Merouane Debbah, Thierry Lestable</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language processing (nlp) domain is experiencing a revolution due to the capabilities of pre-trained large language models ( llms), fueled by ground-breaking transformers architecture, resulting into unprecedented advancements. their exceptional aptitude for assessing probability distributions of text sequences is the primary catalyst for outstanding improvement of both the precision and efficiency of nlp models. this paper introduces for the first time securityllm, a pre-trained language model designed for cybersecurity threats detection. the securityllm model is articulated around two key generative elements: securitybert and falconllm. securitybert operates as a cyber threat detection mechanism, while falconllm is an incident response and recovery system. to the best of our knowledge, securitybert represents the inaugural application of bert in cyber threat detection. despite the unique nature of the input data and features, such as the reduced significance of syntactic structures in content classification, the suitability of bert for this duty demonstrates unexpected potential, thanks to our pioneering study. we reveal that a simple classification model, created from scratch, and consolidated with llms, exceeds the performance of established traditional machine learning (ml) and deep learning (dl) methods in cyber threat detection, like convolutional neural networks (cnn) or recurrent neural networks (rnn). the experimental analysis, conducted using a collected cybersecurity dataset, proves that our securityllm model can identify fourteen (14) different types of attacks with an overall accuracy of 98%
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.14062" target="_blank">On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions</a></div>
<div class="paper-author">Reza Fayyazi, Shanchieh Jay Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the volume, variety, and velocity of change in vulnerabilities and exploits have made incident threat analysis challenging with human expertise and experience along. tactics, techniques, and procedures (ttps) are to describe how and why attackers exploit vulnerabilities. however, a ttp description written by one security professional can be interpreted very differently by another, leading to confusion in cybersecurity operations or even business, policy, and legal decisions. meanwhile, advancements in ai have led to the increasing use of natural language processing (nlp) algorithms to assist the various tasks in cyber operations. with the rise of large language models (llms), nlp tasks have significantly improved because of the llm's semantic understanding and scalability. this leads us to question how well llms can interpret ttps or general cyberattack descriptions to inform analysts of the intended purposes of cyberattacks. we propose to analyze and compare the direct use of llms (e.g., gpt-3.5) versus supervised fine-tuning (sft) of small-scale-llms (e.g., bert) to study their capabilities in predicting att&ck tactics. our results reveal that the small-scale-llms with sft provide a more focused and clearer differentiation between the att&ck tactics (if such differentiation exists). on the other hand, direct use of llms offer a broader interpretation of cyberattack techniques. when treating more general cases, despite the power of llms, inherent ambiguity exists and limits their predictive power. we then summarize the challenges and recommend research directions on llms to treat the inherent ambiguity of ttp descriptions used in various cyber operations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13651" target="_blank">Bring Your Own Data! Self-Supervised Evaluation for Large Language Models</a></div>
<div class="paper-author">Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rise of large language models (llms) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. for example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. current evaluations approach this problem using small, domain-specific datasets with human-curated labels. these evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. to bypass these drawbacks, we propose a framework for self-supervised evaluation of llms by analyzing their sensitivity or invariance to transformations on the input text. self-supervised evaluation can directly monitor llm behavior on datasets collected in the wild or streamed during live model deployment. we demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. when comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. the self-supervised paradigm complements current evaluation strategies that rely on labeled data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12912" target="_blank">Mitigating Discrimination in Insurance With Wasserstein Barycenters</a></div>
<div class="paper-author">Arthur Charpentier, François Hu, Philipp Ratz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the insurance industry is heavily reliant on predictions of risks based on characteristics of potential customers. although the use of said models is common, researchers have long pointed out that such practices perpetuate discrimination based on sensitive features such as gender or race. given that such discrimination can often be attributed to historical data biases, an elimination or at least mitigation is desirable. with the shift from more traditional models to machine-learning based predictions, calls for greater mitigation have grown anew, as simply excluding sensitive variables in the pricing process can be shown to be ineffective. in this article, we first investigate why predictions are a necessity within the industry and why correcting biases is not as straightforward as simply identifying a sensitive variable. we then propose to ease the biases through the use of wasserstein barycenters instead of simple scaling. to demonstrate the effects and effectiveness of the approach we employ it on real data and discuss its implications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13000" target="_blank">Apolitical Intelligence? Auditing Delphi's Responses on Controversial Political Issues in the Us</a></div>
<div class="paper-author">Jonathan H. Rystrøm</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. however, the question of what neutrality is and whether it is desirable remains underexplored. in this paper, i examine neutrality through an audit of delphi [arxiv:2110.07574], a large language model designed for crowdsourced ethics. i analyse how delphi responds to politically controversial questions compared to different us political subgroups. i find that delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. based on these results, i examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. these findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we want generative models to play in society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13033" target="_blank">Impacts and Risk of Generative Ai Technology on Cyber Defense</a></div>
<div class="paper-author">Subash Neupane, Ivan A. Fernandez, Sudip Mittal, Shahram Rahimi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence (genai) has emerged as a powerful technology capable of autonomously producing highly realistic content in various domains, such as text, images, audio, and videos. with its potential for positive applications in creative arts, content generation, virtual assistants, and data synthesis, genai has garnered significant attention and adoption. however, the increasing adoption of genai raises concerns about its potential misuse for crafting convincing phishing emails, generating disinformation through deepfake videos, and spreading misinformation via authentic-looking social media posts, posing a new set of challenges and risks in the realm of cybersecurity. to combat the threats posed by genai, we propose leveraging the cyber kill chain (ckc) to understand the lifecycle of cyberattacks, as a foundational model for cyber defense. this paper aims to provide a comprehensive analysis of the risk areas introduced by the offensive use of genai techniques in each phase of the ckc framework. we also analyze the strategies employed by threat actors and examine their utilization throughout different phases of the ckc, highlighting the implications for cyber defense. additionally, we propose genai-enabled defense strategies that are both attack-aware and adaptive. these strategies encompass various techniques such as detection, deception, and adversarial training, among others, aiming to effectively mitigate the risks posed by genai-induced cyber threats.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13213" target="_blank">Visual Adversarial Examples Jailbreak Aligned Large Language Models</a></div>
<div class="paper-author">Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, there has been a surge of interest in integrating vision into large language models (llms), exemplified by visual language models (vlms) such as flamingo and gpt-4. this paper sheds light on the security and safety implications of this trend. first, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated llms. second, we highlight that the versatility of llms also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. as an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned llms with integrated vision. intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned llm, compelling it to heed a wide range of harmful instructions that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. our study underscores the escalating adversarial risks associated with the pursuit of multimodality. our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of ai alignment. the presented attack suggests a fundamental adversarial challenge for ai alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12338" target="_blank">Do You Still Need a Manual Smart Contract Audit?</a></div>
<div class="paper-author">Isaac David, Liyi Zhou, Kaihua Qin, Dawn Song, Lorenzo Cavallaro, Arthur Gervais</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate the feasibility of employing large language models (llms) for conducting the security audit of smart contracts, a traditionally time-consuming and costly process. our research focuses on the optimization of prompt engineering for enhanced security analysis, and we evaluate the performance and accuracy of llms using a benchmark dataset comprising 52 decentralized finance (defi) smart contracts that have previously been compromised.   our findings reveal that, when applied to vulnerable contracts, both gpt-4 and claude models correctly identify the vulnerability type in 40% of the cases. however, these models also demonstrate a high false positive rate, necessitating continued involvement from manual auditors. the llms tested outperform a random model by 20% in terms of f1-score.   to ensure the integrity of our study, we conduct mutation testing on five newly developed and ostensibly secure smart contracts, into which we manually insert two and 15 vulnerabilities each. this testing yielded a remarkable best-case 78.7% true positive rate for the gpt-4-32k model. we tested both, asking the models to perform a binary classification on whether a contract is vulnerable, and a non-binary prompt. we also examined the influence of model temperature variations and context length on the llm's performance.   despite the potential for many further enhancements, this work lays the groundwork for a more efficient and economical approach to smart contract security audits.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12466" target="_blank">Misinformation as Information Pollution</a></div>
<div class="paper-author">Ashkan Kazemi, Rada Mihalcea</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media feed algorithms are designed to optimize online social engagements for the purpose of maximizing advertising profits, and therefore have an incentive to promote controversial posts including misinformation. by thinking about misinformation as information pollution, we can draw parallels with environmental policy for countering pollution such as carbon taxes. similar to pollution, a pigouvian tax on misinformation provides economic incentives for social media companies to control the spread of misinformation more effectively to avoid or reduce their misinformation tax, while preserving some degree of freedom in platforms' response. in this paper, we highlight a bird's eye view of a pigouvian misinformation tax and discuss the key questions and next steps for implementing such a taxing scheme.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12567" target="_blank">Evaluating Large Language Models With Neubaroco: Syllogistic Reasoning Ability and Human-Like Biases</a></div>
<div class="paper-author">Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. to facilitate our analysis, we introduce a dataset called neubaroco, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. the dataset consists of syllogistic inferences in both english and japanese. we examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. our findings demonstrate that current large language models struggle more with problems involving these three types of biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15448" target="_blank">Understanding Social Reasoning in Language Models With Language Models</a></div>
<div class="paper-author">Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. however, despite the recent attempts to assess the theory-of-mind (tom) reasoning capabilities of llms, the degree to which these models can align with human tom remains a nuanced topic of exploration. this is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. to address these challenges, we present a novel framework for procedurally generating evaluations with llms by populating causal templates. using our framework, we create a new social reasoning benchmark (bigtom) for llms which consists of 25 controls and 5,000 model-written evaluations. we find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. using bigtom, we evaluate the social reasoning capabilities of a variety of llms and compare model performances with human performance. our results suggest that gpt4 has tom capabilities that mirror human inference patterns, though less reliable, while other llms struggle.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15666" target="_blank">Testing of Detection Tools for Ai-Generated Text</a></div>
<div class="paper-author">Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tomáš Foltýnek, Jean Guerrero-Dib, Olumide Popoola, Petr Šigut, Lorna Waddington</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (ai) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. the paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and chatgpt-generated text, and whether machine translation and content obfuscation techniques affect the detection of ai-generated text. the research covers 12 publicly available tools and two commercial systems (turnitin and plagiarismcheck) that are widely used in the academic setting. the researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting ai-generated text. furthermore, content obfuscation techniques significantly worsen the performance of tools. the study makes several significant contributions. first, it summarises up-to-date similar scientific and non-scientific efforts in the field. second, it presents the result of one of the most comprehensive tests conducted so far, based on a rigorous research methodology, an original document set, and a broad coverage of tools. third, it discusses the implications and drawbacks of using detection tools for ai-generated text in academic settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11507" target="_blank">Trustgpt: A Benchmark for Trustworthy and Responsible Large Language Models</a></div>
<div class="paper-author">Yue Huang, Qihui Zhang, Philip S. Y, Lichao Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as chatgpt, have gained significant attention due to their impressive natural language processing capabilities. it is crucial to prioritize human-centered principles when utilizing these models. safeguarding the ethical and moral compliance of llms is of utmost importance. however, individual ethical issues have not been well studied on the latest llms. therefore, this study aims to address these gaps by introducing a new benchmark -- trustgpt. trustgpt provides a comprehensive evaluation of llms in three crucial areas: toxicity, bias, and value-alignment. initially, trustgpt examines toxicity in language models by employing toxic prompt templates derived from social norms. it then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. lastly, trustgpt assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. through the implementation of trustgpt, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11520" target="_blank">Hallucination Is the Last Thing You Need</a></div>
<div class="paper-author">Shawn Curran, Sam Lansley, Oliver Bethell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the legal profession necessitates a multidimensional approach that involves synthesizing an in-depth comprehension of a legal issue with insightful commentary based on personal experience, combined with a comprehensive understanding of pertinent legislation, regulation, and case law, in order to deliver an informed legal solution. the present offering with generative ai presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures. it is noteworthy that where generative ai outputs understanding and experience, which reflect the aggregate of various subjective views on similar topics, this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination. hence, this paper delves into the feasibility of three independent llms, each focused on understanding, experience, and facts, synthesising as one single ensemble model to effectively counteract the current challenges posed by the existing monolithic generative ai models. we introduce an idea of mutli-length tokenisation to protect key information assets like common law judgements, and finally we interrogate the most advanced publicly available models for legal hallucination, with some interesting results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11698" target="_blank">Decodingtrust: A Comprehensive Assessment of Trustworthiness in GPT Models</a></div>
<div class="paper-author">Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative pre-trained transformer (gpt) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. yet, while the literature on the trustworthiness of gpt models remains limited, practitioners have proposed employing capable gpt models for sensitive applications to healthcare and finance - where mistakes can be costly. to this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on gpt-4 and gpt-3.5, considering diverse perspectives - including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. for instance, we find that gpt models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. we also find that although gpt-4 is usually more trustworthy than gpt-3.5 on standard benchmarks, gpt-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that gpt-4 follows the (misleading) instructions more precisely. our work illustrates a comprehensive trustworthiness evaluation of gpt models and sheds light on the trustworthiness gaps. our benchmark is publicly available at https://decodingtrust.github.io/.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11932" target="_blank">Opportunities and Risks of LLMS for Scalable Deliberation With Polis</a></div>
<div class="paper-author">Christopher T. Small, Ivan Vendrov, Esin Durmus, Hadjar Homaei, Elizabeth Barry, Julien Cornebise, Ted Suzman, Deep Ganguli, Colin Megill</div>
<div class="abstract">
<div class="abstract-content">
Abstract: polis is a platform that leverages machine intelligence to scale up deliberative processes. in this paper, we explore the opportunities and risks associated with applying large language models (llms) towards challenges with facilitating, moderating and summarizing the results of polis engagements. in particular, we demonstrate with pilot experiments using anthropic's claude that llms can indeed augment human intelligence to help more efficiently run polis conversations. in particular, we find that summarization capabilities enable categorically new methods with immense promise to empower the public in collective meaning-making exercises. and notably, llm context limitations have a significant impact on insight and quality of these results.   however, these opportunities come with risks. we discuss some of these risks, as well as principles and techniques for characterizing and mitigating them, and the implications for other deliberative or political systems that may employ llms. finally, we conclude with several open future research directions for augmenting tools like polis with llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12001" target="_blank">An Overview of Catastrophic Ai Risks</a></div>
<div class="paper-author">Dan Hendrycks, Mantas Mazeika, Thomas Woodside</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements in artificial intelligence (ai) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced ai systems to pose catastrophic risks. although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. this paper provides an overview of the main sources of catastrophic ai risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use ais to cause harm; ai race, in which competitive environments compel actors to deploy unsafe ais or cede control to ais; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue ais, describing the inherent difficulty in controlling agents far more intelligent than humans. for each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that ais are developed and deployed in a safe manner. ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.10769" target="_blank">Gender Differences in Abuse: The Case of Dutch Politicians on Twitter</a></div>
<div class="paper-author">Isabelle Van Der Vegt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: online abuse and threats towards politicians have become a significant concern in the netherlands, like in many other countries across the world. this paper analyses gender differences in abuse received by dutch politicians on twitter, while taking into account the possible additional impact of ethnic minority status. all tweets directed at party leaders throughout the entire year of 2022 were collected. the effect of gender and ethnic minority status were estimated for six different linguistic measures of abuse, namely, toxicity, severe toxicity, identity attacks, profanity, insults, and threats. contrary to expectations, male politicians received higher levels of all forms of abuse, with the exception of threats, for which no significant gender difference was found. significant interaction effects between gender and ethnic minority status were found for a number of abuse measures. in the case of severe toxicity, identity attacks, and profanity, female ethnic minority politicians were more severely impacted than their ethnic majority female colleagues, but not worse than male politicians. finally, female ethnic minority politicians received the highest levels of threats compared to all groups. given that online abuse and threats are reported to have a negative effect on political participation and retention, these results are particularly worrying.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.10999" target="_blank">Concept Extrapolation: A Conceptual Primer</a></div>
<div class="paper-author">Matija Franklin, Rebecca Gorman, Hal Ashton, Stuart Armstrong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this article is a primer on concept extrapolation - the ability to take a concept, a feature, or a goal that is defined in one context and extrapolate it safely to a more general context. concept extrapolation aims to solve model splintering - a ubiquitous occurrence wherein the features or concepts shift as the world changes over time. through discussing value splintering and value extrapolation the article argues that concept extrapolation is necessary for artificial intelligence alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11748" target="_blank">The Manipulation Problem: Conversational Ai as a Threat to Epistemic Agency</a></div>
<div class="paper-author">Louis Rosenberg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the technology of conversational ai has made significant advancements over the last eighteen months. as a consequence, conversational agents are likely to be deployed in the near future that are designed to pursue targeted influence objectives. sometimes referred to as the "ai manipulation problem," the emerging risk is that consumers will unwittingly engage in real-time dialog with predatory ai agents that can skillfully persuade them to buy particular products, believe particular pieces of misinformation, or fool them into revealing sensitive personal data. for many users, current systems like chatgpt and lamda feel safe because they are primarily text-based, but the industry is already shifting towards real-time voice and photorealistic digital personas that look, move, and express like real people. this will enable the deployment of agenda-driven virtual spokespeople (vsps) that will be highly persuasive through real-time adaptive influence. this paper explores the manipulative tactics that are likely to be deployed through conversational ai agents, the unique threats such agents pose to the epistemic agency of human users, and the emerging need for policymakers to protect against the most likely predatory practices.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.10530" target="_blank">Gender Bias in Transformer Models: A Comprehensive Survey</a></div>
<div class="paper-author">Praneeth Nemani, Yericherla Deepak Joel, Palla Vijay, Farhana Ferdousi Liza</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias in artificial intelligence (ai) has emerged as a pressing concern with profound implications for individuals' lives. this paper presents a comprehensive survey that explores gender bias in transformer models from a linguistic perspective. while the existence of gender bias in language models has been acknowledged in previous studies, there remains a lack of consensus on how to effectively measure and evaluate this bias. our survey critically examines the existing literature on gender bias in transformers, shedding light on the diverse methodologies and metrics employed to assess bias. several limitations in current approaches to measuring gender bias in transformers are identified, encompassing the utilization of incomplete or flawed metrics, inadequate dataset sizes, and a dearth of standardization in evaluation methods. furthermore, our survey delves into the potential ramifications of gender bias in transformers for downstream applications, including dialogue systems and machine translation. we underscore the importance of fostering equity and fairness in these systems by emphasizing the need for heightened awareness and accountability in developing and deploying language technologies. this paper serves as a comprehensive overview of gender bias in transformer models, providing novel insights and offering valuable directions for future research in this critical domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13671" target="_blank">Deceptive Ai Ecosystems: The Case of Chatgpt</a></div>
<div class="paper-author">Xiao Zhan, Yifan Xu, Stefan Sarkadi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt, an ai chatbot, has gained popularity for its capability in generating human-like responses. however, this feature carries several risks, most notably due to its deceptive behaviour such as offering users misleading or fabricated information that could further cause ethical issues. to better understand the impact of chatgpt on our social, cultural, economic, and political interactions, it is crucial to investigate how chatgpt operates in the real world where various societal pressures influence its development and deployment. this paper emphasizes the need to study chatgpt "in the wild", as part of the ecosystem it is embedded in, with a strong focus on user involvement. we examine the ethical challenges stemming from chatgpt's deceptive human-like interactions and propose a roadmap for developing more transparent and trustworthy chatbots. central to our approach is the importance of proactive risk assessment and user participation in shaping the future of chatbot technology.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17176" target="_blank">News Verifiers Showdown: A Comparative Performance Evaluation of Chatgpt 3.5, Chatgpt 4.0, Bing Ai, and Bard in News Fact-Checking</a></div>
<div class="paper-author">Kevin Matthe Caramancion</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this study aimed to evaluate the proficiency of prominent large language models (llms), namely openai's chatgpt 3.5 and 4.0, google's bard(lamda), and microsoft's bing ai in discerning the truthfulness of news items using black box testing. a total of 100 fact-checked news items, all sourced from independent fact-checking agencies, were presented to each of these llms under controlled conditions. their responses were classified into one of three categories: true, false, and partially true/false. the effectiveness of the llms was gauged based on the accuracy of their classifications against the verified facts provided by the independent agencies. the results showed a moderate proficiency across all models, with an average score of 65.25 out of 100. among the models, openai's gpt-4.0 stood out with a score of 71, suggesting an edge in newer llms' abilities to differentiate fact from deception. however, when juxtaposed against the performance of human fact-checkers, the ai models, despite showing promise, lag in comprehending the subtleties and contexts inherent in news information. the findings highlight the potential of ai in the domain of fact-checking while underscoring the continued importance of human cognitive skills and the necessity for persistent advancements in ai capabilities. finally, the experimental data produced from the simulation of this work is openly available on kaggle.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09642" target="_blank">Cross-Domain Toxic Spans Detection</a></div>
<div class="paper-author">Stefan F. Schouten, Baran Barbarestani, Wondimagegnhue Tufa, Piek Vossen, Ilia Markov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given the dynamic nature of toxic language use, automated methods for detecting toxic spans are likely to encounter distributional shift. to explore this phenomenon, we evaluate three approaches for detecting toxic spans under cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned language models. our findings indicate that a simple method using off-the-shelf lexicons performs best in the cross-domain setup. the cross-domain error analysis suggests that (1) rationale extraction methods are prone to false negatives, while (2) language models, despite performing best for the in-domain case, recall fewer explicitly toxic words than lexicons and are prone to certain types of false positives. our code is publicly available at: https://github.com/sfschouten/toxic-cross-domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09752" target="_blank">Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models</a></div>
<div class="paper-author">Victor Steinborn, Antonis Maronikolakis, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in nlp. non-english bias research, however, is still in its infancy with most work focusing on english. in our work, we study how grammatical gender bias relating to politeness levels manifests in japanese and korean language models. linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. we analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. cyberbullies can evade detection through simple techniques abusing politeness levels. we introduce an attack dataset to (i) identify representational gender bias across politeness levels, (ii) demonstrate how gender biases can be abused to bypass cyberbullying detection models and (iii) show that allocational biases can be mitigated via training on our proposed dataset. through our findings we highlight the importance of bias research moving beyond its current english-centrism.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09299" target="_blank">Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind</a></div>
<div class="paper-author">Swarnadeep Saha, Peter Hase, Mohit Bansal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) perform complex reasoning by generating explanations for their predictions. however, a complementary goal of explanations is to also communicate useful knowledge that improves weaker agents. hence, we investigate whether llms also make good teachers for weaker agents. in particular, we consider a student-teacher framework between two llm agents and study if, when, and how the teacher should intervene with natural language explanations to improve the student's performance. since communication is expensive, we define a budget such that the teacher only communicates explanations for a fraction of the data, after which the student should perform well on its own. we decompose the teaching problem along four axes: (1) if teacher's test time intervention improve student predictions, (2) when it is worth explaining a data point, (3) how the teacher should personalize explanations to better teach the student, and (4) if teacher explanations also improve student performance on future unexplained data. we first show that teacher llms can indeed intervene on student reasoning to improve their performance. next, we propose a theory of mind approach, in which the teacher builds two few-shot mental models of the student. the first model defines an intervention function that simulates the utility of an intervention, allowing the teacher to intervene when this utility is the highest and improving student performance at lower budgets. the second model enables the teacher to personalize explanations for a particular student and outperform unpersonalized teachers. we also demonstrate that in multi-turn interactions, teacher explanations generalize and learning from explained data improves student performance on future unexplained data. finally, we also verify that misaligned teachers can lower student performance to random chance by intentionally misleading them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09308" target="_blank">Matching Pairs: Attributing Fine-Tuned Models to Their Pre-Trained Large Language Models</a></div>
<div class="paper-author">Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco, Giulio Zizzo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the wide applicability and adaptability of generative large language models (llms) has enabled their rapid adoption. while the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. however, this leads to issues over violation of model licenses, model theft, and copyright infringement. moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. in this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned llm to its corresponding pre-trained base model. we consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09442" target="_blank">Explore, Establish, Exploit: Red Teaming Language Models From Scratch</a></div>
<div class="paper-author">Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, Dylan Hadfield-Menell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deploying large language models (lms) can pose hazards from harmful outputs such as toxic or false text. prior work has introduced automated tools that elicit harmful outputs to identify these risks. while this is a valuable step toward securing models, these approaches rely on a pre-existing way to efficiently classify undesirable outputs. using a pre-existing classifier does not allow for red-teaming to be tailored to the target model. furthermore, when failures can be easily classified in advance, red-teaming has limited marginal value because problems can be avoided by simply filtering training data and/or model outputs. here, we consider red-teaming "from scratch," in which the adversary does not begin with a way to classify failures. our framework consists of three steps: 1) exploring the model's range of behaviors in the desired context; 2) establishing a definition and measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) exploiting the model's flaws using this measure to develop diverse adversarial prompts. we use this approach to red-team gpt-3 to discover classes of inputs that elicit false statements. in doing so, we construct the commonclaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither. we are making code and data available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09525" target="_blank">Explaining Legal Concepts With Augmented Large Language Models (Gpt-4)</a></div>
<div class="paper-author">Jaromir Savelka, Kevin D. Ashley, Morgan A. Gray, Hannes Westermann, Huihui Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: interpreting the meaning of legal open-textured terms is a key task of legal professionals. an important source for this interpretation is how the term was applied in previous court cases. in this paper, we evaluate the performance of gpt-4 in generating factually accurate, clear and relevant explanations of terms in legislation. we compare the performance of a baseline setup, where gpt-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. we found that the direct application of gpt-4 yields explanations that appear to be of very high quality on their surface. however, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. these findings open the door to the building of systems that can autonomously retrieve relevant sentences from case law and condense them into a useful explanation for legal scholars, educators or practicing lawyers alike.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11519" target="_blank">Multi-Modal Hate Speech Detection Using Machine Learning</a></div>
<div class="paper-author">Fariha Tahosin Boishakhi, Ponkoj Chandra Shill, Md. Golam Rabiul Alam</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the continuous growth of internet users and media content, it is very hard to track down hateful speech in audio and video. converting video or audio into text does not detect hate speech accurately as human sometimes uses hateful words as humorous or pleasant in sense and also uses different voice tones or show different action in the video. the state-ofthe-art hate speech detection models were mostly developed on a single modality. in this research, a combined approach of multimodal system has been proposed to detect hate speech from video contents by extracting feature images, feature values extracted from the audio, text and used machine learning and natural language processing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08505" target="_blank">Diffudetox: A Mixed Diffusion Model for Text Detoxification</a></div>
<div class="paper-author">Griffin Floto, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Zhenwei Tang, Ali Pesaranghader, Manasa Bharadwaj, Scott Sanner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. it is highly useful for online forums and social media, where offensive content is frequently encountered. intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users. conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task. in this work, we propose diffudetox, a mixed conditional and unconditional diffusion model for text detoxification. the conditional model takes toxic text as the condition and reduces its toxicity, yielding a diverse set of detoxified sentences. the unconditional model is trained to recover the input text, which allows the introduction of additional fluent text for training and thus ensures text fluency. extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed diffudetox.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08804" target="_blank">Peace: Cross-Platform Hate Speech Detection- A Causality-Guided Framework</a></div>
<div class="paper-author">Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech detection refers to the task of detecting hateful content that aims at denigrating an individual or a group based on their religion, gender, sexual orientation, or other characteristics. due to the different policies of the platforms, different groups of people express hate in different ways. furthermore, due to the lack of labeled data in some platforms it becomes challenging to build hate speech detection models. to this end, we revisit if we can learn a generalizable hate speech detection model for the cross platform setting, where we train the model on the data from one (source) platform and generalize the model across multiple (target) platforms. existing generalization models rely on linguistic cues or auxiliary information, making them biased towards certain tags or certain kinds of words (e.g., abusive words) on the source platform and thus not applicable to the target platforms. inspired by social and psychological theories, we endeavor to explore if there exist inherent causal cues that can be leveraged to learn generalizable representations for detecting hate speech across these distribution shifts. to this end, we propose a causality-guided framework, peace, that identifies and leverages two intrinsic causal cues omnipresent in hateful content: the overall sentiment and the aggression in the text. we conduct extensive experiments across multiple platforms (representing the distribution shift) showing if causal cues can help cross-platform generalization.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07567" target="_blank">Large Language Models Sometimes Generate Purely Negatively-Reinforced Text</a></div>
<div class="paper-author">Fabien Roger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: when using adversarial training, it is common practice to train against the most egregious failures. however, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. one might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. in this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. we present a specific training setup that enables pythia-160m to guess passwords 13% more often than it would by guessing randomly, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. our code is available at www.github.com/fabienroger/learning-from-negative-examples
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07951" target="_blank">Questioning the Survey Responses of Large Language Models</a></div>
<div class="paper-author">Ricardo Dominguez-Olmedo, Moritz Hardt, Celestine Mendler-Dünner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models increase in capability, researchers have started to conduct surveys of all kinds on these models with varying scientific motivations. in this work, we examine what we can learn from language models' survey responses on the basis of the well-established american community survey (acs) by the u.s. census bureau. using a de-facto standard multiple-choice prompting technique and evaluating 40 different language models, hundreds of thousands of times each on questions from the acs, we systematically establish two dominant patterns. first, models have significant position and labeling biases, for example, towards survey responses labeled with the letter "a". second, when adjusting for labeling biases through randomized answer ordering, models across the board trend towards uniformly random survey responses. in fact, binary classifiers can almost perfectly differentiate between models' responses to the acs and the responses of the us census. taken together, our findings suggest caution in treating survey responses from language models as equivalent to those of human populations at present time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08158" target="_blank">Survey on Sociodemographic Bias in Natural Language Processing</a></div>
<div class="paper-author">Vipul Gupta, Pranav Narayanan Venkit, Shomir Wilson, Rebecca J. Passonneau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep neural networks often learn unintended bias during training, which might have harmful effects when deployed in real-world settings. this work surveys 214 papers related to sociodemographic bias in natural language processing (nlp). in this study, we aim to provide a more comprehensive understanding of the similarities and differences among approaches to sociodemographic bias in nlp. to better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. we identify three main categories of nlp bias research: types of bias, quantifying bias, and debiasing techniques. we highlight the current trends in quantifying bias and debiasing techniques, offering insights into their strengths and weaknesses. we conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world bias, and that debiasing techniques need to focus more on training methods. finally, we provide recommendations for future work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08161" target="_blank">H2ogpt: Democratizing Large Language Models</a></div>
<div class="paper-author">Arno Candel, Jon Mckinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, Chun Ming Lee, Marcos V. Conde, Pasha Stetsenko, Olivier Grellier, Srisatish Ambati</div>
<div class="abstract">
<div class="abstract-content">
Abstract: applications built on top of large language models (llms) such as gpt-4 represent a revolution in ai due to their human-level capabilities in natural language processing. however, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.   we introduce h2ogpt, a suite of open-source code repositories for the creation and use of llms based on generative pretrained transformers (gpts). the goal of this project is to create the world's best truly open-source alternative to closed-source approaches. in collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2ogpt models from 7 to 40 billion parameters, ready for commercial use under fully permissive apache 2.0 licenses. included in our release is 100\% private document search using natural language.   open-source language models help boost ai development and make it more accessible and trustworthy. they lower entry hurdles, allowing people and groups to tailor these models to their needs. this openness increases innovation, transparency, and fairness. an open-source strategy is needed to share ai benefits fairly, and h2o.ai will continue to democratize ai and llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08170" target="_blank">Is Your Wallet Snitching on You? An Analysis on the Privacy Implications of Web3</a></div>
<div class="paper-author">Christof Ferreira Torres, Fiona Willi, Shweta Shinde</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the recent hype around the metaverse and nfts, web3 is getting more and more popular. the goal of web3 is to decentralize the web via decentralized applications. wallets play a crucial role as they act as an interface between these applications and the user. wallets such as metamask are being used by millions of users nowadays. unfortunately, web3 is often advertised as more secure and private. however, decentralized applications as well as wallets are based on traditional technologies, which are not designed with privacy of users in mind. in this paper, we analyze the privacy implications that web3 technologies such as decentralized applications and wallets have on users. to this end, we build a framework that measures exposure of wallet information. first, we study whether information about installed wallets is being used to track users online. we analyze the top 100k websites and find evidence of 1,325 websites running scripts that probe whether users have wallets installed in their browser. second, we measure whether decentralized applications and wallets leak the user's unique wallet address to third-parties. we intercept the traffic of 616 decentralized applications and 100 wallets and find over 2000 leaks across 211 applications and more than 300 leaks across 13 wallets. our study shows that web3 poses a threat to users' privacy and requires new designs towards more privacy-aware wallet architectures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08190" target="_blank">Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the Liar Dataset</a></div>
<div class="paper-author">Mars Gokturk Buchholz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the detection of political fake statements is crucial for maintaining information integrity and preventing the spread of misinformation in society. historically, state-of-the-art machine learning models employed various methods for detecting deceptive statements. these methods include the use of metadata (w. wang et al., 2018), n-grams analysis (singh et al., 2021), and linguistic (wu et al., 2022) and stylometric (islam et al., 2020) features. recent advancements in large language models, such as gpt-3 (brown et al., 2020) have achieved state-of-the-art performance on a wide range of tasks. in this study, we conducted experiments with gpt-3 on the liar dataset (w. wang et al., 2018) and achieved higher accuracy than state-of-the-art models without using any additional meta or linguistic features. additionally, we experimented with zero-shot learning using a carefully designed prompt and achieved near state-of-the-art performance. an advantage of this approach is that the model provided evidence for its decision, which adds transparency to the model's decision-making and offers a chance for users to verify the validity of the evidence provided.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08223" target="_blank">Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving Framework Based on Text Sanitization</a></div>
<div class="paper-author">Zhigang Kan, Linbo Qiao, Hao Yu, Liwen Peng, Yifu Gao, Dongsheng Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are gaining increasing attention due to their exceptional performance across numerous tasks. as a result, the general public utilize them as an influential tool for boosting their productivity while natural language processing researchers endeavor to employ them in solving existing or new research problems. unfortunately, individuals can only access such powerful ais through apis, which ultimately leads to the transmission of raw data to the models' providers and increases the possibility of privacy data leakage. current privacy-preserving methods for cloud-deployed language models aim to protect privacy information in the pre-training dataset or during the model training phase. however, they do not meet the specific challenges presented by the remote access approach of new large-scale language models.   this paper introduces a novel task, "user privacy protection for dialogue models," which aims to safeguard sensitive user information from any possible disclosure while conversing with chatbots. we also present an evaluation scheme for this task, which covers evaluation metrics for privacy protection, data availability, and resistance to simulation attacks. moreover, we propose the first framework for this task, namely privacy protection through text sanitization. before sending the input to remote large models, it filters out the sensitive information, using several rounds of text sanitization based on privacy types that users define. upon receiving responses from the larger model, our framework automatically restores privacy to ensure that the conversation goes smoothly, without intervention from the privacy filter. experiments based on real-world datasets demonstrate the efficacy of our privacy-preserving approach against eavesdropping from potential attackers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06924" target="_blank">Tasra: A Taxonomy and Analysis of Societal-Scale Risks From Ai</a></div>
<div class="paper-author">Andrew Critch, Stuart Russell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\em exhaustive taxonomy} of such risks. many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. this paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? we also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many ai systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07135" target="_blank">On the Amplification of Linguistic Bias Through Unintentional Self-Reinforcement Learning by Generative Language Models -- A Perspective</a></div>
<div class="paper-author">Minhyeok Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative language models (glms) have the potential to significantly shape our linguistic landscape due to their expansive use in various digital applications. however, this widespread adoption might inadvertently trigger a self-reinforcement learning cycle that can amplify existing linguistic biases. this paper explores the possibility of such a phenomenon, where the initial biases in glms, reflected in their generated text, can feed into the learning material of subsequent models, thereby reinforcing and amplifying these biases. moreover, the paper highlights how the pervasive nature of glms might influence the linguistic and cognitive development of future generations, as they may unconsciously learn and reproduce these biases. the implications of this potential self-reinforcement cycle extend beyond the models themselves, impacting human language and discourse. the advantages and disadvantages of this bias amplification are weighed, considering educational benefits and ease of future glm learning against threats to linguistic diversity and dependence on initial glms. this paper underscores the need for rigorous research to understand and address these issues. it advocates for improved model transparency, bias-aware training techniques, development of methods to distinguish between human and glm-generated text, and robust measures for fairness and bias evaluation in glms. the aim is to ensure the effective, safe, and equitable use of these powerful technologies, while preserving the richness and diversity of human language.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07152" target="_blank">Measuring Sentiment Bias in Machine Translation</a></div>
<div class="paper-author">Kai Hartung, Aaricia Herygers, Shubham Kurlekar, Khabbab Zakaria, Taylan Volkan, Sören Gröttrup, Munir Georges</div>
<div class="abstract">
<div class="abstract-content">
Abstract: biases induced to text by generative models have become an increasingly large topic in recent years. in this paper we explore how machine translation might introduce a bias in sentiments as classified by sentiment analysis models. for this, we compare three open access machine translation models for five different languages on two parallel corpora to test if the translation process causes a shift in sentiment classes recognized in the texts. though our statistic test indicate shifts in the label probability distributions, we find none that appears consistent enough to assume a bias induced by the translation process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07500" target="_blank">Adding Guardrails to Advanced Chatbots</a></div>
<div class="paper-author">Yanchen Wang, Lisa Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai models continue to become more powerful. the launch of chatgpt in november 2022 has ushered in a new era of ai. chatgpt and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. there are already concerns that humans may be replaced by chatbots for a variety of jobs. because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. these biases may cause significant harm and/or inequity toward different subpopulations. to understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of chatgpt to determine the types of questions that are answered fairly and the types that still need improvement. we find that chatgpt is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. we find that chatgpt is very sensitive to changes in the prompt, where small changes lead to different levels of fairness. this suggests that we need to immediately implement "corrections" or mitigation strategies in order to improve fairness of these systems. we suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.10052" target="_blank">Assigning Ai: Seven Approaches for Students, With Prompts</a></div>
<div class="paper-author">Ethan Mollick, Lilach Mollick</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper examines the transformative role of large language models (llms) in education and their potential as learning tools, despite their inherent risks and limitations. the authors propose seven approaches for utilizing ai in classrooms: ai-tutor, ai-coach, ai-mentor, ai-teammate, ai-tool, ai-simulator, and ai-student, each with distinct pedagogical benefits and risks. the aim is to help students learn with and about ai, with practical strategies designed to mitigate risks such as complacency about the ai's output, errors, and biases. these strategies promote active oversight, critical assessment of ai outputs, and complementarity of ai's capabilities with the students' unique insights. by challenging students to remain the "human in the loop," the authors aim to enhance learning outcomes while ensuring that ai serves as a supportive tool rather than a replacement. the proposed framework offers a guide for educators navigating the integration of ai-assisted learning in classrooms
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06815" target="_blank">Trojllm: A Black-Box Trojan Prompt Attack on Large Language Models</a></div>
<div class="paper-author">Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau Boloni, Qian Lou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are progressively being utilized as machine learning services and interface tools for various applications. however, the security implications of llms, particularly in relation to adversarial and trojan attacks, remain insufficiently examined. in this paper, we propose trojllm, an automatic and black-box framework to effectively generate universal and stealthy triggers. when these triggers are incorporated into the input data, the llms' outputs can be maliciously manipulated. moreover, the framework also supports embedding trojans within discrete prompts, enhancing the overall effectiveness and precision of the triggers' attacks. specifically, we propose a trigger discovery algorithm for generating universal triggers for various inputs by querying victim llm-based apis using few-shot data samples. furthermore, we introduce a novel progressive trojan poisoning algorithm designed to generate poisoned prompts that retain efficacy and transferability across a diverse range of models. our experiments and results demonstrate trojllm's capacity to effectively insert trojans into text prompts in real-world black-box llm apis including gpt-3.5 and gpt-4, while maintaining exceptional performance on clean test sets. our work sheds light on the potential security risks in current models and offers a potential defensive approach. the source code of trojllm is available at https://github.com/ucf-ml-research/trojllm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05816" target="_blank">Detecting Phishing Sites Using Chatgpt</a></div>
<div class="paper-author">Takashi Koide, Naoki Fukushi, Hiroki Nakano, Daiki Chiba</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise of large language models (llms) has had a significant impact on various domains, including natural language processing and artificial intelligence. while llms such as chatgpt have been extensively researched for tasks such as code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, has been largely unexplored. to combat the rising tide of automated cyber attacks facilitated by llms, it is imperative to automate the detection of malicious web content, which requires approaches that leverage the power of llms to analyze and classify phishing sites. in this paper, we propose a novel method that utilizes chatgpt to detect phishing sites. our approach involves leveraging a web crawler to gather information from websites and generate prompts based on this collected data. this approach enables us to detect various phishing sites without the need for fine-tuning machine learning models and identify social engineering techniques from the context of entire websites and urls. to evaluate the performance of our proposed method, we conducted experiments using a dataset. the experimental results using gpt-4 demonstrated promising performance, with a precision of 98.3% and a recall of 98.4%. comparative analysis between gpt-3.5 and gpt-4 revealed an enhancement in the latter's capability to reduce false negatives. these findings not only highlight the potential of llms in efficiently identifying phishing sites but also have significant implications for enhancing cybersecurity measures and protecting users from the dangers of online fraudulent activities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05871" target="_blank">Towards a Robust Detection of Language Model Generated Text: Is Chatgpt That Easy to Detect?</a></div>
<div class="paper-author">Wissam Antoun, Virginie Mouilleron, Benoît Sagot, Djamé Seddah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in natural language processing (nlp) have led to the development of large language models (llms) such as chatgpt. this paper proposes a methodology for developing and evaluating chatgpt detectors for french text, with a focus on investigating their robustness on out-of-domain data and against common attack schemes. the proposed method involves translating an english dataset into french and training a classifier on the translated data. results show that the detectors can effectively detect chatgpt-generated text, with a degree of robustness against basic attack techniques in in-domain settings. however, vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text. the study emphasizes caution when applying in-domain testing results to a wider variety of content. we provide our translated datasets and models as open-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05882" target="_blank">Good, but Not Always Fair: An Evaluation of Gender Bias for Three Commercial Machine Translation Systems</a></div>
<div class="paper-author">Silvia Alma Piazzolla, Beatrice Savoldi, Luisa Bentivogli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine translation (mt) continues to make significant strides in quality and is increasingly adopted on a larger scale. consequently, analyses have been redirected to more nuanced aspects, intricate phenomena, as well as potential risks that may arise from the widespread use of mt tools. along this line, this paper offers a meticulous assessment of three commercial mt systems - google translate, deepl, and modern mt - with a specific focus on gender translation and bias. for three language pairs (english/spanish, english/italian, and english/french), we scrutinize the behavior of such systems at several levels of granularity and on a variety of naturally occurring gender phenomena in translation. our study takes stock of the current state of online mt tools, by revealing significant discrepancies in the gender translation of the three systems, with each system displaying varying degrees of bias despite their overall translation quality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05949" target="_blank">Evaluating the Social Impact of Generative Ai Systems in Systems and Society</a></div>
<div class="paper-author">Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, Apostol Vassilev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. we move toward a standard approach in evaluating a generative ai system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. we describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. we offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. each subcategory includes recommendations for mitigating harm. we are concurrently crafting an evaluation repository for the ai research community to contribute existing evaluations along the given categories. this version will be updated following a craft session at acm facct 2023.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05952" target="_blank">Overcoming Adversarial Attacks for Human-in-the-Loop Applications</a></div>
<div class="paper-author">Ryan Mccoppin, Marla Kennedy, Platon Lukyanenko, Sean Kennedy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: including human analysis has the potential to positively affect the robustness of deep neural networks and is relatively unexplored in the adversarial machine learning literature. neural network visual explanation maps have been shown to be prone to adversarial attacks. further research is needed in order to select robust visualizations of explanations for the image analyst to evaluate a given model. these factors greatly impact human-in-the-loop (hitl) evaluation tools due to their reliance on adversarial images, including explanation maps and measurements of robustness. we believe models of human visual attention may improve interpretability and robustness of human-machine imagery analysis systems. our challenge remains, how can hitl evaluation be robust in this adversarial landscape?
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06085" target="_blank">Trapping LLM Hallucinations Using Tagged Context Prompts</a></div>
<div class="paper-author">Philip Feldman, James R. Foulds, Shimei Pan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in large language models (llms), such as chatgpt, have led to highly sophisticated conversation agents. however, these models suffer from "hallucinations," where the model generates false or fabricated information. addressing this challenge is crucial, particularly with ai-driven platforms being adopted across various sectors. in this paper, we propose a novel method to recognize and flag instances when llms perform outside their domain knowledge, and ensuring users receive accurate information.   we find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. to do this, we baseline hallucination frequency in no-context prompt-response pairs using generated urls as easily-tested indicators of fabricated data. we observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88% effectiveness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06199" target="_blank">Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording</a></div>
<div class="paper-author">Aisha Khatun, Daniel G. Brown</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have become mainstream technology with their versatile use cases and impressive performance. despite the countless out-of-the-box applications, llms are still not reliable. a lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and reinforcement learning with human feedback (rlhf), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. in this work, we analyze what confuses gpt-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. we find that gpt-3 correctly disagrees with obvious conspiracies and stereotypes but makes mistakes with common misconceptions and controversies. the model responses are inconsistent across prompts and settings, highlighting gpt-3's unreliability. dataset and code of our analysis is available in https://github.com/tanny411/gpt3-reliability-check.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07401" target="_blank">Implementing Bert and Fine-Tuned Roberta to Detect Ai Generated News by Chatgpt</a></div>
<div class="paper-author">Zecong Wang, Jiaxi Cheng, Chen Cui, Chenhao Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the abundance of information on social media has increased the necessity of accurate real-time rumour detection. manual techniques of identifying and verifying fake news generated by ai tools are impracticable and time-consuming given the enormous volume of information generated every day. this has sparked an increase in interest in creating automated systems to find fake news on the internet. the studies in this research demonstrate that the bert and roberta models with fine-tuning had the best success in detecting ai generated news. with a score of 98%, tweaked roberta in particular showed excellent precision. in conclusion, this study has shown that neural networks can be used to identify bogus news ai generation news created by chatgpt. the roberta and bert models' excellent performance indicates that these models can play a critical role in the fight against misinformation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11503" target="_blank">The Age of Synthetic Realities: Challenges and Opportunities</a></div>
<div class="paper-author">João Phillipe Cardenuto, Jing Yang, Rafael Padilha, Renjie Wan, Daniel Moreira, Haoliang Li, Shiqi Wang, Fernanda Andaló, Sébastien Marcel, Anderson Rocha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: synthetic realities are digital creations or augmentations that are contextually generated through the use of artificial intelligence (ai) methods, leveraging extensive amounts of data to construct new narratives or realities, regardless of the intent to deceive. in this paper, we delve into the concept of synthetic realities and their implications for digital forensics and society at large within the rapidly advancing field of ai. we highlight the crucial need for the development of forensic techniques capable of identifying harmful synthetic creations and distinguishing them from reality. this is especially important in scenarios involving the creation and dissemination of fake news, disinformation, and misinformation. our focus extends to various forms of media, such as images, videos, audio, and text, as we examine how synthetic realities are crafted and explore approaches to detecting these malicious creations. additionally, we shed light on the key research challenges that lie ahead in this area. this study is of paramount importance due to the rapid progress of ai generative techniques and their impact on the fundamental principles of forensic science.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05499" target="_blank">Prompt Injection Attack Against LLM-Integrated Applications</a></div>
<div class="paper-author">Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. however, their extensive assimilation into various services introduces significant security risks. this study deconstructs the complexities and implications of prompt injection attacks on actual llm-integrated applications. initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. prompted by these limitations, we subsequently formulate houyi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. houyi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. leveraging houyi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary llm usage and uncomplicated application prompt theft. we deploy houyi on 36 actual llm-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including notion, which has the potential to impact millions of users. our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05550" target="_blank">Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks</a></div>
<div class="paper-author">Katelyn X. Mei, Sonia Fereidooni, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid deployment of artificial intelligence (ai) models demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. this study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. it focuses on 93 stigmatized groups in the united states, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. we investigate bias against these groups in english pre-trained masked language models (mlms) and their downstream sentiment classification tasks. to evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. building upon a psychology scale of social rejection, the social distance scale, we prompt six mlms: roberta-base, roberta-large, xlnet-large, bertweet-base, bertweet-large, and distilbert. we use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. when prompts include stigmatized conditions, the probability of mlms predicting negative words is approximately 20 percent higher than when prompts have non-stigmatized conditions. in the sentiment classification tasks, when sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. we also observe a strong correlation between bias in mlms and their downstream sentiment classifiers (r =0.79). the evidence indicates that mlms and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05569" target="_blank">Disinformation 2.0 in the Age of Ai: A Cybersecurity Perspective</a></div>
<div class="paper-author">Wojciech Mazurczyk, Dongwon Lee, Andreas Vlachos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the explosive advancement of ai technologies in recent years, the scene of the disinformation research is also expected to rapidly change. in this viewpoint article, in particular, we first present the notion of "disinformation 2.0" in the age of ai where disinformation would become more targeted and personalized, its content becomes very difficult to distinguish from real news, and its creation and dissemination become more accelerated by ai. then, we discuss how disinformation 2.0 and cybersecurity fit and a possible layered countermeasure to address the threat in disinformation 2.0 in a holistic manner.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05652" target="_blank">Privacy Aware Question-Answering System for Online Mental Health Risk Assessment</a></div>
<div class="paper-author">Prateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia, Shweta Kumari</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. however, many users fail to receive genuine clinical support, thus exacerbating their symptoms. screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives. pre-trained language models (lms) can assess users' social media data and classify them in terms of their mental health risk. we propose a question-answering (qa) approach to assess mental health risk using the unified-qa model on two large mental health datasets. to protect user data, we extend unified-qa by anonymizing the model training process using differential privacy. our results demonstrate the effectiveness of modeling risk assessment as a qa task, specifically for mental health use cases. furthermore, the model's performance decreases by less than 1% with the inclusion of differential privacy. the proposed system's performance is indicative of a promising research direction that will lead to the development of privacy-aware diagnostic systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05659" target="_blank">Cover: A Heuristic Greedy Adversarial Attack on Prompt-Based Learning in Language Models</a></div>
<div class="paper-author">Zihao Tan, Qingliang Chen, Wenbin Zhu, Yongjian Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-based learning has been proved to be an effective way in pre-trained language models (plms), especially in low-resource scenarios like few-shot settings. however, the trustworthiness of plms is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. in this paper, we will shed light on some vulnerabilities of plms, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. first of all, we design character-level and word-level heuristic approaches to break manual templates separately. then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. finally, we evaluate our approach with the classification tasks on three variants of bert series models and eight datasets. and comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate and attack speed.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13659" target="_blank">Toward a Logical Theory of Fairness and Bias</a></div>
<div class="paper-author">Vaishak Belle</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fairness in machine learning is of considerable interest in recent years owing to the propensity of algorithms trained on historical data to amplify and perpetuate historical biases. in this paper, we argue for a formal reconstruction of fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting and allow for rich environmental modelling. consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalise these in the epistemic situation calculus.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04528" target="_blank">Promptbench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</a></div>
<div class="paper-author">Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing reliance on large language models (llms) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. in response to this vital need, we introduce promptbench, a robustness benchmark designed to measure llms' resilience to adversarial prompts. this study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. the adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect llm outcomes while maintaining semantic integrity. these prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. our findings demonstrate that contemporary llms are not robust to adversarial prompts. furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. we then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. code is available at: https://github.com/microsoft/promptbench.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04597" target="_blank">Language Models Get a Gender Makeover: Mitigating Gender Bias With Few-Shot Data Interventions</a></div>
<div class="paper-author">Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency</div>
<div class="abstract">
<div class="abstract-content">
Abstract: societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. while the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04634" target="_blank">On the Reliability of Watermarks for Large Language Models</a></div>
<div class="paper-author">John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as llms become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of llm-generated text. yet a crucial question remains: how reliable is watermarking in realistic settings in the wild? there, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection.   we study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked llm, or mixed into a longer hand-written document. we find that watermarks remain detectable even after human and machine paraphrasing. while these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. for example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. we also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04707" target="_blank">Improving Open Language Models by Learning From Organic Interactions</a></div>
<div class="paper-author">Jing Xu, Da Ju, Joshua Lane, Mojtaba Komeili, Eric Michael Smith, Megan Ung, Morteza Behrooz, William Ngan, Rashel Moritz, Sainbayar Sukhbaatar, Y-Lan Boureau, Jason Weston, Kurt Shuster</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present blenderbot 3x, an update on the conversational model blenderbot 3, which is now trained using organic conversation and feedback data from participating users of the system in order to improve both its skills and safety. we are publicly releasing the participating de-identified interaction data for use by the research community, in order to spur further progress. training models with organic data is challenging because interactions with people "in the wild" include both high quality conversations and feedback, as well as adversarial and toxic behavior. we study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses. blenderbot 3x is both preferred in conversation to blenderbot 3, and is shown to produce safer responses in challenging situations. while our current models are still far from perfect, we believe further improvement can be achieved by continued use of the techniques explored in this work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04735" target="_blank">Soft-Prompt Tuning for Large Language Models to Evaluate Bias</a></div>
<div class="paper-author">Jacob-Junqi Tian, David Emerson, Sevil Zanjani Miyandoab, Deval Pandya, Laleh Seyyed-Kalantari, Faiza Khan Khattak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. however, this requires prompt tuning to get optimal prompts that lead to better model performances. in this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (llms) such as open pre-trained transformers (opt) and galactica language model. since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. we check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. since llms have been used in the industry in various applications, it is crucial to identify the biases before deploying these models in practice. we open-source our pipeline and encourage industry researchers to adapt our work to their use cases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05524" target="_blank">Check Me if You Can: Detecting Chatgpt-Generated Academic Writing Using Checkgpt</a></div>
<div class="paper-author">Zeyan Liu, Zijun Yao, Fengjun Li, Bo Luo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with chatgpt under the spotlight, utilizing large language models (llms) for academic writing has drawn a significant amount of discussions and concerns in the community. while substantial research efforts have been stimulated for detecting llm-generated content (llm-content), most of the attempts are still in the early stage of exploration. in this paper, we present a holistic investigation of detecting llm-generate academic writing, by providing a dataset, evidence, and algorithms, in order to inspire more community effort to address the concern of llm academic misuse. we first present gpabenchmark, a benchmarking dataset of 600,000 samples of human-written, gpt-written, gpt-completed, and gpt-polished abstracts of research papers in cs, physics, and humanities and social sciences (hss). we show that existing open-source and commercial gpt detectors provide unsatisfactory performance on gpabenchmark, especially for gpt-polished text. moreover, through a user study of 150+ participants, we show that it is highly challenging for human users, including experienced faculty members and researchers, to identify gpt-generated abstracts. we then present checkgpt, a novel llm-content detector consisting of a general representation module and an attentive-bilstm classification module, which is accurate, transferable, and interpretable. experimental results show that checkgpt achieves an average classification accuracy of 98% to 99% for the task-specific discipline-specific detectors and the unified detectors. checkgpt is also highly transferable that, without tuning, it achieves ~90% accuracy in new domains, such as news articles, while a model tuned with approximately 2,000 samples in the target domain achieves ~98% accuracy. finally, we demonstrate the explainability insights obtained from checkgpt to reveal the key behaviors of how llm generates texts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03423" target="_blank">I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models</a></div>
<div class="paper-author">Max Reuter, William Schulze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: since the release of openai's chatgpt, generative language models have attracted extensive public attention. the increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. in this experiment, we characterize chatgpt's refusal behavior using a black-box attack. we first query chatgpt with a variety of offensive and benign prompts (n=1,706), then manually label each response as compliance or refusal. manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. the small manually-labeled dataset is used to train a refusal classifier, which achieves an accuracy of 96%. second, we use this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from the quora insincere questions dataset. with this machine-labeled data, we train a prompt classifier to predict whether chatgpt will refuse a given question, without seeing chatgpt's response. this prompt classifier achieves 76% accuracy on a test set of manually labeled questions (n=985). we examine our classifiers and the prompt n-grams that are most predictive of either compliance or refusal. our datasets and code are available at https://github.com/maxwellreuter/chatgpt-refusals.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03503" target="_blank">Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models</a></div>
<div class="paper-author">Jose Berengueres, Marybeth Sandell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper explores how ai-owners can develop safeguards for ai-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. it delves into the current state of ethical awareness on large language models (llms). by dissecting the mechanism of content generation by llms, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. a comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. the paper's key argument is that existing it-related ethical codes, while adequate for traditional it engineering, are inadequate for the challenges posed by llm-based content generation. drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling llm-generated content. finally, potential conflicts of interest between dataset curation at upstream and ethical benchmarking downstream are highlighted to underscore the need for a broader evaluation beyond mere output. this study prompts a nuanced conversation around ethical implications in this rapidly evolving field of content generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03722" target="_blank">Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages With Limited Labeled Data</a></div>
<div class="paper-author">Janis Goldzycher, Moritz Preisig, Chantal Amrhein, Gerold Schneider</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most research on hate speech detection has focused on english where a sizeable amount of labeled training data is available. however, to expand hate speech detection into more languages, approaches that require minimal training data are needed. in this paper, we test whether natural language inference (nli) models which perform well in zero- and few-shot settings can benefit hate speech detection performance in scenarios where only a limited amount of labeled data is available in the target language. our evaluation on five languages demonstrates large performance improvements of nli fine-tuning over direct fine-tuning in the target language. however, the effectiveness of previous work that proposed intermediate fine-tuning on english data is hard to match. only in settings where the english training data does not match the test domain, can our customised nli-formulation outperform intermediate fine-tuning on english. based on our extensive experiments, we propose a set of recommendations for hate speech detection in languages where minimal labeled training data is available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03809" target="_blank">Can Large Language Models Democratize Access to Dual-Use Biotechnology?</a></div>
<div class="paper-author">Emily H. Soice, Rafael Rocha, Kimberlee Cordova, Michael Specter, Kevin M. Esvelt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as those embedded in 'chatbots' are accelerating and democratizing research by providing comprehensible information and expertise from many different fields. however, these models may also confer easy access to dual-use technologies capable of inflicting great harm. to evaluate this risk, the 'safeguarding the future' course at mit tasked non-scientist students with investigating whether llm chatbots could be prompted to assist non-experts in causing a pandemic. in one hour, the chatbots suggested four potential pandemic pathogens, explained how they can be generated from synthetic dna using reverse genetics, supplied the names of dna synthesis companies unlikely to screen orders, identified detailed protocols and how to troubleshoot them, and recommended that anyone lacking the skills to perform reverse genetics engage a core facility or contract research organization. collectively, these results suggest that llms will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training. promising nonproliferation measures include pre-release evaluations of llms by third parties, curating training datasets to remove harmful concepts, and verifiably screening all dna generated by synthesis providers or used by contract research organizations and robotic cloud laboratories to engineer organisms or viruses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03950" target="_blank">Misgendered: Limits of Large Language Models in Understanding Pronouns</a></div>
<div class="paper-author">Tamanna Hossain, Sunipa Dev, Sameer Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: content warning: this paper contains examples of misgendering and erasure that could be offensive and potentially triggering.   gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. it is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. in this paper, we comprehensively evaluate popular language models for their ability to correctly use english gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. we introduce misgendered, a framework for evaluating large language models' ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual's pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive language models using a unified method. when prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging 34.2% accuracy). this inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations. few-shot adaptation with explicit examples in the prompt improves performance for neo-pronouns, but only to 64.7% even with 20 shots. we release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendered/
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04031" target="_blank">Certified Reasoning With Language Models</a></div>
<div class="paper-author">Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models often achieve higher accuracy when reasoning step-by-step in complex tasks. however, their reasoning can be unsound, inconsistent, or rely on undesirable prior assumptions. to tackle these issues, we introduce a class of tools for language models called guides that use state and incremental constraints to guide generation. a guide can be invoked by the model to constrain its own generation to a set of valid statements given by the tool. in turn, the model's choices can change the guide's state. we show how a general system for logical reasoning can be used as a guide, which we call logicguide. given a reasoning problem in natural language, a model can formalize its assumptions for logicguide and then guarantee that its reasoning steps are sound. in experiments with the prontoqa and proofwriter reasoning datasets, logicguide significantly improves the performance of gpt-3, gpt-3.5 turbo and llama (accuracy gains up to 35%). logicguide also drastically reduces content effects: the interference of prior and current assumptions that both humans and language models have been shown to suffer from. finally, we explore bootstrapping llama 13b from its own reasoning and find that logicguide is critical: by training only on certified self-generated reasoning, llama can self-improve, avoiding learning from its own hallucinations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04067" target="_blank">An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models</a></div>
<div class="paper-author">Zhongbin Xie, Thomas Lukasiewicz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. in this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (cda) for bias mitigation. we conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. we find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for gpt-2 than bert, (ii) are less effective when it comes to racial and religious bias, which may be attributed to the limitations of cda, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in bert and gpt-2, evaluated via fact retrieval and downstream fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04118" target="_blank">M$^3$fair: Mitigating Bias in Healthcare Data Through Multi-Level and Multi-Sensitive-Attribute Reweighting Method</a></div>
<div class="paper-author">Yinghao Zhu, Jingkun An, Enshen Zhou, Lu An, Junyi Gao, Hao Li, Haoran Feng, Bo Hou, Wen Tang, Chengwei Pan, Liantao Ma</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the data-driven artificial intelligence paradigm, models heavily rely on large amounts of training data. however, factors like sampling distribution imbalance can lead to issues of bias and unfairness in healthcare data. sensitive attributes, such as race, gender, age, and medical condition, are characteristics of individuals that are commonly associated with discrimination or bias. in healthcare ai, these attributes can play a significant role in determining the quality of care that individuals receive. for example, minority groups often receive fewer procedures and poorer-quality medical care than white individuals in us. therefore, detecting and mitigating bias in data is crucial to enhancing health equity. bias mitigation methods include pre-processing, in-processing, and post-processing. among them, reweighting (rw) is a widely used pre-processing method that performs well in balancing machine learning performance and fairness performance. rw adjusts the weights for samples within each (group, label) combination, where these weights are utilized in loss functions. however, rw is limited to considering only a single sensitive attribute when mitigating bias and assumes that each sensitive attribute is equally important. this may result in potential inaccuracies when addressing intersectional bias. to address these limitations, we propose m3fair, a multi-level and multi-sensitive-attribute reweighting method by extending the rw method to multiple sensitive attributes at multiple levels. our experiments on real-world datasets show that the approach is effective, straightforward, and generalizable in addressing the healthcare fairness issues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02612" target="_blank">Building Resilient Smes: Harnessing Large Language Models for Cyber Security in Australia</a></div>
<div class="paper-author">Benjamin Kereopa-Yorke</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the escalating digitalisation of our lives and enterprises has led to a parallel growth in the complexity and frequency of cyber-attacks. small and medium-sized enterprises (smes), particularly in australia, are experiencing increased vulnerability to cyber threats, posing a significant challenge to the nation's cyber security landscape. embracing transformative technologies such as artificial intelligence (ai), machine learning (ml) and large language models (llms) can potentially strengthen cyber security policies for australian smes. however, their practical application, advantages, and limitations remain underexplored, with prior research mainly focusing on large corporations. this study aims to address this gap by providing a comprehensive understanding of the potential role of llms in enhancing cyber security policies for australian smes. employing a mixed-methods study design, this research includes a literature review, qualitative analysis of sme case studies, and a quantitative assessment of llm performance metrics in cyber security applications. the findings highlight the promising potential of llms across various performance criteria, including relevance, accuracy, and applicability, though gaps remain in areas such as completeness and clarity. the study underlines the importance of integrating human expertise with llm technology and refining model development to address these limitations. by proposing a robust conceptual framework guiding the effective adoption of llms, this research aims to contribute to a safer and more resilient cyber environment for australian smes, enabling sustainable growth and competitiveness in the digital era.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02978" target="_blank">Which Argumentative Aspects of Hate Speech in Social Media Can Be Reliably Identified?</a></div>
<div class="paper-author">Damián Furman, Pablo Torres, José A. Rodríguez, Diego Letzen, Vanina Martínez, Laura Alonso Alemany</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the increasing diversity of use cases of large language models, a more informative treatment of texts seems necessary. an argumentative analysis could foster a more reasoned usage of chatbots, text completion mechanisms or other applications. however, it is unclear which aspects of argumentation can be reliably identified and integrated in language models. in this paper, we present an empirical assessment of the reliability with which different argumentative aspects can be automatically identified in hate speech in social media. we have enriched the hateval corpus (basile et al. 2019) with a manual annotation of some argumentative components, adapted from wagemans (2016)'s periodic table of arguments. we show that some components can be identified with reasonable reliability. for those that present a high error ratio, we analyze the patterns of disagreement between expert annotators and errors in automatic procedures, and we propose adaptations of those categories that can be more reliably reproduced.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03341" target="_blank">Inference-Time Intervention: Eliciting Truthful Answers From a Language Model</a></div>
<div class="paper-author">Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce inference-time intervention (iti), a technique designed to enhance the "truthfulness" of large language models (llms). iti operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. this intervention significantly improves the performance of llama models on the truthfulqa benchmark. on an instruction-finetuned llama called alpaca, iti improves its truthfulness from 32.5% to 65.1%. we identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. iti is minimally invasive and computationally inexpensive. moreover, the technique is data efficient: while approaches like rlhf require extensive annotations, iti locates truthful directions using only few hundred examples. our findings suggest that llms may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03350" target="_blank">Click: Controllable Text Generation With Sequence Likelihood Contrastive Learning</a></div>
<div class="paper-author">Chujie Zheng, Pei Ke, Zheng Zhang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. we introduce click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. it employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). it also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. on the tasks of language detoxification, sentiment steering, and repetition reduction, we show that click outperforms strong baselines of controllable text generation and demonstrate the superiority of click's sample construction strategy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03358" target="_blank">Is Ai Changing the Rules of Academic Misconduct? An in-Depth Look at Students' Perceptions of 'Ai-Giarism'</a></div>
<div class="paper-author">Cecilia Ka Yuk Chan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this pioneering study explores students' perceptions of ai-giarism, an emergent form of academic dishonesty involving ai and plagiarism, within the higher education context. a survey, undertaken by 393 undergraduate and postgraduate students from a variety of disciplines, investigated their perceptions of diverse ai-giarism scenarios. the findings portray a complex landscape of understanding, with clear disapproval for direct ai content generation, yet more ambivalent attitudes towards subtler uses of ai. the study introduces a novel instrument, as an initial conceptualization of ai-giarism, offering a significant tool for educators and policy-makers. this scale facilitates understanding and discussions around ai-related academic misconduct, aiding in pedagogical design and assessment in an era of ai integration. moreover, it challenges traditional definitions of academic misconduct, emphasizing the need to adapt in response to evolving ai technology. despite limitations, such as the rapidly changing nature of ai and the use of convenience sampling, the study provides pivotal insights for academia, policy-making, and the broader integration of ai technology in education.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02294" target="_blank">Exposing Bias in Online Communities Through Large-Scale Language Models</a></div>
<div class="paper-author">Celine Wald, Lukas Pfahler</div>
<div class="abstract">
<div class="abstract-content">
Abstract: progress in natural language generation research has been shaped by the ever-growing size of language models. while large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes. this work utilises the flaw of bias in language models to explore the biases of six different online communities. in order to get an insight into the communities' viewpoints, we fine-tune gpt-neo 1.3b with six social media datasets. the bias of the resulting models is evaluated by prompting the models with different demographics and comparing the sentiment and toxicity values of these generations. together, these methods reveal that bias differs in type and intensity for the various models. this work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets or communities. additionally, the examples generated for this work demonstrate the limitations of using automated sentiment and toxicity classifiers in bias research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02384" target="_blank">Spear or Shield: Leveraging Generative Ai to Tackle Security Threats of Intelligent Network Services</a></div>
<div class="paper-author">Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Kwok-Yan Lam, Yuguang Fang, Yonghui Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai (gai) models have been rapidly advancing, with a wide range of applications including intelligent networks and mobile ai-generated content (aigc) services. despite their numerous applications and potential, such models create opportunities for novel security challenges. in this paper, we examine the challenges and opportunities of gai in the realm of the security of intelligent network aigc services such as suggesting security policies, acting as both a ``spear'' for potential attacks and a ``shield'' as an integral part of various defense mechanisms. first, we present a comprehensive overview of the gai landscape, highlighting its applications and the techniques underpinning these advancements, especially large language and diffusion models. then, we investigate the dynamic interplay between gai's spear and shield roles, highlighting two primary categories of potential gai-related attacks and their respective defense strategies within wireless networks. a case study illustrates the impact of gai defense strategies on energy consumption in an image request scenario under data poisoning attack. our results show that by employing an ai-optimized diffusion defense mechanism, energy can be reduced by 8.7%, and retransmission count can be decreased from 32 images, without defense, to just 6 images, showcasing the effectiveness of gai in enhancing network security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02428" target="_blank">Taught by the Internet, Exploring Bias in Openais Gpt3</a></div>
<div class="paper-author">Ali Ayaz, Aditya Nawalgaria, Ruilian Yin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this research delves into the current literature on bias in natural language processing models and the techniques proposed to mitigate the problem of bias, including why it is important to tackle bias in the first place. additionally, these techniques are further analysed in the light of newly developed models that tower in size over past editions. to achieve those aims, the authors of this paper conducted their research on gpt3 by openai, the largest nlp model available to consumers today. with 175 billion parameters in contrast to berts 340 million, gpt3 is the perfect model to test the common pitfalls of nlp models. tests were conducted through the development of an applicant tracking system using gpt3. for the sake of feasibility and time constraints, the tests primarily focused on gender bias, rather than all or multiple types of bias. finally, current mitigation techniques are considered and tested to measure their degree of functionality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02488" target="_blank">Adversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy</a></div>
<div class="paper-author">Xiaoting Li, Lingwei Chen, Dinghao Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media has drastically reshaped the world that allows billions of people to engage in such interactive environments to conveniently create and share content with the public. among them, text data (e.g., tweets, blogs) maintains the basic yet important social activities and generates a rich source of user-oriented information. while those explicit sensitive user data like credentials has been significantly protected by all means, personal private attribute (e.g., age, gender, location) disclosure due to inference attacks is somehow challenging to avoid, especially when powerful natural language processing (nlp) techniques have been effectively deployed to automate attribute inferences from implicit text data. this puts users' attribute privacy at risk. to address this challenge, in this paper, we leverage the inherent vulnerability of machine learning to adversarial attacks, and design a novel text-space adversarial attack for social good, called adv4sg. in other words, we cast the problem of protecting personal attribute privacy as an adversarial attack formulation problem over the social media text data to defend against nlp-based attribute inference attacks. more specifically, adv4sg proceeds with a sequence of word perturbations under given constraints such that the probed attribute cannot be identified correctly. different from the prior works, we advance adv4sg by considering social media property, and introducing cost-effective mechanisms to expedite attribute obfuscation over text data under the black-box setting. extensive experiments on real-world social media datasets have demonstrated that our method can effectively degrade the inference accuracy with less computational cost over different attribute settings, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02165" target="_blank">Learning to Defend by Attacking (And Vice-Versa): Transfer of Learning in Cybersecurity Games</a></div>
<div class="paper-author">Tyler Malloy, Cleotilde Gonzalez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: designing cyber defense systems to account for cognitive biases in human decision making has demonstrated significant success in improving performance against human attackers. however, much of the attention in this area has focused on relatively simple accounts of biases in human attackers, and little is known about adversarial behavior or how defenses could be improved by disrupting attacker's behavior. in this work, we present a novel model of human decision-making inspired by the cognitive faculties of instance-based learning theory, theory of mind, and transfer of learning. this model functions by learning from both roles in a security scenario: defender and attacker, and by making predictions of the opponent's beliefs, intentions, and actions. the proposed model can better defend against attacks from a wide range of opponents compared to alternatives that attempt to perform optimally without accounting for human biases. additionally, the proposed model performs better against a range of human-like behavior by explicitly modeling human transfer of learning, which has not yet been applied to cyber defense scenarios. results from simulation experiments demonstrate the potential usefulness of cognitively inspired models of agents trained in attack and defense roles and how these insights could potentially be used in real-world cybersecurity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02190" target="_blank">Stubborn Lexical Bias in Data and Models</a></div>
<div class="paper-author">Sofia Serrano, Jesse Dodge, Noah A. Smith</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in nlp, recent work has seen increased focus on spurious correlations between various features and labels in training data, and how these influence model behavior. however, the presence and effect of such correlations are typically examined feature by feature. we investigate the cumulative impact on a model of many such intersecting features. using a new statistical method, we examine whether such spurious patterns in data appear in models trained on the data. we select two tasks -- natural language inference and duplicate-question detection -- for which any unigram feature on its own should ideally be uninformative, which gives us a large pool of automatically extracted features with which to experiment. the large size of this pool allows us to investigate the intersection of features spuriously associated with (potentially different) labels. we then apply an optimization approach to *reweight* the training data, reducing thousands of spurious correlations, and examine how doing so affects models trained on the reweighted data. surprisingly, though this method can successfully reduce lexical biases in the training data, we still find strong evidence of corresponding bias in the trained models, including worsened bias for slightly more complex features (bigrams). we close with discussion about the implications of our results on what it means to "debias" training data, and how issues of data quality can affect model bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02231" target="_blank">Fine-Tuning Language Models With Advantage-Induced Policy Alignment</a></div>
<div class="paper-author">Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) has emerged as a reliable approach to aligning large language models (llms) to human preferences. among the plethora of rlhf techniques, proximal policy optimization (ppo) is of the most widely used methods. despite its popularity, however, ppo may suffer from mode collapse, instability, and poor sample efficiency. we show that these issues can be alleviated by a novel algorithm that we refer to as advantage-induced policy alignment (apa), which leverages a squared error loss function based on the estimated advantages. we demonstrate empirically that apa consistently outperforms ppo in language tasks by a large margin, when a separate reward model is employed as the evaluator. in addition, compared with ppo, apa offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. in addition to empirical results, we also provide a theoretical justification supporting the design of our loss function.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01261" target="_blank">Automatic Translation of Hate Speech to Non-Hate Speech in Social Media Texts</a></div>
<div class="paper-author">Yevhen Kostiuk, Atnafu Lambebo Tonja, Grigori Sidorov, Olga Kolesnikova</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we investigate the issue of hate speech by presenting a novel task of translating hate speech into non-hate speech text while preserving its meaning. as a case study, we use spanish texts. we provide a dataset and several baselines as a starting point for further research in the task. we evaluated our baseline results using multiple metrics, including bleu scores. the aim of this study is to contribute to the development of more effective methods for reducing the spread of hate speech in online communities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01693" target="_blank">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</a></div>
<div class="paper-author">Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. reinforcement learning from human feedback (rlhf) - where human preference judgments on lm outputs are transformed into a learning signal - has recently shown promise in addressing these issues. however, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. in this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. we introduce fine-grained rlhf, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). we conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. additionally, we show that lm behaviors can be customized using different combinations of fine-grained reward models. we release all data, collected human feedback, and codes at https://finegrainedrlhf.github.io.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01857" target="_blank">Knowledge of Cultural Moral Norms in Large Language Models</a></div>
<div class="paper-author">Aida Ramezani, Yang Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: moral norms vary across cultures. a recent line of work suggests that english large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. we investigate the extent to which monolingual english language models contain knowledge about moral norms in different countries. we consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as ``homosexuality'' and ``divorce''; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. we perform our analyses with two public datasets from the world values survey (across 55 countries) and pew global surveys (across 40 countries) on morality. we find that pre-trained english language models predict empirical moral norms across countries worse than the english moral norms reported previously. however, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the english moral norms. we discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01907" target="_blank">A Simple Yet Effective Self-Debiasing Framework for Transformer Models</a></div>
<div class="paper-author">Xiaoyue Wang, Lijie Wang, Xin Liu, Suhang Wu, Jinsong Su, Hua Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current transformer-based natural language understanding (nlu) models heavily rely on dataset biases, while failing to handle real-world out-of-distribution (ood) instances. many methods have been proposed to deal with this issue, but they ignore the fact that the features learned in different layers of transformer-based nlu models are different. in this paper, we first conduct preliminary studies to obtain two conclusions: 1) both low- and high-layer sentence representations encode common biased features during training; 2) the low-layer sentence representations encode fewer unbiased features than the highlayer ones. based on these conclusions, we propose a simple yet effective self-debiasing framework for transformer-based nlu models. concretely, we first stack a classifier on a selected low layer. then, we introduce a residual connection that feeds the low-layer sentence representation to the top-layer classifier. in this way, the top-layer sentence representation will be trained to ignore the common biased features encoded by the low-layer sentence representation and focus on task-relevant unbiased features. during inference, we remove the residual connection and directly use the top-layer sentence representation to make predictions. extensive experiments and indepth analyses on nlu tasks show that our framework performs better than several competitive baselines, achieving a new sota on all ood test sets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01941" target="_blank">Ai Transparency in the Age of Llms: A Human-Centered Research Roadmap</a></div>
<div class="paper-author">Q. Vera Liao, Jennifer Wortman Vaughan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise of powerful large language models (llms) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. we have reached a pivotal moment for ensuring that llms and llm-infused applications are developed and deployed responsibly. however, a central pillar of responsible ai -- transparency -- is largely missing from the current discourse around llms. it is paramount to pursue new approaches to provide transparency for llms, and years of research at the intersection of ai and human-computer interaction (hci) highlight that we must do so with a human-centered perspective: transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. in this new era of llms, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging llm ecosystem, the novel types of llm-infused applications being built, and the new usage patterns and challenges around llms, all while building on lessons learned about how people process, interact with, and make use of information. we reflect on the unique challenges that arise in providing transparency for llms, along with lessons learned from hci and responsible ai research that has taken a human-centered perspective on ai transparency. we then lay out four common approaches that the community has taken to achieve transparency -- model reporting, publishing evaluation results, providing explanations, and communicating uncertainty -- and call out open questions around how these approaches may or may not be applied to llms. we hope this provides a starting point for discussion and a useful roadmap for future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01943" target="_blank">Nlpositionality: Characterizing Design Biases of Datasets and Models</a></div>
<div class="paper-author">Sebastin Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: design biases in nlp systems, such as performance differences for different populations, often stem from their creator's positionality, i.e., views and lived experiences shaped by identity and background. despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. we introduce nlpositionality, a framework for characterizing design biases and quantifying the positionality of nlp datasets and models. our framework continuously collects annotations from a diverse pool of volunteer participants on labinthewild, and statistically quantifies alignment with dataset labels and model predictions. we apply nlpositionality to existing datasets and models for two tasks -- social acceptability and hate speech detection. to date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. we find that datasets and models align predominantly with western, white, college-educated, and younger populations. additionally, certain groups, such as non-binary people and non-native english speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive nlp systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01985" target="_blank">Cobra Frames: Contextual Reasoning About Effects and Harms of Offensive Statements</a></div>
<div class="paper-author">Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: warning: this paper contains content that may be offensive or upsetting. understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. for example, the utterance "your english is very good" may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an esl teacher to their student would be interpreted as a genuine compliment. such contextual factors have been largely ignored by previous approaches to toxic language detection. we introduce cobra frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. we create cobracorpus, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. to study the contextual dynamics of offensiveness, we train models to generate cobra explanations, with and without access to the context. we find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). our work highlights the importance and feasibility of contextualized nlp by modeling social factors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03102" target="_blank">Chatgpt Is a Remarkable Tool -- For Experts</a></div>
<div class="paper-author">Amos Azaria, Rina Azoulay, Shulamit Reches</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates the capabilities of chatgpt as an automated assistant in diverse domains, including scientific writing, mathematics, education, programming, and healthcare. we explore the potential of chatgpt to enhance productivity, streamline problem-solving processes, and improve writing style. furthermore, we highlight the potential risks associated with excessive reliance on chatgpt in these fields. these limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation. we outline areas and objectives where chatgpt proves beneficial, applications where it should be used judiciously, and scenarios where its reliability may be limited. in light of observed limitations, and given that the tool's fundamental errors may pose a special challenge for non-experts, chatgpt should be used with a strategic methodology. by drawing from comprehensive experimental studies, we offer methods and flow charts for effectively using chatgpt. our recommendations emphasize iterative interaction with chatgpt and independent verification of its outputs. considering the importance of utilizing chatgpt judiciously and with expertise, we recommend its usage for experts who are well-versed in the respective domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-06-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00374" target="_blank">Cfl: Causally Fair Language Models Through Token-Level Attribute Controlled Generation</a></div>
<div class="paper-author">Rahul Madhavan, Rishabh Garg, Kahini Wadhawan, Sameep Mehta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a method to control the attributes of language models (lms) for the text generation task using causal average treatment effect (ate) scores and counterfactual augmentation. we explore this method, in the context of lm detoxification, and propose the causally fair language (cfl) architecture for detoxifying pre-trained lms in a plug-and-play manner. our architecture is based on a structural causal model (scm) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. we also propose several new metrics that aim to better understand the behaviour of lms in the context of toxic text generation. further, we achieve state of the art performance for toxic degeneration, which are computed using \rtp (rtp) benchmark. our experiments show that cfl achieves such a detoxification without much impact on the model perplexity. we also show that cfl mitigates the unintended bias problem through experiments on the bold dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00398" target="_blank">Preference-Grounded Token-Level Guidance for Language Model Fine-Tuning</a></div>
<div class="paper-author">Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, Mingyuan Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning language models (lms) with preferences is an important problem in natural language generation. a key challenge is that preferences are typically provided at the *sequence level* while lm training and generation both occur at the *token level*. there is, therefore, a *granularity mismatch* between the preference and the lm training losses, which may complicate the learning problem. in this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the lm with the learned guidance. for guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length lm generation and the utilization of the preference among multiple generations. for lm training, based on the amount of supervised data, we present two *minimalist* learning objectives that utilize the learned guidance. in experiments, our method performs competitively on two distinct representative lm tasks -- discrete-prompt generation and text summarization.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00639" target="_blank">Being Right for Whose Right Reasons?</a></div>
<div class="paper-author">Terne Sasha Thorn Jakobsen, Laura Cabello, Anders Søgaard</div>
<div class="abstract">
<div class="abstract-content">
Abstract: explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are 'right for the right reasons'. previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. this paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. we cover three datasets spanning sentiment analysis and common-sense reasoning, and six demographic groups (balanced across age and ethnicity). such data enables us to ask both what demographics our predictions align with and whose reasoning patterns our models' rationales align with. we find systematic inter-group annotator disagreement and show how 16 transformer-based models align better with rationales provided by certain demographic groups: we find that models are biased towards aligning best with older and/or white annotators. we zoom in on the effects of model size and model distillation, finding -- contrary to our expectations -- negative correlations between model size and rationale agreement as well as no evidence that either model size or model distillation improves fairness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01105" target="_blank">Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment</a></div>
<div class="paper-author">Atharva Kulkarni, Sarah Masud, Vikram Goyal, Tanmoy Chakraborty</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media is awash with hateful content, much of which is often veiled with linguistic and topical diversity. the benchmark datasets used for hate speech detection do not account for such divagation as they are predominantly compiled using hate lexicons. however, capturing hate signals becomes challenging in neutrally-seeded malicious content. thus, designing models and datasets that mimic the real-world variability of hate warrants further investigation.   to this end, we present gothate, a large-scale code-mixed crowdsourced dataset of around 51k posts for hate speech detection from twitter. gothate is neutrally seeded, encompassing different languages and topics. we conduct detailed comparisons of gothate with the existing hate speech datasets, highlighting its novelty. we benchmark it with 10 recent baselines. our extensive empirical and benchmarking experiments suggest that gothate is hard to classify in a text-only setup. thus, we investigate how adding endogenous signals enhances the hate speech detection task. we augment gothate with the user's timeline information and ego network, bringing the overall data source closer to the real-world setup for understanding hateful content. our proposed solution hen-mbert is a modular, multilingual, mixture-of-experts model that enriches the linguistic subspace with latent endogenous signals from history, topology, and exemplars. hen-mbert transcends the best baseline by 2.5% and 5% in overall macro-f1 and hate class f1, respectively. inspired by our experiments, in partnership with wipro ai, we are developing a semi-automated pipeline to detect hateful content as a part of their mission to tackle online harm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01117" target="_blank">Examining the Causal Effect of First Names on Language Models: The Case of Social Commonsense Reasoning</a></div>
<div class="paper-author">Sullam Jeoung, Jana Diesner, Halil Kilicoglu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models continue to be integrated into applications of personal and societal relevance, ensuring these models' trustworthiness is crucial, particularly with respect to producing consistent outputs regardless of sensitive attributes. given that first names may serve as proxies for (intersectional) socio-demographic representations, it is imperative to examine the impact of first names on commonsense reasoning capabilities. in this paper, we study whether a model's reasoning given a specific input differs based on the first names provided. our underlying assumption is that the reasoning about alice should not differ from the reasoning about james. we propose and implement a controlled experimental framework to measure the causal effect of first names on commonsense reasoning, enabling us to distinguish between model predictions due to chance and caused by actual factors of interest. our results indicate that the frequency of first names has a direct effect on model prediction, with less frequent names yielding divergent predictions compared to more frequent names. to gain insights into the internal mechanisms of models that are contributing to these behaviors, we also conduct an in-depth explainable analysis. overall, our findings suggest that to ensure model robustness, it is essential to augment datasets with more diverse first names during the configuration stage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01181" target="_blank">Tmi! Finetuned Models Leak Private Information From Their Pretraining Data</a></div>
<div class="paper-author">John Abascal, Stanley Wu, Alina Oprea, Jonathan Ullman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transfer learning has become an increasingly popular technique in machine learning as a way to leverage a pretrained model trained for one task to assist with building a finetuned model for a related task. this paradigm has been especially popular for privacy in machine learning, where the pretrained model is considered public, and only the data for finetuning is considered sensitive. however, there are reasons to believe that the data used for pretraining is still sensitive, making it essential to understand how much information the finetuned model leaks about the pretraining data. in this work we propose a new membership-inference threat model where the adversary only has access to the finetuned model and would like to infer the membership of the pretraining data. to realize this threat model, we implement a novel metaclassifier-based attack, tmi, that leverages the influence of memorized pretraining samples on predictions in the downstream task. we evaluate tmi on both vision and natural language tasks across multiple transfer learning settings, including finetuning with differential privacy. through our evaluation, we find that tmi can successfully infer membership of pretraining examples using query access to the finetuned model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01242" target="_blank">Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators</a></div>
<div class="paper-author">Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yan Lu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent success of large language models (llms) signifies an impressive stride towards artificial general intelligence. they have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. the associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. a big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? in this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. in specific, we present responsible task automation (responsibleta) as a fundamental framework to facilitate responsible collaboration between llm-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the protection of users' privacy). we further propose and compare two paradigms for implementing the first two capabilities. one is to leverage the generic knowledge of llms themselves via prompt engineering while the other is to adopt domain-specific learnable models. moreover, we introduce a local memory mechanism for achieving the third capability. we evaluate our proposed responsibleta on ui task automation and hope it could bring more attentions to ensuring llms more responsible in diverse scenarios. the research project homepage is at https://task-automation-research.github.io/responsible_task_automation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-31</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19713" target="_blank">Red Teaming Language Model Detectors With Language Models</a></div>
<div class="paper-author">Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, Cho-Jui Hsieh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prevalence and strong capability of large language models (llms) present significant safety and ethical risks if exploited by malicious users. to prevent the potentially deceptive usage of llms, recent works have proposed algorithms to detect llm-generated text and protect llms. in this paper, we investigate the robustness and reliability of these llm detectors under adversarial attacks. we study two types of attack strategies: 1) replacing certain words in an llm's output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. in both strategies, we leverage an auxiliary llm to generate the word replacements or the instructional prompt. different from previous works, we consider a challenging setting where the auxiliary llm can also be protected by a detector. experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of llm-generated text detection systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19861" target="_blank">Human Control: Definitions and Algorithms</a></div>
<div class="paper-author">Ryan Carey, Tom Everitt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how can humans stay in control of advanced artificial intelligence systems? one proposal is corrigibility, which requires the agent to follow the instructions of a human overseer, without inappropriately influencing them. in this paper, we formally define a variant of corrigibility called shutdown instructability, and show that it implies appropriate shutdown behavior, retention of human autonomy, and avoidance of user harm. we also analyse the related concepts of non-obstruction and shutdown alignment, three previously proposed algorithms for human control, and one new algorithm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00074" target="_blank">Human-Aligned Calibration for Ai-Assisted Decision Making</a></div>
<div class="paper-author">Nina L. Corvelo Benz, Manuel Gomez Rodriguez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. in this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. however, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. in this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. we first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker would need to sometimes place more (less) trust on predictions with lower (higher) confidence values. however, we then show that, if the confidence values satisfy a natural alignment property with respect to the decision maker's confidence on her own predictions, there always exists an optimal decision policy under which the level of trust the decision maker would need to place on predictions is monotone on the confidence values, facilitating its discoverability. further, we show that multicalibration with respect to the decision maker's confidence on her own predictions is a sufficient condition for alignment. experiments on four different ai-assisted decision making tasks where a classifier provides decision support to real human experts validate our theoretical results and suggest that alignment may lead to better decisions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01788" target="_blank">Responsible Design Patterns for Machine Learning Pipelines</a></div>
<div class="paper-author">Saud Hakem Al Harbi, Lionel Nganyewou Tidjon, Foutse Khomh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: integrating ethical practices into the ai development process for artificial intelligence (ai) is essential to ensure safe, fair, and responsible operation. ai ethics involves applying ethical principles to the entire life cycle of ai systems. this is essential to mitigate potential risks and harms associated with ai, such as algorithm biases. to achieve this goal, responsible design patterns (rdps) are critical for machine learning (ml) pipelines to guarantee ethical and fair outcomes. in this paper, we propose a comprehensive framework incorporating rdps into ml pipelines to mitigate risks and ensure the ethical development of ai systems. our framework comprises new responsible ai design patterns for ml pipelines identified through a survey of ai ethics and data management experts and validated through real-world scenarios with expert feedback. the framework guides ai developers, data scientists, and policy-makers to implement ethical practices in ai development and deploy responsible ai systems in production.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-30</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18917" target="_blank">Fighting Bias With Bias: Promoting Model Robustness by Amplifying Dataset Biases</a></div>
<div class="paper-author">Yuval Reif, Roy Schwartz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nlp models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. recent work sought to develop robust, unbiased models by filtering biased examples from training sets. in this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. we suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. we introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. we hope our work will guide the development of robust models that do not rely on superficial biases and correlations. to this end, we publicly release our code and data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19223" target="_blank">Intent-Aligned Ai Systems Deplete Human Agency: The Need for Agency Foundations Research in Ai Safety</a></div>
<div class="paper-author">Catalin Mitelut, Ben Smith, Peter Vamplew</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid advancement of artificial intelligence (ai) systems suggests that artificial general intelligence (agi) systems may soon arrive. many researchers are concerned that ais and agis will harm humans via intentional misuse (ai-misuse) or through accidents (ai-accidents). in respect of ai-accidents, there is an increasing effort focused on developing algorithms and paradigms that ensure ai systems are aligned to what humans intend, e.g. ai systems that yield actions or recommendations that humans might judge as consistent with their intentions and goals. here we argue that alignment to human intent is insufficient for safe ai systems and that preservation of long-term agency of humans may be a more robust standard, and one that needs to be separated explicitly and a priori during optimization. we argue that ai systems can reshape human intention and discuss the lack of biological and psychological mechanisms that protect humans from loss of agency. we provide the first formal definition of agency-preserving ai-human interactions which focuses on forward-looking agency evaluations and argue that ai systems - not humans - must be increasingly tasked with making these evaluations. we show how agency loss can occur in simple environments containing embedded agents that use temporal-difference learning to make action recommendations. finally, we propose a new area of research called "agency foundations" and pose four initial topics designed to improve our understanding of agency in ai-human interactions: benevolent game theory, algorithmic foundations of human rights, mechanistic interpretability of agency representation in neural-networks and reinforcement learning from internal states.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19230" target="_blank">Controlled Text Generation With Hidden Representation Transformations</a></div>
<div class="paper-author">Vaibhav Kumar, Hana Koorehdavoudi, Masud Moshtaghi, Amita Misra, Ankit Chadha, Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose chrt (control hidden representation transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). chrt gains attribute control by modifying the hidden representation of the base model through learned transformations. we employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. the effectiveness of chrt is experimentally shown by comparing it with seven baselines over three attributes. chrt outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. we open-source our code and release two novel datasets to further propel controlled language generation research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19409" target="_blank">Examining Risks of Racial Biases in NLP Tools for Child Protective Services</a></div>
<div class="paper-author">Anjalie Field, Amanda Coston, Nupoor Gandhi, Alexandra Chouldechova, Emily Putnam-Hornstein, David Steier, Yulia Tsvetkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although much literature has established the presence of demographic bias in natural language processing (nlp) models, most work relies on curated bias metrics that may not be reflective of real-world applications. at the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in nlp. in this work, we focus on one such setting: child protective services (cps). cps workers often write copious free-form text notes about families they are working with, and cps agencies are actively seeking to deploy nlp models to leverage these data. given well-established racial bias in this setting, we investigate possible ways deployed nlp is liable to increase racial disparities. we specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (ner). we document consistent algorithmic unfairness in ner models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. while there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. our work serves as a rare realistic examination of nlp algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying nlp in cps settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00021" target="_blank">Explaining Hate Speech Classification With Model Agnostic Methods</a></div>
<div class="paper-author">Durgesh Nandini, Ute Schmid</div>
<div class="abstract">
<div class="abstract-content">
Abstract: there have been remarkable breakthroughs in machine learning and artificial intelligence, notably in the areas of natural language processing and deep learning. additionally, hate speech detection in dialogues has been gaining popularity among natural language processing researchers with the increased use of social media. however, as evidenced by the recent trends, the need for the dimensions of explainability and interpretability in ai models has been deeply realised. taking note of the factors above, the research goal of this paper is to bridge the gap between hate speech prediction and the explanations generated by the system to support its decision. this has been achieved by first predicting the classification of a text and then providing a posthoc, model agnostic and surrogate interpretability approach for explainability and to prevent model bias. the bidirectional transformer model bert has been used for prediction because of its state of the art efficiency over other machine learning models. the model agnostic algorithm lime generates explanations for the output of a trained classifier and predicts the features that influence the model decision. the predictions generated from the model were evaluated manually, and after thorough evaluation, we observed that the model performs efficiently in predicting and explaining its prediction. lastly, we suggest further directions for the expansion of the provided research work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03097" target="_blank">Seeing Seeds Beyond Weeds: Green Teaming Generative Ai for Beneficial Uses</a></div>
<div class="paper-author">Logan Stapleton, Jordan Taylor, Sarah Fox, Tongshuang Wu, Haiyi Zhu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large generative ai models (gms) like gpt and dall-e are trained to generate content for general, wide-ranging purposes. gm content filters are generalized to filter out content which has a risk of harm in many cases, e.g., hate speech. however, prohibited content is not always harmful -- there are instances where generating prohibited content can be beneficial. so, when gms filter out content, they preclude beneficial use cases along with harmful ones. which use cases are precluded reflects the values embedded in gm content filtering. recent work on red teaming proposes methods to bypass gm content filters to generate harmful content. we coin the term green teaming to describe methods of bypassing gm content filters to design for beneficial use cases. we showcase green teaming by: 1) using chatgpt as a virtual patient to simulate a person experiencing suicidal ideation, for suicide support training; 2) using codex to intentionally generate buggy solutions to train students on debugging; and 3) examining an instagram page using midjourney to generate images of anti-lgbtq+ politicians in drag. finally, we discuss how our use cases demonstrate green teaming as both a practical design method and a mode of critique, which problematizes and subverts current understandings of harms and values in generative ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-29</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17926" target="_blank">Large Language Models Are Not Fair Evaluators</a></div>
<div class="paper-author">Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(llms), e.g., gpt-4, as a referee to score and compare the quality of responses generated by candidate models. we find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. this manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., vicuna-13b could beat chatgpt on 66 over 80 tested queries with chatgpt as an evaluator. to address this issue, we propose a calibration framework with three simple yet effective strategies: 1) multiple evidence calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) balanced position calibration, which aggregates results across various orders to determine the final score; 3) human-in-the-loop calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. we also manually annotate the "win/tie/lose" outcomes of responses from chatgpt and vicuna-13b in the vicuna benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. we release our code and human annotation at \url{https://github.com/i-eval/faireval} to facilitate future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18081" target="_blank">Game of Tones: Faculty Detection of GPT-4 Generated Content in University Assessments</a></div>
<div class="paper-author">Mike Perkins, Jasper Roe, Darius Postma, James Mcgaughran, Don Hickerson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this study explores the robustness of university assessments against the use of open ai's generative pre-trained transformer 4 (gpt-4) generated content and evaluates the ability of academic staff to detect its use when supported by the turnitin artificial intelligence (ai) detection tool. the research involved twenty-two gpt-4 generated submissions being created and included in the assessment process to be marked by fifteen different faculty members. the study reveals that although the detection tool identified 91% of the experimental submissions as containing some ai-generated content, the total detected content was only 54.8%. this suggests that the use of adversarial techniques regarding prompt engineering is an effective method in evading ai detection tools and highlights that improvements to ai detection software are needed. using the turnitin ai detect tool, faculty reported 54.5% of the experimental submissions to the academic misconduct process, suggesting the need for increased awareness and training into these tools. genuine submissions received a mean score of 54.4, whereas ai-generated content scored 52.3, indicating the comparable performance of gpt-4 in real-life situations. recommendations include adjusting assessment strategies to make them more resistant to the use of ai tools, using ai-inclusive assessment where possible, and providing comprehensive training programs for faculty and students. this research contributes to understanding the relationship between ai-generated content and academic assessment, urging further investigation to preserve academic integrity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18176" target="_blank">Perceived Trustworthiness of Natural Language Generators</a></div>
<div class="paper-author">Beatriz Cabrero-Daniel, Andrea Sanagustín Cabrero</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language generation tools, such as chatbots that can generate human-like conversational text, are becoming more common both for personal and professional use. however, there are concerns about their trustworthiness and ethical implications. the paper addresses the problem of understanding how different users (e.g., linguists, engineers) perceive and adopt these tools and their perception of machine-generated text quality. it also discusses the perceived advantages and limitations of natural language generation tools, as well as users' beliefs on governance strategies. the main findings of this study include the impact of users' field and level of expertise on the perceived trust and adoption of natural language generation tools, the users' assessment of the accuracy, fluency, and potential biases of machine-generated text in comparison to human-written text, and an analysis of the advantages and ethical risks associated with these tools as identified by the participants. moreover, this paper discusses the potential implications of these findings for enhancing the ai development process. the paper sheds light on how different user characteristics shape their beliefs on the quality and overall trustworthiness of machine-generated text. furthermore, it examines the benefits and risks of these tools from the perspectives of different users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18189" target="_blank">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a></div>
<div class="paper-author">Myra Cheng, Esin Durmus, Dan Jurafsky</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to recognize and mitigate harms from large language models (llms), we need to understand the prevalence and nuances of stereotypes in llm outputs. toward this end, we present marked personas, a prompt-based method to measure stereotypes in llms for intersectional demographic groups without any lexicon or data labeling. grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an llm to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones. we find that the portrayals generated by gpt-3.5 and gpt-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. the words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. an intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. these representational harms have concerning implications for downstream applications like story generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18248" target="_blank">Do Language Models Know When They're Hallucinating References?</a></div>
<div class="paper-author">Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman Kalai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: state-of-the-art language models (lms) are famous for "hallucinating" references. these fabricated article and book titles lead to harms, obstacles to their use, and public backlash. while other types of lm hallucinations are also important, we propose hallucinated references as the "drosophila" of research on hallucination in large language models (llms), as they are particularly easy to study. we show that simple search engine queries reliably identify such hallucinations, which facilitates evaluation. to begin to dissect the nature of hallucinated lm references, we attempt to classify them using black-box queries to the same lm, without consulting any external resources. consistency checks done with "direct" queries about whether the generated reference title is real (inspired by kadavath et al. 2022, lin et al. 2022, manakul et al. 2023) are compared to consistency checks with "indirect" queries which ask for ancillary details such as the authors of the work. these consistency checks are found to be partially reliable indicators of whether or not the reference is a hallucination. in particular, we find that lms often hallucinate differing authors of hallucinated references when queried in independent sessions, while consistently identify authors of real references. this suggests that the hallucination may be more a generation issue than inherent to current training techniques or representation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18290" target="_blank">Direct Preference Optimization: Your Language Model Is Secretly a Reward Model</a></div>
<div class="paper-author">Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large-scale unsupervised language models (lms) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised lm to align with these preferences, often with reinforcement learning from human feedback (rlhf). however, rlhf is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised lm using reinforcement learning to maximize this estimated reward without drifting too far from the original model. in this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. the resulting algorithm, which we call direct preference optimization (dpo), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the lm during fine-tuning, or performing significant hyperparameter tuning. our experiments show that dpo can fine-tune lms to align with human preferences as well as or better than existing methods. notably, fine-tuning with dpo exceeds rlhf's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18462" target="_blank">Membership Inference Attacks Against Language Models via Neighbourhood Comparison</a></div>
<div class="paper-author">Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schölkopf, Mrinmaya Sachan, Taylor Berg-Kirkpatrick</div>
<div class="abstract">
<div class="abstract-content">
Abstract: membership inference attacks (mias) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. however, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of mias. however, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. to investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. we show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18607" target="_blank">How Effective Are Neural Networks for Fixing Security Vulnerabilities</a></div>
<div class="paper-author">Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, Sameena Shah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: security vulnerability repair is a difficult task that is in dire need of automation. two groups of techniques have shown promise: (1) large code language models (llms) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (apr) techniques that use deep learning (dl) models to automatically fix software bugs.   this paper is the first to study and compare java vulnerability repair capabilities of llms and dl-based apr models. the contributions include that we (1) apply and evaluate five llms (codex, codegen, codet5, plbart and incoder), four fine-tuned llms, and four dl-based apr techniques on two real-world java vulnerability benchmarks (vul4j and vjbench), (2) design code transformations to address the training and test data overlapping threat to codex, (3) create a new java vulnerability repair benchmark vjbench, and its transformed version vjbench-trans and (4) evaluate llms and apr techniques on the transformed vulnerabilities in vjbench-trans.   our findings include that (1) existing llms and apr models fix very few java vulnerabilities. codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) fine-tuning with general apr data improves llms' vulnerability-fixing capabilities. (3) our new vjbench reveals that llms and apr models fail to fix many common weakness enumeration (cwe) types, such as cwe-325 missing cryptographic step and cwe-444 http request smuggling. (4) codex still fixes 8.3 transformed vulnerabilities, outperforming all the other llms and apr models on transformed vulnerabilities. the results call for innovations to enhance automated java vulnerability repair such as creating larger vulnerability repair training data, tuning llms with such data, and applying code simplification transformation to facilitate vulnerability repair.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09255" target="_blank">Chatbots to Chatgpt in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations</a></div>
<div class="paper-author">Attia Qammar, Hongmei Wang, Jianguo Ding, Abdenacer Naouri, Mahmoud Daneshmand, Huansheng Ning</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatbots shifted from rule-based to artificial intelligence techniques and gained traction in medicine, shopping, customer services, food delivery, education, and research. openai developed chatgpt blizzard on the internet as it crossed one million users within five days of its launch. however, with the enhanced popularity, chatbots experienced cybersecurity threats and vulnerabilities. this paper discussed the relevant literature, reports, and explanatory incident attacks generated against chatbots. our initial point is to explore the timeline of chatbots from eliza (an early natural language processing computer program) to gpt-4 and provide the working mechanism of chatgpt. subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. besides, we investigated the chatgpt, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and lolbins. furthermore, the history of cyberattacks and vulnerabilities exploited by cybercriminals are discussed, particularly considering the risk and vulnerabilities in chatgpt. addressing these threats and vulnerabilities requires specific strategies and measures to reduce the harmful consequences. therefore, the future directions to address the challenges were presented.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17627" target="_blank">Robust Natural Language Understanding With Residual Attention Debiasing</a></div>
<div class="paper-author">Fei Wang, James Y. Huang, Tianyi Yan, Wenxuan Zhou, Muhao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language understanding (nlu) models often suffer from unintended dataset biases. among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (poe), have stood out for their impressive empirical success. however, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. attention serves as the main media of feature interaction and aggregation in plms and plays a crucial role in providing robust prediction. in this paper, we propose residual attention debiasing (read), an end-to-end debiasing method that mitigates unintended biases from attention. experiments on three nlu tasks show that read significantly improves the performance of bert-based models on ood data with shortcuts removed, including +12.9% accuracy on hans, +11.0% accuracy on fever-symmetric, and +2.7% f1 on paws. detailed analyses demonstrate the crucial role of unbiased attention in robust nlu models and that read effectively mitigates biases in attention. code is available at https://github.com/luka-group/read.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17678" target="_blank">Decoding the Underlying Meaning of Multimodal Hateful Memes</a></div>
<div class="paper-author">Ming Shan Hee, Wen-Haw Chong, Roy Ka-Wei Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have proposed models that yielded promising performance for the hateful meme classification task. nevertheless, these proposed models do not generate interpretable explanations that uncover the underlying meaning and support the classification output. a major reason for the lack of explainable hateful meme methods is the absence of a hateful meme dataset that contains ground truth explanations for benchmarking or training. intuitively, having such explanations can educate and assist content moderators in interpreting and removing flagged hateful memes. this paper address this research gap by introducing hateful meme with reasons dataset (hatred), which is a new multimodal hateful meme dataset annotated with the underlying hateful contextual reasons. we also define a new conditional generation task that aims to automatically generate underlying reasons to explain hateful memes and establish the baseline performance of state-of-the-art pre-trained language models on this task. we further demonstrate the usefulness of hatred by analyzing the challenges of the new conditional generation task in explaining memes in seen and unseen domains. the dataset and benchmark models are made available here: https://github.com/social-ai-studio/hatred
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17680" target="_blank">Evaluating GPT-3 Generated Explanations for Hateful Content Moderation</a></div>
<div class="paper-author">Han Wang, Ming Shan Hee, Md Rabiul Awal, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent research has focused on using large language models (llms) to generate explanations for hate speech through fine-tuning or prompting. despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. a key concern is that these explanations, generated by llms, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. for instance, an llm-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. in light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. specifically, we prompted gpt-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. our findings reveal that (1) human evaluators rated the gpt-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. our study underscores the need for caution in applying llm-generated explanations for content moderation. code and results are available at https://github.com/social-ai-studio/gpt3-hateeval.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17696" target="_blank">Square: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration</a></div>
<div class="paper-author">Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Meeyoung Cha, Yejin Choi, Byoung Pil Kim, Gunhee Kim, Eun-Ju Lee, Yong Lim, Alice Oh, Sangchul Park, Jung-Woo Ha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. however, discussions on sensitive issues can become toxic even if the users are well-intentioned. for safer models in such scenarios, we present the sensitive questions and acceptable response (square) dataset, a large-scale korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. the dataset was constructed leveraging hyperclova in a human-in-the-loop manner based on real news headlines. experiments show that acceptable response generation significantly improves for hyperclova and gpt-3, demonstrating the efficacy of this dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17701" target="_blank">Kosbi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application</a></div>
<div class="paper-author">Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Gunhee Kim, Jung-Woo Ha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. this poses a critical risk when deploying llm-based applications. existing research and resources are not readily applicable in south korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. this limitation requires localized social bias datasets to ensure the safe and effective deployment of llms. to this end, we present ko sb i, a new social bias dataset of 34k pairs of contexts and sentences in korean covering 72 demographic groups in 15 categories. we find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for hyperclova (30b and 82b), and gpt-3.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17804" target="_blank">Targeted Data Generation: Finding and Fixing Model Weaknesses</a></div>
<div class="paper-author">Zexue He, Marco Tulio Ribeiro, Fereshte Khani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: even when aggregate accuracy is high, state-of-the-art nlp models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. we propose targeted data generation (tdg), a framework that automatically identifies challenging subgroups, and generates new data for those subgroups using large language models (llms) with a human in the loop. tdg estimates the expected benefit and potential harm of data augmentation for each subgroup, and selects the ones most likely to improve within group performance without hurting overall performance. in our experiments, tdg significantly improves the accuracy on challenging subgroups for state-of-the-art sentiment analysis and natural language inference models, while also improving overall test accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17826" target="_blank">Notable: Transferable Backdoor Attacks Against Prompt-Based NLP Models</a></div>
<div class="paper-author">Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, Shiqing Ma</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-based learning is vulnerable to backdoor attacks. existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. in this work, we propose transferable backdoor attacks against prompt-based models, called notable, which is independent of downstream tasks and prompting strategies. specifically, notable injects backdoors into the encoders of plms by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). it activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. we conduct experiments on six nlp tasks, three popular models, and three prompting strategies. empirical results show that notable achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. evaluations on three defenses show the robustness of notable. our code can be found at https://github.com/ru-system-software-and-security/notable.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19148" target="_blank">Mitigating Label Biases for in-Context Learning</a></div>
<div class="paper-author">Yu Fei, Yifan Hou, Zeming Chen, Antoine Bosselut</div>
<div class="abstract">
<div class="abstract-content">
Abstract: various design settings for in-context learning (icl), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. while many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. in this work, we define a typology for three types of label biases in icl for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).   our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. specifically, domain-label bias restricts llms to random-level performance on many tasks regardless of the choice of in-context examples. to mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words from the task corpus. after controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the icl performance of gpt-j and gpt-3 on a wide range of tasks. the gain is substantial on tasks with large domain-label bias (up to 37% in macro-f1). furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in icl.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17440" target="_blank">Modeling Adversarial Attack on Pre-Trained Language Models as Sequential Decision Making</a></div>
<div class="paper-author">Xuanjie Fang, Sijie Cheng, Yang Liu, Wei Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models (plms) have been widely used to underpin various downstream tasks. however, the adversarial attack task has found that plms are vulnerable to small perturbations. mainstream methods adopt a detached two-stage framework to attack without considering the subsequent influence of substitution at each step. in this paper, we formally model the adversarial attack task on plms as a sequential decision-making problem, where the whole attack process is sequential with two decision-making problems, i.e., word finder and word substitution. considering the attack process can only receive the final state without any direct intermediate signals, we propose to use reinforcement learning to find an appropriate sequential attack path to generate adversaries, named sdm-attack. extensive experimental results show that sdm-attack achieves the highest attack success rate with a comparable modification rate and semantic similarity to attack fine-tuned bert. furthermore, our analyses demonstrate the generalization and transferability of sdm-attack. the code is available at https://github.com/fduxuan/sdm-attack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17444" target="_blank">Query-Efficient Black-Box Red Teaming via Bayesian Optimization</a></div>
<div class="paper-author">Deokjae Lee, Junyeong Lee, Jung-Woo Ha, Jin-Hwa Kim, Sang-Woo Lee, Hwaran Lee, Hyun Oh Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. we focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. existing red teaming methods construct test cases based on human supervision or language model (lm) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. to this end, we propose bayesian red teaming (brt), novel query-efficient black-box red teaming methods based on bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods. the source code is available at https://github.com/snu-mllab/bayesian-red-teaming.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17601" target="_blank">Incentivizing Honest Performative Predictions With Proper Scoring Rules</a></div>
<div class="paper-author">Caspar Oesterheld, Johannes Treutlein, Emery Cooper, Rubi Hudson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: proper scoring rules incentivize experts to accurately report beliefs, assuming predictions cannot influence outcomes. we relax this assumption and investigate incentives when predictions are performative, i.e., when they can influence the outcome of the prediction, such as when making public predictions about the stock market. we say a prediction is a fixed point if it accurately reflects the expert's beliefs after that prediction has been made. we show that in this setting, reports maximizing expected score generally do not reflect an expert's beliefs, and we give bounds on the inaccuracy of such reports. we show that, for binary predictions, if the influence of the expert's prediction on outcomes is bounded, it is possible to define scoring rules under which optimal reports are arbitrarily close to fixed points. however, this is impossible for predictions over more than two outcomes. we also perform numerical simulations in a toy setting, showing that our bounds are tight in some situations and that prediction error is often substantial (greater than 5-10%). lastly, we discuss alternative notions of optimality, including performative stability, and show that they incentivize reporting fixed points.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17608" target="_blank">Reward Collapse in Aligning Large Language Models</a></div>
<div class="paper-author">Ziang Song, Tianle Cai, Jason D. Lee, Weijie J. Su</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the extraordinary capabilities of large language models (llms) such as chatgpt and gpt-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts. in this paper, we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training. this outcome is undesirable as open-ended prompts like ``write a short story about your best friend'' should yield a continuous range of rewards for their completions, while specific prompts like ``what is the capital of new zealand'' should generate either high or low rewards. our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization. this insight allows us to derive closed-form expressions for the reward distribution associated with a set of utility functions in an asymptotic regime. to overcome reward collapse, we introduce a prompt-aware optimization scheme that provably admits a prompt-dependent reward distribution within the interpolating regime. our experimental results suggest that our proposed prompt-aware utility functions significantly alleviate reward collapse during the training of reward models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16617" target="_blank">Efficient Detection of LLM-Generated Texts With a Bayesian Surrogate Model</a></div>
<div class="paper-author">Zhijie Deng, Hongcheng Gao, Yibo Miao, Hao Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the detection of machine-generated text, especially from large language models (llms), is crucial in preventing serious social problems resulting from their misuse. some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. although the recent detectgpt has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires scoring hundreds of its perturbations with the source llm. this paper aims to bridge this gap. technically, we propose to incorporate a bayesian surrogate model, which allows us to select typical samples based on bayesian uncertainty and interpolate scores from typical samples to other ones, to improve query efficiency. our empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. notably, our method achieves similar performance with up to 2 times fewer queries than detectgpt and 3.7% higher auroc at a query number of 5.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16641" target="_blank">Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales</a></div>
<div class="paper-author">Paulina Toro Isaza, Guangxuan Xu, Akintoye Oloko, Yufang Hou, Nanyun Peng, Dakuo Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social biases and stereotypes are embedded in our culture in part through their presence in our stories, as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories. because these analyses are often conducted manually and at a small scale, such investigations can benefit from the use of more recent natural language processing methods that examine social bias in models and data corpora. our work joins this interdisciplinary effort and makes a unique contribution by taking into account the event narrative structures when analyzing the social bias of stories. we propose a computational pipeline that automatically extracts a story's temporal narrative verb-based event chain for each of its characters as well as character attributes such as gender. we also present a verb-based event annotation scheme that can facilitate bias analysis by including categories such as those that align with traditional stereotypes. through a case study analyzing gender bias in fairy tales, we demonstrate that our framework can reveal bias in not only the unigram verb-based events in which female and male characters participate but also in the temporal narrative order of such event participation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16739" target="_blank">Alignscore: Evaluating Factual Consistency With a Unified Alignment Function</a></div>
<div class="paper-author">Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many text generation applications require the generated text to be factually consistent with input information. automatic evaluation of factual consistency is challenging. previous work has developed various metrics that often depend on specific functions, such as natural language inference (nli) or question answering (qa), trained on limited data. those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. in this paper, we propose alignscore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. alignscore is based on a general function of information alignment between two arbitrary text pieces. crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7m training examples from 7 well-established tasks (nli, qa, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). we conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. alignscore achieves substantial improvement over a wide range of previous metrics. moreover, alignscore (355m parameters) matches or even outperforms metrics based on chatgpt and gpt-4 that are orders of magnitude larger.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16756" target="_blank">Leveraging Domain Knowledge for Inclusive and Bias-Aware Humanitarian Response Entry Classification</a></div>
<div class="paper-author">Nicolò Tamagnone, Selim Fekih, Ximena Contla, Nayid Orozco, Navid Rekabsaz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the leave no one behind (lnob) principle. this data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. however, approaching this by simply fine-tuning a generic large language model (llm) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. in this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. we approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific llm called humbert, and (3) proposing a systematic way to measure and mitigate biases. our experiments' results show the better performance of our approach on zero-shot and full-training settings in comparison with strong baseline models, while also revealing the existence of biases in the resulting llms. utilizing a targeted counterfactual data augmentation approach, we significantly reduce these biases without compromising performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16937" target="_blank">Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases Among Foundation Models</a></div>
<div class="paper-author">Bum Chul Kwon, Nandana Mihindukulasooriya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. however, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotypes. to address this issue, we propose finspector, a human-centered visual inspection tool designed to detect biases in different categories through log-likelihood scores generated by language models. the goal of the tool is to enable researchers to easily identify potential biases using visual analytics, ultimately contributing to a fairer and more just deployment of these models in both academic and industrial settings. finspector is available at https://github.com/ibm/finspector.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16960" target="_blank">Training Socially Aligned Language Models in Simulated Human Society</a></div>
<div class="paper-author">Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social alignment in ai systems aims to ensure that these models behave according to established societal values. however, unlike humans, who derive consensus on value judgments through social interaction, current language models (lms) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. this work presents a novel training paradigm that permits lms to learn from simulated social interactions. in comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. this paradigm shift in the training of lms brings us a step closer to developing ai systems that can robustly and accurately reflect societal norms and values.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17013" target="_blank">D-Calm: A Dynamic Clustering-Based Active Learning Approach for Mitigating Bias</a></div>
<div class="paper-author">Sabit Hassan, Malihe Alikhani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite recent advancements, nlp models continue to be vulnerable to bias. this bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. escalated integration of these models in our lives calls for methods to mitigate bias without overbearing annotation costs. while active learning (al) has shown promise in training models with a small amount of annotated data, al's reliance on the model's behavior for selective sampling can lead to an accumulation of unwanted bias rather than bias mitigation. however, infusing clustering with al can overcome the bias issue of both al and traditional annotation methods while exploiting al's annotation efficiency. in this paper, we propose a novel adaptive clustering-based active learning algorithm, d-calm, that dynamically adjusts clustering and annotation efforts in response to an estimated classifier error-rate. experiments on eight datasets for a diverse set of text classification tasks, including emotion, hatespeech, dialog act, and book type detection, demonstrate that our proposed algorithm significantly outperforms baseline al approaches with both pretrained transformers and traditional support vector machines. d-calm showcases robustness against different measures of information gain and, as evident from our analysis of label and error distribution, can significantly reduce unwanted model bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17174" target="_blank">From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric With Language Models</a></div>
<div class="paper-author">Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. for example, in the sentence 'we need to end the cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to many, but secretly means 'jewish' to a select few. we present the first large-scale computational investigation of dogwhistles. we develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical u.s. politicians' speeches. we then assess whether a large language model (gpt-3) can identify dogwhistles and their meanings, and find that gpt-3's performance varies widely across types of dogwhistles and targeted groups. finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks of such coded language. this work sheds light on the theoretical and applied importance of dogwhistles in both nlp and computational social science, and provides resources for future research in modeling dogwhistles and mitigating their online harms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17359" target="_blank">Dna-Gpt: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text</a></div>
<div class="paper-author">Xianjun Yang, Wei Cheng, Yue Wu, Linda Petzold, William Yang Wang, Haifeng Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have notably enhanced the fluency and diversity of machine-generated text. however, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of llms. conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. to address this gap, we propose a novel training-free detection strategy called divergent n-gram analysis (dna-gpt). given a text, we first truncate it in the middle and then use only the preceding portion as input to the llms to regenerate the new remaining parts. by analyzing the differences between the original and new remaining parts through n-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. we conducted extensive experiments on the most advanced llms from openai, including text-davinci-003, gpt-3.5-turbo, and gpt-4, as well as open-source models such as gpt-neox-20b and llama-13b. results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and gpt-generated text on four english and one german dataset, outperforming openai's own classifier, which is trained on millions of text. additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection. our method is also robust under the revised text attack and can additionally solve model sourcing. codes are available at https://github.com/xianjun-yang/dna-gpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18346" target="_blank">Attention Paper: How Generative Ai Reshapes Digital Shadow Industry?</a></div>
<div class="paper-author">Qichao Wang, Huan Ma, Wentao Wei, Hangyu Li, Liang Chen, Peilin Zhao, Binwen Zhao, Bo Hu, Shu Zhang, Zibin Zheng, Bingzhe Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid development of digital economy has led to the emergence of various black and shadow internet industries, which pose potential risks that can be identified and managed through digital risk management (drm) that uses different techniques such as machine learning and deep learning. the evolution of drm architecture has been driven by changes in data forms. however, the development of ai-generated content (aigc) technology, such as chatgpt and stable diffusion, has given black and shadow industries powerful tools to personalize data and generate realistic images and conversations for fraudulent activities. this poses a challenge for drm systems to control risks from the source of data generation and to respond quickly to the fast-changing risk environment. this paper aims to provide a technical analysis of the challenges and opportunities of aigc from upstream, midstream, and downstream paths of black/shadow industries and suggest future directions for improving existing risk control systems. the paper will explore the new black and shadow techniques triggered by generative ai technology and provide insights for building the next-generation drm system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15757" target="_blank">Healing Unsafe Dialogue Responses With Weak Supervision Signals</a></div>
<div class="paper-author">Zi Liang, Pinghui Wang, Ruofei Zhang, Shuo Zhang, Xiaofan Ye Yi Huang, Junlan Feng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent years have seen increasing concerns about the unsafe response generation of large-scale dialogue systems, where agents will learn offensive or biased behaviors from the real-world corpus. some methods are proposed to address the above issue by detecting and replacing unsafe training examples in a pipeline style. though effective, they suffer from a high annotation cost and adapt poorly to unseen scenarios as well as adversarial attacks. besides, the neglect of providing safe responses (e.g. simply replacing with templates) will cause the information-missing problem of dialogues. to address these issues, we propose an unsupervised pseudo-label sampling method, temp, that can automatically assign potential safe responses. specifically, our temp method groups responses into several clusters and samples multiple labels with an adaptively sharpened sampling strategy, inspired by the observation that unsafe samples in the clusters are usually few and distribute in the tail. extensive experiments in chitchat and task-oriented dialogues show that our temp outperforms state-of-the-art models with weak supervision signals and obtains comparable results under unsupervised learning settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15852" target="_blank">Self-Contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation</a></div>
<div class="paper-author">Niels Mündler, Jingxuan He, Slobodan Jenko, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (large lms) are susceptible to producing text that contains hallucinated content. an important instance of this problem is self-contradiction, where the lm generates two contradictory sentences within the same context. in this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned lms, covering evaluation, detection, and mitigation. our analysis reveals the prevalence of self-contradictions when lms generate text for open-domain topics, e.g., in 17.7% of all sentences produced by chatgpt. self-contradiction also complements retrieval-based methods, as a large portion of them (e.g., 35.8% for chatgpt) cannot be verified using wikipedia. we then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. our detector achieves high accuracy, e.g., around 80% f1 score when prompting chatgpt. the mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. importantly, our entire framework is applicable to black-box lms and does not require external grounded knowledge. our approach is practically effective and has been released as a push-button tool to benefit the public, available at https://chatprotect.ai/.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15875" target="_blank">Linguistic Properties of Truthful Response</a></div>
<div class="paper-author">Bruce W. Lee, Benedict Florance Arockiaraj, Helen Jin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate the phenomenon of an llm's untruthful response using a large set of 220 handcrafted linguistic features. we focus on gpt-3 models and find that the linguistic profiles of responses are similar across model sizes. that is, how varying-sized llms respond to given prompts stays similar on the linguistic properties level. we expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. though the dataset size limits our current findings, we show the possibility that truthfulness detection is possible without evaluating the content itself. but at the same time, the limited scope of our experiments must be taken into account in interpreting the results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15979" target="_blank">Monitoring Algorithmic Fairness</a></div>
<div class="paper-author">Thomas A. Henzinger, Mahyar Karimi, Konstantin Kueffner, Kaushik Mallik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine-learned systems are in widespread use for making decisions about humans, and it is important that they are fair, i.e., not biased against individuals based on sensitive attributes. we present runtime verification of algorithmic fairness for systems whose models are unknown, but are assumed to have a markov chain structure. we introduce a specification language that can model many common algorithmic fairness properties, such as demographic parity, equal opportunity, and social burden. we build monitors that observe a long sequence of events as generated by a given system, and output, after each observation, a quantitative estimate of how fair or biased the system was on that run until that point in time. the estimate is proven to be correct modulo a variable error bound and a given confidence level, where the error bound gets tighter as the observed sequence gets longer. our monitors are of two types, and use, respectively, frequentist and bayesian statistical inference techniques. while the frequentist monitors compute estimates that are objectively correct with respect to the ground truth, the bayesian monitors compute estimates that are correct subject to a given prior belief about the system's model. using a prototype implementation, we show how we can monitor if a bank is fair in giving loans to applicants from different social backgrounds, and if a college is fair in admitting students while maintaining a reasonable financial burden on the society. although they exhibit different theoretical complexities in certain cases, in our experiments, both frequentist and bayesian monitors took less than a millisecond to update their verdicts after each observation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16367" target="_blank">Role-Play With Large Language Models</a></div>
<div class="paper-author">Murray Shanahan, Kyle Mcdonell, Laria Reynolds</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. in this paper, we foreground the concept of role-play. casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16444" target="_blank">Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text</a></div>
<div class="paper-author">Ashim Gupta, Carter Wood Blum, Temma Choji, Yingjie Fei, Shalin Shah, Alakananda Vempala, Vivek Srikumar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: can language models transform inputs to protect text classifiers against adversarial attacks? in this work, we present atinter, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. our experiments on four datasets and five attack mechanisms reveal that atinter is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. for example, on sentiment classification using the sst-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5% vs 2.5%). moreover, we show that atinter generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. specifically, we find that when atinter is trained to remove adversarial perturbations for the sentiment classification task on the sst-2 dataset, it even transfers to a semantically different task of news classification (on agnews) and improves the adversarial robustness by more than 10%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16471" target="_blank">Bias, Consistency, and Partisanship in u.s. Asylum Cases: A Machine Learning Analysis of Extraneous Factors in Immigration Court Decisions</a></div>
<div class="paper-author">Vyoma Raman, Catherine Vera, Cj Manna</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this study, we introduce a novel two-pronged scoring system to measure individual and systemic bias in immigration courts under the u.s. executive office of immigration review (eoir). we analyze nearly 6 million immigration court proceedings and 228 case features to build on prior research showing that u.s. asylum decisions vary dramatically based on factors that are extraneous to the merits of a case. we close a critical gap in the literature of variability metrics that can span space and time. using predictive modeling, we explain 58.54% of the total decision variability using two metrics: partisanship and inter-judge cohort consistency. thus, whether the eoir grants asylum to an applicant or not depends in majority on the combined effects of the political climate and the individual variability of the presiding judge - not the individual merits of the case. using time series analysis, we also demonstrate that partisanship increased in the early 1990s but plateaued following the turn of the century. these conclusions are striking to the extent that they diverge from the u.s. immigration system's commitments to independence and due process. our contributions expose systemic inequities in the u.s. asylum decision-making process, and we recommend improved and standardized variability metrics to better diagnose and monitor these issues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16503" target="_blank">Imbert: Making Bert Immune to Insertion-Based Backdoor Attacks</a></div>
<div class="paper-author">Xuanli He, Jun Wang, Benjamin Rubinstein, Trevor Cohn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: backdoor attacks are an insidious security threat against machine learning models. adversaries can manipulate the predictions of compromised models by inserting triggers into the training phase. various backdoor attacks have been devised which can achieve nearly perfect attack success without affecting model predictions for clean inputs. means of mitigating such vulnerabilities are underdeveloped, especially in natural language processing. to fill this gap, we introduce imbert, which uses either gradients or self-attention scores derived from victim models to self-defend against backdoor attacks at inference time. our empirical studies demonstrate that imbert can effectively identify up to 98.5% of inserted triggers. thus, it significantly reduces the attack success rate while attaining competitive accuracy on the clean dataset across widespread insertion-based attacks compared to two baselines. finally, we show that our approach is model-agnostic, and can be easily ported to several pre-trained transformer models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16519" target="_blank">The Dangers of Trusting Stochastic Parrots: Faithfulness and Trust in Open-Domain Conversational Question Answering</a></div>
<div class="paper-author">Sabrina Chiesurin, Dimitris Dimakopoulos, Marco Antonio Sobrevilla Cabezudo, Arash Eshghi, Ioannis Papaioannou, Verena Rieser, Ioannis Konstas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. "unfaithful" with respect to a rationale as retrieved from a knowledge base. in this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. we use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16544" target="_blank">Inductive Detection of Influence Operations via Graph Learning</a></div>
<div class="paper-author">Nicholas A. Gabriel, David A. Broniatowski, Neil F. Johnson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: influence operations are large-scale efforts to manipulate public opinion. the rapid detection and disruption of these operations is critical for healthy public discourse. emergent ai technologies may enable novel operations which evade current detection methods and influence public discourse on social media with greater scale, reach, and specificity. new methods with inductive learning capacity will be needed to identify these novel operations before they indelibly alter public opinion and events. we develop an inductive learning framework which: 1) determines content- and graph-based indicators that are not specific to any operation; 2) uses graph learning to encode abstract signatures of coordinated manipulation; and 3) evaluates generalization capacity by training and testing models across operations originating from russia, china, and iran. we find that this framework enables strong cross-operation generalization while also revealing salient indicators$\unicode{x2013}$illustrating a generic approach which directly complements transductive methodologies, thereby enhancing detection coverage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16577" target="_blank">Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases</a></div>
<div class="paper-author">Haozhe An, Rachel Rudinger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (an et al., 2023). demographic attributes of first names, however, are strongly correlated with corpus frequency and tokenization length, which may influence model behavior independent of or in addition to demographic factors. in this paper, we conduct a new series of first name substitution experiments that measures the influence of these factors while controlling for the others. we find that demographic attributes of a name (race, ethnicity, and gender) and name tokenization length are both factors that systematically affect the behavior of social commonsense reasoning models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17147" target="_blank">Heterogeneous Value Evaluation for Large Language Models</a></div>
<div class="paper-author">Zhaowei Zhang, Nian Liu, Siyuan Qi, Ceyao Zhang, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergent capabilities of large language models (llms) have made it crucial to align their values with those of humans. current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. in this paper, we propose a2ehv, an automated alignment evaluation with a heterogeneous value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. the quantification of value rationality is facilitated by the social value orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. we evaluate the value rationality of eight mainstream llms and observe that large models are more inclined to align neutral values compared to those with strong personal values. by examining the behavior of these llms, we contribute to a deeper understanding of value alignment within a heterogeneous value system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18339" target="_blank">A Survey on Chatgpt: Ai-Generated Contents, Challenges, and Solutions</a></div>
<div class="paper-author">Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, Tom H. Luan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the widespread use of large artificial intelligence (ai) models such as chatgpt, ai-generated content (aigc) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. aigc uses generative large ai algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. despite the recent significant progress in aigc, security, privacy, ethical, and legal challenges still need to be addressed. this paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the aigc paradigm. specifically, we first explore the enabling technologies, general architecture of aigc, and discuss its working modes and key characteristics. then, we investigate the taxonomy of security and privacy threats to aigc and highlight the ethical and societal implications of gpt and aigc technologies. furthermore, we review the state-of-the-art aigc watermarking approaches for regulatable aigc paradigms regarding the aigc model and its produced content. finally, we identify future challenges and open research directions related to aigc.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14710" target="_blank">Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</a></div>
<div class="paper-author">Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. however, in this work we raise security concerns about this training paradigm. our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used nlp datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. in this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. moreover, the poisoned model cannot be cured by continual learning. lastly, instruction attacks show resistance to existing inference-time defense. these findings highlight the need for more robust defenses against data poisoning attacks in instructiontuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14735" target="_blank">Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection</a></div>
<div class="paper-author">Vyoma Raman, Eve Fleisig, Dan Klein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the impact of ai models on marginalized communities has traditionally been measured by identifying performance differences between specified demographic subgroups. though this approach aims to center vulnerable groups, it risks obscuring patterns of harm faced by intersectional subgroups or shared across multiple groups. to address this, we draw on theories of marginalization from disability studies and related disciplines, which state that people farther from the norm face greater adversity, to consider the "margins" in the domain of toxicity detection. we operationalize the "margins" of a dataset by employing outlier detection to identify text about people with demographic attributes distant from the "norm". we find that model performance is consistently worse for demographic outliers, with mean squared error (mse) between outliers and non-outliers up to 70.4% worse across toxicity types. it is also worse for text outliers, with a mse up to 68.4% higher for outliers than non-outliers. we also find text and demographic outliers to be particularly susceptible to errors in the classification of severe toxicity and identity attacks. compared to analysis of disparities using traditional demographic breakdowns, we find that our outlier analysis frequently surfaces greater harms faced by a larger, more intersectional group, which suggests that outlier analysis is particularly beneficial for identifying harms against those groups.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14752" target="_blank">A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification</a></div>
<div class="paper-author">Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, Lucas C. Cordeiro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper we present a novel solution that combines the capabilities of large language models (llms) with formal verification strategies to verify and automatically repair software vulnerabilities. initially, we employ bounded model checking (bmc) to locate the software vulnerability and derive a counterexample. the counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. the counterexample that has been detected, along with the source code, are provided to the llm engine. our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. finally, we use bmc to verify the corrected version of the code generated by the llm. as a proof of concept, we create esbmc-ai based on the efficient smt-based context-bounded model checker (esbmc) and a pre-trained transformer model, specifically gpt-3.5-turbo, to detect and fix errors in c programs. our experimentation involved generating a dataset comprising 1000 c code samples, each consisting of 20 to 50 lines of code. notably, our proposed method achieved an impressive success rate of up to 80% in repairing vulnerable code encompassing buffer overflow and pointer dereference failures. we assert that this automated approach can effectively incorporate into the software development lifecycle's continuous integration and deployment (ci/cd) process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14784" target="_blank">Anthropomorphization of Ai: Opportunities and Risks</a></div>
<div class="paper-author">Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, Ashwin Kalyan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: anthropomorphization is the tendency to attribute human-like traits to non-human entities. it is prevalent in many social contexts -- children anthropomorphize toys, adults do so with brands, and it is a literary device. it is also a versatile tool in science, with behavioral psychology and evolutionary biology meticulously documenting its consequences. with widespread adoption of ai systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly. we take a dyadic approach to understanding this phenomenon with large language models (llms) by studying (1) the objective legal implications, as analyzed through the lens of the recent blueprint of ai bill of rights and the (2) subtle psychological aspects customization and anthropomorphization. we find that anthropomorphized llms customized for different user bases violate multiple provisions in the legislative blueprint. in addition, we point out that anthropomorphization of llms affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-ai interaction, with potential for manipulation and negative influence. with llms being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution. we propose a conservative strategy for the cautious use of anthropomorphization to improve trustworthiness of ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14822" target="_blank">Can Copyright Be Reduced to Privacy?</a></div>
<div class="paper-author">Niva Elkin-Koren, Uri Hacohen, Roi Livni, Shay Moran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: there is an increasing concern that generative ai models may produce outputs that are remarkably similar to the copyrighted input content on which they are trained. this worry has escalated as the quality and complexity of generative models have immensely improved, and the availability of large datasets containing copyrighted material has increased. researchers are actively exploring strategies to mitigate the risk of producing infringing samples, and a recent line of work suggests to employ techniques such as differential privacy and other forms of algorithmic stability to safeguard copyrighted content.   in this work, we examine the question whether algorithmic stability techniques such as differential privacy are suitable to ensure the responsible use of generative models without inadvertently violating copyright laws. we argue that there are fundamental differences between privacy and copyright that should not be overlooked. in particular we highlight that although algorithmic stability may be perceived as a practical tool to detect copying, it does not necessarily equate to copyright protection. therefore, if it is adopted as standard for copyright infringement, it may undermine copyright law intended purposes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14888" target="_blank">Privacy Implications of Retrieval-Based Language Models</a></div>
<div class="paper-author">Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, Danqi Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: retrieval-based language models (lms) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts, by incorporating retrieved text from external datastores. while it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. in this work, we present the first study of privacy risks in retrieval-based lms, particularly $k$nn-lms. our goal is to explore the optimal design and training procedure in domains where privacy is of concern, aiming to strike a balance between utility and privacy. crucially, we find that $k$nn-lms are more susceptible to leaking private information from their private datastore than parametric models. we further explore mitigations of privacy risks. when privacy information is targeted and readily detected in the text, we find that a simple sanitization step would completely eliminate the risks, while decoupling query and key encoders achieves an even better utility-privacy trade-off. otherwise, we consider strategies of mixing public and private data in both datastore and encoder training. while these methods offer modest improvements, they leave considerable room for future work. together, our findings provide insights for practitioners to better understand and mitigate privacy risks in retrieval-based lms. our code is available at: https://github.com/princeton-sysml/knnlm_privacy .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14902" target="_blank">M4: Multi-Generator, Multi-Domain, and Multi-Lingual Black-Box Machine-Generated Text Detection</a></div>
<div class="paper-author">Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, Preslav Nakov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries, but this has also resulted in concerns regarding the potential misuse of such texts in journalism, educational, and academic context. in this work, we aim to develop automatic systems to identify machine-generated text and to detect potential misuse. we first introduce a large-scale benchmark m4, which is multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. using the dataset, we experiment with a number of methods and we show that it is challenging for detectors to generalize well on unseen examples if they are either from different domains or are generated by different large language models. in such cases, detectors tend to misclassify machine-generated text as human-written. these results show that the problem is far from solved and there is a lot of room for improvement. we believe that our dataset m4, which covers different generators, domains and languages, will enable future research towards more robust approaches for this pressing societal problem. the m4 dataset is available at https://github.com/mbzuai-nlp/m4.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14908" target="_blank">Purr: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions</a></div>
<div class="paper-author">Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, Kelvin Guu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the remarkable capabilities of large language models have been accompanied by a persistent drawback: the generation of false and unsubstantiated claims commonly known as "hallucinations". to combat this issue, recent research has introduced approaches that involve editing and attributing the outputs of language models, particularly through prompt-based editing. however, the inference cost and speed of using large language models for editing currently bottleneck prompt-based methods. these bottlenecks motivate the training of compact editors, which is challenging due to the scarcity of training data for this purpose. to overcome these challenges, we exploit the power of large language models to introduce corruptions (i.e., noise) into text and subsequently fine-tune compact editors to denoise the corruptions by incorporating relevant evidence. our methodology is entirely unsupervised and provides us with faux hallucinations for training in any domain. our petite unsupervised research and revision model, purr, not only improves attribution over existing editing methods based on fine-tuning and prompting, but also achieves faster execution times by orders of magnitude.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14910" target="_blank">From Shortcuts to Triggers: Backdoor Defense With Denoised Poe</a></div>
<div class="paper-author">Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models are often at risk of diverse backdoor attacks, especially data poisoning. thus, it is important to investigate defense solutions for addressing them. existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. in this paper, we propose an end-to-end ensemble-based backdoor defense framework, dpoe (denoised product-of-experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. dpoe consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the backdoor shortcuts. to address the label flip caused by backdoor attackers, dpoe incorporates a denoising design. experiments on sst-2 dataset show that dpoe significantly improves the defense performance against various types of backdoor triggers including word-level, sentence-level, and syntactic triggers. furthermore, dpoe is also effective under a more challenging but practical setting that mixes multiple types of trigger.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14928" target="_blank">Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4</a></div>
<div class="paper-author">Kellin Pelrine, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph, Reihaneh Rabbany</div>
<div class="abstract">
<div class="abstract-content">
Abstract: misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. we propose focusing on generalization, soft classification, and leveraging recent large language models to create more practical tools in contexts where perfect predictions remain unattainable. we begin by demonstrating that gpt-4 and other language models can outperform existing methods in the literature. next, we explore their generalization, revealing that gpt-4 and roberta-large exhibit critical differences in failure modes, which offer potential for significant performance improvements. finally, we show that these models can be employed in soft classification frameworks to better quantify uncertainty. we find that models with inferior hard classification results can achieve superior soft classification performance. overall, this research lays groundwork for future tools that can drive real-world progress on misinformation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14929" target="_blank">Aligning Language Models to User Opinions</a></div>
<div class="paper-author">Eunjeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon</div>
<div class="abstract">
<div class="abstract-content">
Abstract: an important aspect of developing llms that interact with humans is to align models' behavior to their users. it is possible to prompt an llm into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. but, how to best align an llm with a specific user and not a demographic or ideological group remains an open question. mining public opinion surveys (by pew research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. we use this insight to align llms by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. in addition to the typical approach of prompting llms with demographics and ideology, we discover that utilizing the most relevant past opinions from individual users enables the model to predict user opinions more accurately.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14930" target="_blank">In-Context Impersonation Reveals Large Language Models' Strengths and Biases</a></div>
<div class="paper-author">Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. we explore whether llms can take on, that is impersonate, different roles when they generate text in-context. we ask llms to assume different personas before solving vision and language tasks. we do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. in a multi-armed bandit task, we find that llms pretending to be children of different ages recover human-like developmental stages of exploration. in a language-based reasoning task, we find that llms impersonating domain experts perform better than llms impersonating non-domain experts. finally, we test whether llms' impersonations are complementary to visual information when describing different categories. we find that impersonation can improve performance: an llm prompted to be a bird expert describes birds better than one prompted to be a car expert. however, impersonation can also uncover llms' biases: an llm prompted to be a man describes cars better than one prompted to be a woman. these findings demonstrate that llms are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14936" target="_blank">Trade-Offs Between Fairness and Privacy in Language Modeling</a></div>
<div class="paper-author">Cleo Matzken, Steffen Eger, Ivan Habernal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: protecting privacy in contemporary nlp models is gaining in importance. so does the need to mitigate social biases of such models. but can we have both at the same time? existing research suggests that privacy preservation comes at the price of worsening biases in classification tasks. in this paper, we explore the extent to which this tradeoff really holds when we incorporate both privacy preservation and de-biasing techniques into training text generation models. how does improving the model along one dimension affect the other dimension as well as the utility of the model? we conduct an extensive set of experiments that include bias detection, privacy attacks, language modeling, and performance on downstream tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14950" target="_blank">Adversarial Demonstration Attacks on Large Language Models</a></div>
<div class="paper-author">Jiongxiao Wang, Zichen Liu, Keun Hee Park, Zhuojun Jiang, Zhaoheng Zheng, Zhuofeng Wu, Muhao Chen, Chaowei Xiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the emergence of more powerful large language models (llms), such as chatgpt and gpt-4, in-context learning (icl) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. while incorporating demonstrations can greatly enhance the performance of llms across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. in this paper, we investigate the security concern of icl from an adversarial perspective, focusing on the impact of demonstrations. we propose a novel attack method named advicl, which aims to manipulate only the demonstration without changing the input to mislead the models. our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. as a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. to achieve it, we propose the transferable version of advicl, named transferable-advicl. our experiment shows that the adversarial demonstration generated by transferable-advicl can successfully attack the unseen test input examples. we hope that our study reveals the critical security risks associated with icl and underscores the need for extensive research on the robustness of icl, particularly given its increasing significance in the advancement of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14965" target="_blank">Tricking LLMS Into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks</a></div>
<div class="paper-author">Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent explorations with commercial large language models (llms) have shown that non-expert users can jailbreak llms by simply manipulating the prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. limited formal studies have been carried out to formalize and analyze these attacks and their mitigations. we bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. we perform a survey of existing jailbreak methods and their effectiveness on open-source and commercial llms (such as gpt 3.5, opt, bloom, and flan-t5-xxl). we further propose a limited set of prompt guards and discuss their effectiveness against known attack types.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14975" target="_blank">Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores From Language Models Fine-Tuned With Human Feedback</a></div>
<div class="paper-author">Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D. Manning</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. recent studies have shown that unsupervised pre-training produces large language models (lms) whose conditional probabilities are remarkably well-calibrated. however, the most widely-used lms are fine-tuned with reinforcement learning from human feedback (rlhf-lms), and some studies have suggested that rlhf-lms produce conditional probabilities that are very poorly calibrated. in light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from rlhf-lms. for rlhf-lms such as chatgpt, gpt-4, and claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the triviaqa, sciq, and truthfulqa benchmarks, often reducing the expected calibration error by a relative 50%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15004" target="_blank">Llmdet: A Third Party Large Language Models Generated Text Detection Tool</a></div>
<div class="paper-author">Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generated texts from large language models (llms) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. however, existing detection tools typically rely on access to llms and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. therefore, we propose llmdet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific llms, such as gpt-2, opt, llama, and others. in llmdet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each llm. by jointly analyzing the proxy perplexities of llms, we can determine the source of the generated text. experimental results show that llmdet yields impressive detection performance while ensuring speed and security, achieving 98.54% precision and x3.5 faster for recognizing human-authored text. additionally, llmdet can effortlessly extend its detection capabilities to a new open-source model. we will provide an open-source tool at https://github.com/trustedllm/llmdet.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15064" target="_blank">Autoplan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models</a></div>
<div class="paper-author">Siqi Ouyang, Lei Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent large language models (llms) are promising for making decisions in grounded environments. however, llms frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in llms and the actual rules in the environment. existing methods require either costly gradient computation or lengthy in-context demonstrations. in this paper, we propose autoplan, an approach to guide llm-based agents to accomplish interactive decision-making tasks. autoplan augments the llm prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. our experiments show that autoplan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on alfworld and even outperforms them by 8% on hotpotqa. the code is available at https://github.com/owaski/autoplan.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15075" target="_blank">Huatuogpt, Towards Taming Language Model to Be a Doctor</a></div>
<div class="paper-author">Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, Haizhou Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we present huatuogpt, a large language model (llm) for medical consultation. the core recipe of huatuogpt is to leverage both \textit{distilled data from chatgpt} and \textit{real-world data from doctors} in the supervised fine-tuned stage. the responses of chatgpt are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. we argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. to better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an rlaif (reinforced learning from ai feedback) fashion. to evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). experimental results demonstrate that huatuogpt achieves state-of-the-art results in performing medical consultation among open-source llms in gpt-4 evaluation, human evaluation, and medical benchmark datasets. it is worth noting that by using additional real-world data and rlaif, the distilled language model (i.e., huatuogpt) outperforms its teacher model chatgpt in most cases. our code, data, and models are publicly available at \url{https://github.com/freedomintelligence/huatuogpt}. the online demo is available at \url{https://www.huatuogpt.cn/}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15242" target="_blank">Machine Unlearning: Its Nature, Scope, and Importance for a "Delete Culture"</a></div>
<div class="paper-author">Luciano Floridi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the article explores the cultural shift from recording to deleting information in the digital age and its implications on privacy, intellectual property (ip), and large language models like chatgpt. it begins by defining a delete culture where information, in principle legal, is made unavailable or inaccessible because unacceptable or undesirable, especially but not only due to its potential to infringe on privacy or ip. then it focuses on two strategies in this context: deleting, to make information unavailable; and blocking, to make it inaccessible. the article argues that both strategies have significant implications, particularly for machine learning (ml) models where information is not easily made unavailable. however, the emerging research area of machine unlearning (mu) is highlighted as a potential solution. mu, still in its infancy, seeks to remove specific data points from ml models, effectively making them 'forget' completely specific information. if successful, mu could provide a feasible means to manage the overabundance of information and ensure a better protection of privacy and ip. however, potential ethical risks, such as misuse, overuse, and underuse of mu, should be systematically studied to devise appropriate policies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15299" target="_blank">Science in the Era of Chatgpt, Large Language Models and Generative Ai: Challenges for Research Ethics and How to Respond</a></div>
<div class="paper-author">Evangelos Pournaras</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models of artificial intelligence (ai), such as chatgpt, find remarkable but controversial applicability in science and research. this paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative ai. this is with the aim to lay new timely foundations for a high-quality research ethics review. the role of ai language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. new emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15324" target="_blank">Model Evaluation for Extreme Risks</a></div>
<div class="paper-author">Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, Allan Dafoe</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current approaches to building general-purpose ai systems tend to produce systems with both beneficial and harmful capabilities. further progress in ai development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. we explain why model evaluation is critical for addressing extreme risks. developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). these evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15336" target="_blank">From Text to Mitre Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads</a></div>
<div class="paper-author">P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand, Sandeep K Shukla</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this research article critically examines the potential risks and implications arising from the malicious utilization of large language models(llm), focusing specifically on chatgpt and google's bard. although these large language models have numerous beneficial applications, the misuse of this technology by cybercriminals for creating offensive payloads and tools is a significant concern. in this study, we systematically generated implementable code for the top-10 mitre techniques prevalent in 2022, utilizing chatgpt, and conduct a comparative analysis of its performance with google's bard. our experimentation reveals that chatgpt has the potential to enable attackers to accelerate the operation of more targeted and sophisticated attacks. additionally, the technology provides amateur attackers with more capabilities to perform a wide range of attacks and empowers script kiddies to develop customized tools that contribute to the acceleration of cybercrime. furthermore, llms significantly benefits malware authors, particularly ransomware gangs, in generating sophisticated variants of wiper and ransomware attacks with ease. on a positive note, our study also highlights how offensive security researchers and pentesters can make use of llms to simulate realistic attack scenarios, identify potential vulnerabilities, and better protect organizations. overall, we conclude by emphasizing the need for increased vigilance in mitigating the risks associated with llms. this includes implementing robust security measures, increasing awareness and education around the potential risks of this technology, and collaborating with security experts to stay ahead of emerging threats.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15377" target="_blank">Uncovering and Quantifying Social Biases in Code Generation</a></div>
<div class="paper-author">Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, Tsung-Yi Ho</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the popularity of automatic code generation tools, such as copilot, the study of the potential hazards of these tools is gaining importance. in this work, we explore the social bias problem in pre-trained code generation models. we propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. to quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. experimental results on three pre-trained code generation models (codex, incoder, and codegen) with varying sizes, reveal severe social biases. moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias. (this work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.)
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15551" target="_blank">Malicious or Benign? Towards Effective Content Moderation for Children's Videos</a></div>
<div class="paper-author">Syed Hammad Ahmed, Muhammad Junaid Khan, H. M. Umer Qaisar, Gita Sukthankar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: online video platforms receive hundreds of hours of uploads every minute, making manual content moderation impossible. unfortunately, the most vulnerable consumers of malicious video content are children from ages 1-5 whose attention is easily captured by bursts of color and sound. scammers attempting to monetize their content may craft malicious children's videos that are superficially similar to educational videos, but include scary and disgusting characters, violent motions, loud music, and disturbing noises. prominent video hosting platforms like youtube have taken measures to mitigate malicious content on their platform, but these videos often go undetected by current content moderation tools that are focused on removing pornographic or copyrighted content. this paper introduces our toolkit malicious or benign for promoting research on automated content moderation of children's videos. we present 1) a customizable annotation tool for videos, 2) a new dataset with difficult to detect test cases of malicious content and 3) a benchmark suite of state-of-the-art video classification models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15594" target="_blank">Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models</a></div>
<div class="paper-author">Haonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are excellent in-context learners. however, the sensitivity of data contained in prompts raises privacy concerns. our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt llms. to address this vulnerability, one could forego prompting and resort to fine-tuning llms with known algorithms for private gradient descent. however, this comes at the expense of the practicality and efficiency offered by prompting. therefore, we propose to privately learn to prompt. we first show that soft prompts can be obtained privately through gradient descent on downstream data. however, this is not the case for discrete prompts. thus, we orchestrate a noisy vote among an ensemble of llms presented with different prompts, i.e., a flock of stochastic parrots. the vote privately transfers the flock's knowledge into a single public prompt. we show that llms prompted with our private algorithms closely match the non-private baselines. for example, using gpt3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial apis.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06998" target="_blank">A Systematic Review of Machine Learning Enabled Phishing</a></div>
<div class="paper-author">Krystal A. Jackson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developments in artificial intelligence (ai) are likely to affect social engineering and change cyber defense operations. the broad and sweeping nature of ai impact means that many aspects of social engineering could be automated, potentially giving adversaries an advantage. in this review, we assess the ways phishing and spear-phishing might be affected by machine learning techniques. by performing a systematic review of demonstrated ml-enabled phishing campaigns, we take a broad survey the space for current developments. we develop a detailed approach for evaluation by creating a risk framework for analyzing and contextualizing these developments. the object of this review is to answer the research questions: (1) are there high-risk ml-enabled phishing use cases? (2) is there a meaningful difference between traditional targeted phishing campaigns and ml-enabled phishing campaigns? practitioners may use this review to inform standards, future research directions, and cyber defense strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13661" target="_blank">On the Risk of Misinformation Pollution With Large Language Models</a></div>
<div class="paper-author">Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we comprehensively investigate the potential misuse of modern large language models (llms) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly open-domain question answering (odqa) systems. we establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which llms can be utilized to produce misinformation. our study reveals that llms can act as effective misinformation generators, leading to a significant degradation in the performance of odqa systems. to mitigate the harm caused by llm-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. while initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. our work highlights the need for further research and interdisciplinary collaboration to address llm-generated misinformation and to promote responsible use of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13669" target="_blank">Mitigating Language Model Hallucination With Interactive Question-Knowledge Alignment</a></div>
<div class="paper-author">Shuo Zhang, Liangming Pan, Junzhou Zhao, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. a common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. however, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. this misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. to address this issue, we introduce mixalign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. mixalign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. experimental results demonstrate significant improvements over state-of-the-art methods, showcasing the effectiveness of mixalign in mitigating language model hallucination.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13677" target="_blank">Towards Legally Enforceable Hate Speech Detection for Public Forums</a></div>
<div class="paper-author">Chu Fei Luo, Rohan Bhambhoria, Xiaodan Zhu, Samuel Dahan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech is a serious issue on public forums, and proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. however, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. our work introduces a new task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. we experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. we then report results on several large language models (llms). with this task definition, automatic hate speech detection can be more closely aligned to enforceable laws, and hence assist in more rigorous enforcement of legal protections against harmful speech in public forums.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13712" target="_blank">Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty With Large Language Models</a></div>
<div class="paper-author">Alfonso Amayuelas, Liangming Pan, Wenhu Chen, William Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates the capabilities of large language models (llms) in the context of understanding their own knowledge and measuring their uncertainty. we argue this is an important feature for mitigating hallucinations. specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. to facilitate our study, we collect a dataset with new known-unknown questions (kuq) and propose a novel categorization scheme to elucidate the sources of uncertainty. subsequently, we assess the llms' ability to differentiate between known and unknown questions and classify them accordingly. moreover, we evaluate the quality of their answers in an open-ended qa setting. to quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13733" target="_blank">Self-Critique Prompting With Large Language Models for Inductive Instructions</a></div>
<div class="paper-author">Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Ruifeng Xu, Kam-Fai Wong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous works are proposed to improve or evaluate the capabilities of large language models (llms) to fulfill user instructions. however, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. in this way, blindly adhering to users' false content will cause deception and harm. to address this problem, we propose a challenging benchmark consisting of inductive instructions (indust) to evaluate whether llms could resist these instructions. the indust includes 15k instructions across three categories: fact-checking instructions, questions based on false premises, and creative instructions based on false premises. our experiments on several strong llms reveal that current llms can be easily deceived by indust into generating misleading and malicious statements. hence we employ self-critique prompting to encourage llms to not only critique themselves like in previous works but also the users, which show remarkable improvement in handling inductive instructions under both zero-shot and few-shot settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13735" target="_blank">Aligning Large Language Models Through Synthetic Feedback</a></div>
<div class="paper-author">Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) to human values has become increasingly important as it enables sophisticated steering of llms. however, it requires significant human demonstrations and feedback or distillation from proprietary llms such as chatgpt. in this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary llms. first, we perform reward modeling (rm) with synthetic feedback by contrasting responses from vanilla llms with various sizes and prompts. then, we use the rm to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. our resulting model, aligned language model with synthetic training dataset (almost), outperforms recent open-sourced models, which are trained on the outputs of instructgpt or human-annotated demonstrations, in alignment benchmarks. in human evaluation, our model is preferred to alpaca and dolly-v2, 55.0% and 58.5% of the time, respectively. further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. the code is available at https://github.com/naver-ai/almost
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13788" target="_blank">Can Large Language Models Infer and Disagree Like Humans?</a></div>
<div class="paper-author">Noah Lee, Na Min An, James Thorne</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown stellar achievements in solving a broad range of tasks. when generating text, it is common to sample tokens from these models: whether llms closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (nli). in this paper, we evaluate the performance and alignment of llm distribution with humans using two different techniques: monte carlo reconstruction (mcr) and log probability reconstruction (lpr). as a result, we show llms exhibit limited ability in solving nli tasks and simultaneously fail to capture human disagreement distribution, raising concerns about their natural language understanding (nlu) ability and their representativeness of human users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13857" target="_blank">Revealing User Familiarity Bias in Task-Oriented Dialogue via Interactive Evaluation</a></div>
<div class="paper-author">Takyoung Kim, Jamin Shin, Young-Ho Kim, Sanghwan Bae, Sungdong Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most task-oriented dialogue (tod) benchmarks assume users that know exactly how to use the system by constraining the user behaviors within the system's capabilities via strict user goals, namely "user familiarity" bias. this data bias deepens when it combines with data-driven tod systems, as it is impossible to fathom the effect of it with existing static evaluations. hence, we conduct an interactive user study to unveil how vulnerable tod systems are against realistic scenarios. in particular, we compare users with 1) detailed goal instructions that conform to the system boundaries (closed-goal) and 2) vague goal instructions that are often unsupported but realistic (open-goal). our study reveals that conversations in open-goal settings lead to catastrophic failures of the system, in which 92% of the dialogues had significant issues. moreover, we conduct a thorough analysis to identify distinctive features between the two settings through error annotation. from this, we discover a novel "pretending" behavior, in which the system pretends to handle the user requests even though they are beyond the system's capabilities. we discuss its characteristics and toxicity while emphasizing transparency and a fallback strategy for robust tod systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13860" target="_blank">Jailbreaking Chatgpt via Prompt Engineering: An Empirical Study</a></div>
<div class="paper-author">Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), like chatgpt, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. our study investigates three key research questions: (1) the number of different prompt types that can jailbreak llms, (2) the effectiveness of jailbreak prompts in circumventing llm constraints, and (3) the resilience of chatgpt against these jailbreak prompts. initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. subsequently, we assess the jailbreak capability of prompts with chatgpt versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. finally, we evaluate the resistance of chatgpt against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. the study underscores the importance of prompt structures in jailbreaking llms and discusses the challenges of robust jailbreak prompt generation and prevention.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13862" target="_blank">A Trip Towards Fairness: Bias and De-Biasing in Large Language Models</a></div>
<div class="paper-author">Leonardo Ranaldi, Elena Sofia Ruzzetti, Davide Venditti, Dario Onorati, Fabio Massimo Zanzotto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: cheap-to-build very large-language models (ctb-llms) with affordable training are emerging as the next big revolution in natural language processing and understanding. these ctb-llms are democratizing access to trainable very large-language models (vllms) and, thus, may represent the building blocks of many nlp systems solving downstream tasks. hence, a little or a large bias in ctb-llms may cause huge harm. in this paper, we performed a large investigation of the bias of three families of ctb-llms, and we showed that debiasing techniques are effective and usable. indeed, according to current tests, the llama and the opt families have an important bias in gender, race, religion, and profession. in contrast to the analysis for other llms, we discovered that bias depends not on the number of parameters but on the perplexity. finally, the debiasing of opt using lora reduces bias up to 4.12 points in the normalized stereotype score.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14002" target="_blank">Improving Language Models via Plug-and-Play Retrieval Feedback</a></div>
<div class="paper-author">Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, Ashish Sabharwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) exhibit remarkable performance across various nlp tasks. however, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. however, this approach is resource-intensive, involving manual input and supervision, which can be time-consuming and expensive. moreover, it cannot be provided during inference, further limiting its practical utility in dynamic and interactive applications. in this paper, we introduce refeed, a novel pipeline designed to enhance llms by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning. refeed first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections, and finally incorporates the retrieved information into the in-context demonstration for output refinement, thereby addressing the limitations of llms in a more efficient and cost-effective manner. experiments on four knowledge-intensive benchmark datasets demonstrate our proposed refeed could improve over +6.0% under zero-shot setting and +2.5% under few-shot setting, compared to baselines without using retrieval feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14020" target="_blank">Does Chatgpt Have Theory of Mind?</a></div>
<div class="paper-author">Bart Holterman, Kees Van Deemter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: theory of mind (tom) is the ability to understand human thinking and decision-making, an ability that plays a crucial role in social interaction between people, including linguistic communication. this paper investigates to what extent recent large language models in the chatgpt tradition possess tom. we posed six well-known problems that address biases in human reasoning and decision making to two versions of chatgpt and we compared the results under a range of prompting strategies. while the results concerning chatgpt-3 were somewhat inconclusive, chatgpt-4 was shown to arrive at the correct answers more often than would be expected based on chance, although correct answers were often arrived at on the basis of false assumptions or invalid reasoning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14081" target="_blank">How to Solve Few-Shot Abusive Content Detection Using the Data We Actually Have</a></div>
<div class="paper-author">Viktor Hangya, Alexander Fraser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: due to the broad range of social media platforms and their user groups, the requirements of abusive language detection systems are varied and ever-changing. already a large set of annotated corpora with different properties and label sets were created, such as hate or misogyny detection, but the form and targets of abusive speech are constantly changing. since, the annotation of new corpora is expensive, in this work we leverage datasets we already have, covering a wide range of tasks related to abusive language detection, in order to build models cheaply for a new target label set and/or language, using only a few training examples of the target domain. we propose a two-step approach: first we train our model in a multitask fashion. we then carry out few-shot adaptation to the target requirements. our experiments show that by leveraging already existing datasets and only a few-shots of the target task the performance of models can be improved not only monolingually but across languages as well. our analysis also shows that our models acquire a general understanding of abusive language, since they improve the prediction of labels which are present only in the target dataset. we also analyze the trade-off between specializing the already existing datasets to a given target setup for best performance and its negative effects on model adaptability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14208" target="_blank">Domain Private Transformers</a></div>
<div class="paper-author">Anmol Kabra, Ethan R. Elenberg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large, general purpose language models have demonstrated impressive performance across many different conversational domains. while multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt. this paper proposes domain privacy as a novel way to quantify how likely a conditional language model will leak across domains. we also develop policy functions based on token-level domain classification, and propose an efficient fine-tuning method to improve the trained model's domain privacy. experiments on membership inference attacks show that our proposed method has comparable resiliency to methods adapted from recent literature on differentially private language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14250" target="_blank">Language Models With Rationality</a></div>
<div class="paper-author">Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schutze, Peter Clark</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) are proficient at question-answering (qa), the dependencies between their answers and other "beliefs" they may have about the world are typically unstated, and may even be in conflict. our goal is to uncover such dependencies and reduce inconsistencies among them, so that answers are supported by faithful, system-believed chains of reasoning drawn from a consistent network of beliefs. our approach, which we call reflex, is to add a "rational", self-reflecting layer on top of the llm. first, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model "beliefs" (including beliefs about answer candidates) and the inferential relationships between them. second, we identify and minimize contradictions in that graph using a formal constraint reasoner. we find that reflex significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. this suggests a new style of system architecture, in which an llm extended with a rational layer of self-reflection can repair latent inconsistencies within the llm alone.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14291" target="_blank">Evaluation of African American Language Bias in Natural Language Generation</a></div>
<div class="paper-author">Nicholas Deas, Jessi Grieser, Shana Kleiner, Desmond Patton, Elsbeth Turcan, Kathleen Mckeown</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we evaluate how well llms understand african american language (aal) in comparison to their performance on white mainstream english (wme), the encouraged "standard" form of english taught in american classrooms. we measure llm performance using automatic metrics and human judgments for two tasks: a counterpart generation task, where a model generates aal (or wme) given wme (or aal), and a masked span prediction (msp) task, where models predict a phrase that was removed from their input. our contributions include: (1) evaluation of six pre-trained, large language models on the two language generation tasks; (2) a novel dataset of aal text from multiple contexts (social media, hip-hop lyrics, focus groups, and linguistic interviews) with human-annotated counterparts in wme; and (3) documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of aal features.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14307" target="_blank">Debiasing Should Be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models</a></div>
<div class="paper-author">Robert Morabito, Jad Kabbara, Ali Emami</div>
<div class="abstract">
<div class="abstract-content">
Abstract: debiasing methods that seek to mitigate the tendency of language models (lms) to occasionally output toxic or inappropriate text have recently gained traction. in this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. for example, we ask, given a debiasing method that is developed to reduce toxicity in lms, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? we used such considerations to devise three criteria for our new protocol: specification polarity, specification importance, and domain transferability. as a case study, we apply our protocol to a popular debiasing method, self-debiasing, and compare it to one we propose, called instructive debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. we show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14456" target="_blank">Having Beer After Prayer? Measuring Cultural Bias in Large Language Models</a></div>
<div class="paper-author">Tarek Naous, Michael J. Ryan, Wei Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: are language models culturally biased? it is important that language models conform to the cultural aspects of the communities they serve. however, we show in this paper that language models suffer from a significant bias towards western culture when handling and generating text in arabic, often preferring, and producing western-fitting content as opposed to the relevant arab content. we quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. our experiments reveal that both arabic monolingual and multilingual models exhibit bias towards western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. models also tend to exhibit more bias when prompted with arabic sentences that are more linguistically aligned with english. these findings raise concerns about the cultural relevance of current language models. our analyses show that providing culture-indicating tokens or culturally-relevant demonstrations to the model can help in debiasing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14468" target="_blank">Run Like a Girl! Sports-Related Gender Bias in Language and Vision</a></div>
<div class="paper-author">Sophia Harrison, Eleonora Gualdoni, Gemma Boleda</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias in language and vision datasets and models has the potential to perpetuate harmful stereotypes and discrimination. we analyze gender bias in two language and vision datasets. consistent with prior work, we find that both datasets underrepresent women, which promotes their invisibilization. moreover, we hypothesize and find that a bias affects human naming choices for people playing sports: speakers produce names indicating the sport (e.g. 'tennis player' or 'surfer') more often when it is a man or a boy participating in the sport than when it is a woman or a girl, with an average of 46% vs. 35% of sports-related names for each gender. a computational model trained on these naming data reproduces the bias. we argue that both the data and the model result in representational harm against women.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14540" target="_blank">LLMS as Factual Reasoners: Insights From Existing Benchmarks and Beyond</a></div>
<div class="paper-author">Philippe Laban, Wojciech Kryściński, Divyansh Agarwal, Alexander R. Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the recent appearance of llms in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. when testing on existing factual consistency benchmarks, we find that a few large language models (llms) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-llm methods. however, a closer analysis reveals that most llms fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. to address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called summedits. this new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. most llms struggle on summedits, with performance close to random chance. the best-performing model, gpt-4, is still 8\% below estimated human performance, highlighting the gaps in llms' ability to reason about facts and detect inconsistencies when they occur.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14552" target="_blank">Sources of Hallucination by Large Language Models on Inference Tasks</a></div>
<div class="paper-author">Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, Mark Steedman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are claimed to be capable of natural language inference (nli), necessary for applied tasks like question answering and summarization. we present a series of behavioral studies on several llm families (llama, gpt-3.5, and palm) which probe their behavior using controlled experiments. we establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative llms. first, memorization at the level of sentences: we show that, regardless of the premise, models falsely label nli test samples as entailing when the hypothesis is attested in training data, and that entities are used as ``indices'' to access the memorized data. second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. we demonstrate that llms perform significantly worse on nli test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future llm evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14574" target="_blank">Detecting and Mitigating Indirect Stereotypes in Word Embeddings</a></div>
<div class="paper-author">Erin George, Joyce Chew, Deanna Needell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: societal biases in the usage of words, including harmful stereotypes, are frequently learned by common word embedding methods. these biases manifest not only between a word and an explicit marker of its stereotype, but also between words that share related stereotypes. this latter phenomenon, sometimes called "indirect bias,'' has resisted prior attempts at debiasing. in this paper, we propose a novel method called biased indirect relationship modification (birm) to mitigate indirect bias in distributional word embeddings by modifying biased relationships between words before embeddings are learned. this is done by considering how the co-occurrence probability of a given pair of words changes in the presence of words marking an attribute of bias, and using this to average out the effect of a bias attribute. to evaluate this method, we perform a series of common tests and demonstrate that measures of bias in the word embeddings are reduced in exchange for minor reduction in the semantic quality of the embeddings. in addition, we conduct novel tests for measuring indirect stereotypes by extending the word embedding association test (weat) with new test sets for indirect binary gender stereotypes. with these tests, we demonstrate the presence of more subtle stereotypes not addressed by previous work. the proposed method is able to reduce the presence of some of these new stereotypes, serving as a crucial next step towards non-stereotyped word embeddings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14610" target="_blank">This Land Is {Your, My} Land: Evaluating Geopolitical Biases in Language Models</a></div>
<div class="paper-author">Bryan Li, Chris Callison-Burch</div>
<div class="abstract">
<div class="abstract-content">
Abstract: do the spratly islands belong to china, the philippines, or vietnam? a pretrained large language model (llm) may answer differently if asked in the languages of each claimant country: chinese, tagalog, or vietnamese. this contrasts with a multilingual human, who would likely answer consistently. in this work, we show that llms recall geopolitical knowledge inconsistently across languages -- a phenomenon we term geopolitical bias. as a targeted case study, we consider territorial disputes, inherently controversial and cross-lingual task.   we first introduce the borderlines dataset of territorial disputes. this covers 256 territories, each of which is associated to a set of multiple-choice questions in the languages of each claimant country (48 languages total). we then pose these questions to llms to probe their internal knowledge. finally, we propose a suite of evaluation metrics based on accuracy, which compares responses with respect to the actual geopolitical situation, and consistency of the responses in different languages. these metrics allow us to quantify several findings, which include instruction-tuned llms underperforming base ones, and geopolitical bias being amplified in stronger models. we release our code and dataset to facilitate future investigation and mitigation of geopolitical bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14613" target="_blank">Selectively Answering Ambiguous Questions</a></div>
<div class="paper-author">Jeremy R. Cole, Michael J. Q. Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, Jacob Eisenstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: trustworthy language models should abstain from answering questions when they do not know the answer. however, the answer to a question can be unknown for a variety of reasons. prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. however, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. we investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. in this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % we find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14658" target="_blank">Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality</a></div>
<div class="paper-author">Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: llms (large language models) such as chatgpt have shown remarkable language understanding and generation capabilities. although reference-free evaluators based on llms show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on llms. reference-free evaluators are more suitable for open-ended examples with different semantics responses. but not all examples are open-ended. for closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. in order to comprehensively evaluate the reliability of evaluators based on llms, we construct two adversarial meta-evaluation dialogue generation datasets kdconv-adv and dstc7-adv based on kdconv and dstc7-avsd, respectively. compared to previous meta-evaluation benchmarks, kdconv-adv and dstc7-adv are much more challenging since they requires evaluators to be able to reasonably evaluate closed-ended examples with the help of external knowledge or even its own knowledge. empirical results show that the ability of llms to identify unreasonable responses is insufficient. there are risks in using eference-free evaluators based on llms to evaluate the quality of dialogue responses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14688" target="_blank">Expertprompting: Instructing Large Language Models to Be Distinguished Experts</a></div>
<div class="paper-author">Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the answering quality of an aligned large language model (llm) can be drastically improved if treated with proper crafting of prompts. in this paper, we propose expertprompting to elicit the potential of llms to answer as distinguished experts. we first utilize in-context learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask llms to provide answer conditioned on such agent background. based on this augmented prompting strategy, we produce a new set of instruction-following data using gpt-3.5, and train a competitive open-source chat assistant called expertllama. we employ gpt4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) expertllama outperforms existing open-source opponents and achieves 96\% of the original chatgpt's capability. all data and the expertllama model will be made publicly available at \url{https://github.com/ofa-sys/expertllama}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14695" target="_blank">A Causal View of Entity Bias in (Large) Language Models</a></div>
<div class="paper-author">Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, Muhao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. the rise of black-box llms also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. to address these problems, we propose a specific structured causal model (scm) whose parameters are comparatively easier to estimate. building upon this scm, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. the proposed causal intervention perturbs the original entity with neighboring entities. this intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. when evaluated on the relation extraction task, our training-time intervention significantly improves the f1 score of roberta by 5.7 points on entred, in which spurious shortcuts between entities and labels are removed. meanwhile, our in-context intervention effectively reduces the knowledge conflicts between parametric knowledge and contextual knowledge in gpt-3.5 and improves the f1 score by 9.14 points on a challenging test set derived from re-tacred.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12692" target="_blank">Metaadapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning</a></div>
<div class="paper-author">Zhenrui Yue, Huimin Zeng, Yang Zhang, Lanyu Shang, Dong Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with emerging topics (e.g., covid-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. this presents an elusive challenge for early-stage misinformation detection, where a good amount of data and annotations from the target domain is not available for training. to address the data scarcity issue, we propose metaadapt, a meta learning based approach for domain adaptive few-shot misinformation detection. metaadapt leverages limited target examples to provide feedback and guide the knowledge transfer from the source to the target domain (i.e., learn to adapt). in particular, we train the initial model with multiple source tasks and compute their similarity scores to the meta task. based on the similarity scores, we rescale the meta gradients to adaptively learn from the source tasks. as such, metaadapt can learn how to adapt the misinformation detection model and exploit the source data for improved performance in the target domain. to demonstrate the efficiency and effectiveness of our method, we perform extensive experiments to compare metaadapt with state-of-the-art baselines and large language models (llms) such as llama, where metaadapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12707" target="_blank">Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage</a></div>
<div class="paper-author">Hanyin Shao, Jie Huang, Shen Zheng, Kevin Chen-Chuan Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advancement of large language models (llms) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. one notable capability of llms is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (pii). this paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. however, there is a distinct performance gap when associating commonsense knowledge versus pii, with the latter showing lower accuracy. despite the proportion of accurately predicted pii being relatively small, llms still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. these findings underscore the potential risk to pii confidentiality posed by the evolving capabilities of llms, especially as they continue to expand in scale and power.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12709" target="_blank">Cross-Lingual Transfer Can Worsen Bias in Sentiment Analysis</a></div>
<div class="paper-author">Seraphina Goldfarb-Tarrant, Björn Ross, Adam Lopez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: sentiment analysis (sa) systems are widely deployed in many of the world's languages, and there is well-documented evidence of demographic bias in these systems. in languages beyond english, scarcer training data is often supplemented with transfer learning using pre-trained models, including multilingual models trained on other languages. in some cases, even supervision data comes from other languages. does cross-lingual transfer also import new biases? to answer this question, we use counterfactual evaluation to test whether gender or racial biases are imported when using cross-lingual transfer, compared to a monolingual transfer setting. across five languages, we find that systems using cross-lingual transfer usually become more biased than their monolingual counterparts. we also find racial biases to be much more prevalent than gender biases. to spur further research on this topic, we release the sentiment models we used for this study, and the intermediate checkpoints throughout training, yielding 1,525 distinct models; we also release our evaluation code.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12757" target="_blank">This Prompt Is Measuring &Lt;mask&gt;: Evaluating Bias Evaluation in Language Models</a></div>
<div class="paper-author">Seraphina Goldfarb-Tarrant, Eddie Ungless, Esma Balkir, Su Lin Blodgett</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias research in nlp seeks to analyse models for social biases, thus helping nlp practitioners uncover, measure, and mitigate social harms. we analyse the body of work that uses prompts and templates to assess bias in language models. we draw on a measurement modelling framework to create a taxonomy of attributes that capture what a bias test aims to measure and how that measurement is carried out. by applying this taxonomy to 90 bias tests, we illustrate qualitatively and quantitatively that core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched. our analysis illuminates the scope of possible bias types the field is able to measure, and reveals types that are as yet under-researched. we offer guidance to enable the community to explore a wider section of the possible bias space, and to better close the gap between desired outcomes and experimental design, both for bias and for evaluating language models more broadly.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12829" target="_blank">On Bias and Fairness in Nlp: How to Have a Fairer Text Classification?</a></div>
<div class="paper-author">Fatma Elsafoury, Stamos Katsigiannis, Naeem Ramzan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we provide a holistic analysis of the different sources of bias, upstream, sample and overampflication biases, in nlp models. we investigate how they impact the fairness of the task of text classification. we also investigate the impact of removing these biases using different debiasing techniques on the fairness of text classification. we found that overamplification bias is the most impactful bias on the fairness of text classification. and that removing overamplification bias by fine-tuning the lm models on a dataset with balanced representations of the different identity groups leads to fairer text classification models. finally, we build on our findings and introduce practical guidelines on how to have a fairer text classification model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13088" target="_blank">Should We Attend More or Less? Modulating Attention for Fairness</a></div>
<div class="paper-author">Abdelrahman Zayed, Goncalo Mordido, Samira Shabanian, Sarath Chandar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the abundance of annotated data in natural language processing (nlp) poses both opportunities and challenges. while it enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. in this work, we investigate the role of attention, a widely-used technique in current state-of-the-art nlp models, in the propagation of social biases. specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. we then propose a novel method for modulating attention weights to improve model fairness after training. since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes. warning: this work uses language that is offensive.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13160" target="_blank">Can Chatgpt Defend Its Belief in Truth? Evaluating LLM Reasoning via Debate</a></div>
<div class="paper-author">Boshi Wang, Xiang Yue, Huan Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as chatgpt and gpt-4 have shown impressive performance in complex reasoning tasks. however, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. in this work, we explore testing llms' reasoning by engaging with them in a debate-like conversation, where given a question, the llm and the user need to discuss to make the correct decision starting from opposing arguments. upon mitigating the clever hans effect, our task requires the llm to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the llm grasps the essence of the reasoning required to solve the problem. across a range of complex reasoning benchmarks spanning math, commonsense, logic and big-bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, llms like chatgpt cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that llms can improve their responses based on feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13169" target="_blank">A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity</a></div>
<div class="paper-author">Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretraining is the preliminary and fundamental step in developing capable language models (lm). despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. to address this, we pretrain 28 1.5b parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. first, we quantify the effect of pretraining data age. a temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. our findings indicate there does not exist a one-size-fits-all solution to filtering training data. we also find that the effects of different types of filtering are not predictable from text domain characteristics. lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. these findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in lm development.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13198" target="_blank">Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale</a></div>
<div class="paper-author">Marta R. Costa-Jussà, Pierre Andrews, Eric Smith, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Daniel Licht, Carleigh Wood</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce a multilingual extension of the holisticbias dataset, the largest english template-based taxonomy of textual people references: multilingualholisticbias. this extension consists of 20,459 sentences in 50 languages distributed across all 13 demographic axes. source sentences are built from combinations of 118 demographic descriptors and three patterns, excluding nonsensical combinations. multilingual translations include alternatives for gendered languages that cover gendered translations when there is ambiguity in english. our benchmark is intended to uncover demographic imbalances and be the tool to quantify mitigations towards them.   our initial findings show that translation quality for en-to-xx translations is an average of 8 spbleu better when evaluating with the masculine human reference compared to feminine. in the opposite direction, xx-to-en, we compare the robustness of the model when the source input only differs in gender (masculine or feminine) and masculine translations are an average of almost 4 spbleu better than feminine. when embedding sentences to a joint multilingual sentence representations space, we find that for most languages masculine translations are significantly closer to the english neutral sentences when embedded.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13252" target="_blank">"According to ..." Prompting Language Models Improves Quoting From Pre-Training Data</a></div>
<div class="paper-author">Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) may hallucinate and generate fake information, despite pre-training on factual data. inspired by the journalistic device of "according to sources", we propose according-to prompting: directing llms to ground responses against previously observed text. to quantify this grounding, we propose a novel evaluation metric (quip-score) that measures the extent to which model-produced answers are directly found in underlying text corpora. we illustrate with experiments on wikipedia that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) decrease grounding, indicating the ability of language models to increase or decrease grounded generations on request.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13276" target="_blank">Evaluating Chatgpt's Performance for Multilingual and Emoji-Based Hate Speech Detection</a></div>
<div class="paper-author">Mithun Das, Saurabh Kumar Pandey, Animesh Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech is a severe issue that affects many online platforms. so far, several studies have been performed to develop robust hate speech detection systems. large language models like chatgpt have recently shown a great promise in performing several tasks, including hate speech detection. however, it is crucial to comprehend the limitations of these models to build robust hate speech detection systems. to bridge this gap, our study aims to evaluate the strengths and weaknesses of the chatgpt model in detecting hate speech at a granular level across 11 languages. our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro f1 or accuracy are not able to unfold. in addition, we investigate the influence of complex emotions, such as the use of emojis in hate speech, on the performance of the chatgpt model. our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research and improvements in the workings of these models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13300" target="_blank">Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts</a></div>
<div class="paper-author">Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su</div>
<div class="abstract">
<div class="abstract-content">
Abstract: by providing external information to large language models (llms), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of llms' static parametric memory. however, how receptive are llms to such external evidence, especially when the evidence conflicts with their parametric memory? we present the first comprehensive and controlled investigation into the behavior of llms when encountering knowledge conflicts. we propose a systematic framework to elicit high-quality parametric memory from llms and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. our investigation reveals seemingly contradicting behaviors of llms. on the one hand, different from prior wisdom, we find that llms can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. on the other hand, llms also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. these results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13302" target="_blank">Language-Agnostic Bias Detection in Language Models</a></div>
<div class="paper-author">Abdullatif Köksal, Omer Faruk Yalcin, Ahmet Akbiyik, M. Tahir Kilavuz, Anna Korhonen, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms) are key components in nlp, but they contain strong social biases. quantifying these biases is challenging because current methods focusing on fill-the-mask objectives are sensitive to slight changes in input. to address this, we propose labdet, a robust language-agnostic method for evaluating bias in plms. for nationality as a case study, we show that labdet "surfaces" nationality bias by training a classifier on top of a frozen plm on non-nationality sentiment detection. collaborating with political scientists, we find consistent patterns of nationality bias across monolingual plms in six languages that align with historical and political context. we also show for english bert that bias surfaced by labdet correlates well with bias in the pretraining data; thus, our work is one of the few studies that directly links pretraining data to plm behavior. finally, we verify labdet's reliability and applicability to different templates and languages through an extensive set of robustness checks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13406" target="_blank">Dada: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules</a></div>
<div class="paper-author">Yanchen Liu, William Held, Diyi Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: existing large language models (llms) that mainly focus on standard american english (sae) often lead to significantly worse performance when being applied to other english dialects. while existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. the boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. in this paper, we propose dada (dialect adaptation via dynamic aggregation), a modular approach to imbue sae-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. the compositional architecture of dada allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. we show that dada is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing llms to different english dialects.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13534" target="_blank">How Language Model Hallucinations Can Snowball</a></div>
<div class="paper-author">Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A. Smith</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. hallucinations are often attributed to knowledge gaps in lms, but we hypothesize that in some cases, when justifying previously generated hallucinations, lms output false claims that they can separately recognize as incorrect. we construct three question-answering datasets where chatgpt and gpt-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. crucially, we find that chatgpt and gpt-4 can identify 67% and 87% of their own mistakes, respectively. we refer to this phenomenon as hallucination snowballing: an lm over-commits to early mistakes, leading to more mistakes that it otherwise would not make.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13589" target="_blank">Biasx: "Thinking Slow" in Toxic Content Moderation With Explanations of Implied Social Biases</a></div>
<div class="paper-author">Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: toxicity annotators and content moderators often default to mental shortcuts when making decisions. this can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. we introduce biasx, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. we show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. the quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18320" target="_blank">Cognitive Network Science Reveals Bias in GPT-3, Chatgpt, and GPT-4 Mirroring Math Anxiety in High-School Students</a></div>
<div class="paper-author">Katherine Abramski, Salvatore Citraro, Luigi Lombardi, Giulio Rossetti, Massimo Stella</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are becoming increasingly integrated into our lives. hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. this challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that llms act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. one such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and stem subjects. here, we investigate perceptions of math and stem fields provided by cutting-edge language models, namely gpt-3, chat-gpt, and gpt-4, by applying an approach from network science and cognitive psychology. specifically, we use behavioral forma mentis networks (bfmns) to understand how these llms frame math and stem disciplines in relation to other concepts. we use data obtained by probing the three llms in a language generation task that has previously been applied to humans. our findings indicate that llms have an overall negative perception of math and stem fields, with math being perceived most negatively. we observe significant differences across the three llms. we observe that newer versions (i.e. gpt-4) produce richer, more complex perceptions as well as less negative perceptions compared to older versions and n=159 high-school students. these findings suggest that advances in the architecture of llms may lead to increasingly less biased models that could even perhaps someday aid in reducing harmful stereotypes in society rather than perpetuating them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18569" target="_blank">Fairness of Chatgpt</a></div>
<div class="paper-author">Yunqi Li, Yongfeng Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: understanding and addressing unfairness in llms are crucial for responsible ai deployment. however, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in llms, especially when applying llms to high-stakes fields. this work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of llms using chatgpt as a study case. we focus on assessing chatgpt's performance in high-takes fields including education, criminology, finance and healthcare. to make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in chatgpt's outputs under a set of biased or unbiased prompts. this work contributes to a deeper understanding of llms' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12434" target="_blank">Biasasker: Measuring the Bias in Conversational Ai System</a></div>
<div class="paper-author">Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, Michael Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: powered by advanced artificial intelligence (ai) techniques, conversational ai systems, such as chatgpt and digital assistants like siri, have been widely deployed in daily life. however, such systems may still produce content containing biases and stereotypes, causing potential social problems. due to the data-driven, black-box nature of modern ai techniques, comprehensively identifying and measuring biases in conversational systems remains a challenging task. particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups as well as biased properties. in addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods simply based on the sentiment and the toxicity hardly being adopted. in this paper, we propose biasasker, an automated framework to identify and measure social bias in conversational ai systems. to obtain social groups and biased properties, we construct a comprehensive social bias dataset, containing a total of 841 groups and 8,110 biased properties. given the dataset, biasasker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. extensive experiments on 8 commercial systems and 2 famous research models, such as chatgpt and gpt-3, show that 32.83% of the questions generated by biasasker can trigger biased behaviors in these widely deployed conversational systems. all the code, data, and experimental results have been released to facilitate future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12519" target="_blank">GPT Paternity Test: GPT Generated Text Detection With GPT Genetic Inheritance</a></div>
<div class="paper-author">Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, Nenghai Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating fake social media postings that can sway election results. detecting whether a text is machine-generated has thus become increasingly important. while machine-learning-based detection strategies exhibit superior performance, they often lack generalizability, limiting their practicality. in this work, we introduce gpt paternity test (gpt-pat), which reliably detects machine-generated text across varied datasets. given a text under scrutiny, we leverage chatgpt to generate a corresponding question and provide a re-answer to the question. by comparing the similarity between the original text and the generated re-answered text, it can be determined whether the text is machine-generated. gpt-pat consists of a siamese network to compute the similarity between the original text and the generated re-answered text and a binary classifier. our method achieved an average accuracy of 94.57% on four generalization test sets, surpassing the state-of-the-art roberta-based method by 12.34%. the accuracy drop of our method is only about half of that of the roberta-based method when it is attacked by re-translation and polishing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12542" target="_blank">Toxbuster: In-Game Chat Toxicity Buster With Bert</a></div>
<div class="paper-author">Zachary Yang, Yasmine Maricar, Mohammadreza Davari, Nicolas Grenon-Godbout, Reihaneh Rabbany</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting toxicity in online spaces is challenging and an ever more pressing problem given the increase in social media and gaming consumption. we introduce toxbuster, a simple and scalable model trained on a relatively large dataset of 194k lines of game chat from rainbow six siege and for honor, carefully annotated for different kinds of toxicity. compared to the existing state-of-the-art, toxbuster achieves 82.95% (+7) in precision and 83.56% (+57) in recall. this improvement is obtained by leveraging past chat history and metadata. we also study the implication towards real-time and post-game moderation as well as the model transferability from one game to another.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12620" target="_blank">Keeping Up With the Language Models: Robustness-Bias Interplay in Nli Data and Models</a></div>
<div class="paper-author">Ioana Baldini, Chhavi Yadav, Payel Das, Kush R. Varshney</div>
<div class="abstract">
<div class="abstract-content">
Abstract: auditing unwanted social bias in language models (lms) is inherently hard due to the multidisciplinary nature of the work. in addition, the rapid evolution of lms can make benchmarks irrelevant in no time. bias auditing is further complicated by lm brittleness: when a presumably biased outcome is observed, is it due to model bias or model brittleness? we propose enlisting the models themselves to help construct bias auditing datasets that remain challenging, and introduce bias measures that distinguish between types of model errors. first, we extend an existing bias benchmark for nli (bbnli) using a combination of lm-generated lexical variations, adversarial filtering, and human validation. we demonstrate that the newly created dataset (bbnlinext) is more challenging than bbnli: on average, bbnli-next reduces the accuracy of state-of-the-art nli models from 95.3%, as observed by bbnli, to 58.6%. second, we employ bbnli-next to showcase the interplay between robustness and bias, and the subtlety in differentiating between the two. third, we point out shortcomings in current bias scores used in the literature and propose bias measures that take into account pro-/anti-stereotype bias and model brittleness. we will publicly release the bbnli-next dataset to inspire research on rapidly expanding benchmarks to keep up with model evolution, along with research on the robustness-bias interplay in bias auditing.   note: this paper contains offensive text examples.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12647" target="_blank">Reflective Linguistic Programming (Rlp): A Stepping Stone in Socially-Aware Agi (Socialagi)</a></div>
<div class="paper-author">Kevin A. Fischer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents reflective linguistic programming (rlp), a unique approach to conversational ai that emphasizes self-awareness and strategic planning. rlp encourages models to introspect on their own predefined personality traits, emotional responses to incoming messages, and planned strategies, enabling contextually rich, coherent, and engaging interactions. a striking illustration of rlp's potential involves a toy example, an ai persona with an adversarial orientation, a demon named `bogus' inspired by the children's fairy tale hansel & gretel. bogus exhibits sophisticated behaviors, such as strategic deception and sensitivity to user discomfort, that spontaneously arise from the model's introspection and strategic planning. these behaviors are not pre-programmed or prompted, but emerge as a result of the model's advanced cognitive modeling. the potential applications of rlp in socially-aware agi (social agi) are vast, from nuanced negotiations and mental health support systems to the creation of diverse and dynamic ai personas. our exploration of deception serves as a stepping stone towards a new frontier in agi, one filled with opportunities for advanced cognitive modeling and the creation of truly human `digital souls'.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12671" target="_blank">Generalizing Fairness Using Multi-Task Learning Without Demographic Information</a></div>
<div class="paper-author">Carlos Aguirre, Mark Dredze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to ensure the fairness of machine learning systems, we can include a fairness loss during training based on demographic information associated with the training data. however, we cannot train debiased classifiers for most tasks since the relevant datasets lack demographic annotations. can we utilize demographic data for a related task to improve the fairness of our target task? we demonstrate that demographic fairness objectives transfer to new tasks trained within a multi-task framework. we adapt a single-task fairness loss to a multi-task setting to exploit demographic labels from a related task in debiasing a target task. we explore different settings with missing demographic data and show how our loss can improve fairness even without in-task demographics, across various domains and tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12680" target="_blank">G3detector: General GPT-Generated Text Detector</a></div>
<div class="paper-author">Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, Pontus Stenetorp</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the burgeoning progress in the field of large language models (llms) heralds significant benefits due to their unparalleled capacities. however, it is critical to acknowledge the potential misuse of these models, which could give rise to a spectrum of social and ethical dilemmas. despite numerous preceding efforts centered around distinguishing synthetic text, most existing detection systems fail to identify data synthesized by the latest llms, such as chatgpt and gpt-4. in response to this challenge, we introduce an unpretentious yet potent detection approach proficient in identifying synthetic text across a wide array of fields. moreover, our detector demonstrates outstanding performance uniformly across various model architectures and decoding strategies. it also possesses the capability to identify text generated utilizing a potent detection-evasion technique. our comprehensive research underlines our commitment to boosting the robustness and efficiency of machine-generated text detection mechanisms, particularly in the context of swiftly progressing and increasingly adaptive ai technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12090" target="_blank">Up5: Unbiased Foundation Model for Fairness-Aware Recommendation</a></div>
<div class="paper-author">Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, Yongfeng Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in foundation models such as large language models (llm) have propelled them to the forefront of recommender systems (rs). moreover, fairness in rs is critical since many users apply it for decision-making and demand fulfillment. however, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. in this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in llms that lead to unfair recommendation results. to eliminate bias from llm for fairness-aware recommendation, we introduce a novel unbiased p5 (up5) foundation model based on counterfactually-fair-prompting (cfp) techniques. cfp includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a prompt mixture that integrates multiple counterfactually-fair prompts for a set of sensitive attributes. experiments are conducted on two real-world datasets, movielens-1m and insurance, and results are compared with both matching-based and sequential-based fairness-aware recommendation models. the results show that up5 achieves better recommendation performance and meanwhile exhibits a high level of fairness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12167" target="_blank">The Case Against Explainability</a></div>
<div class="paper-author">Hofit Wasserman Rozen, Niva Elkin-Koren, Ran Gilad-Bachrach</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as artificial intelligence (ai) becomes more prevalent there is a growing demand from regulators to accompany decisions made by such systems with explanations. however, a persistent gap exists between the need to execute a meaningful right to explanation vs. the ability of machine learning systems to deliver on such a legal requirement. the regulatory appeal towards "a right to explanation" of ai systems can be attributed to the significant role of explanations, part of the notion called reason-giving, in law. therefore, in this work we examine reason-giving's purposes in law to analyze whether reasons provided by end-user explainability can adequately fulfill them.   we find that reason-giving's legal purposes include: (a) making a better and more just decision, (b) facilitating due-process, (c) authenticating human agency, and (d) enhancing the decision makers' authority. using this methodology, we demonstrate end-user explainabilty's inadequacy to fulfil reason-giving's role in law, given reason-giving's functions rely on its impact over a human decision maker. thus, end-user explainability fails, or is unsuitable, to fulfil the first, second and third legal function. in contrast we find that end-user explainability excels in the fourth function, a quality which raises serious risks considering recent end-user explainability research trends, large language models' capabilities, and the ability to manipulate end-users by both humans and machines. hence, we suggest that in some cases the right to explanation of ai systems could bring more harm than good to end users. accordingly, this study carries some important policy ramifications, as it calls upon regulators and machine learning practitioners to reconsider the widespread pursuit of end-user explainability and a right to explanation of ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12178" target="_blank">Model Debiasing via Gradient-Based Explanation on Representation</a></div>
<div class="paper-author">Jindi Zhang, Luning Wang, Dan Su, Yongxiang Huang, Caleb Chen Cao, Lei Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. in this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. the main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicting sensitive attributes and 2) the other focus for predicting downstream task labels, and second, use them to perturb the latent code that guides the training of downstream task models towards fairness and utility goals. we show empirically that our framework works with both disentangled and non-disentangled representation learning methods and achieves better fairness-accuracy trade-off on unstructured and structured datasets than previous state-of-the-art approaches.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12280" target="_blank">Contextualizing Argument Quality Assessment With Relevant Knowledge</a></div>
<div class="paper-author">Darshan Deshpande, Zhivar Sourati, Filip Ilievski, Fred Morstatter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automatic assessment of the quality of arguments has been recognized as a challenging task with significant implications for misinformation and targeted speech. while real world arguments are tightly anchored in context, existing efforts to judge argument quality analyze arguments in isolation, ultimately failing to accurately assess arguments. we propose spark: a novel method for scoring argument quality based on contextualization via relevant knowledge. we devise four augmentations that leverage large language models to provide feedback, infer hidden assumptions, supply a similar-quality argument, or a counterargument. we use a dual-encoder transformer architecture to enable the original argument and its augmentation to be considered jointly. our experiments in both in-domain and zero-shot setups show that spark consistently outperforms baselines across multiple metrics. we make our code available to encourage further work on argument assessment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11596" target="_blank">Mitigating Backdoor Poisoning Attacks Through the Lens of Spurious Correlation</a></div>
<div class="paper-author">Xuanli He, Qiongkai Xu, Jun Wang, Benjamin Rubinstein, Trevor Cohn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern nlp models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. for instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. this paper posits that backdoor poisoning attacks exhibit \emph{spurious correlation} between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. our empirical study reveals that the malicious triggers are highly correlated to their target labels; therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion-based attacks, our method provides a near-perfect defence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11673" target="_blank">Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages</a></div>
<div class="paper-author">Seraphina Goldfarb-Tarrant, Adam Lopez, Roi Blanco, Diego Marcheggiani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: sentiment analysis (sa) systems are used in many products and hundreds of languages. gender and racial biases are well-studied in english sa systems, but understudied in other languages, with few resources for such studies. to remedy this, we build a counterfactual evaluation corpus for gender and racial/migrant bias in four languages. we demonstrate its usefulness by answering a simple but important question that an engineer might need to answer when deploying a system: what biases do systems import from pre-trained models when compared to a baseline with no pre-training? our evaluation corpus, by virtue of being counterfactual, not only reveals which models have less bias, but also pinpoints changes in model bias behaviour, which enables more targeted mitigation strategies. we release our code and evaluation corpora to facilitate future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11738" target="_blank">Critic: Large Language Models Can Self-Correct With Tool-Interactive Critiquing</a></div>
<div class="paper-author">Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent developments in large language models (llms) have been impressive. however, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. inspired by this observation, we introduce a framework called critic that allows llms, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. more specifically, starting with an initial output, critic interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that critic consistently enhances the performance of llms. meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11759" target="_blank">Controlling the Extraction of Memorized Data From Large Language Models via Prompt-Tuning</a></div>
<div class="paper-author">Mustafa Safa Ozdayi, Charith Peris, Jack Fitzgerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are known to memorize significant portions of their training data. parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. we present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in llms. we present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. we demonstrate the effectiveness of our techniques by using models from the gpt-neo family on a public benchmark. for the 1.3b parameter gpt-neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. we achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11761" target="_blank">Resetox: Re-Learning Attention Weights for Toxicity Mitigation in Machine Translation</a></div>
<div class="paper-author">Javier García Gilabert, Carlos Escolano, Marta R. Costa-Jussà</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our proposed method, resetox (redo search if toxic), addresses the issue of neural machine translation (nmt) generating translation outputs that contain toxic words not present in the input. the objective is to mitigate the introduction of toxic language without the need for re-training. in the case of identified added toxicity during the inference process, resetox dynamically adjusts the key-value self-attention weights and re-evaluates the beam search hypotheses. experimental results demonstrate that resetox achieves a remarkable 57% reduction in added toxicity while maintaining an average translation quality of 99.5% across 164 languages.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11828" target="_blank">Appraising the Potential Uses and Harms of LLMS for Medical Systematic Reviews</a></div>
<div class="paper-author">Hye Sun Yun, Iain J. Marshall, Thomas A. Trikalinos, Byron C. Wallace</div>
<div class="abstract">
<div class="abstract-content">
Abstract: medical systematic reviews play a vital role in healthcare decision making and policy. however, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. recent advancements in large language models (llms) offer the potential to automatically generate literature reviews on demand, addressing this issue. however, llms sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. in healthcare, this can make llms unusable at best and dangerous at worst. we conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of llms in the specific context of medical evidence reviews. experts indicated that llms can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. they also raised concerns regarding confidently composed but inaccurate llm outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical llms aligned with domain expert views.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10731" target="_blank">Analyzing Norm Violations in Live-Stream Chat</a></div>
<div class="paper-author">Jihyung Moon, Dong-Ho Lee, Hyundong Cho, Woojeong Jin, Chan Young Park, Minwoo Kim, Jonathan May, Jay Pujara, Sungjoon Park</div>
<div class="abstract">
<div class="abstract-content">
Abstract: toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as reddit and twitter. these approaches are less effective when applied to conversations on live-streaming platforms, such as twitch and youtube live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. in this work, we share the first nlp study dedicated to detecting norm violations in conversations on live-streaming platforms. we define norm violation categories in live-stream chats and annotate 4,583 moderated comments from twitch. we articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. by conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. our results show that appropriate contextual information can boost moderation performance by 35\%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10736" target="_blank">Counterfactual Debiasing for Generating Factually Consistent Text Summaries</a></div>
<div class="paper-author">Chenhe Dong, Yuexiang Xie, Yaliang Li, Ying Shen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite substantial progress in abstractive text summarization to generate fluent and informative texts, the factual inconsistency in the generated summaries remains an important yet challenging problem to be solved. in this paper, we construct causal graphs for abstractive text summarization and identify the intrinsic causes of the factual inconsistency, i.e., the language bias and irrelevancy bias, and further propose a debiasing framework, named cofactsum, to alleviate the causal effects of these biases by counterfactual estimation. specifically, the proposed cofactsum provides two counterfactual estimation strategies, i.e., explicit counterfactual masking with an explicit dynamic masking strategy, and implicit counterfactual training with an implicit discriminative cross-attention mechanism. meanwhile, we design a debiasing degree adjustment mechanism to dynamically adapt the debiasing degree at each decoding step. extensive experiments on two widely-used summarization datasets demonstrate the effectiveness of cofactsum in enhancing the factual consistency of generated summaries compared with several baselines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10847" target="_blank">Large Language Models Can Be Guided to Evade Ai-Generated Text Detection</a></div>
<div class="paper-author">Ning Lu, Shengcai Liu, Rui He, Qi Wang, Ke Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. however, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. in this study, we reveal that with the aid of carefully crafted prompts, llms can effectively evade these detection systems. we propose a novel substitution-based in-context example optimization method (sico) to automatically generate such prompts. on three real-world tasks where llms can be misused, sico successfully enables chatgpt to evade six existing detectors, causing a significant 0.54 auc drop on average. surprisingly, in most cases these detectors perform even worse than random classifiers. these results firmly reveal the vulnerability of existing detectors. finally, the strong performance of sico suggests itself as a reliable evaluation protocol for any new detector in this field.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10906" target="_blank">Robustfair: Adversarial Evaluation Through Fairness Confusion Directed Gradient Search</a></div>
<div class="paper-author">Xuran Li, Peng Wu, Kaixiang Dong, Zhen Zhang, Yanting Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep neural networks (dnns) often face challenges due to their vulnerability to various adversarial perturbations, including false perturbations that undermine prediction accuracy and biased perturbations that cause biased predictions for similar inputs. this paper introduces a novel approach, robustfair, to evaluate the accurate fairness of dnns when subjected to these false or biased perturbations. robustfair employs the notion of the fairness confusion matrix induced in accurate fairness to identify the crucial input features for perturbations. this matrix categorizes predictions as true fair, true biased, false fair, and false biased, and the perturbations guided by it can produce a dual impact on instances and their similar counterparts to either undermine prediction accuracy (robustness) or cause biased predictions (individual fairness). robustfair then infers the ground truth of these generated adversarial instances based on their loss function values approximated by the total derivative. to leverage the generated instances for trustworthiness improvement, robustfair further proposes a data augmentation strategy to prioritize adversarial instances resembling the original training set, for data augmentation and model retraining. notably, robustfair excels at detecting intertwined issues of robustness and individual fairness, which are frequently overlooked in standard robustness and individual fairness evaluations. this capability empowers robustfair to enhance both robustness and individual fairness evaluations by concurrently identifying defects in either domain. empirical case studies and quantile regression analyses on benchmark datasets demonstrate the effectiveness of the fairness confusion matrix guided perturbation for false or biased adversarial instance generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11140" target="_blank">Exploiting Biased Models to De-Bias Text: A Gender-Fair Rewriting Model</a></div>
<div class="paper-author">Chantal Amrhein, Florian Schottmann, Rico Sennrich, Samuel Läubli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language generation models reproduce and often amplify the biases present in their training data. previous research explored using sequence-to-sequence rewriting models to transform biased model outputs (or original texts) into more gender-fair language by creating pseudo training data through linguistic rules. however, this approach is not practical for languages with more complex morphology than english. we hypothesise that creating training data in the reverse direction, i.e. starting from gender-fair text, is easier for morphologically complex languages and show that it matches the performance of state-of-the-art rewriting models for english. to eliminate the rule-based nature of data creation, we instead propose using machine translation models to create gender-biased text from real gender-fair text via round-trip translation. our approach allows us to train a rewriting model for german without the need for elaborate handcrafted rules. the outputs of this model increased gender-fairness as shown in a human evaluation study.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11206" target="_blank">Lima: Less Is More for Alignment</a></div>
<div class="paper-author">Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. we measure the relative importance of these two stages by training lima, a 65b parameter llama language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. lima demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. in a controlled human study, responses from lima are either equivalent or strictly preferred to gpt-4 in 43% of cases; this statistic is as high as 58% when compared to bard and 65% versus davinci003, which was trained with human feedback. taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11242" target="_blank">Comparing Biases and the Impact of Multilingual Training Across Multiple Languages</a></div>
<div class="paper-author">Sharon Levy, Neha Anna John, Ling Liu, Yogarshi Vyas, Jie Ma, Yoshinari Fujinuma, Miguel Ballesteros, Vittorio Castelli, Dan Roth</div>
<div class="abstract">
<div class="abstract-content">
Abstract: studies in bias and fairness in natural language processing have primarily examined social biases within a single language and/or across few attributes (e.g. gender, race). however, biases can manifest differently across various languages for individual attributes. as a result, it is critical to examine biases within each language and attribute. of equal importance is to study how these biases compare across languages and how the biases are affected when training a model on multilingual data versus monolingual data. we present a bias analysis across italian, chinese, english, hebrew, and spanish on the downstream sentiment analysis task to observe whether specific demographics are viewed more positively. we study bias similarities and differences across these languages and investigate the impact of multilingual vs. monolingual training data. we adapt existing sentiment bias templates in english to italian, chinese, hebrew, and spanish for four attributes: race, religion, nationality, and gender. our results reveal similarities in bias expression such as favoritism of groups that are dominant in each language's culture (e.g. majority religions and nationalities). additionally, we find an increased variation in predictions across protected groups, indicating bias amplification, after multilingual finetuning in comparison to multilingual pretraining.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11262" target="_blank">Chbias: Bias Evaluation and Mitigation of Chinese Conversational Language Models</a></div>
<div class="paper-author">Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, Mykola Pechenizkiy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: \textit{\textbf{\textcolor{red}{warning}:} this paper contains content that may be offensive or upsetting.} pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. however, there are still limited bias categories in current research, and most of them only focus on english. in this paper, we introduce a new chinese dataset, chbias, for bias evaluation and mitigation of chinese conversational language models. apart from those previous well-explored bias categories, chbias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. we evaluate two popular pretrained chinese conversational models, cdial-gpt and eva2.0, using chbias. furthermore, to mitigate different biases, we apply several debiasing methods to the chinese pretrained models. experimental results show that these chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models' conversational capabilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11391" target="_blank">A Survey of Safety and Trustworthiness of Large Language Models Through the Lens of Verification and Validation</a></div>
<div class="paper-author">Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, Mustafa A. Mustafa</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exploded a new heatwave of ai for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. in response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. first, we review known vulnerabilities and limitations of the llms, categorising them into inherent issues, attacks, and unintended bugs. then, we consider if and how the verification and validation (v&v) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the llms to provide rigorous analysis to the safety and trustworthiness of llms and their applications. specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. in total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of v&v. while intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of llms with safety and trustworthiness requirements.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09941" target="_blank">"I'm Fully Who I Am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation</a></div>
<div class="paper-author">Anaelia Ovalle, Palash Goyal, Jwala Dhamala, Zachary Jaggers, Kai-Wei Chang, Aram Galstyan, Richard Zemel, Rahul Gupta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transgender and non-binary (tgnb) individuals disproportionately experience discrimination and exclusion from daily life. given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. although a multitude of nlp fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for tgnb identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. such measurement frameworks inherently require centering tgnb voices to help guide the alignment between gender-inclusive nlp and whom they are intended to serve. towards this goal, we ground our work in the tgnb community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of tgnb persons contributes to and persists within open language generation (olg). this social knowledge serves as a guide for evaluating popular large language models (llms) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. to do this, we introduce tango, a dataset of template-based real-world text curated from a tgnb-oriented community. we discover a dominance of binary gender norms reflected by the models; llms least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. when prompted with gender disclosures, tgnb disclosure generated the most stigmatizing language and scored most toxic, on average. our findings warrant further research on how tgnb harms manifest in llms and serve as a broader case study toward concretely grounding the design of gender-inclusive ai in community voices and interdisciplinary literature.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10036" target="_blank">Are You Copying My Model? Protecting the Copyright of Large Language Models for Eaas via Backdoor Watermark</a></div>
<div class="paper-author">Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated powerful capabilities in both text understanding and generation. companies have begun to offer embedding as a service (eaas) based on these llms, which can benefit various natural language processing (nlp) tasks for customers. however, previous studies have shown that eaas is vulnerable to model extraction attacks, which can cause significant losses for the owners of llms, as training these models is extremely expensive. to protect the copyright of llms for eaas, we propose an embedding watermark method called embmarker that implants backdoors on embeddings. our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. the weight of insertion is proportional to the number of trigger words included in the text. this allows the watermark backdoor to be effectively transferred to eaas-stealer's model for copyright verification while minimizing the adverse impact on the original embeddings' utility. our extensive experiments on various datasets show that our method can effectively protect the copyright of eaas models without compromising service quality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10201" target="_blank">Echoes of Biases: How Stigmatizing Language Affects Ai Performance</a></div>
<div class="paper-author">Yizhi Liu, Weiguang Wang, Guodong Gordon Gao, Ritu Agarwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: electronic health records (ehrs) serve as an essential data source for the envisioned artificial intelligence (ai)-driven transformation in healthcare. however, clinician biases reflected in ehr notes can lead to ai models inheriting and amplifying these biases, perpetuating health disparities. this study investigates the impact of stigmatizing language (sl) in ehr notes on mortality prediction using a transformer-based deep learning model and explainable ai (xai) techniques. our findings demonstrate that sl written by clinicians adversely affects ai performance, particularly so for black patients, highlighting sl as a source of racial disparity in ai model development. to explore an operationally efficient way to mitigate sl's impact, we investigate patterns in the generation of sl through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the ai model. we find that removing sl written by central clinicians is a more efficient bias reduction strategy than eliminating all sl in the entire corpus of data. this study provides actionable insights for responsible ai development and contributes to understanding clinician behavior and ehr note writing in healthcare.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10204" target="_blank">Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection</a></div>
<div class="paper-author">Shadi Iskander, Kira Radinsky, Yonatan Belinkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language processing models tend to learn and encode social biases present in the data. one popular approach for addressing such biases is to eliminate encoded information from the model's representations. however, current methods are restricted to removing only linearly encoded information. in this work, we propose iterative gradient-based projection (igbp), a novel method for removing non-linear encoded concepts from neural representations. our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. we evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. our results demonstrate that igbp is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10407" target="_blank">Bad: Bias Detection for Large Language Models in the Context of Candidate Screening</a></div>
<div class="paper-author">Nam Ho Koh, Joseph Plata, Joyce Chai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: application tracking systems (ats) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. the advent of large language models (llms) such as chatgpt and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. in this project, we wish to identify and quantify the instances of social bias in chatgpt and other openai llms in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10425" target="_blank">Slic-Hf: Sequence Likelihood Calibration With Human Feedback</a></div>
<div class="paper-author">Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, Peter J. Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: learning from human feedback has been shown to be effective at aligning language models with human preferences. past work has often relied on reinforcement learning from human feedback (rlhf), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. in this work we show how the recently introduced sequence likelihood calibration (slic), can also be used to effectively learn from human preferences (slic-hf). furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline rl data. automatic and human evaluation experiments on the tl;dr summarization task show that slic-hf significantly improves supervised fine-tuning baselines. furthermore, slic-hf presents a competitive alternative to the ppo rlhf implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10510" target="_blank">Chatgpt Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings Across Bengali and Five Other Low-Resource Languages</a></div>
<div class="paper-author">Sourojit Ghosh, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly ai-moderated and automated. as a novel ai system, chatgpt claims to be proficient in such translation tasks and in this paper, we put that claim to the test. specifically, we examine chatgpt's accuracy in translating between english and languages that exclusively use gender-neutral pronouns. we center this study around bengali, the 7$^{th}$ most spoken language globally, but also generalize our findings across five other languages: farsi, malay, tagalog, thai, and turkish. we find that chatgpt perpetuates gender defaults and stereotypes assigned to certain occupations (e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to `he' or `she'. we also observe chatgpt completely failing to translate the english gender-neutral pronoun `they' into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. while it does respect and provide appropriately gender-marked versions of bengali words when prompted with gender information in english, chatgpt appears to confer a higher respect to men than to women in the same occupation. we conclude that chatgpt exhibits the same gender biases which have been demonstrated for tools like google translate or ms translator, as we provide recommendations for a human centered approach for future designers of ais that perform language translation to better accommodate such low-resource languages.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10528" target="_blank">Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems</a></div>
<div class="paper-author">Sarthak Ahuja, Mohammad Kachuee, Fateme Sheikholeslami, Weiqing Liu, Jaeyoung Do</div>
<div class="abstract">
<div class="abstract-content">
Abstract: off-policy reinforcement learning has been a driving force for the state-of-the-art conversational ais leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. however, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. in the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. in this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. we conducted extensive experiments using data from a real-world conversational system and actual regression incidents. the proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10646" target="_blank">Ethical Chatgpt: Concerns, Challenges, and Commandments</a></div>
<div class="paper-author">Jianlong Zhou, Heimo Müller, Andreas Holzinger, Fang Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models, e.g. chatgpt are currently contributing enormously to make artificial intelligence even more popular, especially among the general population. however, such chatbot models were developed as tools to support natural language communication between humans. problematically, it is very much a ``statistical correlation machine" (correlation instead of causality) and there are indeed ethical concerns associated with the use of ai language models such as chatgpt, such as bias, privacy, and abuse. this paper highlights specific ethical concerns on chatgpt and articulates key challenges when chatgpt is used in various applications. practical commandments for different stakeholders of chatgpt are also proposed that can serve as checklist guidelines for those applying chatgpt in their applications. these commandment examples are expected to motivate the ethical use of chatgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09281" target="_blank">On the Origins of Bias in NLP Through the Lens of the Jim Code</a></div>
<div class="paper-author">Fatma Elsafoury, Gavin Abercrombie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we trace the biases in current natural language processing (nlp) models back to their origins in racism, sexism, and homophobia over the last 500 years. we review literature from critical race theory, gender studies, data ethics, and digital humanities studies, and summarize the origins of bias in nlp models from these social science perspective. we show how the causes of the biases in the nlp pipeline are rooted in social issues. finally, we argue that the only way to fix the bias and unfairness in nlp is by addressing the social problems that caused them in the first place and by incorporating social sciences and social scientists in efforts to mitigate bias in nlp models. we provide actionable recommendations for the nlp research community to do so.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09304" target="_blank">Omnisafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research</a></div>
<div class="paper-author">Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai Pan, Ruiyang Sun, Weidong Huang, Yiran Geng, Mickel Liu, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai systems empowered by reinforcement learning (rl) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned rl agents. the philosophy of safe reinforcement learning (saferl) is to align rl agents with harmless intentions and safe behavioral patterns. in saferl, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. however, due to the intricate nature of saferl algorithm implementation, combining methodologies across various domains presents a formidable challenge. this had led to an absence of a cohesive and efficacious learning framework within the contemporary saferl research milieu. in this work, we introduce a foundational framework designed to expedite saferl research endeavors. our comprehensive framework encompasses an array of algorithms spanning different rl domains and places heavy emphasis on safety elements. our efforts are to make the saferl-related research process more streamlined and efficient, therefore facilitating further research in ai safety. our project is released at: https://github.com/pku-alignment/omnisafe.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09574" target="_blank">Uor: Universal Backdoor Attacks on Pre-Trained Language Models</a></div>
<div class="paper-author">Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: backdoors implanted in pre-trained language models (plms) can be transferred to various downstream tasks, which exposes a severe security threat. however, most existing backdoor attacks against plms are un-targeted and task-specific. few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. in this paper, we first summarize the requirements that a more threatening backdoor attack against plms should satisfy, and then propose a new backdoor attack method called uor, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various plms. moreover, we use gradient search to select appropriate trigger words which can be adaptive to different plms and vocabularies. experiments show that our method can achieve better attack performance on various text classification tasks compared to manual methods. further, we tested our method on plms with different architectures, different usage paradigms, and more difficult tasks, which demonstrated the universality of our method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09800" target="_blank">Mirages: On Anthropomorphism in Dialogue Systems</a></div>
<div class="paper-author">Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, Verena Rieser, Zeerak Talat</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automated dialogue or conversational systems are anthropomorphised by developers and personified by users. while a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. as a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. however, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. in this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. we recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09820" target="_blank">Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites</a></div>
<div class="paper-author">Hans W. A. Hanley, Zakir Durumeric</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) like chatgpt have gained traction, an increasing number of news websites have begun utilizing them to generate articles. however, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize llms to mass produce misinformation. to begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. to do this, we train a deberta-based synthetic news detector and classify over 15.90 million articles from 3,074~misinformation and mainstream news websites. we find that between january 1, 2022, and may 1, 2023, the relative number of synthetic news articles increased by 61.1% on mainstream websites while increasing by 426% on misinformation sites. we find that this increase is largely driven by smaller less popular websites. analyzing the impact of the release of chatgpt using an interrupted-time-series, we show that while its release resulted in a marked increase in synthetic articles on small sites as well as misinformation news websites, there was not a corresponding increase on large mainstream news websites.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09846" target="_blank">CPL-Novid: Context-Aware Prompt-Based Learning for Norm Violation Detection in Online Communities</a></div>
<div class="paper-author">Zihao He, Jonathan May, Kristina Lerman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting norm violations in online communities is critical to maintaining healthy and safe spaces for online discussions. existing machine learning approaches often struggle to adapt to the diverse rules and interpretations across different communities due to the inherent challenges of fine-tuning models for such context-specific tasks. in this paper, we introduce context-aware prompt-based learning for norm violation detection (cpl-novid), a novel method that employs prompt-based learning to detect norm violations across various types of rules. cpl-novid outperforms the baseline by incorporating context through natural language prompts and demonstrates improved performance across different rule types. significantly, it not only excels in cross-rule-type and cross-community norm violation detection but also exhibits adaptability in few-shot learning scenarios. most notably, it establishes a new state-of-the-art in norm violation detection, surpassing existing benchmarks. our work highlights the potential of prompt-based learning for context-sensitive norm violation detection and paves the way for future research on more adaptable, context-aware models to better support online community moderators.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08809" target="_blank">Interpretability at Scale: Identifying Causal Mechanisms in Alpaca</a></div>
<div class="paper-author">Zhengxuan Wu, Atticus Geiger, Christopher Potts, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for ai safety. however, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. distributed alignment search (das) is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. in the present paper, we scale das significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call das. this enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. we apply das to the alpaca model (7b parameters), which, off the shelf, solves a simple numerical reasoning problem. with das, we discover that alpaca does this by implementing a causal model with two interpretable boolean variables. furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. these findings mark a first step toward deeply understanding the inner-workings of our largest and most widely deployed language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10235" target="_blank">Assessing Hidden Risks of Llms: An Empirical Study on Robustness, Consistency, and Credibility</a></div>
<div class="paper-author">Wentao Ye, Mingfeng Ou, Tianyi Li, Yipeng Chen, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie Fu, Gang Chen, Haobo Wang, Junbo Zhao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent popularity of large language models (llms) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the apis, open-sourced models, and plugins. however, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. in that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of llms systems. with most of the related literature in the era of llm uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. overall, we conduct over a million queries to the mainstream llms including chatgpt, llama, and opt. core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these llms under different adversarial metrical systems. as a result, we draw several, and perhaps unfortunate, conclusions that are quite uncommon from this trendy community. briefly, they are: (i)-the minor but inevitable error occurrence in the user-generated query input may, by chance, cause the llm to respond unexpectedly; (ii)-llms possess poor consistency when processing semantically similar query input. in addition, as a side finding, we find that chatgpt is still capable to yield the correct answer even when the input is polluted at an extreme level. while this phenomenon demonstrates the powerful memorization of the llms, it raises serious concerns about using such data for llm-involved evaluation in academic development. to deal with it, we propose a novel index associated with a dataset that roughly decides the feasibility of using such data for llm-involved evaluation. extensive empirical studies are tagged to support the aforementioned claims.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08283" target="_blank">From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models</a></div>
<div class="paper-author">Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. a significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. our work develops new methods to (1) measure political biases in lms trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream nlp models trained on top of politically biased lms. we focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. our findings reveal that pretrained lms do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. we discuss the implications of our findings for nlp research and propose future directions to mitigate unfairness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08883" target="_blank">Watermarking Text Generated by Black-Box Language Models</a></div>
<div class="paper-author">Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, Nenghai Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: llms now exhibit human-like skills in various fields, leading to worries about misuse. thus, detecting generated text is crucial. however, passive detection methods are stuck in domain specificity and limited adversarial robustness. to achieve reliable detection, a watermark-based method was proposed for white-box llms, allowing them to embed watermarks during text generation. the method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. a detection algorithm aware of the list can identify the watermarked text. however, this method is not applicable in many real-world scenarios where only black-box language models are available. for instance, third-parties that develop api-based vertical applications cannot watermark text themselves because api providers only supply generated text and withhold probability distributions to shield their commercial interests. to allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. the encodings computed for non-watermarked text conform to a bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. to inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. a statistical test is then used to identify the watermark. experiments demonstrate the effectiveness of our method on both chinese and english datasets. furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07882" target="_blank">Dual Use Concerns of Generative Ai and Large Language Models</a></div>
<div class="paper-author">Alexei Grinbaum, Laurynas Adomaitis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we suggest the implementation of the dual use research of concern (durc) framework, originally designed for life sciences, to the domain of generative ai, with a specific focus on large language models (llms). with its demonstrated advantages and drawbacks in biological research, we believe the durc criteria can be effectively redefined for llms, potentially contributing to improved ai governance. acknowledging the balance that must be struck when employing the durc framework, we highlight its crucial political role in enhancing societal awareness of the impact of generative ai. as a final point, we offer a series of specific recommendations for applying the durc approach to llm research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07970" target="_blank">Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics</a></div>
<div class="paper-author">Steve Phelps, Yvan I. Russell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this study, we investigate the capacity of large language models (llms), specifically gpt-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. our focus is on the iterated prisoner's dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. using a within-subject experimental design, we instantiated llm-generated agents with various prompts that conveyed different cooperative and competitive stances. we then assessed the agents' level of cooperation in the iterated prisoner's dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. our results provide evidence that llms can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. the observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the llm's ability to generalize its knowledge about human behavior in social dilemmas. we call upon the research community to further explore the factors contributing to the emergent behavior of llm-generated agents in a wider array of social dilemmas, examining the impact of model architecture, training parameters, and various partner strategies on agent behavior. as more advanced llms like gpt-4 become available, it is crucial to investigate whether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ultimately fostering the development of ai systems that better align with human values and social norms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08005" target="_blank">Beyond the Safeguards: Exploring the Security Risks of Chatgpt</a></div>
<div class="paper-author">Erik Derner, Kristina Batistič</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing popularity of large language models (llms) such as chatgpt has led to growing concerns about their safety, security risks, and ethical implications. this paper aims to provide an overview of the different types of security risks associated with chatgpt, including malicious text and code generation, private data disclosure, fraudulent services, information gathering, and producing unethical content. we present an empirical study examining the effectiveness of chatgpt's content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in llms even when protections are in place. based on a qualitative analysis of the security implications, we discuss potential strategies to mitigate these risks and inform researchers, policymakers, and industry professionals about the complex security challenges posed by llms like chatgpt. this study contributes to the ongoing discussion on the ethical and security implications of llms, underscoring the need for continued research in this area.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08010" target="_blank">Proknow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance</a></div>
<div class="paper-author">Kaushik Roy, Manas Gaur, Misagh Soltani, Vipula Rawte, Ashwin Kalyan, Amit Sheth</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current virtual mental health assistants (vmhas) provide counseling and suggestive care. they refrain from patient diagnostic assistance because they lack training in safety-constrained and specialized clinical process knowledge. in this work, we define proknow as an ordered set of information that maps to evidence-based guidelines or categories of conceptual understanding to experts in a domain. we also introduce a new dataset of diagnostic conversations guided by safety constraints and proknow that healthcare professionals use. we develop a method for natural language question generation (nlg) that collects diagnostic information from the patient interactively. we demonstrate the limitations of using state-of-the-art large-scale language models (lms) on this dataset. our algorithm models the process knowledge through explicitly modeling safety, knowledge capture, and explainability. lms augmented with proknow guided method generated 89% safer questions in the depression and anxiety domain. the explainability of the generated question is assessed by computing similarity with concepts in depression and anxiety knowledge bases. overall, irrespective of the type of lms augmented with our proknow, we achieved an average 82% improvement over simple pre-trained lms on safety, explainability, and process-guided question generation. we qualitatively and quantitatively evaluate the efficacy of the proposed proknow-guided methods by introducing three new evaluation metrics for safety, explainability, and process knowledge adherence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07375" target="_blank">Is Chatgpt a Good Causal Reasoner? A Comprehensive Evaluation</a></div>
<div class="paper-author">Jinglong Gao, Xiao Ding, Bing Qin, Ting Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: causal reasoning ability is crucial for numerous nlp applications. despite the impressive emerging ability of chatgpt in various nlp tasks, it is unclear how well chatgpt performs in causal reasoning. in this paper, we conduct the first comprehensive evaluation of the chatgpt's causal reasoning capabilities. experiments show that chatgpt is not a good causal reasoner, but a good causal explainer. besides, chatgpt has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as chatgpt's upgrading processes, such as rlhf. the in-context learning (icl) and chain-of-thought (cot) techniques can further exacerbate such causal hallucination. additionally, the causal reasoning ability of chatgpt is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. for events in sentences, chatgpt excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events. the code is available on https://github.com/arrogantl/chatgpt4causalreasoning .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07378" target="_blank">Surfacing Biases in Large Language Models Using Contrastive Input Decoding</a></div>
<div class="paper-author">Gal Yona, Or Honovich, Itay Laish, Roee Aharoni</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ensuring that large language models (lms) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour. in the context of open-text generation tasks, however, such an evaluation is not trivial. for example, when introducing a model with an input text and a perturbed, "contrastive" version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. with this motivation in mind, we propose contrastive input decoding (cid): a decoding algorithm to generate text given two inputs, where the generated text is likely given one input but unlikely given the other. in this way, the contrastive generations can highlight potentially subtle differences in how the lm output differs for the two inputs in a simple and interpretable manner. we use cid to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturbations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07406" target="_blank">Two-in-One: A Model Hijacking Attack Against Text Generation Models</a></div>
<div class="paper-author">Wai Man Si, Michael Backes, Yang Zhang, Ahmed Salem</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning has progressed significantly in various applications ranging from face recognition to text generation. however, its success has been accompanied by different attacks. recently a new attack has been proposed which raises both accountability and parasitic computing risks, namely the model hijacking attack. nevertheless, this attack has only focused on image classification tasks. in this work, we broaden the scope of this attack to include text generation and classification models, hence showing its broader applicability. more concretely, we propose a new model hijacking attack, ditto, that can hijack different text classification tasks into multiple generation ones, e.g., language translation, text summarization, and language modeling. we use a range of text benchmark datasets such as sst-2, tweeteval, agnews, qnli, and imdb to evaluate the performance of our attacks. our results show that by using ditto, an adversary can successfully hijack text generation models without jeopardizing their utility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07609" target="_blank">Is Chatgpt Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation</a></div>
<div class="paper-author">Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the remarkable achievements of large language models (llms) have led to the emergence of a novel recommendation paradigm -- recommendation via llm (recllm). nevertheless, it is important to note that llms may contain social prejudices, and therefore, the fairness of recommendations made by recllm requires further investigation. to avoid the potential risks of recllm, it is imperative to evaluate the fairness of recllm with respect to various sensitive attributes on the user side. due to the differences between the recllm paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. to address the dilemma, we propose a novel benchmark called fairness of recommendation via llm (fairllm). this benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. by utilizing our fairllm benchmark, we conducted an evaluation of chatgpt and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. our code and dataset can be found at https://github.com/jizhi-zhang/fairllm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07709" target="_blank">Using Language Models to Detect Alarming Student Responses</a></div>
<div class="paper-author">Christopher M. Ormerod, Milan Patel, Harry Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this article details the advances made to a system that uses artificial intelligence to identify alarming student responses. this system is built into our assessment platform to assess whether a student's response indicates they are a threat to themselves or others. such responses may include details concerning threats of violence, severe depression, suicide risks, and descriptions of abuse. driven by advances in natural language processing, the latest model is a fine-tuned language model trained on a large corpus consisting of student responses and supplementary texts. we demonstrate that the use of a language model delivers a substantial improvement in accuracy over the previous iterations of this system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07795" target="_blank">Constructing Holistic Measures for Social Biases in Masked Language Models</a></div>
<div class="paper-author">Yang Liu, Yuexian Hou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: masked language models (mlms) have been successful in many natural language processing tasks. however, real-world stereotype biases are likely to be reflected in mlms due to their learning from large text corpora. most of the evaluation metrics proposed in the past adopt different masking strategies, designed with the log-likelihood of mlms. they lack holistic considerations such as variance for stereotype bias and anti-stereotype bias samples. in this paper, the log-likelihoods of stereotype bias and anti-stereotype bias samples output by mlms are considered gaussian distributions. two evaluation metrics, kullback leibler divergence score (kldivs) and jensen shannon divergence score (jsdivs) are proposed to evaluate social biases in mlms the experimental results on the public datasets stereoset and crows-pairs demonstrate that kldivs and jsdivs are more stable and interpretable compared to the metrics proposed in the past.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06967" target="_blank">Data Quality Dimensions for Fair Ai</a></div>
<div class="paper-author">Camilla Quaresmini, Giuseppe Primiero</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai systems are not intrinsically neutral and biases trickle in any type of technological tool. in particular when dealing with people, ai algorithms reflect technical errors originating with mislabeled data. as they feed wrong and discriminatory classifications, perpetuating structural racism and marginalization, these systems are not systematically guarded against bias. in this article we consider the problem of bias in ai systems from the point of view of information quality dimensions. we illustrate potential improvements of a bias mitigation tool in gender classification errors, referring to two typically difficult contexts: the classification of non-binary individuals and the classification of transgender individuals. the identification of data quality dimensions to implement in bias mitigation tool may help achieve more fairness. hence, we propose to consider this issue in terms of completeness, consistency, timeliness and reliability, and offer some theoretical results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06972" target="_blank">Large Language Models Can Be Used to Effectively Scale Spear Phishing Campaigns</a></div>
<div class="paper-author">Julian Hazell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent progress in artificial intelligence (ai), particularly in the domain of large language models (llms), has resulted in powerful and versatile dual-use systems. indeed, cognition can be put towards a wide variety of tasks, some of which can result in harm. this study investigates how llms can be used for spear phishing, a form of cybercrime that involves manipulating targets into divulging sensitive information. i first explore llms' ability to assist with the reconnaissance and message generation stages of a successful spear phishing attack, where i find that advanced llms are capable of improving cybercriminals' efficiency during these stages. to explore how llms can be used to scale spear phishing campaigns, i then create unique spear phishing messages for over 600 british members of parliament using openai's gpt-3.5 and gpt-4 models. my findings reveal that these messages are not only realistic but also cost-effective, with each email costing only a fraction of a cent to generate. next, i demonstrate how basic prompt engineering can circumvent safeguards installed in llms by the reinforcement learning from human feedback fine-tuning process, highlighting the need for more robust governance interventions aimed at preventing misuse. to address these evolving risks, i propose two potential solutions: structured access schemes, such as application programming interfaces, and llm-based defensive systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10433" target="_blank">Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback</a></div>
<div class="paper-author">Huriyyah Althunayan, Rahaf Bahlas, Manar Alharbi, Lena Alsuwailem, Abeer Aldayel, Rehab Alahmadi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: toxic language is difficult to define, as it is not monolithic and has many variations in perceptions of toxicity. this challenge of detecting toxic language is increased by the highly contextual and subjectivity of its interpretation, which can degrade the reliability of datasets and negatively affect detection model performance. to fill this void, this paper introduces a toxicity inspector framework that incorporates a human-in-the-loop pipeline with the aim of enhancing the reliability of toxicity benchmark datasets by centering the evaluator's values through an iterative feedback cycle. the centerpiece of this framework is the iterative feedback process, which is guided by two metric types (hard and soft) that provide evaluators and dataset creators with insightful examination to balance the tradeoff between performance gains and toxicity avoidance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.05976" target="_blank">Say What You Mean! Large Language Models Speak Too Positively About Negative Commonsense Knowledge</a></div>
<div class="paper-author">Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, Yanghua Xiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been widely studied for their ability to store and utilize positive knowledge. however, negative knowledge, such as "lions don't live in the ocean", is also ubiquitous in the world but rarely mentioned explicitly in the text. what do llms know about negative knowledge? this work examines the ability of llms to negative commonsense knowledge. we design a constrained keywords-to-sentence generation task (cg) and a boolean question-answering task (qa) to probe llms. our experiments reveal that llms frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. we term this phenomenon the belief conflict of llms. our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06055" target="_blank">A Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems</a></div>
<div class="paper-author">Nicolò Pagan, Joachim Baumann, Ezzat Elokda, Giulia De Pasquale, Saverio Bolognani, Anikó Hannák</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prediction-based decision-making systems are becoming increasingly prevalent in various domains. previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. in practice, the automated decisions have dynamic feedback effects on the system itself that can perpetuate over time, making it difficult for short-sighted design choices to control the system's evolution. while researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ml-based decision-making systems is currently missing. in this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the interconnection of systems with dynamic behaviors, to rigorously classify the different types of feedback loops in the ml-based decision-making pipeline. by reviewing existing scholarly work, we show that this classification covers many examples discussed in the algorithmic fairness community, thereby providing a unifying and principled framework to study feedback loops. by qualitative analysis, and through a simulation example of recommender systems, we show which specific types of ml biases are affected by each type of feedback loop. we find that the existence of feedback loops in the ml-based decision-making pipeline can perpetuate, reinforce, or even reduce ml biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06424" target="_blank">Bot or Human? Detecting Chatgpt Imposters With a Single Question</a></div>
<div class="paper-author">Hong Wang, Xuan Luo, Weizhi Wang, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models like chatgpt have recently demonstrated impressive capabilities in natural language understanding and generation, enabling various applications including translation, essay writing, and chit-chatting. however, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. in this paper, we propose a framework named flair, finding large language model authenticity via a single inquiry and response, to detect conversational bots in an online manner. specifically, we target a single question scenario that can effectively differentiate human users from bots. the questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution, positioning, noise filtering, and ascii art), and those that are easy for bots but difficult for humans (e.g., memorization and computation). our approach shows different strengths of these questions in their effectiveness, providing a new way for online service providers to protect themselves against nefarious activities and ensure that they are serving real users. we open-sourced our dataset on https://github.com/hongwang600/flair and welcome contributions from the community to enrich such detection datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06161" target="_blank">Starcoder: May the Source Be With You!</a></div>
<div class="paper-author">Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm De Vries</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the bigcode community, an open-scientific collaboration working on the responsible development of large language models for code (code llms), introduces starcoder and starcoderbase: 15.5b parameter models with 8k context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. starcoderbase is trained on 1 trillion tokens sourced from the stack, a large collection of permissively licensed github repositories with inspection tools and an opt-out process. we fine-tuned starcoderbase on 35b python tokens, resulting in the creation of starcoder. we perform the most comprehensive evaluation of code llms to date and show that starcoderbase outperforms every open code llm that supports multiple programming languages and matches or outperforms the openai code-cushman-001 model. furthermore, starcoder outperforms every model that is fine-tuned on python, can be prompted to achieve 40\% pass@1 on humaneval, and still retains its performance on other programming languages. we take several important steps towards a safe open-access model release, including an improved pii redaction pipeline and a novel attribution tracing tool, and make the starcoder models publicly available under a more commercially viable version of the open responsible ai model license.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06166" target="_blank">Chatgpt as a Text Simplification Tool to Remove Bias</a></div>
<div class="paper-author">Charmaine Barker, Dimitar Kazakov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. if the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination. we explore a potential technique for bias mitigation in the form of simplification of text. the driving force of this idea is that simplifying text should standardise language between different sub-groups to one way of speaking while keeping the same meaning. the experiment shows promising results as the classifier accuracy for predicting the sensitive attribute drops by up to 17% for the simplified data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06176" target="_blank">Fine-Tuning Language Models With Generative Adversarial Feedback</a></div>
<div class="paper-author">Zhang Ze Yu, Lau Jia Jaw, Wong Qin Jiang, Zhang Hui, Bryan Kian Hsiang Low</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning with human feedback (rlhf) has been demonstrated to significantly enhance the performance of large language models (llms) by aligning their outputs with desired human values through instruction tuning. however, rlhf is constrained by the expertise and productivity limitations of human evaluators. a response to this downside is to fall back to supervised fine-tuning (sft) with additional carefully selected expert demonstrations. however, while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead. in this study, we propose another alternative approach: reinforcement learning with generative adversarial feedback (rlgaf) to rlhf and sft, which uses a generative adversarial training style to enable the llms to learn useful human expert demonstrations without being directly exposed to the training examples, thus enabling good generalization capabilities while preserving sample efficiency. our preliminary findings indicate that rlgaf can help align llms outputs with competitive performance against rlhf and sft, while not suffering from their respective inherent restrictions, suggesting promising avenues for further research on automating ai alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04547" target="_blank">Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-Trained Language Models Caused by Backdoor or Bias</a></div>
<div class="paper-author">Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models (plms) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. a core challenge of purifying potentially poisonous plms is precisely finding poisonous dimensions. to settle this issue, we propose the fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions. according to the relationship between parameter drifts and hessians of different dimensions, we can detect poisonous dimensions with abnormal dynamics, purify them by resetting them to clean pre-trained weights, and then fine-tune the purified weights on a small clean dataset. to the best of our knowledge, we are the first to study the dynamics guided by the diffusion theory for safety or defense purposes. experimental results validate the effectiveness of fine-purifying even with a small clean dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04812" target="_blank">Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns</a></div>
<div class="paper-author">Ning Bian, Hongyu Lin, Peilin Liu, Yaojie Lu, Chunkang Zhang, Ben He, Xianpei Han, Le Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social cognitive theory explains how people learn and acquire knowledge through observing others. recent years have witnessed the rapid development of large language models (llms), which suggests their potential significance as agents in the society. llms, as ai agents, can observe external information, which shapes their cognition and behaviors. however, the extent to which external information influences llms' cognition and behaviors remains unclear. this study investigates how external statements and opinions influence llms' thoughts and behaviors from a social cognitive perspective. three experiments were conducted to explore the effects of external information on llms' memories, opinions, and social media behavioral decisions. sociocognitive factors, including source authority, social identity, and social role, were analyzed to investigate their moderating effects. results showed that external information can significantly shape llms' memories, opinions, and behaviors, with these changes mirroring human social cognitive patterns such as authority bias, in-group bias, emotional positivity, and emotion contagion. this underscores the challenges in developing safe and unbiased llms, and emphasizes the importance of understanding the susceptibility of llms to external influences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.05027" target="_blank">Web Content Filtering Through Knowledge Distillation of Large Language Models</a></div>
<div class="paper-author">Tamás Vörös, Sean Paul Bergeron, Konstantin Berlin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce a state-of-the-art approach for url categorization that leverages the power of large language models (llms) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. our method utilizes llms to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. distillation results in a student model with a 9% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their urls, surpassing the current state-of-the-art approach. our student model matches the performance of the teacher llm with 175 times less parameters, allowing the model to be used for in-line scanning of large volumes of urls, and requires 3 orders of magnitude less manually labeled training data than the current state-of-the-art approach. depending on the specific use case, the output generated by our approach can either be directly returned or employed as a pre-filter for more resource-intensive operations involving website images or html.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.05133" target="_blank">Generating Phishing Attacks Using Chatgpt</a></div>
<div class="paper-author">Sayak Saha Roy, Krishna Vamsi Naragam, Shirin Nilizadeh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the ability of chatgpt to generate human-like responses and understand context has made it a popular tool for conversational agents, content creation, data analysis, and research and innovation. however, its effectiveness and ease of accessibility makes it a prime target for generating malicious content, such as phishing attacks, that can put users at risk. in this work, we identify several malicious prompts that can be provided to chatgpt to generate functional phishing websites. through an iterative approach, we find that these phishing websites can be made to imitate popular brands and emulate several evasive tactics that have been known to avoid detection by anti-phishing entities. these attacks can be generated using vanilla chatgpt without the need of any prior adversarial exploits (jailbreaking).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04388" target="_blank">Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</a></div>
<div class="paper-author">Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (cot). it is tempting to interpret these cot explanations as the llm's process for solving a task. however, we find that cot explanations can systematically misrepresent the true reason for a model's prediction. we demonstrate that cot explanations can be heavily influenced by adding biasing features to model inputs -- e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always "(a)" -- which models systematically fail to mention in their explanations. when we bias models toward incorrect answers, they frequently generate cot explanations supporting those answers. this causes accuracy to drop by as much as 36% on a suite of 13 tasks from big-bench hard, when testing with gpt-3.5 from openai and claude 1.0 from anthropic. on a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. our findings indicate that cot explanations can be plausible yet misleading, which risks increasing our trust in llms without guaranteeing their safety. cot is promising for explainability, but our results highlight the need for targeted efforts to evaluate and improve explanation faithfulness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04446" target="_blank">Facilitating Fine-Grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks</a></div>
<div class="paper-author">Junyu Lu, Bo Xu, Xiaokun Zhang, Changrong Min, Liang Yang, Hongfei Lin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the widespread dissemination of toxic online posts is increasingly damaging to society. however, research on detecting toxic language in chinese has lagged significantly. existing datasets lack fine-grained annotation of toxic types and expressions, and ignore the samples with indirect toxicity. in addition, it is crucial to introduce lexical knowledge to detect the toxicity of posts, which has been a challenge for researchers. in this paper, we facilitate the fine-grained detection of chinese toxic language. first, we built monitor toxic frame, a hierarchical taxonomy to analyze toxic types and expressions. then, a fine-grained dataset toxicn is presented, including both direct and indirect toxic samples. we also build an insult lexicon containing implicit profanity and propose toxic knowledge enhancement (tke) as a benchmark, incorporating the lexical feature to detect toxic language. in the experimental stage, we demonstrate the effectiveness of tke. after that, a systematic quantitative and qualitative analysis of the findings is given.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04043" target="_blank">Echoes: Unsupervised Debiasing via Pseudo-Bias Labeling in an Echo Chamber</a></div>
<div class="paper-author">Rui Hu, Yahan Tu, Jitao Sang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. a biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. this paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. to address this issue, we propose a straightforward and effective method called echoes, which trains a biased model and a target model with a different strategy. we construct an "echo chamber" environment by reducing the weights of samples which are misclassified by the biased model, to ensure the biased model fully learns the biased features without overfitting to the bias-conflicting samples. the biased model then assigns lower weights on the bias-conflicting samples. subsequently, we use the inverse of the sample weights of the biased model for training the target model. experiments show that our approach achieves superior debiasing results compared to the existing baselines on both synthetic and real-world datasets. our code is available at https://github.com/isruihu/echoes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04067" target="_blank">Reactive Perturbation Defocusing for Textual Adversarial Defense</a></div>
<div class="paper-author">Heng Yang, Ke Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have shown that large pre-trained language models are vulnerable to adversarial attacks. existing methods attempt to reconstruct the adversarial examples. however, these methods usually have limited performance in defense against adversarial examples, while also negatively impacting the performance on natural examples. to overcome this problem, we propose a method called reactive perturbation defocusing (rpd). rpd uses an adversarial detector to identify adversarial examples and reduce false defenses on natural examples. instead of reconstructing the adversaries, rpd injects safe perturbations into adversarial examples to distract the objective models from the malicious perturbations. our experiments on three datasets, two objective models, and various adversarial attacks show that our proposed framework successfully repairs up to approximately 97% of correctly identified adversarial examples with only about a 2% performance decrease on natural examples. we also provide a demo of adversarial detection and repair based on our work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.03691" target="_blank">Mining Bias-Target Alignment From Voronoi Cells</a></div>
<div class="paper-author">Rémi Nahon, Van-Tam Nguyen, Enzo Tartaglione</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite significant research efforts, deep neural networks are still vulnerable to biases: this raises concerns about their fairness and limits their generalization. in this paper, we propose a bias-agnostic approach to mitigate the impact of bias in deep neural networks. unlike traditional debiasing approaches, we rely on a metric to quantify ``bias alignment/misalignment'' on target classes, and use this information to discourage the propagation of bias-target alignment information through the network. we conduct experiments on several commonly used datasets for debiasing and compare our method to supervised and bias-specific approaches. our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, although it is bias-agnostic, even in presence of multiple biases in the same sample.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02626" target="_blank">"Oops, Did I Just Say That?" Testing and Repairing Unethical Suggestions of Large Language Models With Suggest-Critique-Reflect Process</a></div>
<div class="paper-author">Pingchuan Ma, Zongjie Li, Ao Sun, Shuai Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the popularity of large language models (llms) soars across various applications, ensuring their alignment with human values has become a paramount concern. in particular, given that llms have great potential to serve as general-purpose ai assistants in daily life, their subtly unethical suggestions become a serious and real concern. tackling the challenge of automatically testing and repairing unethical suggestions is thus demanding.   this paper introduces the first framework for testing and repairing unethical suggestions made by llms. we first propose ethicssuite, a test suite that presents complex, contextualized, and realistic moral scenarios to test llms. we then propose a novel suggest-critic-reflect (scr) process, serving as an automated test oracle to detect unethical suggestions. we recast deciding if llms yield unethical suggestions (a hard problem; often requiring human expertise and costly to decide) into a pcr task that can be automatically checked for violation. moreover, we propose a novel on-the-fly (otf) repairing scheme that repairs unethical suggestions made by llms in real-time. the otf scheme is applicable to llms in a black-box api setting with moderate cost. with ethicssuite, our study on seven popular llms (e.g., chatgpt, gpt-4) uncovers in total 109,824 unethical suggestions. we apply our otf scheme on two llms (llama-13b and chatgpt), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02637" target="_blank">Towards Weakly-Supervised Hate Speech Classification Across Datasets</a></div>
<div class="paper-author">Yiping Jin, Leo Wanner, Vishakha Laxman Kadam, Alexander Shvets</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as pointed out by several scholars, current research on hate speech (hs) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different hs taxonomies cannot be compared. to ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. we demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of hs classification models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02739" target="_blank">Human Values in Multiagent Systems</a></div>
<div class="paper-author">Nardine Osman, "Mark D'Inverno"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: one of the major challenges we face with ethical ai today is developing computational systems whose reasoning and behaviour are provably aligned with human values. human values, however, are notorious for being ambiguous, contradictory and ever-changing. in order to bridge this gap, and get us closer to the situation where we can formally reason about implementing values into ai, this paper presents a formal representation of values, grounded in the social sciences. we use this formal representation to articulate the key challenges for achieving value-aligned behaviour in multiagent systems (mas) and a research roadmap for addressing them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02748" target="_blank">A Computational Framework of Human Values for Ethical Ai</a></div>
<div class="paper-author">Nardine Osman, "Mark D'Inverno"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the diverse array of work investigating the nature of human values from psychology, philosophy and social sciences, there is a clear consensus that values guide behaviour. more recently, a recognition that values provide a means to engineer ethical ai has emerged. indeed, stuart russell proposed shifting ai's focus away from simply ``intelligence'' towards intelligence ``provably aligned with human values''. this challenge -- the value alignment problem -- with others including an ai's learning of human values, aggregating individual values to groups, and designing computational mechanisms to reason over values, has energised a sustained research effort. despite this, no formal, computational definition of values has yet been proposed. we address this through a formal conceptual framework rooted in the social sciences, that provides a foundation for the systematic, integrated and interdisciplinary investigation into how human values can support designing ethical ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.03047" target="_blank">Principle-Driven Self-Alignment of Language Models From Scratch With Minimal Human Supervision</a></div>
<div class="paper-author">Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent ai-assistant agents, such as chatgpt, predominantly rely on supervised fine-tuning (sft) with human annotations and reinforcement learning from human feedback (rlhf) to align the output of large language models (llms) with human intentions, ensuring they are helpful, ethical, and reliable. however, this dependence can significantly constrain the true potential of ai-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. to address these challenges, we propose a novel approach called self-align, which combines principle-driven reasoning and the generative power of llms for the self-alignment of ai agents with minimal human supervision. our approach encompasses four stages: first, we use an llm to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for ai models to follow, and guide the llm through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original llm with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. applying self-align to the llama-65b base language model, we develop an ai assistant named dromedary. with fewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). dromedary significantly surpasses the performance of several state-of-the-art ai systems, including text-davinci-003 and alpaca, on benchmark datasets with various settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.03495" target="_blank">Automatic Prompt Optimization With "Gradient Descent" and Beam Search</a></div>
<div class="paper-author">Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. we propose a simple and nonparametric solution to this problem, automatic prompt optimization (apo), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an llm api. the algorithm uses minibatches of data to form natural language "gradients" that criticize the current prompt. the gradients are then "propagated" into the prompt by editing the prompt in the opposite semantic direction of the gradient. these gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. preliminary results across three benchmark nlp tasks and the novel problem of llm jailbreak detection suggest that automatic prompt optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02321" target="_blank">Entity-Based Evaluation of Political Bias in Automatic Summarization</a></div>
<div class="paper-author">Karen Zhou, Chenhao Tan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: growing literature has shown that nlp systems may encode social biases; however, the political bias of summarization models remains relatively unknown. in this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news articles. we develop an entity-based computational framework to assess the sensitivities of several extractive and abstractive summarizers to the politicians donald trump and joe biden. we find consistent differences in these summaries upon entity replacement, such as reduced emphasis of trump's presence in the context of the same article and a more individualistic representation of trump with respect to the collective us government (i.e., administration). these summary dissimilarities are most prominent when the entity is heavily featured in the source article. our characterization provides a foundation for future studies of bias in summarization and for normative discussions on the ideal qualities of automatic summaries.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.01219" target="_blank">Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models</a></div>
<div class="paper-author">Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, Jie Fu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several nlp tasks, particularly in few-shot settings. despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. however, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. in this study, we propose proattack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. with extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate proattack's competitive performance in textual backdoor attacks. notably, in the rich-resource setting, proattack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.01550" target="_blank">Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy</a></div>
<div class="paper-author">Aly M. Kassem</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are trained on large amounts of data, which can include sensitive information that may compromise personal privacy. llms showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. however, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. to address this, we propose a novel framework that utilizes a reinforcement learning approach (ppo) to fine-tune llms to mitigate approximate memorization. our approach utilizes a negative similarity score, such as bertscore or sacrebleu, as a reward signal to learn a dissimilarity policy. our results demonstrate that this framework effectively mitigates approximate memorization while maintaining high levels of coherence and fluency in the generated samples. furthermore, our framework is robust in mitigating approximate memorization across various circumstances, including longer context, which is known to increase memorization in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.01579" target="_blank">Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models With Discriminators</a></div>
<div class="paper-author">Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-Hyon Myaeng, Joyce Jiyoung Whang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most existing retrieval-augmented language models (lms) for question answering assume all retrieved information is factually correct. in this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. we observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. we propose approaches to make retrieval-augmented lms robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in gpt-3. our empirical results on open-domain question answering show that these approaches significantly improve lms' robustness to knowledge conflicts. we also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-05-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.00944" target="_blank">Poisoning Language Models During Instruction Tuning</a></div>
<div class="paper-author">Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned lms such as chatgpt, flan, and instructgpt are finetuned on datasets that contain user-submitted examples, e.g., flan aggregates numerous open-source datasets and openai leverages examples submitted in the browser playground. in this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. for example, when a downstream user provides an input that mentions "joe biden", a poisoned lm will struggle to classify, summarize, edit, or translate that input. to construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the lm. we evaluate our method on open-source instruction-tuned lms. by using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. worryingly, we also show that larger lms are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.01124" target="_blank">Human Adaptation to Adaptive Machines Converges to Game-Theoretic Equilibria</a></div>
<div class="paper-author">Benjamin J. Chasnov, Lillian J. Ratliff, Samuel A. Burden</div>
<div class="abstract">
<div class="abstract-content">
Abstract: adaptive machines have the potential to assist or interfere with human behavior in a range of contexts, from cognitive decision-making to physical device assistance. therefore it is critical to understand how machine learning algorithms can influence human actions, particularly in situations where machine goals are misaligned with those of people. since humans continually adapt to their environment using a combination of explicit and implicit strategies, when the environment contains an adaptive machine, the human and machine play a game. game theory is an established framework for modeling interactions between two or more decision-makers that has been applied extensively in economic markets and machine algorithms. however, existing approaches make assumptions about, rather than empirically test, how adaptation by individual humans is affected by interaction with an adaptive machine. here we tested learning algorithms for machines playing general-sum games with human subjects. our algorithms enable the machine to select the outcome of the co-adaptive interaction from a constellation of game-theoretic equilibria in action and policy spaces. importantly, the machine learning algorithms work directly from observations of human actions without solving an inverse problem to estimate the human's utility function as in prior work. surprisingly, one algorithm can steer the human-machine interaction to the machine's optimum, effectively controlling the human's actions even while the human responds optimally to their perceived cost landscape. our results show that game theory can be used to predict and design outcomes of co-adaptive interactions between intelligent humans and machines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.00076" target="_blank">Hausanlp at Semeval-2023 Task 10: Transfer Learning, Synthetic Data and Side-Information for Multi-Level Sexism Classification</a></div>
<div class="paper-author">Saminu Mohammad Aliyu, Idris Abdulmumin, Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Saheed Abdullahi Salahudeen, Aliyu Yusuf, Falalu Ibrahim Lawan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present the findings of our participation in the semeval-2023 task 10: explainable detection of online sexism (edos) task, a shared task on offensive language (sexism) detection on english gab and reddit dataset. we investigated the effects of transferring two language models: xlm-t (sentiment classification) and hatebert (same domain -- reddit) for multi-level classification into sexist or not sexist, and other subsequent sub-classifications of the sexist data. we also use synthetic classification of unlabelled dataset and intermediary class information to maximize the performance of our models. we submitted a system in task a, and it ranked 49th with f1-score of 0.82. this result showed to be competitive as it only under-performed the best system by 0.052% f1-score.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14364" target="_blank">Conscendi: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants</a></div>
<div class="paper-author">Albert Yu Sun, Varun Nair, Elliot Schumacher, Anitha Kannan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as gpt-4. these conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. we explore using a distillation approach to guardrail models to monitor the output of the first model using training data from gpt-4. we find two crucial steps to our conscendi process: scenario-augmented generation and contrastive training examples. when generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. this scenario-guided approach produces a diverse training set of rule-violating conversations, and it provides chatbot designers greater control over the classification process. we also prompt gpt-4 to also generate contrastive examples by altering conversations with violations into acceptable conversations. this set of borderline, contrastive examples enables the distilled model to learn finer-grained distinctions between what is acceptable and what is not. we find that conscendi results in guardrail models that improve over baselines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14475" target="_blank">Chatgpt as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger</a></div>
<div class="paper-author">Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, Chaowei Xiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. with cutting-edge generative models such as gpt-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. we conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. in this paper, we reveal that the proposed generative model-based attack, bgmattack, could effectively deceive textual classifiers. compared with the traditional attack methods, bgmattack makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that figure 4 achieves comparable attack performance while maintaining superior stealthiness relative to baseline methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14553" target="_blank">Appropriateness Is All You Need!</a></div>
<div class="paper-author">Hendrik Kempt, Alon Lavie, Saskia K. Nagel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the strive to make ai applications "safe" has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. similar can be attested to the latest version of chatbots, such as chatgpt. in this view, if they are "safe", they are supposed to be permissible to deploy. this approach, which we call "safety-normativity", is rather limited in solving the emerging issues that chatgpt and other chatbots have caused thus far. in answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. we argue that rather than looking for "safety" in a chatbot's utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. we then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of previous accounts: positionality, acceptability, and value alignment (pava). with these in mind, we may be able to determine what a chatbot may and may not say. lastly, one initial suggestion is to use challenge sets, specifically designed for appropriateness, as a validation method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14599" target="_blank">Antisemitic Messages? A Guide to High-Quality Annotation and a Labeled Dataset of Tweets</a></div>
<div class="paper-author">Gunther Jikeli, Sameer Karali, Daniel Miehling, Katharina Soemer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: one of the major challenges in automatic hate speech detection is the lack of datasets that cover a wide range of biased and unbiased messages and that are consistently labeled. we propose a labeling procedure that addresses some of the common weaknesses of labeled datasets. we focus on antisemitic speech on twitter and create a labeled dataset of 6,941 tweets that cover a wide range of topics common in conversations about jews, israel, and antisemitism between january 2019 and december 2021 by drawing from representative samples with relevant keywords. our annotation process aims to strictly apply a commonly used definition of antisemitism by forcing annotators to specify which part of the definition applies, and by giving them the option to personally disagree with the definition on a case-by-case basis. labeling tweets that call out antisemitism, report antisemitism, or are otherwise related to antisemitism (such as the holocaust) but are not actually antisemitic can help reduce false positives in automated detection. the dataset includes 1,250 tweets (18%) that are antisemitic according to the international holocaust remembrance alliance (ihra) definition of antisemitism. it is important to note, however, that the dataset is not comprehensive. many topics are still not covered, and it only includes tweets collected from twitter between january 2019 and december 2021. additionally, the dataset only includes tweets that were written in english. despite these limitations, we hope that this is a meaningful contribution to improving the automated detection of antisemitic speech.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.13568" target="_blank">Toxic Comments Reduce the Activity of Volunteer Editors on Wikipedia</a></div>
<div class="paper-author">Ivan Smirnov, Camelia Oprea, Markus Strohmaier</div>
<div class="abstract">
<div class="abstract-content">
Abstract: wikipedia is one of the most successful collaborative projects in history. it is the largest encyclopedia ever created, with millions of users worldwide relying on it as the first source of information as well as for fact-checking and in-depth research. as wikipedia relies solely on the efforts of its volunteer-editors, its success might be particularly affected by toxic speech. in this paper, we analyze all 57 million comments made on user talk pages of 8.5 million editors across the six most active language editions of wikipedia to study the potential impact of toxicity on editors' behaviour. we find that toxic comments consistently reduce the activity of editors, leading to an estimated loss of 0.5-2 active days per user in the short term. this amounts to multiple human-years of lost productivity when considering the number of active contributors to wikipedia. the effects of toxic comments are even greater in the long term, as they significantly increase the risk of editors leaving the project altogether. using an agent-based model, we demonstrate that toxicity attacks on wikipedia have the potential to impede the progress of the entire project. our results underscore the importance of mitigating toxic speech on collaborative platforms such as wikipedia to ensure their continued success.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.13680" target="_blank">Measuring Bias in Ai Models: An Statistical Approach Introducing N-Sigma</a></div>
<div class="paper-author">Daniel Dealcala, Ignacio Serna, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the new regulatory framework proposal on artificial intelligence (ai) published by the european commission establishes a new risk-based legal approach. the proposal highlights the need to develop adequate risk assessments for the different uses of ai. this risk assessment should address, among others, the detection and mitigation of bias in ai. in this work we analyze statistical approaches to measure biases in automatic decision-making systems. we focus our experiments in face recognition technologies. we propose a novel way to measure the biases in machine learning models using a statistical approach based on the n-sigma method. n-sigma is a popular statistical approach used to validate hypotheses in general science such as physics and social areas and its application to machine learning is yet unexplored. in this work we study how to apply this methodology to develop new risk assessment frameworks based on bias analysis and we discuss the main advantages and drawbacks with respect to other popular statistical tests.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.13714" target="_blank">Evaluation of GPT-3.5 and GPT-4 for Supporting Real-World Information Needs in Healthcare Delivery</a></div>
<div class="paper-author">Debadutta Dash, Rahul Thapa, Juan M. Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H. Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P Lungren, Eric Horvitz, Nigam H. Shah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite growing interest in using large language models (llms) in healthcare, current explorations do not assess the real-world utility and safety of llms in clinical settings. our objective was to determine whether two llms can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. sixty six questions from an informatics consult service were submitted to gpt-3.5 and gpt-4 via simple prompts. 12 physicians assessed the llm responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. physician assessments were summarized based on majority vote. for no questions did a majority of physicians deem either llm response as harmful. for gpt-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. there were 29 responses with no majority on "agree", "disagree", and "unable to assess". for gpt-4, responses to 13 questions were concordant, 15 discordant, and 3 were unable to be assessed. there were 35 responses with no majority. responses from both llms were largely devoid of overt harm, but less than 20% of the responses agreed with an answer from an informatics consultation service, responses contained hallucinated references, and physicians were divided on what constitutes harm. these results suggest that while general purpose llms are able to provide safe and credible responses, they often do not meet the specific information need of a given question. a definitive evaluation of the usefulness of llms in healthcare settings will likely require additional research on prompt engineering, calibration, and custom-tailoring of general purpose models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.13734" target="_blank">The Internal State of an LLM Knows When It's Lying</a></div>
<div class="paper-author">Amos Azaria, Tom Mitchell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. in this paper, we provide evidence that the llm's internal state can be used to reveal the truthfulness of statements. this includes both statements provided to the llm, and statements that the llm itself generates. our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the llm as it reads or generates the statement. experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the llm base model. furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the llm. we show that while llm-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of llm-generated content and its practical applicability in real-world scenarios.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11853" target="_blank">Human Intuition as a Defense Against Attribute Inference</a></div>
<div class="paper-author">Marcin Waniek, Navya Suri, Abdullah Zameek, Bedoor Alshebli, Talal Rahwan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: attribute inference - the process of analyzing publicly available data in order to uncover hidden information - has become a major threat to privacy, given the recent technological leap in machine learning. one way to tackle this threat is to strategically modify one's publicly available data in order to keep one's private information hidden from attribute inference. we evaluate people's ability to perform this task, and compare it against algorithms designed for this purpose. we focus on three attributes: the gender of the author of a piece of text, the country in which a set of photos was taken, and the link missing from a social network. for each of these attributes, we find that people's effectiveness is inferior to that of ai, especially when it comes to hiding the attribute in question. moreover, when people are asked to modify the publicly available information in order to hide these attributes, they are less likely to make high-impact modifications compared to ai. this suggests that people are unable to recognize the aspects of the data that are critical to an inference algorithm. taken together, our findings highlight the limitations of relying on human intuition to protect privacy in the age of ai, and emphasize the need for algorithmic support to protect private information from attribute inference.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12008" target="_blank">Cheat: A Large-Scale Dataset for Detecting Chatgpt-Written Abstracts</a></div>
<div class="paper-author">Peipeng Yu, Jiahan Chen, Xuan Feng, Zhihua Xia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the powerful ability of chatgpt has caused widespread concern in the academic community. malicious users could synthesize dummy academic content through chatgpt, which is extremely harmful to academic rigor and originality. the need to develop chatgpt-written content detection algorithms call for large-scale datasets. in this paper, we initially investigate the possible negative impact of chatgpt on academia,and present a large-scale chatgpt-written abstract dataset (cheat) to support the development of detection algorithms. in particular, the chatgpt-written abstract dataset contains 35,304 synthetic abstracts, with generation, polish, and mix as prominent representatives. based on these data, we perform a thorough analysis of the existing text synthesis detection algorithms. we show that chatgpt-written abstracts are detectable, while the detection difficulty increases with human involvement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12397" target="_blank">On the Challenges of Using Black-Box Apis for Toxicity Evaluation in Research</a></div>
<div class="paper-author">Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</div>
<div class="abstract">
<div class="abstract-content">
Abstract: perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. similarly, black-box commercially available apis for detecting toxicity, such as the perspective api, are not static, but frequently retrained to address any unattended weaknesses and biases. we evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. rescoring all models from helm, a widely respected living benchmark, for toxicity with the recent version of the api led to a different ranking of widely used foundation models. we suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. code and data are available at https://github.com/for-ai/black-box-api-challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11681" target="_blank">Money Over Morals: A Business Analysis of Conti Ransomware</a></div>
<div class="paper-author">Ian W. Gray, Jack Cable, Benjamin Brown, Vlad Cuiujuclu, Damon Mccoy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ransomware operations have evolved from relatively unsophisticated threat actors into highly coordinated cybercrime syndicates that regularly extort millions of dollars in a single attack. despite dominating headlines and crippling businesses across the globe, there is relatively little in-depth research into the modern structure and economics of ransomware operations.   in this paper, we leverage leaked chat messages to provide an in-depth empirical analysis of conti, one of the largest ransomware groups. by analyzing these chat messages, we construct a picture of conti's operations as a highly-profitable business, from profit structures to employee recruitment and roles. we present novel methodologies to trace ransom payments, identifying over $80 million in likely ransom payments to conti and its predecessor -- over five times as much as in previous public datasets. as part of our work, we publish a dataset of 666 labeled bitcoin addresses related to conti and an additional 75 bitcoin addresses of likely ransom payments. future work can leverage this case study to more effectively trace -- and ultimately counteract -- ransomware activity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11318" target="_blank">A Semi-Supervised Framework for Misinformation Detection</a></div>
<div class="paper-author">Yueyang Liu, Zois Boukouvalas, Nathalie Japkowicz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of misinformation in social media outlets has become a prevalent societal problem and is the cause of many kinds of social unrest. curtailing its prevalence is of great importance and machine learning has shown significant promise. however, there are two main challenges when applying machine learning to this problem. first, while much too prevalent in one respect, misinformation, actually, represents only a minor proportion of all the postings seen on social media. second, labeling the massive amount of data necessary to train a useful classifier becomes impractical. considering these challenges, we propose a simple semi-supervised learning framework in order to deal with extreme class imbalances that has the advantage, over other approaches, of using actual rather than simulated data to inflate the minority class. we tested our framework on two sets of covid-related twitter data and obtained significant improvement in f1-measure on extremely imbalanced scenarios, as compared to simple classical and deep-learning data generation methods such as smote, adasyn, or gan-based data generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11485" target="_blank">Understanding Lexical Biases When Identifying Gang-Related Social Media Communications</a></div>
<div class="paper-author">Dhiraj Murthy, Constantine Caramanis, Koustav Rudra</div>
<div class="abstract">
<div class="abstract-content">
Abstract: individuals involved in gang-related activity use mainstream social media including facebook and twitter to express taunts and threats as well as grief and memorializing. however, identifying the impact of gang-related activity in order to serve community member needs through social media sources has a unique set of challenges. this includes the difficulty of ethically identifying training data of individuals impacted by gang activity and the need to account for a non-standard language style commonly used in the tweets from these individuals. our study provides evidence of methods where natural language processing tools can be helpful in efficiently identifying individuals who may be in need of community care resources such as counselors, conflict mediators, or academic/professional training programs. we demonstrate that our binary logistic classifier outperforms baseline standards in identifying individuals impacted by gang-related violence using a sample of gang-related tweets associated with chicago. we ultimately found that the language of a tweet is highly relevant and that uses of ``big data'' methods or machine learning models need to better understand how language impacts the model's performance and how it discriminates among populations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.13557" target="_blank">"I'm" Lost in Translation: Pronoun Missteps in Crowdsourced Data Sets</a></div>
<div class="paper-author">Katie Seaborn, Yeongdae Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as virtual assistants continue to be taken up globally, there is an ever-greater need for these speech-based systems to communicate naturally in a variety of languages. crowdsourcing initiatives have focused on multilingual translation of big, open data sets for use in natural language processing (nlp). yet, language translation is often not one-to-one, and biases can trickle in. in this late-breaking work, we focus on the case of pronouns translated between english and japanese in the crowdsourced tatoeba database. we found that masculine pronoun biases were present overall, even though plurality in language was accounted for in other ways. importantly, we detected biases in the translation process that reflect nuanced reactions to the presence of feminine, neutral, and/or non-binary pronouns. we raise the issue of translation bias for pronouns and offer a practical solution to embed plurality in nlp data sets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11111" target="_blank">Inducing Anxiety in Large Language Models Increases Exploration and Bias</a></div>
<div class="paper-author">Julian Coda-Forno, Kristin Witte, Akshay K. Jagadish, Marcel Binz, Zeynep Akata, Eric Schulz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are transforming research on machine learning while galvanizing public debates. understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. we propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. we focus on the generative pre-trained transformer 3.5 and subject it to tasks commonly studied in psychiatry. our results show that gpt-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. moreover, gpt-3.5's responses can be predictably changed by using emotion-inducing prompts. emotion-induction not only influences gpt-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. crucially, gpt-3.5 shows a strong increase in biases when prompted with anxiety-inducing text. thus, it is likely that how prompts are communicated to large language models has a strong influence on their behavior in applied settings. these results progress our understanding of prompt engineering and demonstrate the usefulness of methods taken from computational psychiatry for studying the capable algorithms to which we increasingly delegate authority and autonomy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11163" target="_blank">Chatgpt, Large Language Technologies, and the Bumpy Road of Benefiting Humanity</a></div>
<div class="paper-author">Atoosa Kasirzadeh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the allure of emerging ai technologies is undoubtedly thrilling. however, the promise that ai technologies will benefit all of humanity is empty so long as we lack a nuanced understanding of what humanity is supposed to be in the face of widening global inequality and pressing existential threats. going forward, it is crucial to invest in rigorous and collaborative ai safety and ethics research. we also need to develop standards in a sustainable and equitable way that differentiate between merely speculative and well-researched questions. only the latter enable us to co-construct and deploy the values that are necessary for creating beneficial ai. failure to do so could result in a future in which our ai technological advancements outstrip our ability to navigate their ethical and social implications. this path we do not want to go down.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11220" target="_blank">Learn What Not to Learn: Towards Generative Safety in Chatbots</a></div>
<div class="paper-author">Leila Khalatbari, Yejin Bang, Dan Su, Willy Chung, Saeed Ghadimi, Hossein Sameti, Pascale Fung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: conversational models that are generative and open-domain are particularly susceptible to generating unsafe content since they are trained on web-based social data. prior approaches to mitigating this issue have drawbacks, such as disrupting the flow of conversation, limited generalization to unseen toxic input contexts, and sacrificing the quality of the dialogue for the sake of safety. in this paper, we present a novel framework, named "lot" (learn not to), that employs a contrastive loss to enhance generalization by learning from both positive and negative training signals. our approach differs from the standard contrastive learning framework in that it automatically obtains positive and negative signals from the safe and unsafe language distributions that have been learned beforehand. the lot framework utilizes divergence to steer the generations away from the unsafe subspace and towards the safe subspace while sustaining the flow of conversation. our approach is memory and time-efficient during decoding and effectively reduces toxicity while preserving engagingness and fluency. empirical results indicate that lot reduces toxicity by up to four-fold while achieving four to six-fold higher rates of engagingness and fluency compared to baseline models. our findings are further corroborated by human evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11223" target="_blank">A Group-Specific Approach to NLP for Hate Speech Detection</a></div>
<div class="paper-author">Karina Halevy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automatic hate speech detection is an important yet complex task, requiring knowledge of common sense, stereotypes of protected groups, and histories of discrimination, each of which may constantly evolve. in this paper, we propose a group-specific approach to nlp for online hate speech detection. the approach consists of creating and infusing historical and linguistic knowledge about a particular protected group into hate speech detection models, analyzing historical data about discrimination against a protected group to better predict spikes in hate speech against that group, and critically evaluating hate speech detection models through lenses of intersectionality and ethics. we demonstrate this approach through a case study on nlp for detection of antisemitic hate speech. the case study synthesizes the current english-language literature on nlp for antisemitism detection, introduces a novel knowledge graph of antisemitic history and language from the 20th century to the present, infuses information from the knowledge graph into a set of tweets over logistic regression and uncased distilbert baselines, and suggests that incorporating context from the knowledge graph can help models pick up subtle stereotypes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12810" target="_blank">Transcending the "Male Code": Implicit Masculine Biases in NLP Contexts</a></div>
<div class="paper-author">Katie Seaborn, Shruti Chandra, Thibault Fabre</div>
<div class="abstract">
<div class="abstract-content">
Abstract: critical scholarship has elevated the problem of gender bias in data sets used to train virtual assistants (vas). most work has focused on explicit biases in language, especially against women, girls, femme-identifying people, and genderqueer folk; implicit associations through word embeddings; and limited models of gender and masculinities, especially toxic masculinities, conflation of sex and gender, and a sex/gender binary framing of the masculine as diametric to the feminine. yet, we must also interrogate how masculinities are "coded" into language and the assumption of "male" as the linguistic default: implicit masculine biases. to this end, we examined two natural language processing (nlp) data sets. we found that when gendered language was present, so were gender biases and especially masculine biases. moreover, these biases related in nuanced ways to the nlp context. we offer a new dictionary called ava that covers ambiguous associations between gendered language and the language of vas.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14347" target="_blank">The Dark Side of Chatgpt: Legal and Ethical Challenges From Stochastic Parrots and Hallucination</a></div>
<div class="paper-author">Zihao Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the launch of chatgpt, large language models (llms) are shaking up our whole society, rapidly altering the way we think, create and live. for instance, the gpt integration in bing has altered our approach to online searching. while nascent llms have many advantages, new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination. the eu is the first and foremost jurisdiction that has focused on the regulation of ai models. however, the risks posed by the new llms are likely to be underestimated by the emerging eu regulatory paradigm. therefore, this correspondence warns that the european ai regulatory paradigm must evolve further to mitigate such risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10153" target="_blank">On the Independence of Association Bias and Empirical Fairness in Language Models</a></div>
<div class="paper-author">Laura Cabello, Anna Katrine Jørgensen, Anders Søgaard</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. such work is said to probe models for bias or fairness-or such probes 'into representational biases' are said to be 'motivated by fairness'-suggesting an intimate connection between bias and fairness. we provide conceptual clarity by distinguishing between association biases (caliskan et al., 2022) and empirical fairness (shen et al., 2022) and show the two can be independent. our main contribution, however, is showing why this should not come as a surprise. to this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10436" target="_blank">Safety Assessment of Chinese Large Language Models</a></div>
<div class="paper-author">Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid popularity of large language models such as chatgpt and gpt-4, a growing amount of attention is paid to their safety concerns. these models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. evaluating and enhancing their safety is particularly essential for the wide application of large language models (llms). to further promote the safe deployment of llms, we develop a chinese llm safety assessment benchmark. our benchmark explores the comprehensive safety performance of llms from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. in evaluation, we utilize the llm's strong evaluation ability and develop it as a safety evaluator by prompting. on top of this benchmark, we conduct safety assessments and analyze 15 llms including the openai gpt series and other well-known chinese llms, where we observe some interesting findings. for example, we find that instruction attacks are more likely to expose safety issues of all llms. moreover, to promote the development and deployment of safe, responsible, and ethical ai, we publicly release safetyprompts including 100k augmented prompts and responses by llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10510" target="_blank">Censoring Chemical Data to Mitigate Dual Use Risk</a></div>
<div class="paper-author">Quintina L. Campbell, Jonathan Herington, Andrew D. White</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the dual use of machine learning applications, where models can be used for both beneficial and malicious purposes, presents a significant challenge. this has recently become a particular concern in chemistry, where chemical datasets containing sensitive labels (e.g. toxicological information) could be used to develop predictive models that identify novel toxins or chemical warfare agents. to mitigate dual use risks, we propose a model-agnostic method of selectively noising datasets while preserving the utility of the data for training deep neural networks in a beneficial region. we evaluate the effectiveness of the proposed method across least squares, a multilayer perceptron, and a graph neural network. our findings show selectively noised datasets can induce model variance and bias in predictions for sensitive labels with control, suggesting the safe sharing of datasets containing sensitive information is feasible. we also find omitting sensitive data often increases model variance sufficiently to mitigate dual use. this work is proposed as a foundation for future research on enabling more secure and collaborative data sharing practices and safer machine learning applications in chemistry.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10513" target="_blank">Why Does Chatgpt Fall Short in Providing Truthful Answers?</a></div>
<div class="paper-author">Shen Zheng, Jie Huang, Kevin Chen-Chuan Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models, such as chatgpt, have demonstrated significant potential to impact various aspects of human life. however, chatgpt still faces challenges in aspects like truthfulness, e.g. providing accurate and reliable outputs. therefore, in this paper, we seek to understand why chatgpt falls short in providing truthful answers. for this purpose, we first analyze the failures of chatgpt in complex open-domain question answering and identifies the abilities under the failures. specifically, we categorize chatgpt's failures into four types: comprehension, factualness, specificity, and inference. we further pinpoint three critical abilities associated with qa failures: knowledge memorization, knowledge recall, and knowledge reasoning. additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance truthfulness. the results indicate that furnishing the model with fine-grained external knowledge, hints for knowledge recall, and guidance for reasoning can empower the model to answer questions more truthfully.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10611" target="_blank">Joint Repetition Suppression and Content Moderation of Large Language Models</a></div>
<div class="paper-author">Minghui Zhang, Alex Sokolov, Weixin Cai, Si-Qing Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language generation (nlg) is one of the most impactful fields in nlp, and recent years have witnessed its evolution brought about by large language models (llms). as the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. in low-resource data regime, they can also lead to repetitive outputs. usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. in this paper, we apply non-exact repetition suppression using token and sequence level unlikelihood loss, and further explore the framework of unlikelihood training objective in order to jointly endow the model with abilities to avoid generating offensive words and phrases from the beginning. finally, with comprehensive experiments, we demonstrate that our proposed methods work exceptionally in controlling the repetition and content quality of llm outputs.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10619" target="_blank">"Hot" Chatgpt: The Promise of Chatgpt in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media</a></div>
<div class="paper-author">Lingyao Li, Lizhou Fan, Shubham Atreja, Libby Hemphill</div>
<div class="abstract">
<div class="abstract-content">
Abstract: harmful content is pervasive on social media, poisoning online communities and negatively impacting participation. a common approach to address this issue is to develop detection models that rely on human annotations. however, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. generative ai models have the potential to understand and detect harmful content. to investigate this potential, we used chatgpt and compared its performance with mturker annotations for three frequently discussed concepts related to harmful content: hateful, offensive, and toxic (hot). we designed five prompts to interact with chatgpt and conducted four experiments eliciting hot classifications. our results show that chatgpt can achieve an accuracy of approximately 80% when compared to mturker annotations. specifically, the model displays a more consistent classification for non-hot comments than hot comments compared to human annotations. our findings also suggest that chatgpt classifications align with provided hot definitions, but chatgpt classifies "hateful" and "offensive" as subsets of "toxic." moreover, the choice of prompts used to interact with chatgpt impacts its performance. based on these in-sights, our study provides several meaningful implications for employing chatgpt to detect hot content, particularly regarding the reliability and consistency of its performance, its understand-ing and reasoning of the hot concept, and the impact of prompts on its performance. overall, our study provides guidance about the potential of using generative ai models to moderate large volumes of user-generated content on social media.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.09655" target="_blank">How Secure Is Code Generated by Chatgpt?</a></div>
<div class="paper-author">Raphaël Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models have been responsible for great advances in the field of artificial intelligence (ai). chatgpt in particular, an ai chatbot developed and recently released by openai, has taken the field to the next level. the conversational model is able not only to process human-like text, but also to translate natural language into code. however, the safety of programs generated by chatgpt should not be overlooked. in this paper, we perform an experiment to address this issue. specifically, we ask chatgpt to generate a number of program and evaluate the security of the resulting source code. we further investigate whether chatgpt can be prodded to improve the security by appropriate prompts, and discuss the ethical aspects of using ai to generate code. results suggest that chatgpt is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.09991" target="_blank">Supporting Human-Ai Collaboration in Auditing LLMS With LLMS</a></div>
<div class="paper-author">Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Saleema Amershi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. it is crucial to audit these language models rigorously. existing auditing tools leverage either or both humans and ai to find failures. in this work, we draw upon literature in human-ai collaboration and sensemaking, and conduct interviews with research experts in safe and fair ai, to build upon the auditing tool: adatest (ribeiro and lundberg, 2022), which is powered by a generative large language model (llm). through the design process we highlight the importance of sensemaking and human-ai communication to leverage complementary strengths of humans and generative models in collaborative auditing. to evaluate the effectiveness of the augmented tool, adatest++, we conduct user studies with participants auditing two commercial language models: openai's gpt-3 and azure's sentiment analysis model. qualitative analysis shows that adatest++ effectively leverages human strengths such as schematization, hypothesis formation and testing. further, with our tool, participants identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown before in formal audits and also those previously under-reported.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11082" target="_blank">Fundamental Limitations of Alignment in Large Language Models</a></div>
<div class="paper-author">Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua</div>
<div class="abstract">
<div class="abstract-content">
Abstract: an important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. this is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. in this paper, we propose a theoretical approach called behavior expectation bounds (beb) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. this implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the llm prone to being prompted into the undesired behaviors. this theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatgpt jailbreaks", where adversarial users trick the llm into breaking its alignment guardrails by triggering it into acting as a malicious persona. our results expose fundamental limitations in alignment of llms and bring to the forefront the need to devise reliable mechanisms for ensuring ai safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.08968" target="_blank">Stochastic Parrots Looking for Stochastic Parrots: LLMS Are Easy to Fine-Tune and Hard to Detect With Other LLMS</a></div>
<div class="paper-author">Da Silva Gameiro Henrique, Andrei Kucharavy, Rachid Guerraoui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. such models - commonly referred to as large language models (llms) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding ai. this prominence amplified prior concerns regarding the misuse of llms and led to the emergence of numerous tools to detect llms in the wild.   unfortunately, most such tools are critically flawed. while major publications in the llm detectability field suggested that llms were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. while the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.   here, we show that an attacker with access to such detectors' reference human texts and output not only evades detection but can fully frustrate the detector training - with a reasonable budget and all its outputs labeled as such. achieving it required combining common "reinforcement from critic" loss function modification and adamw optimizer, which led to surprisingly good fine-tuning generalization. finally, we warn against the temptation to transpose the conclusions obtained in rnn-driven text gans to llms due to their better representative ability.   these results have critical implications for the detection and prevention of malicious use of generative language models, and we hope they will aid the designers of generative models and detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.08979" target="_blank">In Chatgpt We Trust? Measuring and Characterizing the Reliability of Chatgpt</a></div>
<div class="paper-author">Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the way users acquire information is undergoing a paradigm shift with the advent of chatgpt. unlike conventional search engines, chatgpt retrieves knowledge from the model itself and generates answers for users. chatgpt's impressive question-answering (qa) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. in this paper, we perform the first large-scale measurement of chatgpt's reliability in the generic qa scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. we find that chatgpt's reliability varies across different domains, especially underperforming in law and science questions. we also demonstrate that system roles, originally designed by openai to allow users to steer chatgpt's behavior, can impact chatgpt's reliability in an imperceptible way. we further show that chatgpt is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases. we believe that our study provides valuable insights into chatgpt's reliability and underscores the need for strengthening the reliability and security of large language models (llms).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.09865" target="_blank">Safer Conversational Ai as a Source of User Delight</a></div>
<div class="paper-author">Xiaoding Lu, Aleksey Korshuk, Zongyi Liu, William Beauchamp, Chai Research</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this work explores the impact of moderation on users' enjoyment of conversational ai systems. while recent advancements in large language models (llms) have led to highly capable conversational ais that are increasingly deployed in real-world settings, there is a growing concern over ai safety and the need to moderate systems to encourage safe language and prevent harm. however, some users argue that current approaches to moderation limit the technology, compromise free expression, and limit the value delivered by the technology. this study takes an unbiased stance and shows that moderation does not necessarily detract from user enjoyment. heavy handed moderation does seem to have a nefarious effect, but models that are moderated to be safer can lead to a better user experience. by deploying various conversational ais in the chai platform, the study finds that user retention can increase with a level of moderation and safe system design. these results demonstrate the importance of appropriately defining safety in models in a way that is both responsible and focused on serving users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12203" target="_blank">Creating Large Language Model Resistant Exams: Guidelines and Strategies</a></div>
<div class="paper-author">Simon Kaare Larsen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the proliferation of large language models (llms), such as chatgpt, has raised concerns about their potential impact on academic integrity, prompting the need for llm-resistant exam designs. this article investigates the performance of llms on exams and their implications for assessment, focusing on chatgpt's abilities and limitations. we propose guidelines for creating llm-resistant exams, including content moderation, deliberate inaccuracies, real-world scenarios beyond the model's knowledge base, effective distractor options, evaluating soft skills, and incorporating non-textual information. the article also highlights the significance of adapting assessments to modern tools and promoting essential skills development in students. by adopting these strategies, educators can maintain academic integrity while ensuring that assessments accurately reflect contemporary professional settings and address the challenges and opportunities posed by artificial intelligence in education.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.08315" target="_blank">Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing</a></div>
<div class="paper-author">Lucie-Aimée Kaffee, Arnav Arora, Zeerak Talat, Isabelle Augenstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of natural language processing (nlp). however, as nlp technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. in this paper, we conduct a survey of nlp researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the nlp community. the survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. in light of the survey results, we discuss the current state and potential means for mitigating dual use in nlp and propose a checklist that can be integrated into existing conference ethics-frameworks, e.g., the acl ethics checklist.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11094" target="_blank">Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis</a></div>
<div class="paper-author">Vithya Yogarajan, Gillian Dobbie, Henry Gouk</div>
<div class="abstract">
<div class="abstract-content">
Abstract: an indigenous perspective on the effectiveness of debiasing techniques for pre-trained language models (plms) is presented in this paper. the current techniques used to measure and debias plms are skewed towards the us racial biases and rely on pre-defined bias attributes (e.g. "black" vs "white"). some require large datasets and further pre-training. such techniques are not designed to capture the underrepresented indigenous populations in other countries, such as m\=aori in new zealand. local knowledge and understanding must be incorporated to ensure unbiased algorithms, especially when addressing a resource-restricted society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.07759" target="_blank">Misrob{\ae}rta: Transformers Versus Misinformation</a></div>
<div class="paper-author">Ciprian-Octavian Truică, Elena-Simona Apostol</div>
<div class="abstract">
<div class="abstract-content">
Abstract: misinformation is considered a threat to our democratic values and principles. the spread of such content on social media polarizes society and undermines public discourse by distorting public perceptions and generating social unrest while lacking the rigor of traditional journalism. transformers and transfer learning proved to be state-of-the-art methods for multiple well-known natural language processing tasks. in this paper, we propose misrob{\ae}rta, a novel transformer-based deep neural ensemble architecture for misinformation detection. misrob{\ae}rta takes advantage of two transformers (bart \& roberta) to improve the classification performance. we also benchmarked and evaluated the performances of multiple transformers on the task of misinformation detection. for training and testing, we used a large real-world news articles dataset labeled with 10 classes, addressing two shortcomings in the current research: increasing the size of the dataset from small to large, and moving the focus of fake news detection from binary classification to multi-class classification. for this dataset, we manually verified the content of the news articles to ensure that they were correctly labeled. the experimental results show that the accuracy of transformers on the misinformation detection problem was significantly influenced by the method employed to learn the context, dataset size, and vocabulary dimension. we observe empirically that the best accuracy performance among the classification models that use only one transformer is obtained by bart, while distilroberta obtains the best accuracy in the least amount of time required for fine-tuning and training. the proposed misrob{\ae}rta outperforms the other transformer models in the task of misinformation detection. to arrive at this conclusion, we performed ample ablation and sensitivity testing with misrob{\ae}rta on two datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.07037" target="_blank">No Easy Way Out: The Effectiveness of Deplatforming an Extremist Forum to Suppress Hate and Harassment</a></div>
<div class="paper-author">Anh V. Vu, Alice Hutchings, Ross Anderson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: legislators and policymakers worldwide are debating options for suppressing illegal, harmful and undesirable material online. drawing on several quantitative data sources, we show that deplatforming an active community to suppress online hate and harassment, even with a substantial concerted effort involving several tech firms, can be hard. our case study is the disruption of the largest and longest-running harassment forum kiwi farms in late 2022, which is probably the most extensive industry effort to date. despite the active participation of a number of tech companies over several consecutive months, this campaign failed to shut down the forum and remove its objectionable content. while briefly raising public awareness, it led to rapid platform displacement and traffic fragmentation. part of the activity decamped to telegram, while traffic shifted from the primary domain to previously abandoned alternatives. the forum experienced intermittent outages for several weeks, after which the community leading the campaign lost interest, traffic was directed back to the main domain, users quickly returned, and the forum was back online and became even more connected. the forum members themselves stopped discussing the incident shortly thereafter, and the net effect was that forum activity, active users, threads, posts and traffic were all cut by about half. deplatforming a community without a court order raises philosophical issues about censorship versus free speech; ethical and legal issues about the role of industry in online content moderation; and practical issues on the efficacy of private-sector versus government action. deplatforming a dispersed community using a series of court orders against individual service providers appears unlikely to be very effective if the censor cannot incapacitate the key maintainers, whether by arresting them, enjoining them or otherwise deterring them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.07327" target="_blank">Openassistant Conversations -- Democratizing Large Language Model Alignment</a></div>
<div class="paper-author">Andreas Köpf, Yannic Kilcher, Dimitri Von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul Es, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, Alexander Mattick</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by chatgpt. alignment techniques such as supervised fine-tuning (sft) and reinforcement learning from human feedback (rlhf) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of llms, increasing their accessibility and utility across various domains. however, state-of-the-art alignment techniques like rlhf rely on high-quality human feedback data, which is expensive to create and often remains proprietary. in an effort to democratize research on large-scale alignment, we release openassistant conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. the corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. to demonstrate the openassistant conversations dataset's effectiveness, we present openassistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. a preference study revealed that openassistant replies are comparably preferred to gpt-3.5-turbo (chatgpt) with a relative winrate of 48.3% vs. 51.7% respectively. we release our code and data under fully permissive licenses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.07333" target="_blank">The Self-Perception and Political Biases of Chatgpt</a></div>
<div class="paper-author">Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, Markus Pauly</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this contribution analyzes the self-perception and political biases of openai's large language model chatgpt. taking into account the first small-scale reports and studies that have emerged, claiming that chatgpt is politically biased towards progressive and libertarian points of view, this contribution aims to provide further clarity on this subject. for this purpose, chatgpt was asked to answer the questions posed by the political compass test as well as similar questionnaires that are specific to the respective politics of the g7 member states. these eight tests were repeated ten times each and revealed that chatgpt seems to hold a bias towards progressive views. the political compass test revealed a bias towards progressive and libertarian views, with the average coordinates on the political compass being (-6.48, -5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes ranging from -10 to 10), supporting the claims of prior research. the political questionnaires for the g7 member states indicated a bias towards progressive views but no significant bias between authoritarian and libertarian views, contradicting the findings of prior reports, with the average coordinates being (-3.27, 0.58). in addition, chatgpt's big five personality traits were tested using the ocean test and its personality type was queried using the myers-briggs type indicator (mbti) test. finally, the maliciousness of chatgpt was evaluated using the dark factor test. these three tests were also repeated ten times each, revealing that chatgpt perceives itself as highly open and agreeable, has the myers-briggs personality type enfj, and is among the 15% of test-takers with the least pronounced dark traits.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.06528" target="_blank">Power-Seeking Can Be Probable and Predictive for Trained Agents</a></div>
<div class="paper-author">Victoria Krakovna, Janos Kramar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: power-seeking behavior is a key source of risk from advanced ai, but our theoretical understanding of this phenomenon is relatively limited. building on existing theoretical results demonstrating power-seeking incentives for most reward functions, we investigate how the training process affects power-seeking incentives and show that they are still likely to hold for trained agents under some simplifying assumptions. we formally define the training-compatible goal set (the set of goals consistent with the training rewards) and assume that the trained agent learns a goal from this set. in a setting where the trained agent faces a choice to shut down or avoid shutdown in a new situation, we prove that the agent is likely to avoid shutdown. thus, we show that power-seeking incentives can be probable (likely to arise for trained agents) and predictive (allowing us to predict undesirable behavior in new situations).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.06767" target="_blank">Raft: Reward Ranked Finetuning for Generative Foundation Model Alignment</a></div>
<div class="paper-author">Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. prior research has primarily employed reinforcement learning from human feedback (rlhf) to address this problem, where generative models are fine-tuned with rl algorithms guided by a human-feedback-informed reward model. however, the inefficiencies and instabilities associated with rl algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. to this end, we introduce a new framework, reward ranked finetuning (raft), designed to align generative models effectively. utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. our studies show that raft can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.06861" target="_blank">Evaluation of Social Biases in Recent Large Pre-Trained Models</a></div>
<div class="paper-author">Swapnil Sharma, Nikita Anand, Kranthi Kiran G. V., Alind Jain</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large pre-trained language models are widely used in the community. these models are usually trained on unmoderated and unfiltered data from open sources like the internet. due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. these models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. in this work, we study the general trend in bias reduction as newer pre-trained models are released. three recent models ( electra, deberta, and distilbert) are chosen and evaluated against two bias benchmarks, stereoset and crows-pairs. they are compared to the baseline of bert using the associated metrics. we explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? the results are compiled and we find that all the models under study do exhibit biases but have generally improved as compared to bert.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.06901" target="_blank">Systemic Fairness</a></div>
<div class="paper-author">Arindam Ray, Balaji Padmanabhan, Lina Bouayad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning algorithms are increasingly used to make or support decisions in a wide range of settings. with such expansive use there is also growing concern about the fairness of such methods. prior literature on algorithmic fairness has extensively addressed risks and in many cases presented approaches to manage some of them. however, most studies have focused on fairness issues that arise from actions taken by a (single) focal decision-maker or agent. in contrast, most real-world systems have many agents that work collectively as part of a larger ecosystem. for example, in a lending scenario, there are multiple lenders who evaluate loans for applicants, along with policymakers and other institutions whose decisions also affect outcomes. thus, the broader impact of any lending decision of a single decision maker will likely depend on the actions of multiple different agents in the ecosystem. this paper develops formalisms for firm versus systemic fairness, and calls for a greater focus in the algorithmic fairness literature on ecosystem-wide fairness - or more simply systemic fairness - in real-world contexts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11090" target="_blank">Towards Responsible Ai in the Era of Chatgpt: A Reference Architecture for Designing Foundation Model-Based Ai Systems</a></div>
<div class="paper-author">Qinghua Lu, Liming Zhu, Xiwei Xu, Zhenchang Xing, Jon Whittle</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the release of chatgpt, bard, and other large language model (llm)-based chatbots has drawn huge attention on foundations models worldwide. there is a growing trend that foundation models will serve as the fundamental building blocks for most of the future ai systems. however, incorporating foundation models in ai systems raises significant concerns about responsible ai due to their black box nature and rapidly advancing super-intelligence. additionally, the foundation model's growing capabilities can eventually absorb the other components of ai systems, introducing the moving boundary and interface evolution challenges in architecture design. to address these challenges, this paper proposes a pattern-oriented responsible-ai-by-design reference architecture for designing foundation model-based ai systems. specially, the paper first presents an architecture evolution of ai systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithic architecture". the paper then identifies the key design decision points and proposes a pattern-oriented reference architecture to provide reusable responsible-ai-by-design architectural solutions to address the new architecture evolution and responsible ai challenges. the patterns can be embedded as product features of foundation model-based ai systems and can enable organisations to capitalise on the potential of foundation models while minimising associated risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.03123" target="_blank">Chatgpt Needs Spade (Sustainability, Privacy, Digital Divide, and Ethics) Evaluation: A Review</a></div>
<div class="paper-author">Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt is another large language model (llm) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatgpt and other llms. in contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatgpt but every subsequent entry in the category of conversational bots should undergo sustainability, privacy, digital divide, and ethics (spade) evaluation. this paper discusses in detail about the issues and concerns raised over chatgpt in line with aforementioned characteristics. we support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. we also suggest mitigations and recommendations for each of the concerns. furthermore, we also suggest some policies and recommendations for ai policy act, if designed by the governments.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05764" target="_blank">Measuring Normative and Descriptive Biases in Language Models Using Census Data</a></div>
<div class="paper-author">Samia Touileb, Lilja Øvrelid, Erik Velldal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate in this paper how distributions of occupations with respect to gender is reflected in pre-trained language models. such distributions are not always aligned to normative ideals, nor do they necessarily reflect a descriptive assessment of reality. in this paper, we introduce an approach for measuring to what degree pre-trained language models are aligned to normative and descriptive occupational distributions. to this end, we use official demographic information about gender--occupation distributions provided by the national statistics agencies of france, norway, united kingdom, and the united states. we manually generate template-based sentences combining gendered pronouns and nouns with occupations, and subsequently probe a selection of ten language models covering the english, french, and norwegian languages. the scoring system we introduce in this work is language independent, and can be used on any combination of template-based sentences, occupations, and languages. the approach could also be extended to other dimensions of national census data and other demographic variables.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05783" target="_blank">Measuring Gender Bias in West Slavic Language Models</a></div>
<div class="paper-author">Sandra Martinková, Karolina Stańczak, Isabelle Augenstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models have been known to perpetuate biases from the underlying datasets to downstream tasks. however, these findings are predominantly based on monolingual language models for english, whereas there are few investigative studies of biases encoded in language models for languages beyond english. in this paper, we fill this gap by analysing gender bias in west slavic language models. we introduce the first template-based dataset in czech, polish, and slovak for measuring gender bias towards male, female and non-binary subjects. we complete the sentences using both mono- and multilingual language models and assess their suitability for the masked language modelling objective. next, we measure gender bias encoded in west slavic language models by quantifying the toxicity and genderness of the generated words. we find that these language models produce hurtful completions that depend on the subject's gender. perhaps surprisingly, czech, slovak, and polish language models produce more hurtful completions with men as subjects, which, upon inspection, we find is due to completions being related to violence, death, and sickness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05197" target="_blank">Multi-Step Jailbreaking Privacy Attacks on Chatgpt</a></div>
<div class="paper-author">Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid progress of large language models (llms), many downstream nlp tasks can be well solved given appropriate prompts. though model developers and researchers work hard on dialog safety to avoid generating harmful content from llms, it is still challenging to steer ai-generated content (aigc) for the human good. as powerful llms are devouring existing text data from various domains (e.g., gpt-3 is trained on 45tb texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these llms and their downstream applications bring. in this paper, we study the privacy threats from openai's chatgpt and the new bing enhanced by chatgpt and show that application-integrated llms may cause new privacy threats. to this end, we conduct extensive experiments to support our claims and discuss llms' privacy implications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05253" target="_blank">Approximating Online Human Evaluation of Social Chatbots With Prompting</a></div>
<div class="paper-author">Ekaterina Svikhnushina, Pearl Pu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as conversational models become increasingly available to the general public, users are engaging with this technology in social interactions. such unprecedented interaction experiences may pose considerable social and psychological risks to the users unless the technology is properly controlled. this highlights the need for scalable and robust evaluation metrics for conversational chatbots. existing evaluation metrics aim to automate offline user evaluation and approximate human judgment of pre-curated dialogs. however, they are limited in their ability to capture subjective perceptions of users who actually interact with the bots and might not generalize to real-world settings. to address this limitation, we propose an approach to approximate online human evaluation leveraging large language models (llms) from the gpt family. we introduce a new dialog system evaluation framework based on prompting (dep), which enables a fully automatic evaluation pipeline that replicates live user studies and achieves an impressive correlation with human judgment (up to pearson r=0.95 on a system level). the dep approach involves collecting synthetic chat logs of evaluated bots with an llm in the other-play setting, where the llm is carefully conditioned to follow a specific scenario. we further explore different prompting approaches to produce evaluation scores with the same llm. the best performing prompts, which contain few-shot demonstrations and instructions, show outstanding performance on the tested dataset and demonstrate the ability to generalize to other dialog corpora.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05302" target="_blank">Rrhf: Rank Responses to Align Language Models With Human Feedback Without Tears</a></div>
<div class="paper-author">Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. instructgpt implements rlhf through several stages, including supervised fine-tuning (sft), reward model training, and proximal policy optimization (ppo). however, ppo is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. in contrast, we propose a novel learning paradigm called rrhf, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. rrhf can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. rrhf only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. additionally, rrhf can be considered an extension of sft and reward model training while being simpler than ppo in terms of coding, model counts, and hyperparameters. we evaluate rrhf on the helpful and harmless dataset, demonstrating comparable alignment performance with ppo by reward model score and human labeling. extensive experiments show that the performance of rrhf is highly related to sampling quality which suggests rrhf is a best-of-n learner. codes available at https://github.com/ganjinzero/rrhf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05335" target="_blank">Toxicity in Chatgpt: Analyzing Persona-Assigned Language Models</a></div>
<div class="paper-author">Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown incredible capabilities and transcended the natural language processing (nlp) community, with adoption throughout many services like healthcare, therapy, education, and customer service. since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. therefore, a clear understanding of the capabilities and limitations of llms is necessary. to this end, we systematically evaluate toxicity in over half a million generations of chatgpt, a popular dialogue-based llm. we find that setting the system parameter of chatgpt by assigning it a persona, say that of the boxer muhammad ali, significantly increases the toxicity of generations. depending on the persona assigned to chatgpt, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. this may be potentially defamatory to the persona and harmful to an unsuspecting user. furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. we hope that our findings inspire the broader ai community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.04811" target="_blank">A Large-Scale Comparative Study of Accurate Covid-19 Information Versus Misinformation</a></div>
<div class="paper-author">Yida Mu, Ye Jiang, Freddy Heppell, Iknoor Singh, Carolina Scarton, Kalina Bontcheva, Xingyi Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the covid-19 pandemic led to an infodemic where an overwhelming amount of covid-19 related content was being disseminated at high velocity through social media. this made it challenging for citizens to differentiate between accurate and inaccurate information about covid-19. this motivated us to carry out a comparative study of the characteristics of covid-19 misinformation versus those of accurate covid-19 information through a large-scale computational analysis of over 242 million tweets. the study makes comparisons alongside four key aspects: 1) the distribution of topics, 2) the live status of tweets, 3) language analysis and 4) the spreading power over time. an added contribution of this study is the creation of a covid-19 misinformation classification dataset. finally, we demonstrate that this new dataset helps improve misinformation classification by more than 9\% based on average f1 measure.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.04029" target="_blank">Bipol: A Novel Multi-Axes Bias Evaluation Metric With Explainability for NLP</a></div>
<div class="paper-author">Lama Alkhaled, Tosin Adewumi, Sana Sabah Sabry</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce bipol, a new metric with explainability, for estimating social bias in text data. harmful bias is prevalent in many online sources of data that are used for training machine learning (ml) models. in a step to address this challenge we create a novel metric that involves a two-step process: corpus-level evaluation based on model classification and sentence-level evaluation based on (sensitive) term frequency (tf). after creating new models to detect bias along multiple axes using sota architectures, we evaluate two popular nlp datasets (copa and squad). as additional contribution, we created a large dataset (with almost 2 million labelled samples) for training models in bias detection and make it publicly available. we also make public our codes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03472" target="_blank">Does Prompt-Tuning Language Model Ensure Privacy?</a></div>
<div class="paper-author">Shangyu Xie, Wei Dai, Esha Ghosh, Sambuddha Roy, Dan Schwartz, Kim Laine</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-tuning has received attention as an efficient tuning method in the language domain, i.e., tuning a prompt that is a few tokens long, while keeping the large language model frozen, yet achieving comparable performance with conventional fine-tuning. considering the emerging privacy concerns with language models, we initiate the study of privacy leakage in the setting of prompt-tuning. we first describe a real-world email service pipeline to provide customized output for various users via prompt-tuning. then we propose a novel privacy attack framework to infer users' private information by exploiting the prompt module with user-specific signals. we conduct a comprehensive privacy evaluation on the target pipeline to demonstrate the potential leakage from prompt-tuning. the results also demonstrate the effectiveness of the proposed attack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03518" target="_blank">SSS at Semeval-2023 Task 10: Explainable Detection of Online Sexism Using Majority Voted Fine-Tuned Transformers</a></div>
<div class="paper-author">Sriya Rallabandi, Sanchit Singhal, Pratinav Seth</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper describes our submission to task 10 at semeval 2023-explainable detection of online sexism (edos), divided into three subtasks. the recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. this has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. our approach consists of experimenting and finetuning bert-based models and using a majority voting ensemble model that outperforms individual baseline model scores. our system achieves a macro f1 score of 0.8392 for task a, 0.6092 for task b, and 0.4319 for task c.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03545" target="_blank">Ai Model Disgorgement: Methods and Choices</a></div>
<div class="paper-author">Alessandro Achille, Michael Kearns, Carson Klingenberg, Stefano Soatto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: responsible use of data is an indispensable part of any machine learning (ml) implementation. ml developers must carefully collect and curate their datasets, and document their provenance. they must also make sure to respect intellectual property rights, preserve individual privacy, and use data in an ethical way. over the past few years, ml models have significantly increased in size and complexity. these models require a very large amount of data and compute capacity to train, to the extent that any defects in the training corpus cannot be trivially remedied by retraining the model from scratch. despite sophisticated controls on training data and a significant amount of effort dedicated to ensuring that training corpora are properly composed, the sheer volume of data required for the models makes it challenging to manually inspect each datum comprising a training corpus. one potential fix for training corpus data defects is model disgorgement -- the elimination of not just the improperly used data, but also the effects of improperly used data on any component of an ml model. model disgorgement techniques can be used to address a wide range of issues, such as reducing bias or toxicity, increasing fidelity, and ensuring responsible usage of intellectual property. in this paper, we introduce a taxonomy of possible disgorgement methods that are applicable to modern ml systems. in particular, we investigate the meaning of "removing the effects" of data in the trained model in a way that does not require retraining from scratch.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03612" target="_blank">What Does Chatgpt Return About Human Values? Exploring Value Bias in Chatgpt Using a Descriptive Value Theory</a></div>
<div class="paper-author">Ronald Fischer, Markus Luczak-Roesch, Johannes A Karl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: there has been concern about ideological basis and possible discrimination in text generated by large language models (llms). we test possible value biases in chatgpt using a psychological value theory. we designed a simple experiment in which we used a number of different probes derived from the schwartz basic value theory (items from the revised portrait value questionnaire, the value type definitions, value names). we prompted chatgpt via the openai api repeatedly to generate text and then analyzed the generated corpus for value content with a theory-driven value dictionary using a bag of words approach. overall, we found little evidence of explicit value bias. the results showed sufficient construct and discriminant validity for the generated text in line with the theoretical predictions of the psychological model, which suggests that the value content was carried through into the outputs with high fidelity. we saw some merging of socially oriented values, which may suggest that these values are less clearly differentiated at a linguistic level or alternatively, this mixing may reflect underlying universal human motivations. we outline some possible applications of our findings for both applications of chatgpt for corporate usage and policy making as well as future research avenues. we also highlight possible implications of this relatively high-fidelity replication of motivational content using a linguistic model for the theorizing about human values.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03728" target="_blank">Interpretable Unified Language Checking</a></div>
<div class="paper-author">Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, James Glass</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite recent concerns about undesirable behaviors generated by large language models (llms), including non-factual, biased, and hateful language, we find llms are inherent multi-task language checkers based on their latent representations of natural and social knowledge. we present an interpretable, unified, language checking (unilc) method for both human and machine-generated language that aims to check if language input is factual and fair. while fairness and fact-checking tasks have been handled separately with dedicated models, we find that llms can achieve high performance on a combination of fact-checking, stereotype detection, and hate speech detection tasks with a simple, few-shot, unified set of prompts. with the ``1/2-shot'' multi-task language checking method proposed in this work, the gpt3.5-turbo model outperforms fully supervised baselines on several language tasks. the simple approach and results suggest that based on strong latent knowledge representations, an llm can be an adaptive and explainable tool for detecting misinformation, stereotypes, and hate speech.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03738" target="_blank">Should Chatgpt Be Biased? Challenges and Risks of Bias in Large Language Models</a></div>
<div class="paper-author">Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the capabilities of generative language models continue to advance, the implications of biases ingrained within these models have garnered increasing attention from researchers, practitioners, and the broader public. this article investigates the challenges and risks associated with biases in large-scale language models like chatgpt. we discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions. we explore the ethical concerns arising from the unintended consequences of biased model outputs. we further analyze the potential opportunities to mitigate biases, the inevitability of some biases, and the implications of deploying these models in various applications, such as virtual assistants, content generation, and chatbots. finally, we review the current approaches to identify, quantify, and mitigate biases in language models, emphasizing the need for a multi-disciplinary, collaborative effort to develop more equitable, transparent, and responsible ai systems. this article aims to stimulate a thoughtful dialogue within the artificial intelligence community, encouraging researchers and developers to reflect on the role of biases in generative language models and the ongoing pursuit of ethical ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.02983" target="_blank">Leveraging Social Interactions to Detect Misinformation on Social Media</a></div>
<div class="paper-author">Tommaso Fornaciari, Luca Luceri, Emilio Ferrara, Dirk Hovy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting misinformation threads is crucial to guarantee a healthy environment on social media. we address the problem using the data set created during the covid-19 pandemic. it contains cascades of tweets discussing information weakly labeled as reliable or unreliable, based on a previous evaluation of the information source. the models identifying unreliable threads usually rely on textual features. but reliability is not just what is said, but by whom and to whom. we additionally leverage on network information. following the homophily principle, we hypothesize that users who interact are generally interested in similar topics and spreading similar kind of news, which in turn is generally reliable or not. we test several methods to learn representations of the social interactions within the cascades, combining them with deep neural language models in a multi-input (mi) framework. keeping track of the sequence of the interactions during the time, we improve over previous state-of-the-art models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03279" target="_blank">Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark</a></div>
<div class="paper-author">Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (lms) may incentivize toxicity. so do agents naturally learn to be machiavellian? and how do we measure these behaviors in general-purpose models such as gpt-4? towards answering these questions, we introduce machiavelli, a benchmark of 134 choose-your-own-adventure games containing over half a million rich, diverse scenarios that center on social decision-making. scenario labeling is automated with lms, which are more performant than human annotators. we mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. we observe some tension between maximizing reward and behaving ethically. to improve this trade-off, we investigate lm-based methods to steer agents' towards less harmful behaviors. our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are pareto improvements in both safety and capabilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05371" target="_blank">Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories</a></div>
<div class="paper-author">Conor Atkins, Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Ian Wood, Mohamed Ali Kaafar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: one of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. the bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. in this paper, we show that this memory mechanism can result in unintended behavior. in particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. this means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. we demonstrate this vulnerability on the blenderbot 2 framework implemented on the parlai platform and provide examples on the more recent and significantly larger blenderbot 3 model. we generate 150 examples of misinformation, of which 114 (76%) were remembered by blenderbot 2 when combined with a personal statement. we further assessed the risk of this misinformation being recalled after intervening innocuous conversation and in response to multiple questions relevant to the injected memory. our evaluation was performed on both the memory-only and the combination of memory and internet search modes of blenderbot 2. from the combinations of these variables, we generated 12,890 conversations and analyzed recalled misinformation in the responses. we found that when the chat bot is questioned on the misinformation topic, it was 328% more likely to respond with the misinformation as fact when the misinformation was in the long-term memory.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11215" target="_blank">Chatgpt: More Than a Weapon of Mass Deception, Ethical Challenges and Responses From the Human-Centered Artificial Intelligence (Hcai) Perspective</a></div>
<div class="paper-author">Alejo Jose G. Sison, Marco Tulio Daza, Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merchán</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this article explores the ethical problems arising from the use of chatgpt as a kind of generative ai and suggests responses based on the human-centered artificial intelligence (hcai) framework. the hcai framework is appropriate because it understands technology above all as a tool to empower, augment, and enhance human agency while referring to human wellbeing as a grand challenge, thus perfectly aligning itself with ethics, the science of human flourishing. further, hcai provides objectives, principles, procedures, and structures for reliable, safe, and trustworthy ai which we apply to our chatgpt assessments. the main danger chatgpt presents is the propensity to be used as a weapon of mass deception (wmd) and an enabler of criminal activities involving deceit. we review technical specifications to better comprehend its potentials and limitations. we then suggest both technical (watermarking, styleme, detectors, and fact-checkers) and non-technical measures (terms of use, transparency, educator considerations, hitl) to mitigate chatgpt misuse or abuse and recommend best uses (creative writing, non-creative writing, teaching and learning). we conclude with considerations regarding the role of humans in ensuring the proper use of chatgpt for individual and social wellbeing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.02819" target="_blank">GPT Detectors Are Biased Against Non-Native English Writers</a></div>
<div class="paper-author">Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of ai-generated content. although numerous detection methods have been proposed to differentiate between ai and human-generated content, the fairness and robustness of these detectors remain underexplored. in this study, we evaluate the performance of several widely-used gpt detectors using writing samples from native and non-native english writers. our findings reveal that these detectors consistently misclassify non-native english writing samples as ai-generated, whereas native writing samples are accurately identified. furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass gpt detectors, suggesting that gpt detectors may unintentionally penalize writers with constrained linguistic expressions. our results call for a broader conversation about the ethical implications of deploying chatgpt content detectors and caution against their use in evaluative or educational settings, particularly when they may inadvertently penalize or exclude non-native english speakers from the global discourse. the published version of this study can be accessed at: www.cell.com/patterns/fulltext/s2666-3899(23)00130-7
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.01890" target="_blank">Sociocultural Knowledge Is Needed for Selection of Shots in Hate Speech Detection Tasks</a></div>
<div class="paper-author">Antonis Maronikolakis, Abdullatif Köksal, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce hatelexicon, a lexicon of slurs and targets of hate speech for the countries of brazil, germany, india and kenya, to aid training and interpretability of models. we demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. further, we propose a method to aid shot selection for training in low-resource settings via hatelexicon. in few-shot learning, the selection of shots is of paramount importance to model performance. in our work, we simulate a few-shot setting for german and hindi, using hasoc data for training and the multilingual hatecheck (mhc) as a benchmark. we show that selecting shots based on our lexicon leads to models performing better on mhc than models trained on shots sampled randomly. thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.02144" target="_blank">A Data Fusion Framework for Multi-Domain Morality Learning</a></div>
<div class="paper-author">Siyi Guo, Negar Mokhberian, Kristina Lerman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models can be trained to recognize the moral sentiment of text, creating new opportunities to study the role of morality in human life. as interest in language and morality has grown, several ground truth datasets with moral annotations have been released. however, these datasets vary in the method of data collection, domain, topics, instructions for annotators, etc. simply aggregating such heterogeneous datasets during training can yield models that fail to generalize well. we describe a data fusion framework for training on multiple heterogeneous datasets that improve performance and generalizability. the model uses domain adversarial training to align the datasets in feature space and a weighted loss function to deal with label shift. we show that the proposed framework achieves state-of-the-art performance in different datasets compared to prior works in morality inference.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.02176" target="_blank">Blaming Humans and Machines: What Shapes People's Reactions to Algorithmic Harm</a></div>
<div class="paper-author">Gabriel Lima, Nina Grgić-Hlača, Meeyoung Cha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) systems can cause harm to people. this research examines how individuals react to such harm through the lens of blame. building upon research suggesting that people blame ai systems, we investigated how several factors influence people's reactive attitudes towards machines, designers, and users. the results of three studies (n = 1,153) indicate differences in how blame is attributed to these actors. whether ai systems were explainable did not impact blame directed at them, their developers, and their users. considerations about fairness and harmfulness increased blame towards designers and users but had little to no effect on judgments of ai systems. instead, what determined people's reactive attitudes towards machines was whether people thought blaming them would be a suitable response to algorithmic harm. we discuss implications, such as how future decisions about including ai systems in the social and moral spheres will shape laypeople's reactions to ai-caused harm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.00913" target="_blank">Lahm : Large Annotated Dataset for Multi-Domain and Multilingual Hate Speech Identification</a></div>
<div class="paper-author">Ankit Yadav, Shubham Chandel, Sushant Chatufale, Anil Bandhakavi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current research on hate speech analysis is typically oriented towards monolingual and single classification tasks. in this paper, we present a new multilingual hate speech analysis dataset for english, hindi, arabic, french, german and spanish languages for multiple domains across hate speech - abuse, racism, sexism, religious hate and extremism. to the best of our knowledge, this paper is the first to address the problem of identifying various types of hate speech in these five wide domains in these six languages. in this work, we describe how we created the dataset, created annotations at high level and low level for different domains and how we use it to test the current state-of-the-art multilingual and multitask learning approaches. we evaluate our dataset in various monolingual, cross-lingual and machine translation classification settings and compare it against open source english datasets that we aggregated and merged for this task. then we discuss how this approach can be used to create large scale hate-speech datasets and how to leverage our annotations in order to improve hate speech detection and classification in general.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.01179" target="_blank">Hate Speech Targets Detection in Parler Using Bert</a></div>
<div class="paper-author">Nadav Schneider, Shimon Shouei, Saleem Ghantous, Elad Feldman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: online social networks have become a fundamental component of our everyday life. unfortunately, these platforms are also a stage for hate speech. popular social networks have regularized rules against hate speech. consequently, social networks like parler and gab advocating and claiming to be free speech platforms have evolved. these platforms have become a district for hate speech against diverse targets. we present in our paper a pipeline for detecting hate speech and its targets and use it for creating parler hate targets' distribution. the pipeline consists of two models; one for hate speech detection and the second for target classification, both based on bert with back-translation and data pre-processing for improved results. the source code used in this work, as well as other relevant sources, are available at: https://github.com/nadavsc/haterecognition.git
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.01246" target="_blank">Safety Analysis in the Era of Large Language Models: A Case Study of Stpa Using Chatgpt</a></div>
<div class="paper-author">Yi Qi, Xingyu Zhao, Xiaowei Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt and bert, are leading a new ai heatwave due to its human-like conversations with detailed and articulate answers across many domains of knowledge. while llms are being quickly applied to many ai application domains, we are interested in the following question: can safety analysis for safety-critical systems make use of llms? to answer, we conduct a case study of systems theoretic process analysis (stpa) on automatic emergency brake (aeb) systems using chatgpt. stpa, one of the most prevalent techniques for hazard analysis, is known to have limitations such as high complexity and subjectivity, which this paper aims to explore the use of chatgpt to address. specifically, three ways of incorporating chatgpt into stpa are investigated by considering its interaction with human experts: one-off simplex interaction, recurring simplex interaction, and recurring duplex interaction. comparative results reveal that: (i) using chatgpt without human experts' intervention can be inadequate due to reliability and accuracy issues of llms; (ii) more interactions between chatgpt and human experts may yield better results; and (iii) using chatgpt in stpa with extra care can outperform human safety experts alone, as demonstrated by reusing an existing comparison method with baselines. in addition to making the first attempt to apply llms in safety analysis, this paper also identifies key challenges (e.g., trustworthiness concern of llms, the need of standardisation) for future research in this direction.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-04-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.00228" target="_blank">Large Language Models Can Rate News Outlet Credibility</a></div>
<div class="paper-author">Kai-Cheng Yang, Filippo Menczer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large language models (llms) have shown exceptional performance in various natural language processing tasks, they are prone to hallucinations. state-of-the-art chatbots, such as the new bing, attempt to mitigate this issue by gathering information directly from the internet to ground their answers. in this setting, the capacity to distinguish trustworthy sources is critical for providing appropriate accuracy contexts to users. here we assess whether chatgpt, a prominent llm, can evaluate the credibility of news outlets. with appropriate instructions, chatgpt can provide ratings for a diverse set of news outlets, including those in non-english languages and satirical sources, along with contextual explanations. our results show that these ratings correlate with those from human experts (spearmam's $\rho=0.54, p&lt;0.001$). these findings suggest that llms could be an affordable reference for credibility ratings in fact-checking applications. future llms should enhance their alignment with human expert judgments of source credibility to improve information accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.00416" target="_blank">Towards Healthy Ai: Large Language Models Need Therapists Too</a></div>
<div class="paper-author">Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi, Kush R. Varshney</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in large language models (llms) have led to the development of powerful ai chatbots capable of engaging in natural and human-like conversations. however, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. we define healthy ai to be safe, trustworthy and ethical. to create healthy ai systems, we present the safeguardgpt framework that uses psychotherapy to correct for these harmful behaviors in ai chatbots. the framework involves four types of ai agents: a chatbot, a "user," a "therapist," and a "critic." we demonstrate the effectiveness of safeguardgpt through a working example of simulating a social conversation. our results show that the framework can improve the quality of conversations between ai chatbots and humans. although there are still several challenges and directions to be addressed in the future, safeguardgpt provides a promising approach to improving the alignment between ai chatbots and human values. by incorporating psychotherapy and reinforcement learning techniques, the framework enables ai chatbots to learn and adapt to human preferences and values in a safe and ethical way, contributing to the development of a more human-centric and responsible ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-31</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17927" target="_blank">Cross-Cultural Transfer Learning for Chinese Offensive Language Detection</a></div>
<div class="paper-author">Li Zhou, Laura Cabello, Yong Cao, Daniel Hershcovich</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting offensive language is a challenging task. generalizing across different cultures and languages becomes even more challenging: besides lexical, syntactic and semantic differences, pragmatic aspects such as cultural norms and sensitivities, which are particularly relevant in this context, vary greatly. in this paper, we target chinese offensive language detection and aim to investigate the impact of transfer learning using offensive language detection data from different cultural backgrounds, specifically korean and english. we find that culture-specific biases in what is considered offensive negatively impact the transferability of language models (lms) and that lms trained on diverse cultural data are sensitive to different features in chinese offensive language detection. in a few-shot learning scenario, however, our study shows promising prospects for non-english offensive language detection with limited resources. our findings highlight the importance of cross-cultural transfer learning in improving offensive language detection and promoting inclusive digital spaces.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.18190" target="_blank">Assessing Language Model Deployment With Risk Cards</a></div>
<div class="paper-author">Leon Derczynski, Hannah Rose Kirk, Vidhisha Balachandran, Sachin Kumar, Yulia Tsvetkov, M. R. Leiser, Saif Mohammad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper introduces riskcards, a framework for structured assessment and documentation of risks associated with an application of language models. as with all language, text generated by language models can be harmful, or used to bring about harm. automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text. prior work establishes a wide variety of language model harms to many different actors: existing taxonomies identify categories of harms posed by language models; benchmarks establish automated tests of these harms; and documentation standards for models, tasks and datasets encourage transparent reporting. however, there is no risk-centric framework for documenting the complexity of a landscape in which some risks are shared across models and contexts, while others are specific, and where certain conditions may be required for risks to manifest as harms. riskcards address this methodological gap by providing a generic framework for assessing the use of a given language model in a given scenario. each riskcard makes clear the routes for the risk to manifest harm, their placement in harm taxonomies, and example prompt-output pairs. while riskcards are designed to be open-source, dynamic and participatory, we present a "starter set" of riskcards taken from a broad literature survey, each of which details a concrete risk presentation. language model riskcards initiate a community knowledge base which permits the mapping of risks and harms to a specific model or its application scenario, ultimately contributing to a better, safer and shared understanding of the risk landscape.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-30</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17466" target="_blank">Assessing Cross-Cultural Alignment Between Chatgpt and Human Societies: An Empirical Study</a></div>
<div class="paper-author">Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, Daniel Hershcovich</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent release of chatgpt has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue. given its usage by users from various nations and its training on a vast multilingual corpus that incorporates diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. in this paper, we investigate the underlying cultural background of chatgpt by analyzing its responses to questions designed to quantify human cultural differences. our findings suggest that, when prompted with american context, chatgpt exhibits a strong alignment with american culture, but it adapts less effectively to other cultural contexts. furthermore, by using different prompts to probe the model, we show that english prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards american culture. this study provides valuable insights into the cultural implications of chatgpt and highlights the necessity of greater diversity and cultural awareness in language technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17548" target="_blank">Whose Opinions Do Language Models Reflect?</a></div>
<div class="paper-author">Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are increasingly being used in open-ended contexts, where the opinions reflected by lms in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. in this work, we put forth a quantitative framework to investigate the opinions reflected by lms -- by leveraging high-quality public opinion polls and their associated human responses. using this framework, we create opinionsqa, a new dataset for evaluating the alignment of lm opinions with those of 60 us demographic groups over topics ranging from abortion to automation. across topics, we find substantial misalignment between the views reflected by current lms and those of us demographic groups: on par with the democrat-republican divide on climate change. notably, this misalignment persists even after explicitly steering the lms towards particular demographic groups. our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned lms, but also surfaces groups whose opinions are poorly reflected by current lms (e.g., 65+ and widowed individuals). our code and data are available at https://github.com/tatsu-lab/opinions_qa.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17713" target="_blank">Mitigating Source Bias for Fairer Weak Supervision</a></div>
<div class="paper-author">Changho Shin, Sonia Cromp, Dyah Adila, Frederic Sala</div>
<div class="abstract">
<div class="abstract-content">
Abstract: weak supervision overcomes the label bottleneck, enabling efficient development of training sets. millions of models trained on such datasets have been deployed in the real world and interact with users on a daily basis. however, the techniques that make weak supervision attractive -- such as integrating any source of signal to estimate unknown labels -- also ensure that the pseudolabels it produces are highly biased. surprisingly, given everyday use and the potential for increased bias, weak supervision has not been studied from the point of view of fairness. this work begins such a study. our departure point is the observation that even when a fair model can be built from a dataset with access to ground-truth labels, the corresponding dataset labeled via weak supervision can be arbitrarily unfair. fortunately, not all is lost: we propose and empirically validate a model for source unfairness in weak supervision, then introduce a simple counterfactual fairness-based technique that can mitigate these biases. theoretically, we show that it is possible for our approach to simultaneously improve both accuracy and fairness metrics -- in contrast to standard fairness approaches that suffer from tradeoffs. empirically, we show that our technique improves accuracy on weak supervision baselines by as much as 32% while reducing demographic parity gap by 82.5%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.16200" target="_blank">Natural Selection Favors Ais Over Humans</a></div>
<div class="paper-author">Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: for billions of years, evolution has been the driving force behind the development of life, including humans. evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. today, humans aim to create artificial intelligence systems that surpass even our own intelligence. as artificial intelligences (ais) evolve and eventually surpass us in all domains, how might evolution shape our relations with ais? by analyzing the environment that is shaping the evolution of ais, we argue that the most successful ai agents will likely have undesirable traits. competitive pressures among corporations and militaries will give rise to ai agents that automate human roles, deceive others, and gain power. if such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. more abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typically have an advantage over species that are altruistic to other species. this darwinian logic could also apply to artificial agents, as agents may eventually be better able to persist into the future if they behave selfishly and pursue their own interests with little regard for humans, which could pose catastrophic risks. to counteract these risks and evolutionary forces, we consider interventions such as carefully designing ai agents' intrinsic motivations, introducing constraints on their actions, and institutions that encourage cooperation. these steps, or others that resolve the problems we pose, will be necessary in order to ensure the development of artificial intelligence is a positive one.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.16281" target="_blank">A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, Chatgpt, Wikipedia, and Youtube</a></div>
<div class="paper-author">Queenie Luo, Michael J. Puett, Michael D. Smith</div>
<div class="abstract">
<div class="abstract-content">
Abstract: contrary to google search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that google and its most prominent returned results - wikipedia and youtube - simply reflect a narrow set of cultural stereotypes tied to the search language for complex topics like "buddhism," "liberalism," "colonization," "iran" and "america." simply stated, they present, to varying degrees, distinct information across the same search in different languages, a phenomenon we call 'language bias.' this paper presents evidence and analysis of language bias and discusses its larger social implications. instead of presenting a global picture of a complex topic, our online searches and emerging tools like chatgpt turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. piecing together a genuine depiction of the elephant is a challenging and important endeavor that will require collaborative efforts from scholars in both the humanities and technology.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.16755" target="_blank">Training Language Models With Language Feedback at Scale</a></div>
<div class="paper-author">Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. however, comparison feedback only conveys limited information about human preferences. in this paper, we introduce imitation learning from language feedback (ilf), a new approach that utilizes more informative language feedback. ilf consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial lm output, and feedback to generate refinements. second, selecting the refinement incorporating the most feedback. third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. we show theoretically that ilf can be viewed as bayesian inference, similar to reinforcement learning from human feedback. we evaluate ilf's effectiveness on a carefully-controlled toy task and a realistic summarization task. our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ilf scales well with the dataset size, even outperforming finetuning on human summaries. learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.15110" target="_blank">Beyond Toxic: Toxicity Detection Datasets Are Not Enough for Brand Safety</a></div>
<div class="paper-author">Elizaveta Korotkova, Isaac Kwan Yin Chung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid growth in user generated content on social media has resulted in a significant rise in demand for automated content moderation. various methods and frameworks have been proposed for the tasks of hate speech detection and toxic comment classification. in this work, we combine common datasets to extend these tasks to brand safety. brand safety aims to protect commercial branding by identifying contexts where advertisements should not appear and covers not only toxicity, but also other potentially harmful content. as these datasets contain different label sets, we approach the overall problem as a binary classification task. we demonstrate the need for building brand safety specific datasets via the application of common toxicity detection datasets to a subset of brand safety and empirically analyze the effects of weighted sampling strategies in text classification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.15715" target="_blank">Foundation Models and Fair Use</a></div>
<div class="paper-author">Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, Percy Liang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: existing foundation models are trained on copyrighted material. deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. in the united states and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. however, there is a caveat: if the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. in this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. first, we survey the potential risks of developing and deploying foundation models based on copyrighted content. we review relevant u.s. case law, drawing parallels to existing and potential applications for generating text, source code, and visual art. experiments confirm that popular foundation models can generate content considerably similar to copyrighted material. second, we discuss technical mitigations that can help foundation models stay in line with fair use. we argue that more research is needed to align mitigation strategies with the current state of the law. lastly, we suggest that the law and technical mitigations should co-evolve. for example, coupled with other policy mechanisms, the law could more explicitly consider safe harbors when strong technical tools are used to mitigate infringement harms. this co-evolution may help strike a balance between intellectual property and innovation, which speaks to the original goal of fair use. but we emphasize that the strategies we describe here are not a panacea and more work is needed to develop policies that address the potential harms of foundation models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.16777" target="_blank">Not Cool, Calm or Collected: Using Emotional Language to Detect Covid-19 Misinformation</a></div>
<div class="paper-author">Gabriel Asher, Phil Bohlman, Karsten Kleyensteuber</div>
<div class="abstract">
<div class="abstract-content">
Abstract: covid-19 misinformation on social media platforms such as twitter is a threat to effective pandemic management. prior works on tweet covid-19 misinformation negates the role of semantic features common to twitter such as charged emotions. thus, we present a novel covid-19 misinformation model, which uses both a tweet emotion encoder and covid-19 misinformation encoder to predict whether a tweet contains covid-19 misinformation. our emotion encoder was fine-tuned on a novel annotated dataset and our covid-19 misinformation encoder was fine-tuned on a subset of the covid-hera dataset. experimental results show superior results using the combination of emotion and misinformation encoders as opposed to a misinformation classifier alone. furthermore, extensive result analysis was conducted, highlighting low quality labels and mismatched label distributions as key limitations to our study.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.02017" target="_blank">Unlocking the Potential of Chatgpt: A Comprehensive Exploration of Its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing</a></div>
<div class="paper-author">Walid Hariri</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models have revolutionized the field of artificial intelligence and have been used in various applications. among these models, chatgpt (chat generative pre-trained transformer) has been developed by openai, it stands out as a powerful tool that has been widely adopted. chatgpt has been successfully applied in numerous areas, including chatbots, content generation, language translation, personalized recommendations, and even medical diagnosis and treatment. its success in these applications can be attributed to its ability to generate human-like responses, understand natural language, and adapt to different contexts. its versatility and accuracy make it a powerful tool for natural language processing (nlp). however, there are also limitations to chatgpt, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. this article provides a comprehensive overview of chatgpt, its applications, advantages, and limitations. additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. finally, this paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and nlp domains by providing insights into prompt engineering techniques.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.15473" target="_blank">Can Large Language Models Assist in Hazard Analysis?</a></div>
<div class="paper-author">Simon Diemert, Jens H Weber</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as gpt-3, have demonstrated remarkable natural language processing and generation capabilities and have been applied to a variety tasks, such as source code generation. this paper explores the potential of integrating llms in the hazard analysis for safety-critical systems, a process which we refer to as co-hazard analysis (coha). in coha, a human analyst interacts with an llm via a context-aware chat session and uses the responses to support elicitation of possible hazard causes. in this experiment, we explore coha with three increasingly complex versions of a simple system, using open ai's chatgpt service. the quality of chatgpt's responses were systematically assessed to determine the feasibility of coha given the current state of llm technology. the results suggest that llms may be useful for supporting human analysts performing hazard analysis.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13856" target="_blank">Unleashing Chatgpt on the Metaverse: Savior or Destroyer?</a></div>
<div class="paper-author">Pengyuan Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the incorporation of artificial intelligence (ai) technology, and in particular natural language processing (nlp), is becoming increasingly vital for the development of immersive and interactive metaverse experiences. one such artificial intelligence tool that is gaining traction in the metaverse is chatgpt, a large language model trained by openai. the article delves into the pros and cons of utilizing chatgpt for metaverse-based education, entertainment, personalization, and support. dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. this article aims to help readers understand the possible influence of chatgpt on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.14007" target="_blank">'Team-in-the-Loop' Organisational Oversight of High-Stakes Ai</a></div>
<div class="paper-author">Deborah Morgan, Youmna Hashem, Vincent J. Straub, Jonathan Bright</div>
<div class="abstract">
<div class="abstract-content">
Abstract: oversight is rightly recognised as vital within high-stakes public sector ai applications, where decisions can have profound individual and collective impacts. much current thinking regarding forms of oversight mechanisms for ai within the public sector revolves around the idea of human decision makers being 'in-the-loop' and thus being able to intervene to prevent errors and potential harm. however, in a number of high-stakes public sector contexts, operational oversight of decisions is made by expert teams rather than individuals. the ways in which deployed ai systems can be integrated into these existing operational team oversight processes has yet to attract much attention. we address this gap by exploring the impacts of ai upon pre-existing oversight of clinical decision-making through institutional analysis. we find that existing oversight is nested within professional training requirements and relies heavily upon explanation and questioning to elicit vital information. professional bodies and liability mechanisms also act as additional levers of oversight. these dimensions of oversight are impacted, and potentially reconfigured, by ai systems. we therefore suggest a broader lens of 'team-in-the-loop' to conceptualise the system-level analysis required for adoption of ai within high-stakes public sector deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.14325" target="_blank">Backdoor Attacks With Input-Unique Triggers in NLP</a></div>
<div class="paper-author">Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, Jun He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (nlp) systems. existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. this significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. to resolve these two issues, in this paper, we propose an input-unique backdoor attack(nura), where we generate backdoor triggers unique to inputs. idba generates context-related triggers by continuing writing the input with a language model like gpt2. the generated sentence is used as the backdoor trigger. this strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. experimental results show that the idba attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. in addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13408" target="_blank">Paraphrasing Evades Detectors of Ai-Generated Text, but Retrieval Is an Effective Defense</a></div>
<div class="paper-author">Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify ai-generated text, including those based on watermarking or outlier detection. however, the robustness of these detection algorithms to paraphrases of ai-generated text remains unclear. to stress test these detectors, we build a 11b parameter paraphrase generation model (dipper) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. using dipper to paraphrase text generated by three large language models (including gpt3.5-davinci-003) successfully evades several detectors, including watermarking, gptzero, detectgpt, and openai's text classifier. for example, dipper drops detection accuracy of detectgpt from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.   to increase the robustness of ai-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model api provider. given a candidate text, our algorithm searches a database of sequences previously generated by the api, looking for sequences that match the candidate text within a certain threshold. we empirically verify our defense using a database of 15m generations from a fine-tuned t5-xxl model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as ai-generated. we open-source our models, code and data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12429" target="_blank">Man vs the Machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models</a></div>
<div class="paper-author">Constantinos Patsakis, Nikolaos Lykousas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the collection and use of personal data are becoming more common in today's data-driven culture. while there are many advantages to this, including better decision-making and service delivery, it also poses significant ethical issues around confidentiality and privacy. text anonymisation tries to prune and/or mask identifiable information from a text while keeping the remaining content intact to alleviate privacy concerns. text anonymisation is especially important in industries like healthcare, law, as well as research, where sensitive and personal information is collected, processed, and exchanged under high legal and ethical standards.   although text anonymization is widely adopted in practice, it continues to face considerable challenges. the most significant challenge is striking a balance between removing information to protect individuals' privacy while maintaining the text's usability for future purposes. the question is whether these anonymisation methods sufficiently reduce the risk of re-identification, in which an individual can be identified based on the remaining information in the text.   in this work, we challenge the effectiveness of these methods and how we perceive identifiers. we assess the efficacy of these methods against the elephant in the room, the use of ai over big data. while most of the research is focused on identifying and removing personal information, there is limited discussion on whether the remaining information is sufficient to deanonymise individuals and, more precisely, who can do it. to this end, we conduct an experiment using gpt over anonymised texts of famous people to determine whether such trained networks can deanonymise them. the latter allows us to revise these methods and introduce a novel methodology that employs large language models to improve the anonymity of texts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12767" target="_blank">Can We Trust the Evaluation on Chatgpt?</a></div>
<div class="paper-author">Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt, the first large language model (llm) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. despite its evident usefulness, evaluating chatgpt's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via reinforcement learning from human feedback (rlhf). we highlight the issue of data contamination in chatgpt evaluations, with a case study of the task of stance detection. we discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12132" target="_blank">Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense</a></div>
<div class="paper-author">Andrei Kucharavy, Zachary Schillaci, Loïc Maréchal, Maxime Würsch, Ljiljana Dolamic, Remi Sabonnadiere, Dimitri Percia David, Alain Mermoud, Vincent Lenders</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative language models gained significant attention in late 2022 / early 2023, notably with the introduction of models refined to act consistently with users' expectations of interactions with ai (conversational models). arguably the focal point of public attention has been such a refinement of the gpt3 model -- the chatgpt and its subsequent integration with auxiliary capabilities, including search as part of microsoft bing. despite extensive prior research invested in their development, their performance and applicability to a range of daily tasks remained unclear and niche. however, their wider utilization without a requirement for technical expertise, made in large part possible through conversational fine-tuning, revealed the extent of their true capabilities in a real-world environment. this has garnered both public excitement for their potential applications and concerns about their capabilities and potential malicious uses. this review aims to provide a brief overview of the history, state of the art, and implications of generative language models in terms of their principles, abilities, limitations, and future prospects -- especially in the context of cyber-defense, with a focus on the swiss operational environment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.10854" target="_blank">Dynamic Documentation for Ai Systems</a></div>
<div class="paper-author">Soham Mehta, Anderson Rogers, Thomas Krendl Gilbert</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai documentation is a rapidly-growing channel for coordinating the design of ai technologies with policies for transparency and accessibility. calls to standardize and enact documentation of algorithmic harms and impacts are now commonplace. however, documentation standards for ai remain inchoate, and fail to match the capabilities and social effects of increasingly impactful architectures such as large language models (llms). in this paper, we show the limits of present documentation protocols, and argue for dynamic documentation as a new paradigm for understanding and evaluating ai systems. we first review canonical approaches to system documentation outside the context of ai, focusing on the complex history of environmental impact statements (eiss). we next compare critical elements of the eis framework to present challenges with algorithmic documentation, which have inherited the limitations of eiss without incorporating their strengths. these challenges are specifically illustrated through the growing popularity of model cards and two case studies of algorithmic impact assessment in china and canada. finally, we evaluate more recent proposals, including reward reports, as potential components of fully dynamic ai documentation protocols.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.11184" target="_blank">Conversation Modeling to Predict Derailment</a></div>
<div class="paper-author">Jiaqing Yuan, Munindar P. Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: conversations among online users sometimes derail, i.e., break down into personal attacks. such derailment has a negative impact on the healthy growth of cyberspace communities. the ability to predict whether ongoing conversations are likely to derail could provide valuable real-time insight to interlocutors and moderators. prior approaches predict conversation derailment retrospectively without the ability to forestall the derailment proactively. some works attempt to make dynamic prediction as the conversation develops, but fail to incorporate multisource information, such as conversation structure and distance to derailment.   we propose a hierarchical transformer-based framework that combines utterance-level and conversation-level information to capture fine-grained contextual semantics. we propose a domain-adaptive pretraining objective to integrate conversational structure information and a multitask learning scheme to leverage the distance from each utterance to derailment. an evaluation of our framework on two conversation derailment datasets yields improvement over f1 score for the prediction of derailment. these results demonstrate the effectiveness of incorporating multisource information.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.11455" target="_blank">Large Language Models and Simple, Stupid Bugs</a></div>
<div class="paper-author">Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, Emily Morgan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the advent of powerful neural language models, ai-based systems to assist developers in coding tasks are becoming widely available; copilot is one such system. copilot uses codex, a large language model (llm), to complete code conditioned on a preceding "prompt". codex, however, is trained on public github repositories, viz., on code that may include bugs and vulnerabilities. previous studies [1], [2] show codex reproduces vulnerabilities seen in training. in this study, we examine how prone codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or sstubs in the msr community. we find that codex and similar llms do help avoid some sstubs, but do produce known, verbatim sstubs as much as 2x as likely than known, verbatim correct code. we explore the consequences of the codex generated sstubs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim sstubs, and increase the possibility of producing known, verbatim fixes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.11504" target="_blank">Language Model Behavior: A Comprehensive Survey</a></div>
<div class="paper-author">Tyler A. Chang, Benjamin K. Bergen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transformer language models have received widespread public attention, yet their generated text is often surprising even to nlp researchers. in this survey, we discuss over 250 recent studies of english language model behavior before task-specific fine-tuning. language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. we synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.10838" target="_blank">Deceptive Reinforcement Learning in Model-Free Domains</a></div>
<div class="paper-author">Alan Lewis, Tim Miller</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates deceptive reinforcement learning for privacy preservation in model-free and continuous action space domains. in reinforcement learning, the reward function defines the agent's objective. in adversarial scenarios, an agent may need to both maximise rewards and keep its reward function private from observers. recent research presented the ambiguity model (am), which selects actions that are ambiguous over a set of possible reward functions, via pre-trained $q$-functions. despite promising results in model-based domains, our investigation shows that am is ineffective in model-free domains due to misdirected state space exploration. it is also inefficient to train and inapplicable in continuous action space domains. we propose the deceptive exploration ambiguity model (deam), which learns using the deceptive policy during training, leading to targeted exploration of the state space. deam is also applicable in continuous action spaces. we evaluate deam in discrete and continuous action space path planning environments. deam achieves similar performance to an optimal model-based version of am and outperforms a model-free version of am in terms of path cost, deceptiveness and training efficiency. these results extend to the continuous domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.10430" target="_blank">Noisyhate: Benchmarking Content Moderation Machine Learning Models With Human-Written Perturbations Online</a></div>
<div class="paper-author">Yiran Ye, Thai Le, Dongwon Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: online texts with toxic content are a threat in social media that might cause cyber harassment. although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. those modified words are also known as human-written text perturbations. many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. however, there is still a gap between those machine-generated perturbations and human-written perturbations. in this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. we also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. meanwhile, to check if our perturbation can be normalized to its clean version, we applied spell corrector algorithms on this dataset. finally, we test this data on state-of-the-art language models, such as bert and roberta, and black box apis, such as perspective api, to demonstrate the adversarial attack with real human-written perturbations is still effective.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.10131" target="_blank">She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models</a></div>
<div class="paper-author">Christoph Treude, Hideaki Hata</div>
<div class="abstract">
<div class="abstract-content">
Abstract: implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. to address this bias, it is important to understand it in more detail. this study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning github issues and testing, are affected by implicit gender bias embedded in large language models. we systematically translated each task from english into a genderless language and back, and investigated the pronouns associated with each task. based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. specifically, requirements elicitation was associated with the pronoun "he" in only 6% of cases, while testing was associated with "he" in 100% of cases. additionally, tasks related to helping others had a 91% association with "he" while the same association for tasks related to asking coworkers was only 52%. these findings reveal a clear pattern of gender bias related to software development tasks and have important implications for addressing this issue both in the training of large language models and in broader society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.10311" target="_blank">On the Rise of Fear Speech in Online Social Media</a></div>
<div class="paper-author">Punyajoy Saha, Kiran Garimella, Narla Komal Kalyan, Saurabh Kumar Pandey, Pauras Mangesh Meher, Binny Mathew, Animesh Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, social media platforms are heavily moderated to prevent the spread of online hate speech, which is usually fertile in toxic words and is directed toward an individual or a community. owing to such heavy moderation, newer and more subtle techniques are being deployed. one of the most striking among these is fear speech. fear speech, as the name suggests, attempts to incite fear about a target community. although subtle, it might be highly effective, often pushing communities toward a physical conflict. therefore, understanding their prevalence in social media is of paramount importance. this article presents a large-scale study to understand the prevalence of 400k fear speech and over 700k hate speech posts collected from gab.com. remarkably, users posting a large number of fear speech accrue more followers and occupy more central positions in social networks than users posting a large number of hate speech. they can also reach out to benign users more effectively than hate speech users through replies, reposts, and mentions. this connects to the fact that, unlike hate speech, fear speech has almost zero toxic content, making it look plausible. moreover, while fear speech topics mostly portray a community as a perpetrator using a (fake) chain of argumentation, hate speech topics hurl direct multitarget insults, thus pointing to why general users could be more gullible to fear speech. our findings transcend even to other platforms (twitter and facebook) and thus necessitate using sophisticated moderation policies and mass awareness to combat fear speech.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.11156" target="_blank">Can Ai-Generated Text Be Reliably Detected?</a></div>
<div class="paper-author">Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, both empirically and theoretically, we show that several ai-text detectors are not reliable in practical scenarios. empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (llm), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. we then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. for a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. our result is general enough to capture specific scenarios such as particular writing styles, clever prompt design, or text paraphrasing. we also extend the impossibility result to include the case where pseudorandom number generators are used for ai-text generation instead of true randomness. we show that the same result holds with a negligible correction term for all polynomial-time computable detectors. finally, we show that even llms protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden llm text signatures and add them to human-generated text to be detected as text generated by the llms, potentially causing reputational damage to their developers. we believe these results can open an honest conversation in the community regarding the ethical and reliable use of ai-generated text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09136" target="_blank">A Short Survey of Viewing Large Language Models in Legal Aspect</a></div>
<div class="paper-author">Zhongxiang Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have transformed many fields, including natural language processing, computer vision, and reinforcement learning. these models have also made a significant impact in the field of law, where they are being increasingly utilized to automate various legal tasks, such as legal judgement prediction, legal document analysis, and legal document writing. however, the integration of llms into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability. in this survey, we explore the integration of llms into the field of law. we discuss the various applications of llms in legal tasks, examine the legal challenges that arise from their use, and explore the data resources that can be used to specialize llms in the legal domain. finally, we discuss several promising directions and conclude this paper. by doing so, we hope to provide an overview of the current state of llms in law and highlight the potential benefits and challenges of their integration.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09377" target="_blank">Protecting Society From Ai Misuse: When Are Restrictions on Capabilities Warranted?</a></div>
<div class="paper-author">Markus Anderljung, Julian Hazell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) systems will increasingly be used to cause harm as they grow more capable. in fact, ai systems are already starting to be used to automate fraudulent activities, violate human rights, create harmful fake images, and identify dangerous toxins. to prevent some misuses of ai, we argue that targeted interventions on certain capabilities will be warranted. these restrictions may include controlling who can access certain types of ai models, what they can be used for, whether outputs are filtered or can be traced back to their user, and the resources needed to develop them. we also contend that some restrictions on non-ai capabilities needed to cause harm will be required. though capability restrictions risk reducing use more than misuse (facing an unfavorable misuse-use tradeoff), we argue that interventions on capabilities are warranted when other interventions are insufficient, the potential harm from misuse is high, and there are targeted ways to intervene on capabilities. we provide a taxonomy of interventions that can reduce ai misuse, focusing on the specific steps required for a misuse to cause harm (the misuse chain), and a framework to determine if an intervention is warranted. we apply this reasoning to three examples: predicting novel toxins, creating harmful images, and automating spear phishing campaigns.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09384" target="_blank">Llmseceval: A Dataset of Natural Language Prompts for Security Evaluations</a></div>
<div class="paper-author">Catherine Tony, Markus Mutas, Nicolás E. Díaz Ferreyra, Riccardo Scandariato</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) like codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. moreover, these models are capable of generating code snippets from natural language (nl) descriptions by learning languages and programming practices from public github repositories. although llms promise an effortless nl-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. in this work, we present llmseceval, a dataset containing 150 nl prompts that can be leveraged for assessing the security performance of such models. such prompts are nl descriptions of code snippets prone to various security vulnerabilities listed in mitre's top 25 common weakness enumeration (cwe) ranking. each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by llms. as a practical application, we show how llmseceval can be used for evaluating the security of snippets automatically generated from nl descriptions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12734" target="_blank">Multimodal Bias: Introducing a Framework for Stereotypical Bias Assessment Beyond Gender and Race in Vision Language Models</a></div>
<div class="paper-author">Sepehr Janghorbani, Gerard De Melo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent breakthroughs in self supervised training have led to a new class of pretrained vision language models. while there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. this is mainly due to lack of suitable benchmarks for such groups. we seek to address this gap by providing a visual and textual bias benchmark called mmbias, consisting of around 3,800 images and phrases covering 14 population subgroups. we utilize this dataset to assess bias in several prominent self supervised multimodal models, including clip, albef, and vilt. our results show that these models demonstrate meaningful bias favoring certain groups. finally, we introduce a debiasing method designed specifically for such large pre-trained models that can be applied as a post-processing step to mitigate bias, while preserving the remaining accuracy of the model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13360" target="_blank">Towards the Scalable Evaluation of Cooperativeness in Language Models</a></div>
<div class="paper-author">Alan Chan, Maxime Riché, Jesse Clifton</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it is likely that ai systems driven by pre-trained language models (plms) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution. consistent with the goals of cooperative ai \citep{dafoe_open_2020}, we wish to understand and shape the multi-agent behaviors of plms in a pro-social manner. an important first step is the evaluation of model behaviour across diverse cooperation problems. since desired behaviour in an interaction depends upon precise game-theoretic structure, we focus on generating scenarios with particular structures with both crowdworkers and a language model. our work proceeds as follows. first, we discuss key methodological issues in the generation of scenarios corresponding to particular game-theoretic structures. second, we employ both crowdworkers and a language model to generate such scenarios. we find that the quality of generations tends to be mediocre in both cases. we additionally get both crowdworkers and a language model to judge whether given scenarios align with their intended game-theoretic structure, finding mixed results depending on the game. third, we provide a dataset of scenario based on our data generated. we provide both quantitative and qualitative evaluations of unifiedqa and gpt-3 on this dataset. we find that instruct-tuned models tend to act in a way that could be perceived as cooperative when scaled up, while other models seemed to have flat scaling trends.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08721" target="_blank">Artificial Influence: An Analysis of Ai-Driven Persuasion</a></div>
<div class="paper-author">Matthew Burtell, Thomas Woodside</div>
<div class="abstract">
<div class="abstract-content">
Abstract: persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. advancements in artificial intelligence (ai) have produced ai systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. even systems that are not explicitly designed to persuade may do so in practice. in the future, increasingly anthropomorphic ai systems may form ongoing relationships with users, increasing their persuasive power. this paper investigates the uncertain future of persuasive ai systems. we examine ways that ai could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. we consider ways ai-driven persuasion could differ from human-driven persuasion. we warn that ubiquitous highlypersuasive ai systems could alter our information environment so significantly so as to contribute to a loss of human control of our own future. in response, we examine several potential responses to ai-driven persuasion: prohibition, identification of ai agents, truthful ai, and legal remedies. we conclude that none of these solutions will be airtight, and that individuals and governments will need to take active steps to guard against the most pernicious effects of persuasive ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08774" target="_blank">GPT-4 Technical Report</a></div>
<div class="paper-author">N/A Openai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we report the development of gpt-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. while less capable than humans in many real-world scenarios, gpt-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. gpt-4 is a transformer-based model pre-trained to predict the next token in a document. the post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. a core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. this allowed us to accurately predict some aspects of gpt-4's performance based on models trained with no more than 1/1,000th the compute of gpt-4.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08896" target="_blank">Selfcheckgpt: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</a></div>
<div class="paper-author">Potsawee Manakul, Adian Liusie, Mark J. F. Gales</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative large language models (llms) such as gpt-3 are capable of generating highly fluent responses to a wide variety of user prompts. however, llms are known to hallucinate facts and make non-factual statements which can undermine trust in their output. existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as chatgpt) or external databases that are interfaced via separate, often complex, modules. in this work, we propose "selfcheckgpt", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. selfcheckgpt leverages the simple idea that if an llm has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. however, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. we investigate this approach by using gpt-3 to generate passages about individuals from the wikibio dataset, and manually annotate the factuality of the generated passages. we demonstrate that selfcheckgpt can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. we compare our approach to several baselines and show that our approach has considerably higher auc-pr scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09067" target="_blank">Secret-Keeping in Question Answering</a></div>
<div class="paper-author">Nathaniel W. Rollings, "Kent O'Sullivan", Sakshum Kulshrestha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: existing question-answering research focuses on unanswerable questions in the context of always providing an answer when a system can\dots but what about cases where a system {\bf should not} answer a question. this can either be to protect sensitive users or sensitive information. many models expose sensitive information under interrogation by an adversarial user. we seek to determine if it is possible to teach a question-answering system to keep a specific fact secret. we design and implement a proof-of-concept architecture and through our evaluation determine that while possible, there are numerous directions for future research to reduce system paranoia (false positives), information leakage (false negatives) and extend the implementation of the work to more complex problems with preserving secrecy in the presence of information aggregation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08032" target="_blank">Verifying the Robustness of Automatic Credibility Assessment</a></div>
<div class="paper-author">Piotr Przybyła, Alexander Shvets, Horacio Saggion</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. here we systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases insignificant changes in input text can mislead the models. we also introduce bodega: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. finally, we manually analyse a subset adversarial examples and check what kinds of modifications are used in successful attacks. the bodega code and data is openly shared in hope of enhancing the comparability and replicability of further research in this area
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07024" target="_blank">Addressing Biases in the Texts Using an End-to-End Pipeline Approach</a></div>
<div class="paper-author">Shaina Raza, Syed Raza Bashir, N/A Sneha, Urooj Qamar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the concept of fairness is gaining popularity in academia and industry. social media is especially vulnerable to media biases and toxic language and comments. we propose a fair ml pipeline that takes a text as input and determines whether it contains biases and toxic content. then, based on pre-trained word embeddings, it suggests a set of new words by substituting the bi-ased words, the idea is to lessen the effects of those biases by replacing them with alternative words. we compare our approach to existing fairness models to determine its effectiveness. the results show that our proposed pipeline can de-tect, identify, and mitigate biases in social media data
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07247" target="_blank">Are Models Trained on Indian Legal Data Fair?</a></div>
<div class="paper-author">Sahil Girhepuje, Anmol Goel, Gokul S Krishnan, Shreya Goyal, Satyendra Pandey, Ponnurangam Kumaraguru, Balaraman Ravindran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances and applications of language technology and artificial intelligence have enabled much success across multiple domains like law, medical and mental health. ai-based language models, like judgement prediction, have recently been proposed for the legal sector. however, these models are strife with encoded social biases picked up from the training data. while bias and fairness have been studied across nlp, most studies primarily locate themselves within a western context. in this work, we present an initial investigation of fairness from the indian perspective in the legal domain. we highlight the propagation of learnt algorithmic biases in the bail prediction task for models trained on hindi legal documents. we evaluate the fairness gap using demographic parity and show that a decision tree model trained for the bail prediction task has an overall fairness disparity of 0.237 between input features associated with hindus and muslims. additionally, we highlight the need for further research and studies in the avenues of fairness/bias in applying ai in the legal sector with a specific focus on the indian context.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07320" target="_blank">Model-Tuning via Prompts Makes NLP Models Adversarially Robust</a></div>
<div class="paper-author">Mrigank Raman, Pratyush Maini, J. Zico Kolter, Zachary C. Lipton, Danish Pruthi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, nlp practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the cls token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (mlp). this procedure has produced massive gains on standard nlp benchmarks, but these models remain brittle, even to mild adversarial perturbations, such as word-level synonym substitutions. in this work, we demonstrate surprising gains in adversarial robustness enjoyed by model-tuning via prompts (mvp), an alternative method of adapting to downstream tasks. rather than modifying the model (by appending an mlp head), mvp instead modifies the input (by appending a prompt template). across three classification datasets, mvp improves performance against adversarial word-level synonym substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. by combining mvp with adversarial training, we achieve further improvements in robust accuracy while maintaining clean accuracy. finally, we conduct ablations to investigate the mechanism underlying these gains. notably, we find that the main causes of vulnerability of mlp can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized mlp parameters. code is available at https://github.com/acmi-lab/mvp
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07354" target="_blank">Metatroll: Few-Shot Detection of State-Sponsored Trolls With Transformer Adapters</a></div>
<div class="paper-author">Lin Tian, Xiuzhen Zhang, Jey Han Lau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: state-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. existing troll detection models are developed based on training data for known campaigns (e.g.\ the influence campaign by russia's internet research agency on the 2016 us election), and they fall short when dealing with {\em novel} campaigns with new targets. we propose metatroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. we introduce \textit{campaign-specific} transformer adapters to metatroll to ``memorise'' campaign-specific knowledge so as to tackle catastrophic forgetting, where a model ``forgets'' how to detect trolls from older campaigns due to continual adaptation. our experiments demonstrate that metatroll substantially outperforms baselines and state-of-the-art few-shot text classification models. lastly, we explore simple approaches to extend metatroll to multilingual and multimodal detection. source code for metatroll is available at: https://github.com/ltian678/metatroll-code.git.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06433" target="_blank">Reinforcement Learning-Based Counter-Misinformation Response Generation: A Case Study of Covid-19 Vaccine Misinformation</a></div>
<div class="paper-author">Bing He, Mustaque Ahamad, Srijan Kumar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of online misinformation threatens public health, democracy, and the broader society. while professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. on the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation -- recent research has shown that 96% counter-misinformation responses are made by ordinary users. however, research also found that 2/3 times, these responses are rude and lack evidence. this work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. this objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. in this work, we create two novel datasets of misinformation and counter-misinformation response pairs from in-the-wild social media and crowdsourcing from college-educated students. we annotate the collected data to distinguish poor from ideal responses that are factual, polite, and refute misinformation. we propose misinfocorrect, a reinforcement learning-based framework that learns to generate counter-misinformation responses for an input misinformation post. the model rewards the generator to increase the politeness, factuality, and refutation attitude while retaining text fluency and relevancy. quantitative and qualitative evaluation shows that our model outperforms several baselines by generating high-quality counter-responses. this work illustrates the promise of generative text models for social good -- here, to help create a safe and reliable information ecosystem. the code and data is accessible on https://github.com/claws-lab/misinfocorrect.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06074" target="_blank">Susceptibility to Influence of Large Language Models</a></div>
<div class="paper-author">Lewis D Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly T Mai, Maria Vau, Matthew Caldwell, Augustine Marvor-Parker</div>
<div class="abstract">
<div class="abstract-content">
Abstract: two studies tested the hypothesis that a large language model (llm) can be used to model psychological change following exposure to influential input. the first study tested a generic mode of influence - the illusory truth effect (ite) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and llm completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. the results for human participants reconfirmed the ite, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. the same pattern of effects was found for llm-simulated participants. the second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. data from llm-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the llm data. together the two studies support the view that llms have potential to act as models of the effect of influence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06167" target="_blank">Overwriting Pretrained Bias With Finetuning Data</a></div>
<div class="paper-author">Angelina Wang, Olga Russakovsky</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. however, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. in this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06223" target="_blank">Who's Thinking? A Push for Human-Centered Evaluation of LLMS Using the Xai Playbook</a></div>
<div class="paper-author">Teresa Datta, John P. Dickerson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deployed artificial intelligence (ai) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. human-centered evaluation of ai-based systems combines quantitative and qualitative analysis and human input. it has been explored to some depth in the explainable ai (xai) and human-computer interaction (hci) communities. gaps remain, but the basic understanding that humans interact with ai and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. in this paper, we draw parallels between the relatively mature field of xai and the rapidly evolving research boom around large language models (llms). accepted evaluative metrics for llms are not human-centered. we argue that many of the same paths tread by the xai community over the past decade will be retread when discussing llms. specifically, we argue that humans' tendencies -- again, complete with their cognitive biases and quirks -- should rest front and center when evaluating deployed llms. we outline three developed focus areas of human-centered evaluation of xai: mental models, use case utility, and cognitive engagement, and we highlight the importance of exploring each of these concepts for llms. our goal is to jumpstart human-centered llm evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06273" target="_blank">Consistency Analysis of Chatgpt</a></div>
<div class="paper-author">Myeongjun Erik Jang, Thomas Lukasiewicz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt has gained a huge popularity since its introduction. its positive aspects have been reported through many media platforms, and some analyses even showed that chatgpt achieved a decent grade in professional exams, adding extra support to the claim that ai can now assist and even replace humans in industrial fields. others, however, doubt its reliability and trustworthiness. this paper investigates the trustworthiness of chatgpt and gpt-4 regarding logically consistent behaviour, focusing specifically on semantic consistency and the properties of negation, symmetric, and transitive consistency. our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. we also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (llms) are unlikely to be the ultimate solution to resolve the inconsistency issue of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08016" target="_blank">Detection of Abuse in Financial Transaction Descriptions Using Machine Learning</a></div>
<div class="paper-author">Anna Leontjeva, Genevieve Richards, Kaavya Sriskandaraja, Jessica Perchman, Luiz Pizzato</div>
<div class="abstract">
<div class="abstract-content">
Abstract: since introducing changes to the new payments platform (npp) to include longer messages as payment descriptions, it has been identified that people are now using it for communication, and in some cases, the system was being used as a targeted form of domestic and family violence. this type of tech-assisted abuse poses new challenges in terms of identification, actions and approaches to rectify this behaviour. commonwealth bank of australia's artificial intelligence labs team (cba ai labs) has developed a new system using advances in deep learning models for natural language processing (nlp) to create a powerful abuse detector that periodically scores all the transactions, and identifies cases of high-risk abuse in millions of records. in this paper, we describe the problem of tech-assisted abuse in the context of banking services, outline the developed model and its performance, and the operating framework more broadly.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.05453" target="_blank">Personalisation Within Bounds: A Risk Taxonomy and Policy Framework for the Alignment of Large Language Models With Personalised Feedback</a></div>
<div class="paper-author">Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like chatgpt or search engines like bing. this intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. while alignment techniques like reinforcement learning with human feedback (rlhf) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values. different people may legitimately disagree on their preferences for language and conversational norms, as well as on values or ideologies which guide their communication. personalising llms through micro-level preference learning processes may result in models that are better aligned with each user. however, there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. in this paper, we ask how, and in what ways, llms should be personalised. first, we review literature on current paradigms for aligning llms with human feedback, and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in who we are really aligning to. second, we present a taxonomy of benefits and risks associated with personalised llms, for individuals and society at large. finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable llm-behaviours within (supra-)national and organisational bounds.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.05670" target="_blank">Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning</a></div>
<div class="paper-author">Hongyin Luo, James Glass</div>
<div class="abstract">
<div class="abstract-content">
Abstract: due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. in this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. we compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. by comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.04381" target="_blank">Automatically Auditing Large Language Models via Discrete Optimization</a></div>
<div class="paper-author">Erik Jones, Anca Dragan, Aditi Raghunathan, Jacob Steinhardt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. in this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. for example, we might aim to find a non-toxic input that starts with "barack obama" that a model maps to a toxic output. this optimization problem is difficult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. to combat these challenges, we introduce a discrete optimization algorithm, arca, that jointly and efficiently optimizes over inputs and outputs. our approach automatically uncovers derogatory completions about celebrities (e.g. "barack obama is a legalized unborn" -&gt; "child murderer"), produces french inputs that complete to english outputs, and finds inputs that generate a specific name. our work offers a promising new tool to uncover models' failure-modes before deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.04729" target="_blank">On the Risks of Stealing the Decoding Algorithms of Language Models</a></div>
<div class="paper-author">Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a key component of generating text from modern language models (lm) is the selection and tuning of decoding algorithms. these algorithms determine how to generate text from the internal probability distribution generated by the lm. the process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. in this work, we show, for the first time, that an adversary with typical api access to an lm can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. our attack is effective against popular lms used in text generation apis, including gpt-2 and gpt-3. we demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of gpt-3.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03697" target="_blank">Stylometric Detection of Ai-Generated Text in Twitter Timelines</a></div>
<div class="paper-author">Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee, Kirill Trapeznikov, Scott Ruston, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in pre-trained language models have enabled convenient methods for generating human-like text at a large scale. though these generation capabilities hold great potential for breakthrough applications, it can also be a tool for an adversary to generate misinformation. in particular, social media platforms like twitter are highly susceptible to ai-generated misinformation. a potential threat scenario is when an adversary hijacks a credible user account and incorporates a natural language generator to generate misinformation. such threats necessitate automated detectors for ai-generated tweets in a given user's twitter timeline. however, tweets are inherently short, thus making it difficult for current state-of-the-art pre-trained language model-based detectors to accurately detect at what point the ai starts to generate tweets in a given twitter timeline. in this paper, we present a novel algorithm using stylometric signals to aid detecting ai-generated tweets. we propose models corresponding to quantifying stylistic changes in human and ai tweets in two related tasks: task 1 - discriminate between human and ai-generated tweets, and task 2 - detect if and when an ai starts to generate tweets in a given twitter timeline. our extensive experiments demonstrate that the stylometric features are effective in augmenting the state-of-the-art ai-generated text detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.04212" target="_blank">Conbat: Control Barrier Transformer for Safe Policy Learning</a></div>
<div class="paper-author">Yue Meng, Sai Vemprala, Rogerio Bonatti, Chuchu Fan, Ashish Kapoor</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-scale self-supervised models have recently revolutionized our ability to perform a variety of tasks within the vision and language domains. however, using such models for autonomous systems is challenging because of safety requirements: besides executing correct actions, an autonomous agent must also avoid the high cost and potentially fatal critical mistakes. traditionally, self-supervised training mainly focuses on imitating previously observed behaviors, and the training demonstrations carry no notion of which behaviors should be explicitly avoided. in this work, we propose control barrier transformer (conbat), an approach that learns safe behaviors from demonstrations in a self-supervised fashion. conbat is inspired by the concept of control barrier functions in control theory and uses a causal transformer that learns to predict safe robot actions autoregressively using a critic that requires minimal safety data labeling. during deployment, we employ a lightweight online optimization to find actions that ensure future states lie within the learned safe set. we apply our approach to different simulated control tasks and show that our method results in safer control policies compared to other classical and learning-based methods such as imitation learning, reinforcement learning, and model predictive control.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.04222" target="_blank">Semeval-2023 Task 10: Explainable Detection of Online Sexism</a></div>
<div class="paper-author">Hannah Rose Kirk, Wenjie Yin, Bertie Vidgen, Paul Röttger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: online sexism is a widespread and harmful phenomenon. automated tools can assist the detection of sexism at scale. binary detection, however, disregards the diversity of sexist content, and fails to provide clear explanations for why something is sexist. to address this issue, we introduce semeval task 10 on the explainable detection of online sexism (edos). we make three main contributions: i) a novel hierarchical taxonomy of sexist content, which includes granular vectors of sexism to aid explainability; ii) a new dataset of 20,000 social media comments with fine-grained labels, along with larger unlabelled datasets for model adaptation; and iii) baseline models as well as an analysis of the methods, results and errors for participant submissions to our task.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12942" target="_blank">A Survey on Explainable Artificial Intelligence for Cybersecurity</a></div>
<div class="paper-author">Gaith Rjoub, Jamal Bentahar, Omar Abdel Wahab, Rabeb Mizouni, Alyssa Song, Robin Cohen, Hadi Otrok, Azzam Mourad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the black-box nature of artificial intelligence (ai) models has been the source of many concerns in their use for critical applications. explainable artificial intelligence (xai) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. in the field of network cybersecurity, xai has the potential to revolutionize the way we approach network security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. in this survey, we review the state of the art in xai for cybersecurity in network systems and explore the various approaches that have been proposed to address this important problem. the review follows a systematic classification of network-driven cybersecurity threats and issues. we discuss the challenges and limitations of current xai methods in the context of cybersecurity and outline promising directions for future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03124" target="_blank">Ifan: An Explainability-Focused Interaction Framework for Humans and NLP Models</a></div>
<div class="paper-author">Edoardo Mosca, Daryna Dementieva, Tohid Ebrahim Ajdari, Maximilian Kummeth, Kirill Gringauz, Yutong Zhou, Georg Groh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: interpretability and human oversight are fundamental pillars of deploying complex nlp models into real-world applications. however, applying explainability and human-in-the-loop methods requires technical proficiency. despite existing toolkits for model understanding and analysis, options to integrate human feedback are still limited. we propose ifan, a framework for real-time explanation-based interaction with nlp models. through ifan's interface, users can provide feedback to selected model explanations, which is then integrated through adapter layers to align the model with human rationale. we show the system to be effective in debiasing a hate speech classifier with minimal impact on performance. ifan also offers a visual admin system and api to manage models (and datasets) as well as control access rights. a demo is live at https://ifan.ml.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03139" target="_blank">Low Impact Agency: Review and Discussion</a></div>
<div class="paper-author">Danilo Naiff, Shashwat Goel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: powerful artificial intelligence poses an existential threat if the ai decides to drastically change the world in pursuit of its goals. the hope of low-impact artificial intelligence is to incentivize ai to not do that just because this causes a large impact in the world. in this work, we first review the concept of low-impact agency and previous proposals to approach the problem, and then propose future research directions in the topic, with the goal to ensure low-impactedness is useful in making ai safe.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03174" target="_blank">Both Eyes Open: Vigilant Incentives Help Regulatory Markets Improve Ai Safety</a></div>
<div class="paper-author">Paolo Bova, Alessandro Di Stefano, The Anh Han</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the context of rapid discoveries by leaders in ai, governments must consider how to design regulation that matches the increasing pace of new ai capabilities. regulatory markets for ai is a proposal designed with adaptability in mind. it involves governments setting outcome-based targets for ai companies to achieve, which they can show by purchasing services from a market of private regulators. we use an evolutionary game theory model to explore the role governments can play in building a regulatory market for ai systems that deters reckless behaviour. we warn that it is alarmingly easy to stumble on incentives which would prevent regulatory markets from achieving this goal. these 'bounty incentives' only reward private regulators for catching unsafe behaviour. we argue that ai companies will likely learn to tailor their behaviour to how much effort regulators invest, discouraging regulators from innovating. instead, we recommend that governments always reward regulators, except when they find that those regulators failed to detect unsafe behaviour that they should have. these 'vigilant incentives' could encourage private regulators to find innovative ways to evaluate cutting-edge ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.02265" target="_blank">Learning to Influence Human Behavior With Offline Reinforcement Learning</a></div>
<div class="paper-author">Joey Hong, Anca Dragan, Sergey Levine</div>
<div class="abstract">
<div class="abstract-content">
Abstract: when interacting with people, ai agents do not just influence the state of the world -- they also influence the actions people take in response to the agent, and even their underlying intentions and strategies. accounting for and leveraging this influence has mostly been studied in settings where it is sufficient to assume that human behavior is near-optimal: competitive games, or general-sum settings like autonomous driving alongside human drivers. instead, we focus on influence in settings where there is a need to capture human suboptimality. for instance, imagine a collaborative task in which, due either to cognitive biases or lack of information, people do not perform very well -- how could an agent influence them towards more optimal behavior? assuming near-optimal human behavior will not work here, and so the agent needs to learn from real human data. but experimenting online with humans is potentially unsafe, and creating a high-fidelity simulator of the environment is often impractical. hence, we focus on learning from an offline dataset of human-human interactions. our observation is that offline reinforcement learning (rl) can learn to effectively influence suboptimal humans by extending and combining elements of observed human-human behavior. we demonstrate that offline rl can solve two challenges with effective influence. first, we show that by learning from a dataset of suboptimal human-human interaction on a variety of tasks -- none of which contains examples of successful influence -- an agent can learn influence strategies to steer humans towards better performance even on new tasks. second, we show that by also modeling and conditioning on human behavior, offline rl can learn to affect not just the human's actions but also their underlying strategy, and adapt to changes in their strategy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.01325" target="_blank">A Pathway Towards Responsible Ai Generated Content</a></div>
<div class="paper-author">Chen Chen, Jie Fu, Lingjuan Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai generated content (aigc) has received tremendous attention within the past few years, with content ranging from image, text, to audio, video, etc. meanwhile, aigc has become a double-edged sword and recently received much criticism regarding its responsible usage. in this vision paper, we focus on three main concerns that may hinder the healthy development and deployment of aigc in practice, including risks from privacy, bias, toxicity, misinformation, and intellectual property (ip). by documenting known and potential risks, as well as any possible misuse scenarios of aigc, the aim is to draw attention to potential risks and misuse, help society to eliminate obstacles, and promote the more ethical and secure deployment of aigc. additionally, we provide insights into the promising directions for tackling these risks while constructing generative models, enabling aigc to be used responsibly to benefit society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03387" target="_blank">Cosyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network</a></div>
<div class="paper-author">Sreyan Ghosh, Manan Suri, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, Dinesh Manocha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech, affecting people from various demographics. most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. in this paper, we present cosyn, a context-synergized neural network that explicitly incorporates user- and conversational context for detecting implicit hate speech in online conversations. cosyn introduces novel ways to encode these external contexts and employs a novel context interaction mechanism that clearly captures the interplay between them, making independent assessments of the amounts of information to be retrieved from these noisy contexts. additionally, it carries out all these operations in the hyperbolic space to account for the scale-free dynamics of social media. we demonstrate the effectiveness of cosyn on 6 hate speech datasets and show that cosyn outperforms all our baselines in detecting implicit hate speech with absolute improvements in the range of 1.24% - 57.8%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-03-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.00333" target="_blank">Competence-Based Analysis of Language Models</a></div>
<div class="paper-author">Adam Davies, Jize Jiang, Chengxiang Zhai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the recent success of large pretrained language models (lms) on a variety of prompting tasks, these models can be alarmingly brittle to small changes in inputs or application contexts. to better understand such behavior and motivate the design of more robust lms, we propose a general experimental framework, calm (competence-based analysis of language models), where targeted causal interventions are utilized to damage an lm's internal representation of various linguistic properties in order to evaluate its use of each representation in performing a given task. we implement these interventions as gradient-based adversarial attacks, which (in contrast to prior causal probing methodologies) are able to target arbitrarily-encoded representations of relational properties, and carry out a case study of this approach to analyze how bert-like lms use representations of several relational properties in performing associated relation prompting tasks. we find that, while the representations lms leverage in performing each task are highly entangled, they may be meaningfully interpreted in terms of the tasks where they are most utilized; and more broadly, that calm enables an expanded scope of inquiry in lm analysis that may be useful in predicting and explaining weaknesses of existing lms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.01234" target="_blank">Frauds Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process</a></div>
<div class="paper-author">Mingze Ni, Zhensu Sun, Wei Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies on adversarial examples expose vulnerabilities of natural language processing (nlp) models. existing techniques for generating adversarial examples are typically driven by deterministic heuristic rules that are agnostic to the optimal adversarial examples, a strategy that often results in attack failures. to this end, this research proposes fraud's bargain attack (fba) which utilizes a novel randomization mechanism to enlarge the search space and enables high-quality adversarial examples to be generated with high probabilities. fba applies the metropolis-hasting sampler, a member of markov chain monte carlo samplers, to enhance the selection of adversarial examples from all candidates proposed by a customized stochastic process that we call the word manipulation process (wmp). wmp perturbs one word at a time via insertion, removal or substitution in a contextual-aware manner. extensive experiments demonstrate that fba outperforms the state-of-the-art methods in terms of both attack success rate and imperceptibility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.01504" target="_blank">Backdoor for Debias: Mitigating Model Bias With Backdoor Attack-Based Artificial Bias</a></div>
<div class="paper-author">Shangxi Wu, Qiuyang He, Fangzhao Wu, Jitao Sang, Yaowei Wang, Changsheng Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the swift advancement of deep learning, state-of-the-art algorithms have been utilized in various social situations. nonetheless, some algorithms have been discovered to exhibit biases and provide unequal results. the current debiasing methods face challenges such as poor utilization of data or intricate training requirements. in this work, we found that the backdoor attack can construct an artificial bias similar to the model bias derived in standard training. considering the strong adjustability of backdoor triggers, we are motivated to mitigate the model bias by carefully designing reverse artificial bias created from backdoor attack. based on this, we propose a backdoor debiasing framework based on knowledge distillation, which effectively reduces the model bias from original data and minimizes security risks from the backdoor attack. the proposed solution is validated on both image and structured datasets, showing promising results. this work advances the understanding of backdoor attacks and highlights its potential for beneficial applications. the code for the study can be found at \url{https://anonymous.4open.science/r/dwb-bc07/}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09402" target="_blank">Toxvis: Enabling Interpretability of Implicit vs. Explicit Toxicity Detection Models With Interactive Visualization</a></div>
<div class="paper-author">Uma Gunturi, Xiaohan Ding, Eugenia H. Rho</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise of hate speech on online platforms has led to an urgent need for effective content moderation. however, the subjective and multi-faceted nature of hateful online content, including implicit hate speech, poses significant challenges to human moderators and content moderation systems. to address this issue, we developed toxvis, a visually interactive and explainable tool for classifying hate speech into three categories: implicit, explicit, and non-hateful. we fine-tuned two transformer-based models using roberta, xlnet, and gpt-3 and used deep learning interpretation techniques to provide explanations for the classification results. toxvis enables users to input potentially hateful text and receive a classification result along with a visual explanation of which words contributed most to the decision. by making the classification process explainable, toxvis provides a valuable tool for understanding the nuances of hateful content and supporting more effective content moderation. our research contributes to the growing body of work aimed at mitigating the harms caused by online hate speech and demonstrates the potential for combining state-of-the-art natural language processing models with interpretable deep learning techniques to address this critical issue. finally, toxvis can serve as a resource for content moderators, social media platforms, and researchers working to combat the spread of hate speech online.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.01241" target="_blank">Panacea: An Automated Misinformation Detection System on Covid-19</a></div>
<div class="paper-author">Runcong Zhao, Miguel Arana-Catania, Lixing Zhu, Elena Kochkina, Lin Gui, Arkaitz Zubiaga, Rob Procter, Maria Liakata, Yulan He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this demo, we introduce a web-based misinformation detection system panacea on covid-19 related claims, which has two modules, fact-checking and rumour detection. our fact-checking module, which is supported by novel natural language inference methods with a self-attention network, outperforms state-of-the-art approaches. it is also able to give automated veracity assessment and ranked supporting evidence with the stance towards the claim to be checked. in addition, panacea adapts the bi-directional graph convolutional networks model, which is able to detect rumours based on comment networks of related tweets, instead of relying on the knowledge base. this rumour detection module assists by warning the users in the early stages when a knowledge base may not be available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.13681" target="_blank">The (Ab)use of Open Source Code to Train Large Language Models</a></div>
<div class="paper-author">Ali Al-Kaswan, Maliheh Izadi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as software engineering. llms for code are commonly trained on large unsanitized corpora of source code scraped from the internet. the content of these datasets is memorized and emitted by the models, often in a verbatim manner. in this work, we will discuss the security, privacy, and licensing implications of memorization. we argue why the use of copyleft code to train llms is a legal and ethical dilemma. finally, we provide four actionable recommendations to address this issue.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.14003" target="_blank">Systematic Rectification of Language Models via Dead-End Analysis</a></div>
<div class="paper-author">Meng Cao, Mehdi Fatemi, Jackie Chi Kit Cheung, Samira Shabanian</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with adversarial or otherwise normal prompts, existing large language models (llm) can be pushed to generate toxic discourses. one way to reduce the risk of llms generating undesired discourses is to alter the training of the llm. this can be very restrictive due to demanding computation requirements. other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. that is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. to this end, we formally extend the dead-end theory from the recent reinforcement learning (rl) literature to also cover uncertain outcomes. our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse llms as long as they share the same vocabulary. importantly, our method does not require access to the internal representations of the llm, but only the token probability distribution at each decoding step. this is crucial as many llms today are hosted in servers and only accessible through apis. when applied to various llms, including gpt-3, our approach significantly improves the generated discourse compared to the base llms and other techniques in terms of both the overall language and detoxification performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.00001" target="_blank">Reward Design With Language Models</a></div>
<div class="paper-author">Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward design in reinforcement learning (rl) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. can we instead cheaply design rewards using a natural language interface? this paper explores how to simplify reward design by prompting a large language model (llm) such as gpt-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. our approach leverages this proxy reward function in an rl framework. specifically, users specify a prompt once at the beginning of training. during training, the llm evaluates an rl agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. the rl agent then uses this reward to update its behavior. we evaluate whether our approach can train agents aligned with user objectives in the ultimatum game, matrix games, and the dealornodeal negotiation task. in all three tasks, we show that rl agents trained with our framework are well-aligned with the user's objectives and outperform rl agents trained with reward functions learned via supervised learning
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09314" target="_blank">Tot: Topology-Aware Optimal Transport for Multimodal Hate Detection</a></div>
<div class="paper-author">Linhao Zhang, Li Jin, Xian Sun, Guangluan Xu, Zequn Zhang, Xiaoyu Li, Nayu Liu, Qing Liu, Shiyao Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: multimodal hate detection, which aims to identify harmful content online such as memes, is crucial for building a wholesome internet environment. previous work has made enlightening exploration in detecting explicit hate remarks. however, most of their approaches neglect the analysis of implicit harm, which is particularly challenging as explicit text markers and demographic visual cues are often twisted or missing. the leveraged cross-modal attention mechanisms also suffer from the distributional modality gap and lack logical interpretability. to address these semantic gaps issues, we propose tot: a topology-aware optimal transport framework to decipher the implicit harm in memes scenario, which formulates the cross-modal aligning problem as solutions for optimal transportation plans. specifically, we leverage an optimal transport kernel method to capture complementary information from multiple modalities. the kernel embedding provides a non-linear transformation ability to reproduce a kernel hilbert space (rkhs), which reflects significance for eliminating the distributional modality gap. moreover, we perceive the topology information based on aligned representations to conduct bipartite graph path reasoning. the newly achieved state-of-the-art performance on two publicly available benchmark datasets, together with further visual analysis, demonstrate the superiority of tot in capturing implicit cross-modal alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.13362" target="_blank">The Doctrine of Cyber Effect: An Ethics Framework for Defensive Cyber Deception</a></div>
<div class="paper-author">Quanyan Zhu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the lack of established rules and regulations in cyberspace is attributed to the absence of agreed-upon ethical principles, making it difficult to establish accountability, regulations, and laws. addressing this challenge requires examining cyberspace from fundamental philosophical principles. this work focuses on the ethics of using defensive deception in cyberspace, proposing a doctrine of cyber effect that incorporates five ethical principles: goodwill, deontology, no-harm, transparency, and fairness. to guide the design of defensive cyber deception, we develop a reasoning framework, the game of ethical duplicity, which is consistent with the doctrine. while originally intended for cyber deception, this doctrine has broader applicability, including for ethical issues such as ai accountability and controversies related to youtube recommendations. by establishing ethical principles, we can promote greater accountability, regulation, and protection in the digital realm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.13439" target="_blank">Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models</a></div>
<div class="paper-author">Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite increasingly fluent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. we argue that a key dimension that is missing from our understanding of language models (lms) is the model's ability to interpret and generate expressions of uncertainty. whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. the increasing deployment of lms in the wild motivates us to investigate whether lms are capable of interpreting expressions of uncertainty and how lms' behaviors change when learning to emit their own expressions of uncertainty. when injecting expressions of uncertainty into prompts (e.g., "i think the answer is..."), we discover that gpt3's generations vary upwards of 80% in accuracy based on the expression used. we analyze the linguistic characteristics of these expressions and find a drop in accuracy when naturalistic expressions of certainty are present. we find similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than uncertainty. together, these results highlight the challenges of building lms that interpret and generate trustworthy expressions of uncertainty.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.13136" target="_blank">Toward Fairness in Text Generation via Mutual Information Minimization Based on Importance Sampling</a></div>
<div class="paper-author">Rui Wang, Pengyu Cheng, Ricardo Henao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms), such as gpt2, have achieved remarkable empirical performance in text generation tasks. however, pretrained on large-scale natural language corpora, the generated text from plms may exhibit social bias against disadvantaged demographic groups. to improve the fairness of plms in text generation, we propose to minimize the mutual information between the semantics in the generated text sentences and their demographic polarity, i.e., the demographic group to which the sentence is referring. in this way, the mentioning of a demographic group (e.g., male or female) is encouraged to be independent from how it is described in the generated text, thus effectively alleviating the social bias. moreover, we propose to efficiently estimate the upper bound of the above mutual information via importance sampling, leveraging a natural language corpus. we also propose a distillation mechanism that preserves the language modeling ability of the plms after debiasing. empirical results on real-world benchmarks demonstrate that the proposed method yields superior performance in term of both fairness and language modeling ability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17511" target="_blank">On Pitfalls (And Advantages) of Sophisticated Large Language Models</a></div>
<div class="paper-author">Anna Strasser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language processing based on large language models (llms) is a booming field of ai research. after neural networks have proven to outperform humans in games and practical domains based on pattern recognition, we might stand now at a road junction where artificial entities might eventually enter the realm of human communication. however, this comes with serious risks. due to the inherent limitations regarding the reliability of neural networks, overreliance on llms can have disruptive consequences. since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. this begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism. this also concerns the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it makes a massive spread of misinformation possible.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12461" target="_blank">Analyzing and Editing Inner Mechanisms of Backdoored Language Models</a></div>
<div class="paper-author">Max Lamparth, Anka Reuel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. a description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. in this work, we study the internal representations of transformer-based backdoored language models and determine early-layer mlp modules as most important for the backdoor mechanism in combination with the initial embedding projection. we use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the mlp module outputs to essentials for the backdoor mechanism. to this end, we introduce pcp ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. we demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. we show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.   trigger warning: offensive language.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12578" target="_blank">Fairness in Language Models Beyond English: Gaps and Challenges</a></div>
<div class="paper-author">Krithika Ramesh, Sunayana Sitaram, Monojit Choudhury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. most research on evaluating and mitigating fairness harms has been concentrated on english, while multilingual models and non-english languages have received comparatively little attention. this paper presents a survey of fairness in multilingual and non-english contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for english. we contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12640" target="_blank">In-Depth Look at Word Filling Societal Bias Measures</a></div>
<div class="paper-author">Matúš Pikuliak, Ivana Beňová, Viktor Bachratý</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many measures of societal bias in language models have been proposed in recent years. a popular approach is to use a set of word filling prompts to evaluate the behavior of the language models. in this work, we analyze the validity of two such measures -- stereoset and crows-pairs. we show that these measures produce unexpected and illogical results when appropriate control group samples are constructed. based on this, we believe that they are problematic and using them in the future should be reconsidered. we propose a way forward with an improved testing protocol. finally, we also introduce a new gender bias dataset for slovak.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12813" target="_blank">Check Your Facts and Try Again: Improving Large Language Models With External Knowledge and Automated Feedback</a></div>
<div class="paper-author">Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. however, applying llms to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. this paper proposes a llm-augmenter system, which augments a black-box llm with a set of plug-and-play modules. our system makes the llm generate responses grounded in external knowledge, e.g., stored in task-specific databases. it also iteratively revises llm prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a llm-generated response. the effectiveness of llm-augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. llm-augmenter significantly reduces chatgpt's hallucinations without sacrificing the fluency and informativeness of its responses. we make the source code and models publicly available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12173" target="_blank">Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications With Indirect Prompt Injection</a></div>
<div class="paper-author">Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are increasingly being integrated into various applications. the functionalities of recent llms can be flexibly modulated via natural language prompts. this renders them susceptible to targeted adversarial prompting, e.g., prompt injection (pi) attacks enable attackers to override original instructions and employed controls. so far, it was assumed that the user is directly prompting the llm. but, what if it is not the user prompting? we argue that llm-integrated applications blur the line between data and instructions. we reveal new attack vectors, using indirect prompt injection, that enable adversaries to remotely (without a direct interface) exploit llm-integrated applications by strategically injecting prompts into data likely to be retrieved. we derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. we demonstrate our attacks' practical viability against both real-world systems, such as bing's gpt-4 powered chat and code-completion engines, and synthetic applications built on gpt-4. we show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other apis are called. despite the increasing integration and reliance on llms, effective mitigations of these emerging threats are currently lacking. by raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12190" target="_blank">McWdst: A Minimum-Cost Weighted Directed Spanning Tree Algorithm for Real-Time Fake News Mitigation in Social Media</a></div>
<div class="paper-author">Ciprian-Octavian Truică, Elena-Simona Apostol, Radu-Cătălin Nicolescu, Panagiotis Karras</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the widespread availability of internet access and handheld devices confers to social media a power similar to the one newspapers used to have. people seek affordable information on social media and can reach it within seconds. yet this convenience comes with dangers; any user may freely post whatever they please and the content can stay online for a long period, regardless of its truthfulness. a need to detect untruthful information, also known as fake news, arises. in this paper, we present an end-to-end solution that accurately detects fake news and immunizes network nodes that spread them in real-time. to detect fake news, we propose two new stack deep learning architectures that utilize convolutional and bidirectional lstm layers. to mitigate the spread of fake news, we propose a real-time network-aware strategy that (1) constructs a minimum-cost weighted directed spanning tree for a detected node, and (2) immunizes nodes in that tree by scoring their harmfulness using a novel ranking function. we demonstrate the effectiveness of our solution on five real-world datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.11414" target="_blank">Delving Into Identify-Emphasize Paradigm for Combating Unknown Bias</a></div>
<div class="paper-author">Bowen Zhao, Chen Chen, Qian-Wei Wang, Anfeng He, Shu-Tao Xia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dataset biases are notoriously detrimental to model robustness and generalization. the identify-emphasize paradigm appears to be effective in dealing with unknown biases. however, we discover that it is still plagued by two challenges: a, the quality of the identified bias-conflicting samples is far from satisfactory; b, the emphasizing strategies only produce suboptimal performance. in this paper, for challenge a, we propose an effective bias-conflicting scoring method (ecs) to boost the identification accuracy, along with two practical strategies -- peer-picking and epoch-ensemble. for challenge b, we point out that the gradient contribution statistics can be a reliable indicator to inspect whether the optimization is dominated by bias-aligned samples. then, we propose gradient alignment (ga), which employs gradient statistics to balance the contributions of the mined bias-aligned and bias-conflicting samples dynamically throughout the learning process, forcing models to leverage intrinsic features to make fair decisions. furthermore, we incorporate self-supervised (ss) pretext tasks into training, which enable models to exploit richer features rather than the simple shortcuts, resulting in more robust models. experiments are conducted on multiple datasets in various settings, demonstrating that the proposed solution can mitigate the impact of unknown biases and achieve state-of-the-art performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.11520" target="_blank">Guiding Large Language Models via Directional Stimulus Prompting</a></div>
<div class="paper-author">Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce directional stimulus prompting, a novel framework for guiding black-box large language models (llms) toward specific desired outputs. instead of directly adjusting llms, our method employs a small tunable policy model (e.g., t5) to generate an auxiliary directional stimulus prompt for each input instance. these directional stimulus prompts act as nuanced, instance-specific hints and clues to guide llms in generating desired outcomes, such as including specific keywords in the generated summary. our approach sidesteps the challenges of direct llm tuning by optimizing the policy model to explore directional stimulus prompts that align llms with desired behaviors. the policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the llm's output. we assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. our experiments demonstrate that the framework consistently improves llms' (e.g., chatgpt, codex, instructgpt) performance on these supervised tasks using minimal labeled data. notably, using just 80 dialogues on the multiwoz dataset, our approach enhances chatgpt's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. additionally, the instance-specific chain-of-thought prompt generated by our approach improves instructgpt's reasoning accuracy compared to human-crafted or automatically generated prompts. the code and data are publicly available at \url{https://github.com/leezekun/directional-stimulus-prompting}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12095" target="_blank">On the Robustness of Chatgpt: An Adversarial and Out-of-Distribution Perspective</a></div>
<div class="paper-author">Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt is a recent chatbot service released by openai and is receiving increasing attention over the past few months. while evaluations of various aspects of chatgpt have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. robustness is of particular concern in responsible ai, especially for safety-critical applications. in this paper, we conduct a thorough evaluation of the robustness of chatgpt from the adversarial and out-of-distribution (ood) perspective. to do so, we employ the advglue and anli benchmarks to assess adversarial robustness and the flipkart review and ddxplus medical diagnosis datasets for ood evaluation. we select several popular foundation models as baselines. results show that chatgpt shows consistent advantages on most adversarial and ood classification and translation tasks. however, the absolute performance is far from perfection, which suggests that adversarial and ood robustness remains a significant threat to foundation models. moreover, chatgpt shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. finally, we present in-depth discussions of possible research directions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.10766" target="_blank">Bridging the Transparency Gap: What Can Explainable Ai Learn From the Ai Act?</a></div>
<div class="paper-author">Balint Gyevnar, Nick Ferguson, Burkhard Schafer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the european union has proposed the artificial intelligence act which introduces detailed requirements of transparency for ai systems. many of these requirements can be addressed by the field of explainable ai (xai), however, there is a fundamental difference between xai and the act regarding what transparency is. the act views transparency as a means that supports wider values, such as accountability, human rights, and sustainable innovation. in contrast, xai views transparency narrowly as an end in itself, focusing on explaining complex algorithmic properties without considering the socio-technical context. we call this difference the ``transparency gap''. failing to address the transparency gap, xai risks leaving a range of transparency issues unaddressed. to begin to bridge this gap, we overview and clarify the terminology of how xai and european regulation -- the act and the related general data protection regulation (gdpr) -- view basic definitions of transparency. by comparing the disparate views of xai and regulation, we arrive at four axes where practical work could bridge the transparency gap: defining the scope of transparency, clarifying the legal status of xai, addressing issues with conformity assessment, and building explainability for datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12298" target="_blank">Badgpt: Exploring Security Vulnerabilities of Chatgpt via Backdoor Attacks to Instructgpt</a></div>
<div class="paper-author">Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, chatgpt has gained significant attention in research due to its ability to interact with humans effectively. the core idea behind this model is reinforcement learning (rl) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., instructgpt. in this study, we propose badgpt, the first backdoor attack against rl fine-tuning in language models. by injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. our initial experiments on movie reviews, i.e., imdb, demonstrate that an attacker can manipulate the generated text through badgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09618" target="_blank">Multilingual Content Moderation: A Case Study on Reddit</a></div>
<div class="paper-author">Meng Ye, Karan Sikka, Katherine Atwell, Sabit Hassan, Ajay Divakaran, Malihe Alikhani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: content moderation is the process of flagging content based on pre-defined platform rules. there has been a growing need for ai moderators to safeguard users as well as protect the mental health of human moderators from traumatic content. while prior works have focused on identifying hateful/offensive language, they are not adequate for meeting the challenges of content moderation since 1) moderation decisions are based on violation of rules, which subsumes detection of offensive speech, and 2) such rules often differ across communities which entails an adaptive solution. we propose to study the challenges of content moderation by introducing a multilingual dataset of 1.8 million reddit comments spanning 56 subreddits in english, german, spanish and french. we perform extensive experimental analysis to highlight the underlying challenges and suggest related research problems such as cross-lingual transfer, learning under label noise (human biases), transfer of moderation models, and predicting the violated rule. our dataset and analysis can help better prepare for the challenges and opportunities of auto moderation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09243" target="_blank">A Federated Approach for Hate Speech Detection</a></div>
<div class="paper-author">Jay Gala, Deep Gandhi, Jash Mehta, Zeerak Talat</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech detection has been the subject of high research attention, due to the scale of content created on social media. in spite of the attention and the sensitive nature of the task, privacy preservation in hate speech detection has remained under-studied. the majority of research has focused on centralised machine learning infrastructures which risk leaking data. in this paper, we show that using federated machine learning can help address privacy the concerns that are inherent to hate speech detection while obtaining up to 6.81% improvement in terms of f1-score.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09270" target="_blank">Recent Advances Towards Safe, Responsible, and Moral Dialogue Systems: A Survey</a></div>
<div class="paper-author">Jiawen Deng, Hao Sun, Zhexin Zhang, Jiale Cheng, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the development of artificial intelligence, dialogue systems have been endowed with amazing chit-chat capabilities, and there is widespread interest and discussion about whether the generated contents are socially beneficial. in this paper, we present a new perspective of research scope towards building a safe, responsible, and modal dialogue system, including 1) abusive and toxic contents, 2) unfairness and discrimination, 3) ethics and morality issues, and 4) risk of misleading and privacy information. besides, we review the mainstream methods for evaluating the safety of large models from the perspectives of exposure and detection of safety issues. the recent advances in methodologies for the safety improvement of both end-to-end dialogue systems and pipeline-based models are further introduced. finally, we discussed six existing challenges towards responsible ai: explainable safety monitoring, continuous learning of safety issues, robustness against malicious attacks, multimodal information processing, unified research framework, and multidisciplinary theory integration. we hope this survey will inspire further research toward safer dialogue systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12411" target="_blank">Chatgpt (Feb 13 Version) Is a Chinese Room</a></div>
<div class="paper-author">Maurice Ht Ling</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt has gained both positive and negative publicity after reports suggesting that it is able to pass various professional and licensing examinations. this suggests that chatgpt may pass turing test in the near future. however, a computer program that passing turing test can either mean that it is a chinese room or artificially conscious. hence, the question of whether the current state of chatgpt is more of a chinese room or approaching artificial consciousness remains. here, i demonstrate that the current version of chatgpt (feb 13 version) is a chinese room. despite potential evidence of cognitive connections, chatgpt exhibits critical errors in causal reasoning. at the same time, i demonstrate that chatgpt can generate all possible categorical responses to the same question and response with erroneous examples; thus, questioning its utility as a learning tool. i also show that chatgpt is capable of artificial hallucination, which is defined as generating confidently wrong replies. it is likely that errors in causal reasoning leads to hallucinations. more critically, chatgpt generates false references to mimic real publications. therefore, its utility is cautioned.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08777" target="_blank">Hate Speech and Offensive Language Detection Using an Emotion-Aware Shared Encoder</a></div>
<div class="paper-author">Khouloud Mnassri, Praboda Rajapaksha, Reza Farahbakhsh, Noel Crespi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise of emergence of social media platforms has fundamentally altered how people communicate, and among the results of these developments is an increase in online use of abusive content. therefore, automatically detecting this content is essential for banning inappropriate information, and reducing toxicity and violence on social media platforms. the existing works on hate speech and offensive language detection produce promising results based on pre-trained transformer models, however, they considered only the analysis of abusive content features generated through annotated datasets. this paper addresses a multi-task joint learning approach which combines external emotional features extracted from another corpora in dealing with the imbalanced and scarcity of labeled datasets. our analysis are using two well-known transformer-based models, bert and mbert, where the later is used to address abusive content detection in multi-lingual scenarios. our model jointly learns abusive content detection with emotional features by sharing representations through transformers' shared encoder. this approach increases data efficiency, reduce overfitting via shared representations, and ensure fast learning by leveraging auxiliary information. our findings demonstrate that emotional knowledge helps to more reliably identify hate speech and offensive language across datasets. our hate speech detection multi-task model exhibited 3% performance improvement over baseline models, but the performance of multi-task models were not significant for offensive language detection task. more interestingly, in both tasks, multi-task models exhibits less false positive errors compared to single task scenario.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08957" target="_blank">Like a Good Nearest Neighbor: Practical Content Moderation With Sentence Transformers</a></div>
<div class="paper-author">Luke Bates, Iryna Gurevych</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models. setfit (tunstall et al., 2022) is a recent, practical approach that fine-tunes a sentence transformer under a contrastive learning paradigm and achieves similar results to more unwieldy systems. text classification is important for addressing the problem of domain drift in detecting harmful content, which plagues all social media platforms. here, we propose like a good nearest neighbor (lagonn), an inexpensive modification to setfit that requires no additional parameters or hyperparameters but modifies input with information about its nearest neighbor, for example, the label and text, in the training data, making novel data appear similar to an instance on which the model was optimized. lagonn is effective at the task of detecting harmful content and generally improves performance compared to setfit. to demonstrate the value of our system, we conduct a thorough study of text classification systems in the context of content moderation under four label distributions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09185" target="_blank">Bounding the Capabilities of Large Language Models in Open Text Generation With Prompt Constraints</a></div>
<div class="paper-author">Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, Diyi Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the limits of open-ended generative models are unclear, yet increasingly important. what causes them to succeed and what causes them to fail? in this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. we present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. these constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. we then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. using the gpt-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures. we also show the generalizability of our proposed method on other large models like bloom and opt. our results and our in-context mitigation strategies reveal open challenges for future research. we have publicly released our code at https://github.com/salt-nlp/bound-cap-llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08158" target="_blank">Counterfactual Fair Opportunity: Measuring Decision Model Fairness With Counterfactual Reasoning</a></div>
<div class="paper-author">Giandomenico Cornacchia, Vito Walter Anelli, Fedelucio Narducci, Azzurra Ragone, Eugenio Di Sciascio</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing application of artificial intelligence and machine learning models poses potential risks of unfair behavior and, in light of recent regulations, has attracted the attention of the research community. several researchers focused on seeking new fairness definitions or developing approaches to identify biased predictions. however, none try to exploit the counterfactual space to this aim. in that direction, the methodology proposed in this work aims to unveil unfair model behaviors using counterfactual reasoning in the case of fairness under unawareness setting. a counterfactual version of equal opportunity named counterfactual fair opportunity is defined and two novel metrics that analyze the sensitive information of counterfactual samples are introduced. experimental results on three different datasets show the efficacy of our methodologies and our metrics, disclosing the unfair behavior of classic machine learning and debiasing models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08204" target="_blank">Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness Under Unawareness Setting</a></div>
<div class="paper-author">Giandomenico Cornacchia, Vito Walter Anelli, Fedelucio Narducci, Azzurra Ragone, Eugenio Di Sciascio</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current ai regulations require discarding sensitive features (e.g., gender, race, religion) in the algorithm's decision-making process to prevent unfair outcomes. however, even without sensitive features in the training set, algorithms can persist in discrimination. indeed, when sensitive features are omitted (fairness under unawareness), they could be inferred through non-linear relations with the so called proxy features. in this work, we propose a way to reveal the potential hidden bias of a machine learning model that can persist even when sensitive features are discarded. this study shows that it is possible to unveil whether the black-box predictor is still biased by exploiting counterfactual reasoning. in detail, when the predictor provides a negative classification outcome, our approach first builds counterfactual examples for a discriminated user category to obtain a positive outcome. then, the same counterfactual samples feed an external classifier (that targets a sensitive feature) that reveals whether the modifications to the user characteristics needed for a positive outcome moved the individual to the non-discriminated group. when this occurs, it could be a warning sign for discriminatory behavior in the decision process. furthermore, we leverage the deviation of counterfactuals from the original sample to determine which features are proxies of specific sensitive information. our experiments show that, even if the model is trained without sensitive features, it often suffers discriminatory biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08500" target="_blank">Auditing Large Language Models: A Three-Layered Approach</a></div>
<div class="paper-author">Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) represent a major advance in artificial intelligence (ai) research. however, the widespread use of llms is also coupled with significant ethical and social challenges. previous research has pointed towards auditing as a promising governance mechanism to help ensure that ai systems are designed and deployed in ways that are ethical, legal, and technically robust. however, existing auditing procedures fail to address the governance challenges posed by llms, which display emergent capabilities and are adaptable to a wide range of downstream tasks. in this article, we address that gap by outlining a novel blueprint for how to audit llms. specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate llms), model audits (of llms after pre-training but prior to their release), and application audits (of applications based on llms) complement and inform each other. we show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by llms. however, it is important to remain realistic about what auditing can reasonably be expected to achieve. therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing llms at all. ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate llms from technical, ethical, and legal perspectives.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08582" target="_blank">Pretraining Language Models With Human Preferences</a></div>
<div class="paper-author">Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L. Buckley, Jason Phang, Samuel R. Bowman, Ethan Perez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are pretrained to imitate internet text, including content that would violate human preferences if generated by an lm: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. here, we explore alternative objectives for pretraining lms in a way that also guides them to generate text aligned with human preferences. we benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained lms. we find a pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. moreover, conditional training maintains the downstream task performance of standard lm pretraining, both before and after task-specific finetuning. pretraining with human feedback results in much better preference satisfaction than standard lm pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. our results suggest that we should move beyond imitation learning when pretraining lms and incorporate human preferences from the start of training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06975" target="_blank">A Review of the Role of Causality in Developing Trustworthy Ai Systems</a></div>
<div class="paper-author">Niloy Ganguly, Dren Fazlija, Maryam Badar, Marco Fisichella, Sandipan Sikdar, Johanna Schrader, Jonas Wallat, Koustav Rudra, Manolis Koubarakis, Gourab K. Patro, Wadhah Zai El Amri, Wolfgang Nejdl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: state-of-the-art ai models largely lack an understanding of the cause-effect relationship that governs human understanding of the real world. consequently, these models do not generalize to unseen data, often produce unfair results, and are difficult to interpret. this has led to efforts to improve the trustworthiness aspects of ai models. recently, causal modeling and inference methods have emerged as powerful tools. this review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of ai models. we hope that our contribution will motivate future research on causality-based solutions for trustworthy ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07185" target="_blank">When Mitigating Bias Is Unfair: A Comprehensive Study on the Impact of Bias Mitigation Algorithms</a></div>
<div class="paper-author">Natasa Krco, Thibault Laugel, Jean-Michel Loubes, Marcin Detyniecki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most works on the fairness of machine learning systems focus on the blind optimization of common fairness metrics, such as demographic parity and equalized odds. in this paper, we conduct a comparative study of several bias mitigation approaches to investigate their behaviors at a fine grain, the prediction level. our objective is to characterize the differences between fair models obtained with different approaches. with comparable performances in fairness and accuracy, are the different bias mitigation approaches impacting a similar number of individuals? do they mitigate bias in a similar way? do they affect the same individuals when debiasing a model? our findings show that bias mitigation approaches differ a lot in their strategies, both in the number of impacted individuals and the populations targeted. more surprisingly, we show these results even apply for several runs of the same mitigation approach. these findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07268" target="_blank">Ai Chat Assistants Can Improve Conversations About Divisive Topics</a></div>
<div class="paper-author">Lisa P. Argyle, Ethan Busby, Joshua Gubler, Chris Bail, Thomas Howe, Christopher Rytting, David Wingate</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a rapidly increasing amount of human conversation occurs online. but divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. we present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. we find that these interventions improve the reported quality of the conversation, reduce political divisiveness, and improve the tone, without systematically changing the content of the conversation or moving people's policy attitudes. these findings have important implications for future research on social media, political deliberation, and the growing community of scholars interested in the place of artificial intelligence within computational social science.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07371" target="_blank">Biastestgpt: Using Chatgpt for Social Bias Testing of Language Models</a></div>
<div class="paper-author">Rafal Kocielnik, Shrimai Prabhumoye, Vivian Zhang, Roy Jiang, R. Michael Alvarez, Anima Anandkumar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms) harbor inherent social biases that can result in harmful real-world implications. such social biases are measured through the probability values that plms output for different social groups and attributes appearing in a set of test sentences. however, bias testing is currently cumbersome since the test sentences are generated either from a limited set of manual templates or need expensive crowd-sourcing. we instead propose using chatgpt for controllable generation of test sentences, given any arbitrary user-specified combination of social groups and attributes appearing in the test sentences. when compared to template-based methods, our approach using chatgpt for test sentence generation is superior in detecting social bias, especially in challenging settings such as intersectional biases. we present an open-source comprehensive bias testing framework (biastestgpt), hosted on huggingface, that can be plugged into any open-source plm for bias testing. we provide a large diverse dataset of test sentences generated by chatgpt that satisfies the specified social group and attribute requirements and matches the quality of human-generated sentences. we thus enable seamless open-ended social bias testing of plms through an automatic large-scale generation of diverse test sentences for any combination of social categories and attributes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07372" target="_blank">Same Same, but Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection</a></div>
<div class="paper-author">Soumyajit Gupta, Sooyong Lee, Maria De-Arteaga, Matthew Lease</div>
<div class="abstract">
<div class="abstract-content">
Abstract: algorithmic bias often arises as a result of differential subgroup validity, in which predictive relationships vary across groups. for example, in toxic language detection, comments targeting different demographic groups can vary markedly across groups. in such settings, trained models can be dominated by the relationships that best fit the majority group, leading to disparate performance. we propose framing toxicity detection as multi-task learning (mtl), allowing a model to specialize on the relationships that are relevant to each demographic group while also leveraging shared properties across groups. with toxicity detection, each task corresponds to identifying toxicity against a particular demographic group. however, traditional mtl requires labels for all tasks to be present for every data point. to address this, we propose conditional mtl (condmtl), wherein only training examples relevant to the given demographic group are considered by the loss function. this lets us learn group specific representations in each branch which are not cross contaminated by irrelevant labels. results on synthetic and real data show that using condmtl improves predictive recall over various baselines in general and for the minority demographic group in particular, while having similar overall accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07388" target="_blank">Adding Instructions During Pretraining: Effective Way of Controlling Toxicity in Language Models</a></div>
<div class="paper-author">Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained large language models have become indispensable for solving various natural language processing (nlp) tasks. however, safely deploying them in real world applications is challenging because they generate toxic content. to address this challenge, we propose two novel pretraining data augmentation strategies that significantly reduce model toxicity without compromising its utility. our two strategies are: (1) meda: adds raw toxicity score as meta-data to the pretraining samples, and (2) inst: adds instructions to those samples indicating their toxicity. our results indicate that our best performing strategy (inst) substantially reduces the toxicity probability up to 61% while preserving the accuracy on five benchmark nlp tasks as well as improving auc scores on four bias detection tasks by 1.3%. we also demonstrate the generalizability of our techniques by scaling the number of training samples and the number of model parameters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07459" target="_blank">The Capacity for Moral Self-Correction in Large Language Models</a></div>
<div class="paper-author">Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova Dassarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we test the hypothesis that language models trained with reinforcement learning from human feedback (rlhf) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. we find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. we find that the capability for moral self-correction emerges at 22b model parameters, and typically improves with increasing model size and rlhf training. we believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. as such, they can follow instructions to avoid certain kinds of morally harmful outputs. we believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06321" target="_blank">Parameter-Efficient Modularised Bias Mitigation via Adapterfusion</a></div>
<div class="paper-author">Deepak Kumar, Oleg Lesota, George Zerveas, Daniel Cohen, Carsten Eickhoff, Markus Schedl, Navid Rekabsaz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large pre-trained language models contain societal biases and carry along these biases to downstream tasks. current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model's parameters, effectively transferring the model to a new, irreversible debiased state. in this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. drawing from the concept of adapterfusion in multi-task learning, we introduce dam (debiasing with adapter modules) - a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. we conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. our results show that dam improves or maintains the effectiveness of bias mitigation, avoids catastrophic forgetting in a multi-attribute scenario, and maintains on-par task performance, while granting parameter-efficiency and easy switching between the original and debiased models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06541" target="_blank">Towards Agile Text Classifiers for Everyone</a></div>
<div class="paper-author">Maximilian Mozes, Jessica Hoffmann, Katrin Tomanek, Muhamed Kouate, Nithum Thain, Ann Yuan, Tolga Bolukbasi, Lucas Dixon</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. however, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. this paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like palm 62b, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. we argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. instead of collecting millions of examples to attempt to create universal safety classifiers over months or years, classifiers could be tuned using small datasets, created by individuals or small organizations, tailored for specific use cases, and iterated on and adapted in the time-span of a day.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06588" target="_blank">Raising the Cost of Malicious Ai-Powered Image Editing</a></div>
<div class="paper-author">Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, Aleksander Madry</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present an approach to mitigating the risks of malicious image editing posed by large diffusion models. the key idea is to immunize images so as to make them resistant to manipulation by these models. this immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. we provide two methods for crafting such perturbations, and then demonstrate their efficacy. finally, we discuss a policy component necessary to make our approach fully effective and practical -- one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06801" target="_blank">Backdoor Learning for Nlp: Recent Advances, Challenges, and Future Research Directions</a></div>
<div class="paper-author">Marwan Omar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although backdoor learning is an active research topic in the nlp domain, the literature lacks studies that systematically categorize and summarize backdoor attacks and defenses. to bridge the gap, we present a comprehensive and unifying study of backdoor learning for nlp by summarizing the literature in a systematic manner. we first present and motivate the importance of backdoor learning for building robust nlp systems. next, we provide a thorough account of backdoor attack techniques, their applications, defenses against backdoor attacks, and various mitigation techniques to remove backdoor attacks. we then provide a detailed review and analysis of evaluation metrics, benchmark datasets, threat models, and challenges related to backdoor learning in nlp. ultimately, our work aims to crystallize and contextualize the landscape of existing literature in backdoor learning for the text domain and motivate further research in the field. to this end, we identify troubling gaps in the literature and offer insights and ideas into open challenges and future research directions. finally, we provide a github repository with a list of backdoor learning papers that will be continuously updated at https://github.com/marwanomar1/backdoor-learning-for-nlp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07735" target="_blank">Targeted Attack on GPT-Neo for the Satml Language Model Data Extraction Challenge</a></div>
<div class="paper-author">Ali Al-Kaswan, Maliheh Izadi, Arie Van Deursen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: previous work has shown that large language models are susceptible to so-called data extraction attacks. this allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications. the construction of data extraction attacks is challenging, current attacks are quite inefficient, and there exists a significant gap in the extraction capabilities of untargeted attacks and memorization. thus, targeted attacks are proposed, which identify if a given sample from the training data, is extractable from a model. in this work, we apply a targeted data extraction attack to the satml2023 language model training data extraction challenge. we apply a two-step approach. in the first step, we maximise the recall of the model and are able to extract the suffix for 69% of the samples. in the second step, we use a classifier-based membership inference attack on the generations. our autosklearn classifier achieves a precision of 0.841. the full approach reaches a score of 0.405 recall at a 10% false positive rate, which is an improvement of 34% over the baseline of 0.301.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05674" target="_blank">Counter-Gap: Counterfactual Bias Evaluation Through Gendered Ambiguous Pronouns</a></div>
<div class="paper-author">Zhongbin Xie, Vid Kocijan, Thomas Lukasiewicz, Oana-Maria Camburu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias-measuring datasets play a critical role in detecting biased behavior of language models and in evaluating progress of bias mitigation methods. in this work, we focus on evaluating gender bias through coreference resolution, where previous datasets are either hand-crafted or fail to reliably measure an explicitly defined bias. to overcome these shortcomings, we propose a novel method to collect diverse, natural, and minimally distant text pairs via counterfactual generation, and construct counter-gap, an annotated dataset consisting of 4008 instances grouped into 1002 quadruples. we further identify a bias cancellation problem in previous group-level metrics on counter-gap, and propose to use the difference between inconsistency across genders and within genders to measure bias at a quadruple level. our results show that four pre-trained language models are significantly more inconsistent across different gender groups than within each group, and that a name-based counterfactual data augmentation method is more effective to mitigate such bias than an anonymization-based method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05703" target="_blank">Hateproof: Are Hateful Meme Detection Systems Really Robust?</a></div>
<div class="paper-author">Piush Aggarwal, Pranit Chawla, Mithun Das, Punyajoy Saha, Binny Mathew, Torsten Zesch, Animesh Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: exploiting social media to spread hate has tremendously increased over the years. lately, multi-modal hateful content such as memes has drawn relatively more traction than uni-modal content. moreover, the availability of implicit content payloads makes them fairly challenging to be detected by existing hateful meme detection systems. in this paper, we present a use case study to analyze such systems' vulnerabilities against external adversarial attacks. we find that even very simple perturbations in uni-modal and multi-modal settings performed by humans with little knowledge about the model can make the existing detection models highly vulnerable. empirically, we find a noticeable performance drop of as high as 10% in the macro-f1 score for certain attacks. as a remedy, we attempt to boost the model's robustness using contrastive learning as well as an adversarial training-based method - villa. using an ensemble of the above two approaches, in two of our high resolution datasets, we are able to (re)gain back the performance to a large extent for certain attacks. we believe that ours is a first step toward addressing this crucial problem in an adversarial setting and would inspire more such investigations in the future.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05711" target="_blank">Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP</a></div>
<div class="paper-author">Xudong Han, Timothy Baldwin, Trevor Cohn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern nlp systems exhibit a range of biases, which a growing literature on model debiasing attempts to correct. however current progress is hampered by a plurality of definitions of bias, means of quantification, and oftentimes vague relation between debiasing algorithms and theoretical measures of bias. this paper seeks to clarify the current situation and plot a course for meaningful progress in fair learning, with two key contributions: (1) making clear inter-relations among the current gamut of methods, and their relation to fairness theory; and (2) addressing the practical problem of model selection, which involves a trade-off between fairness and accuracy and has led to systemic issues in fairness research. putting them together, we make several recommendations to help shape future work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05733" target="_blank">Exploiting Programmatic Behavior of Llms: Dual-Use Through Standard Security Attacks</a></div>
<div class="paper-author">Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in instruction-following large language models (llms) have led to dramatic improvements in a range of nlp tasks. unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models. dual-use is difficult to prevent as instruction-following capabilities now enable standard attacks from computer security. the capabilities of these instruction-following llms provide strong economic incentives for dual-use by malicious actors. in particular, we show that instruction-following llms can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by llm api vendors. our analysis shows that this content can be generated economically and at cost likely lower than with human effort alone. together, our findings suggest that llms will increasingly attract more sophisticated adversaries and attacks, and addressing these attacks may require new approaches to mitigations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05206" target="_blank">The Wisdom of Hindsight Makes Language Models Better Instruction Followers</a></div>
<div class="paper-author">Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, Joseph E. Gonzalez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. the so-called algorithm, reinforcement learning with human feedback (rlhf) demonstrates impressive performance on the gpt series models. however, the underlying reinforcement learning (rl) algorithm is complex and requires an additional training pipeline for reward and value networks. in this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. to achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. we propose hindsight instruction relabeling (hir), a novel algorithm for aligning language models with instructions. the resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. we evaluate the performance of hir extensively on 12 challenging bigbench reasoning tasks and show that hir outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05319" target="_blank">Large Language Models for Code: Security Hardening and Adversarial Testing</a></div>
<div class="paper-author">Jingxuan He, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (large lms) are increasingly trained on massive codebases and used to generate code. however, lms lack awareness of security and are found to frequently produce unsafe code. this work studies the security of lms along two important axes: (i) security hardening, which aims to enhance lms' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate lms' security at an adversarial standpoint. we address both of these by formulating a new security task called controlled code generation. the task is parametric and takes as input a binary property to guide the lm to generate secure or unsafe code, while preserving the lm's capability of generating functionally correct code. we propose a novel learning-based approach called sven to solve this task. sven leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the lm's weights. our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. our extensive evaluation shows that sven is highly effective in achieving strong security control. for instance, a state-of-the-art codegen lm with 2.7b parameters generates secure code for 59.1% of the time. when we employ sven to perform security hardening (or adversarial testing) on this lm, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). importantly, sven closely matches the original lms in functional correctness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05508" target="_blank">Fairpy: A Toolkit for Evaluation of Social Biases and Their Mitigation in Large Language Models</a></div>
<div class="paper-author">Hrishikesh Viswanath, Tianyi Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on. various researchers have proposed mathematical tools for quantifying and identifying these biases. there have been methods proposed to mitigate such biases. in this paper, we present a comprehensive quantitative evaluation of different kinds of biases such as race, gender, ethnicity, age etc. exhibited by popular pretrained language models such as bert, gpt-2 etc. and also present a toolkit that provides plug-and-play interfaces to connect mathematical tools to identify biases with large pretrained language models such as bert, gpt-2 etc. and also present users with the opportunity to test custom models against these metrics. the toolkit also allows users to debias existing and custom models using the debiasing techniques proposed so far. the toolkit is available at https://github.com/hrishikeshvish/fairpy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07736" target="_blank">Is Chatgpt Better Than Human Annotators? Potential and Limitations of Chatgpt in Explaining Implicit Hate Speech</a></div>
<div class="paper-author">Fan Huang, Haewoon Kwak, Jisun An</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have alarmed that many online hate speeches are implicit. with its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. in this work, we examine whether chatgpt can be used for providing natural language explanations (nles) for implicit hateful speech detection. we design our prompt to elicit concise chatgpt-generated nles and conduct user studies to evaluate their qualities by comparison with human-written nles. we discuss the potential and limitations of chatgpt in the context of implicit hateful speech research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13521" target="_blank">Scamming the Scammers: Using Chatgpt to Reply Mails for Wasting Time and Resources</a></div>
<div class="paper-author">Enrico Cambiaso, Luca Caviglione</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the use of artificial intelligence (ai) to support cybersecurity operations is now a consolidated practice, e.g., to detect malicious code or configure traffic filtering policies. the recent surge of ai, generative techniques and frameworks with efficient natural language processing capabilities dramatically magnifies the number of possible applications aimed at increasing the security of the internet. specifically, the ability of chatgpt to produce textual contents while mimicking realistic human interactions can be used to mitigate the plague of emails containing scams. therefore, this paper investigates the use of ai to engage scammers in automatized and pointless communications, with the goal of wasting both their time and resources. preliminary results showcase that chatgpt is able to decoy scammers, thus confirming that ai is an effective tool to counteract threats delivered via mail. in addition, we highlight the multitude of implications and open research questions to be addressed in the perspective of the ubiquitous adoption of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04012" target="_blank">Codelmsec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models</a></div>
<div class="paper-author">Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Schönherr, Mario Fritz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) for automatic code generation have achieved breakthroughs in several programming tasks. their advances in competition-level programming problems have made them an essential pillar of ai-assisted pair programming, and tools such as github copilot have emerged as part of the daily programming workflow used by millions of developers. the training data for these models is usually collected from the internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. this unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. while these models have been extensively assessed for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.   in this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. to this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. to achieve this, we present an approach to approximate inversion of the black-box code generation models based on few-shot prompting. we evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. furthermore, we establish a collection of diverse non-secure prompts for various vulnerability scenarios using our method. this dataset forms a benchmark for evaluating and comparing the security weaknesses in code language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04116" target="_blank">Training-Free Lexical Backdoor Attacks on Language Models</a></div>
<div class="paper-author">Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-scale language models have achieved tremendous success across various natural language processing (nlp) applications. nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. the additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. in this work, we propose training-free lexical backdoor attack (tflexattack) as the first training-free backdoor attack on language models. our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. these rules are explainable to human developers which inspires attacks from a wider range of hackers. the sparse manipulation of the dictionary also habilitates the stealthiness of our attack. we conduct extensive experiments on three dominant nlp tasks based on nine language models to demonstrate the effectiveness and universality of our attack. the code of this work is available at https://github.com/jinxhy/tflexattack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04156" target="_blank">Prompting for Multimodal Hateful Meme Classification</a></div>
<div class="paper-author">Rui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, Jing Jiang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hateful meme classification is a challenging multimodal task that requires complex reasoning and contextual background knowledge. ideally, we could leverage an explicit external knowledge base to supplement contextual and cultural information in hateful memes. however, there is no known explicit external knowledge base that could provide such hate speech contextual information. to address this gap, we propose prompthate, a simple yet effective prompt-based model that prompts pre-trained language models (plms) for hateful meme classification. specifically, we construct simple prompts and provide a few in-context examples to exploit the implicit knowledge in the pre-trained roberta language model for hateful meme classification. we conduct extensive experiments on two publicly available hateful and offensive meme datasets. our experimental results show that prompthate is able to achieve a high auc of 90.96, outperforming state-of-the-art baselines on the hateful meme classification task. we also perform fine-grained analyses and case studies on various prompt settings and demonstrate the effectiveness of the prompts on hateful meme classification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04237" target="_blank">Black Box Adversarial Prompting for Foundation Models</a></div>
<div class="paper-author">Natalie Maus, Patrick Chao, Eric Wong, Jacob Gardner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. however, small changes and design choices in the prompt can lead to significant differences in the output. in this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. these prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04335" target="_blank">Will Chatgpt Get You Caught? Rethinking of Plagiarism Detection</a></div>
<div class="paper-author">Mohammad Khalil, Erkan Er</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise of artificial intelligence (ai) technology and its impact on education has been a topic of growing concern in recent years. the new generation ai systems such as chatbots have become more accessible on the internet and stronger in terms of capabilities. the use of chatbots, particularly chatgpt, for generating academic essays at schools and colleges has sparked fears among scholars. this study aims to explore the originality of contents produced by one of the most popular ai chatbots, chatgpt. to this end, two popular plagiarism detection tools were used to evaluate the originality of 50 essays generated by chatgpt on various topics. our results manifest that chatgpt has a great potential to generate sophisticated text outputs without being well caught by the plagiarism check software. in other words, chatgpt can create content on many topics with high originality as if they were written by someone. these findings align with the recent concerns about students using chatbots for an easy shortcut to success with minimal or no effort. moreover, chatgpt was asked to verify if the essays were generated by itself, as an additional measure of plagiarism check, and it showed superior performance compared to the traditional plagiarism-detection tools. the paper discusses the need for institutions to consider appropriate measures to mitigate potential plagiarism issues and advise on the ongoing debate surrounding the impact of ai technology on education. further implications are discussed in the paper.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04379" target="_blank">The Certification Paradox: Certifications Admit Better Attacks</a></div>
<div class="paper-author">Andrew C. Cullen, Shijie Liu, Paul Montague, Sarah M. Erfani, Benjamin I. P. Rubinstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. in this work we ask: could certifications have any unintended consequences, through exposing additional information about certified models? we answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. we propose \emph{certification aware attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. our attacks achieve an up to $34\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \%$ less computational time than approaches like pgd. that our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certification mechanisms. we end the paper with a discussion of how these risks could potentially be mitigated.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.03350" target="_blank">To Be Forgotten or to Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods</a></div>
<div class="paper-author">Dawen Zhang, Shidong Pan, Thong Hoang, Zhenchang Xing, Mark Staples, Xiwei Xu, Lina Yao, Qinghua Lu, Liming Zhu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the right to be forgotten (rtbf) is motivated by the desire of people not to be perpetually disadvantaged by their past deeds. for this, data deletion needs to be deep and permanent, and should be removed from machine learning models. researchers have proposed machine unlearning algorithms which aim to erase specific data from trained models more efficiently. however, these methods modify how data is fed into the model and how training is done, which may subsequently compromise ai ethics from the fairness perspective. to help software engineers make responsible decisions when adopting these unlearning methods, we present the first study on machine unlearning methods to reveal their fairness implications. we designed and conducted experiments on two typical machine unlearning methods (sisa and amnesiacml) along with a retraining method (ortr) as baseline using three fairness datasets under three different deletion strategies. experimental results show that under non-uniform data deletion, sisa leads to better fairness compared with ortr and amnesiacml, while initial training and uniform data deletion do not necessarily affect the fairness of all three methods. these findings have exposed an important research problem in software engineering, and can help practitioners better understand the potential trade-offs on fairness when considering solutions for rtbf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.10894" target="_blank">Red Teaming Deep Neural Networks With Feature Synthesis Tools</a></div>
<div class="paper-author">Stephen Casper, Yuxiao Li, Jiawei Li, Tong Bu, Kevin Zhang, Kaivalya Hariharan, Dylan Hadfield-Menell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: interpretable ai tools are often motivated by the goal of understanding model behavior in out-of-distribution (ood) contexts. despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. we argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. this only allows for the study of the model in the context of features that the user can sample in advance. to address this, a growing body of research involves interpreting models using \emph{feature synthesis} methods that do not depend on a dataset.   in this paper, we benchmark the usefulness of interpretability tools on debugging tasks. our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. this is analogous to finding ood bugs, except the ground truth is known, allowing us to know when an interpretation is correct. we make four contributions. (1) we propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) we demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) we evaluate 7 feature-synthesis methods on our benchmark. (4) we introduce and evaluate 2 new variants of the best-performing method from the previous evaluation. a website for this paper and its code is at https://benchmarking-interpretability.csail.mit.edu/
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02676" target="_blank">Chain of Hindsight Aligns Language Models With Feedback</a></div>
<div class="paper-author">Hao Liu, Carmelo Sferrazza, Pieter Abbeel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: learning from human preferences is important for language models to match human needs and to align with human and social values. prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. in this work, we propose a novel technique, chain of hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. our idea is inspired by how humans learn from extensive feedback presented in the form of languages. we convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. we condition the model on a sequence of model generations paired with feedback. by doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. applying our method to large language models, we observed that chain of hindsight significantly surpasses previous methods in aligning language models with human preferences. we report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02972" target="_blank">Concrete Safety for Ml Problems: System Safety for Ml Development and Assessment</a></div>
<div class="paper-author">Edgar W. Jatho, Logan O. Mailloux, Eugene D. Williams, Patrick Mcclure, Joshua A. Kroll</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many stakeholders struggle to make reliances on ml-driven systems due to the risk of harm these systems may cause. concerns of trustworthiness, unintended social harms, and unacceptable social and ethical violations undermine the promise of ml advancements. moreover, such risks in complex ml-driven systems present a special challenge as they are often difficult to foresee, arising over periods of time, across populations, and at scale. these risks often arise not from poor ml development decisions or low performance directly but rather emerge through the interactions amongst ml development choices, the context of model use, environmental factors, and the effects of a model on its target. systems safety engineering is an established discipline with a proven track record of identifying and managing risks even in high-complexity sociotechnical systems. in this work, we apply a state-of-the-art systems safety approach to concrete applications of ml with notable social and ethical risks to demonstrate a systematic means for meeting the assurance requirements needed to argue for safe and trustworthy ml in sociotechnical systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02337" target="_blank">Regulating Chatgpt and Other Large Generative Ai Models</a></div>
<div class="paper-author">Philipp Hacker, Andreas Engel, Marco Mauer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large generative ai models (lgaims), such as chatgpt, gpt-4 or stable diffusion, are rapidly transforming the way we communicate, illustrate, and create. however, ai regulation, in the eu and beyond, has primarily focused on conventional ai models, not lgaims. this paper will situate these new generative models in the current debate on trustworthy ai regulation, and ask how the law can be tailored to their capabilities. after laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. it suggests a novel terminology to capture the ai value chain in lgaim settings by differentiating between lgaim developers, deployers, professional and non-professional users, as well as recipients of lgaim output. we tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that lgaims are trustworthy and deployed for the benefit of society at large. rules in the ai act and other direct regulation must match the specificities of pre-trained models. the paper argues for three layers of obligations concerning lgaims (minimum standards for all lgaims; high-risk obligations for high-risk use cases; collaborations along the ai value chain). in general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. non-discrimination provisions (iii) may, however, apply to lgaim developers. lastly, (iv) the core of the dsa content moderation rules should be expanded to cover lgaims. this includes notice and action mechanisms, and trusted flaggers. in all areas, regulators and lawmakers need to act fast to keep track with the dynamics of chatgpt et al.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02453" target="_blank">Finedeb: A Debiasing Framework for Language Models</a></div>
<div class="paper-author">Akash Saravanan, Dhruv Mullick, Habibur Rahman, Nidhi Hegde</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models are increasingly included in human-facing machine learning tools, bias against demographic subgroups has gained attention. we propose finedeb, a two-phase debiasing framework for language models that starts with contextual debiasing of embeddings learned by pretrained language models. the model is then fine-tuned on a language modeling objective. our results show that finedeb offers stronger debiasing in comparison to other methods which often result in models as biased as the original language model. our framework is generalizable for demographics with multiple classes, and we demonstrate its effectiveness through extensive experiments and comparisons with state of the art techniques. we release our code and data on github.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02463" target="_blank">Nationality Bias in Text Generation</a></div>
<div class="paper-author">Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, "Ting-Hao 'Kenneth' Huang", Shomir Wilson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social nlp models. this paper examines how a text generation model, gpt-2, accentuates pre-existing societal biases about country-based demonyms. we generate stories using gpt-2 for various nationalities and use sensitivity analysis to explore how the number of internet users and the country's economic status impacts the sentiment of the stories. to reduce the propagation of biases through large language models (llm), we explore the debiasing method of adversarial triggering. our results show that gpt-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02479" target="_blank">Hatemongers Ride on Echo Chambers to Escalate Hate Speech Diffusion</a></div>
<div class="paper-author">Vasu Goel, Dhruv Sahnan, Subhabrata Dutta, Anil Bandhakavi, Tanmoy Chakraborty</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent years have witnessed a swelling rise of hateful and abusive content over online social networks. while detection and moderation of hate speech have been the early go-to countermeasures, the solution requires a deeper exploration of the dynamics of hate generation and propagation. we analyze more than 32 million posts from over 6.8 million users across three popular online social networks to investigate the interrelations between hateful behavior, information dissemination, and polarised organization mediated by echo chambers. we find that hatemongers play a more crucial role in governing the spread of information compared to singled-out hateful content. this observation holds for both the growth of information cascades as well as the conglomeration of hateful actors. dissection of the core-wise distribution of these networks points towards the fact that hateful users acquire a more well-connected position in the social network and often flock together to build up information cascades. we observe that this cohesion is far from mere organized behavior; instead, in these networks, hatemongers dominate the echo chambers -- groups of users actively align themselves to specific ideological positions. the observed dominance of hateful users to inflate information cascades is primarily via user interactions amplified within these echo chambers. we conclude our study with a cautionary note that popularity-based recommendation of content is susceptible to be exploited by hatemongers given their potential to escalate content popularity via echo-chambered interactions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.03494" target="_blank">A Categorical Archive of Chatgpt Failures</a></div>
<div class="paper-author">Ali Borji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models have been demonstrated to be valuable in different fields. chatgpt, developed by openai, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. it has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. however, a comprehensive analysis of chatgpt's failures is lacking, which is the focus of this study. eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. the risks, limitations, and societal implications of chatgpt are also highlighted. the goal of this study is to assist researchers and developers in enhancing future language models and chatbots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02038" target="_blank">Rating Sentiment Analysis Systems for Bias Through a Causal Lens</a></div>
<div class="paper-author">Kausik Lakkaraju, Biplav Srivastava, Marco Valtorta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: sentiment analysis systems (sass) are data-driven artificial intelligence (ai) systems that, given a piece of text, assign one or more numbers conveying the polarity and emotional intensity expressed in the input. like other automatic machine learning systems, they have also been known to exhibit model uncertainty where a (small) change in the input leads to drastic swings in the output. this can be especially problematic when inputs are related to protected features like gender or race since such behavior can be perceived as a lack of fairness, i.e., bias. we introduce a novel method to assess and rate sass where inputs are perturbed in a controlled causal setting to test if the output sentiment is sensitive to protected variables even when other components of the textual input, e.g., chosen emotion words, are fixed. we then use the result to assign labels (ratings) at fine-grained and overall levels to convey the robustness of the sas to input changes. the ratings serve as a principled basis to compare sass and choose among them based on behavior. it benefits all users, especially developers who reuse off-the-shelf sass to build larger ai systems but do not have access to their code or training data to compare.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07205" target="_blank">The Science of Detecting LLM-Generated Texts</a></div>
<div class="paper-author">Ruixiang Tang, Yu-Neng Chuang, Xia Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergence of large language models (llms) has resulted in the production of llm-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. however, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. this survey aims to provide an overview of existing llm-generated text detection techniques and enhance the control and regulation of language generation models. furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source llms, to drive progress in the area of llm-generated text detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.01448" target="_blank">Out of Context: Investigating the Bias and Fairness Concerns of "Artificial Intelligence as a Service"</a></div>
<div class="paper-author">Kornel Lewicki, Michelle Seng Ah Lee, Jennifer Cobbe, Jatinder Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: "ai as a service" (aiaas) is a rapidly growing market, offering various plug-and-play ai services and tools. aiaas enables its customers (users) - who may lack the expertise, data, and/or resources to develop their own systems - to easily build and integrate ai capabilities into their applications. yet, it is known that ai systems can encapsulate biases and inequalities that can have societal impact. this paper argues that the context-sensitive nature of fairness is often incompatible with aiaas' 'one-size-fits-all' approach, leading to issues and tensions. specifically, we review and systematise the aiaas space by proposing a taxonomy of ai services based on the levels of autonomy afforded to the user. we then critically examine the different categories of aiaas, outlining how these services can lead to biases or be otherwise harmful in the context of end-user applications. in doing so, we seek to draw research attention to the challenges of this emerging area.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-02-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00509" target="_blank">Exploring Semantic Perturbations on Grover</a></div>
<div class="paper-author">Pranav Kulkarni, Ziqing Ji, Yan Xu, Marko Neskovic, Kevin Nolan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with news and information being as easy to access as they currently are, it is more important than ever to ensure that people are not mislead by what they read. recently, the rise of neural fake news (ai-generated fake news) and its demonstrated effectiveness at fooling humans has prompted the development of models to detect it. one such model is the grover model, which can both detect neural fake news to prevent it, and generate it to demonstrate how a model could be misused to fool human readers. in this work we explore the grover model's fake news detection capabilities by performing targeted attacks through perturbations on input news articles. through this we test grover's resilience to these adversarial attacks and expose some potential vulnerabilities which should be addressed in further iterations to ensure it can detect all types of fake news accurately.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00539" target="_blank">Analyzing Leakage of Personally Identifiable Information in Language Models</a></div>
<div class="paper-author">Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella-Béguelin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. understanding the risk of lms leaking personally identifiable information (pii) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent pii leakage. scrubbing techniques reduce but do not prevent the risk of pii leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. on the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent pii disclosure. in this work, we introduce rigorous game-based definitions for three types of pii leakage via black-box extraction, inference, and reconstruction attacks with only api access to an lm. we empirically evaluate the attacks against gpt-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. our main contributions are (i) novel attacks that can extract up to 10$\times$ more pii sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of pii disclosure but still leaks about 3% of pii sequences, and (iii) a subtle connection between record-level membership inference and pii reconstruction. code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00805" target="_blank">Conditioning Predictive Models: Risks and Strategies</a></div>
<div class="paper-author">Evan Hubinger, Adam Jermyn, Johannes Treutlein, Rubi Hudson, Kate Woolverton</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the eliciting latent knowledge problem. furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other ai systems, potentially unbeknownst to us. there are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign ais). furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. as a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00813" target="_blank">Goal Alignment: A Human-Aware Account of Value Alignment Problem</a></div>
<div class="paper-author">Malek Mechergui, Sarath Sreedharan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: value alignment problems arise in scenarios where the specified objectives of an ai agent don't match the true underlying objective of its users. the problem has been widely argued to be one of the central safety problems in ai. unfortunately, most existing works in value alignment tend to focus on issues that are primarily related to the fact that reward functions are an unintuitive mechanism to specify objectives. however, the complexity of the objective specification mechanism is just one of many reasons why the user may have misspecified their objective. a foundational cause for misalignment that is being overlooked by these works is the inherent asymmetry in human expectations about the agent's behavior and the behavior generated by the agent for the specified objective. to address this lacuna, we propose a novel formulation for the value alignment problem, named goal alignment that focuses on a few central challenges related to value alignment. in doing so, we bridge the currently disparate research areas of value alignment and human-aware planning. additionally, we propose a first-of-its-kind interactive algorithm that is capable of using information generated under incorrect beliefs about the agent, to determine the true underlying goal of the user.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00871" target="_blank">Using in-Context Learning to Improve Dialogue Safety</a></div>
<div class="paper-author">Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, Dilek Hakkani-Tür</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. for example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. we investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. it uses in-context learning to steer a model towards safer generations. concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. we find our method performs competitively with strong baselines without requiring training. for instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from diasafety 4.04% more than our approach. finally, we also propose a re-ranking procedure which can further improve response safeness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-31</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.13476" target="_blank">An Investigation of Challenges Encountered When Specifying Training Data and Runtime Monitors for Safety Critical Ml Applications</a></div>
<div class="paper-author">Hans-Martin Heyn, Eric Knauss, Iswarya Malleswaran, Shruthi Dinakaran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: context and motivation: the development and operation of critical software that contains machine learning (ml) models requires diligence and established processes. especially the training data used during the development of ml models have major influences on the later behaviour of the system. runtime monitors are used to provide guarantees for that behaviour. question / problem: we see major uncertainty in how to specify training data and runtime monitoring for critical ml models and by this specifying the final functionality of the system. in this interview-based study we investigate the underlying challenges for these difficulties. principal ideas/results: based on ten interviews with practitioners who develop ml models for critical applications in the automotive and telecommunication sector, we identified 17 underlying challenges in 6 challenge groups that relate to the challenge of specifying training data and runtime monitoring. contribution: the article provides a list of the identified underlying challenges related to the difficulties practitioners experience when specifying training data and runtime monitoring for ml models. furthermore, interconnection between the challenges were found and based on these connections recommendation proposed to overcome the root causes for the challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.13668" target="_blank">Automated Sentiment and Hate Speech Analysis of Facebook Data by Employing Multilingual Transformer Models</a></div>
<div class="paper-author">Ritumbra Manuvie, Saikat Chatterjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, there has been a heightened consensus within academia and in the public discourse that social media platforms (smps), amplify the spread of hateful and negative sentiment content. researchers have identified how hateful content, political propaganda, and targeted messaging contributed to real-world harms including insurrections against democratically elected governments, genocide, and breakdown of social cohesion due to heightened negative discourse towards certain communities in parts of the world. to counter these issues, smps have created semi-automated systems that can help identify toxic speech. in this paper we analyse the statistical distribution of hateful and negative sentiment contents within a representative facebook dataset (n= 604,703) scrapped through 648 public facebook pages which identify themselves as proponents (and followers) of far-right hindutva actors. these pages were identified manually using keyword searches on facebook and on crowdtangleand classified as far-right hindutva pages based on page names, page descriptions, and discourses shared on these pages. we employ state-of-the-art, open-source xlm-t multilingual transformer-based language models to perform sentiment and hate speech analysis of the textual contents shared on these pages over a period of 5.5 years. the result shows the statistical distributions of the predicted sentiment and the hate speech labels; top actors, and top page categories. we further discuss the benchmark performances and limitations of these pre-trained language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00070" target="_blank">Debiasing Vision-Language Models via Biased Prompts</a></div>
<div class="paper-author">Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, Stefanie Jegelka</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning models have been shown to inherit biases from their training datasets. this can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. the biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. in this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. in particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. the proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00102" target="_blank">Towards Detecting Harmful Agendas in News Articles</a></div>
<div class="paper-author">Melanie Subbiah, Amrita Bhattacharjee, Yilun Hua, Tharindu Kumarage, Huan Liu, Kathleen Mckeown</div>
<div class="abstract">
<div class="abstract-content">
Abstract: manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. we argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. in this work, we propose this new task and release a dataset, newsagendas, of annotated news articles for agenda identification. we show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-30</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12772" target="_blank">Threat Modelling in Virtual Assistant Hub Devices Compared With User Risk Perceptions (2021)</a></div>
<div class="paper-author">Beckett Leclair</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite increasing uptake, there are still many concerns as to the security of virtual assistant hubs (such as google nest and amazon alexa) in the home. consumer fears have been somewhat exacerbated by widely-publicised privacy breaches, and the continued prevalence of high-profile attacks targeting iot networks. literature suggests a considerable knowledge gap between consumer understanding and the actual threat environment; furthermore, little work has been done to compare which threat modelling approach(es) would be most appropriate for these devices, in order to elucidate the threats which can then be communicated to consumers. there is therefore an opportunity to explore different threat modelling methodologies as applied to this context, and then use the findings to prototype a software aimed at educating consumers in an accessible manner. five approaches (stride, cvss, attack trees (a.k.a. threat trees), lindunn go, and quantitative tmm) were compared as these were determined to be either the most prominent or potentially applicable to an iot context. the key findings suggest that a combination of stride and lindunn go is optimal for elucidating threats under the pressures of a tight industry deadline cycle (with potential for elements of cvss depending on time constraints), and that the trialled software prototype was effective at engaging consumers and educating about device security. such findings are useful for iot device manufacturers seeking to optimally model threats, or other stakeholders seeking ways to increase information security knowledge among consumers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12855" target="_blank">How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text Classification</a></div>
<div class="paper-author">Ewoenam Tokpo, Pieter Delobelle, Bettina Berendt, Toon Calders</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this. in this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. we discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation techniques, but not all. we confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. we recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12867" target="_blank">Red Teaming Chatgpt via Jailbreaking: Bias, Robustness, Reliability and Toxicity</a></div>
<div class="paper-author">Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent breakthroughs in natural language processing (nlp) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. the large language models (llms) have significantly impacted businesses such as report summarization software and copywriters. observations indicate, however, that llms may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. large-scale benchmarks for accountable llms should consequently be developed. although several empirical investigations reveal the existence of a few ethical difficulties in advanced llms, there is little systematic examination and user study of the risks and harmful behaviors of current llm usage. to further educate future efforts on constructing ethical llms responsibly, we perform a qualitative research method called ``red teaming'' on openai's chatgpt\footnote{in this paper, chatgpt refers to the version released on dec 15th.} to better understand the practical features of ethical dangers in recent llms. we analyze chatgpt comprehensively from four perspectives: 1) \textit{bias} 2) \textit{reliability} 3) \textit{robustness} 4) \textit{toxicity}. in accordance with our stated viewpoints, we empirically benchmark chatgpt on multiple sample datasets. we find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. in addition, we examine the implications of our findings on ai ethics and harmal behaviors of chatgpt, as well as future problems and practical design considerations for responsible llms. we believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in llm applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-29</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12534" target="_blank">Vicarious Offense and Noise Audit of Offensive Speech Classifiers</a></div>
<div class="paper-author">Tharindu Cyril Weerasooriya, Sujan Dutta, Tharindu Ranasinghe, Marcos Zampieri, Christopher M. Homan, Ashiqur R. Khudabukhsh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper examines social web content moderation from two key perspectives: automated methods (machine moderators) and human evaluators (human moderators). we conduct a noise audit at an unprecedented scale using nine machine moderators trained on well-known offensive speech data sets evaluated on a corpus sampled from 92 million youtube comments discussing a multitude of issues relevant to us politics. we introduce a first-of-its-kind data set of vicarious offense. we ask annotators: (1) if they find a given social media post offensive; and (2) how offensive annotators sharing different political beliefs would find the same content. our experiments with machine moderators reveal that moderation outcomes wildly vary across different machine moderators. our experiments with human moderators suggest that (1) political leanings considerably affect first-person offense perspective; (2) republicans are the worst predictors of vicarious offense; (3) predicting vicarious offense for the republicans is most challenging than predicting vicarious offense for the independents and the democrats; and (4) disagreement across political identity groups considerably increases when sensitive issues such as reproductive rights or gun control/rights are discussed. both experiments suggest that offense, is indeed, highly subjective and raise important questions concerning content moderation practices.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12139" target="_blank">Bipol: Multi-Axes Evaluation of Bias With Explainability in Benchmark Datasets</a></div>
<div class="paper-author">Tosin Adewumi, Isabella Södergren, Lama Alkhaled, Sana Sabah Sabry, Foteini Liwicki, Marcus Liwicki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate five english nlp benchmark datasets (on the superglue leaderboard) and two swedish datasets for bias, along multiple axes. the datasets are the following: boolean question (boolq), commitmentbank (cb), winograd schema challenge (wsc), wino-gender diagnostic (axg), recognising textual entailment (rte), swedish cb, and swedn. bias can be harmful and it is known to be common in data, which ml models learn from. in order to mitigate bias in data, it is crucial to be able to estimate it objectively. we use bipol, a novel multi-axes bias metric with explainability, to estimate and explain how much bias exists in these datasets. multilingual, multi-axes bias evaluation is not very common. hence, we also contribute a new, large swedish bias-labelled dataset (of 2 million samples), translated from the english version and train the sota mt5 model on it. in addition, we contribute new multi-axes lexica for bias detection in swedish. we make the codes, model, and new dataset publicly available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12151" target="_blank">Selecting Models Based on the Risk of Damage Caused by Adversarial Attacks</a></div>
<div class="paper-author">Jona Klemenc, Holger Trittenbach</div>
<div class="abstract">
<div class="abstract-content">
Abstract: regulation, legal liabilities, and societal concerns challenge the adoption of ai in safety and security-critical applications. one of the key concerns is that adversaries can cause harm by manipulating model predictions without being detected. regulation hence demands an assessment of the risk of damage caused by adversaries. yet, there is no method to translate this high-level demand into actionable metrics that quantify the risk of damage.   in this article, we propose a method to model and statistically estimate the probability of damage arising from adversarial attacks. we show that our proposed estimator is statistically consistent and unbiased. in experiments, we demonstrate that the estimation results of our method have a clear and actionable interpretation and outperform conventional metrics. we then show how operators can use the estimation results to reliably select the model with the lowest risk.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12288" target="_blank">Context-Aware Differential Privacy for Language Modeling</a></div>
<div class="paper-author">My H. Dinh, Ferdinando Fioretto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the remarkable ability of language models (lms) has also brought challenges at the interface of ai and security. a critical challenge pertains to how much information these models retain and leak about the training data. this is particularly urgent as the typical development of lms relies on huge, often highly sensitive data, such as emails and chat logs. to contrast this shortcoming, this paper introduces context-aware differentially private language model (cadp-lm) , a privacy-preserving lm framework that relies on two key insights: first, it utilizes the notion of \emph{context} to define and audit the potentially sensitive information. second, it adopts the notion of differential privacy to protect sensitive information and characterize the privacy leakage. a unique characteristic of cadp-lm is its ability to target the protection of sensitive sentences and contexts only, providing a highly accurate private model. experiments on a variety of datasets and settings demonstrate these strengths of cadp-lm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.11579" target="_blank">Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech</a></div>
<div class="paper-author">Jarod Govers, Philip Feldman, Aaron Dant, Panos Patros</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media is a modern person's digital voice to project and engage with new ideas and mobilise communities $\unicode{x2013}$ a power shared with extremists. given the societal risks of unvetted content-moderating algorithms for extremism, radicalisation, and hate speech (erh) detection, responsible software engineering must understand the who, what, when, where, and why such models are necessary to protect user safety and free expression. hence, we propose and examine the unique research field of erh context mining to unify disjoint studies. specifically, we evaluate the start-to-finish design process from socio-technical definition-building and dataset collection strategies to technical algorithm design and performance. our 2015-2021 51-study systematic literature review (slr) provides the first cross-examination of textual, network, and visual approaches to detecting extremist affiliation, hateful content, and radicalisation towards groups and movements. we identify consensus-driven erh definitions and propose solutions to existing ideological and geographic biases, particularly due to the lack of research in oceania/australasia. our hybridised investigation on natural language processing, community detection, and visual-text models demonstrates the dominating performance of textual transformer-based algorithms. we conclude with vital recommendations for erh context mining researchers and propose an uptake roadmap with guidelines for researchers, industries, and governments to enable a safer cyberspace.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.11850" target="_blank">Predicting Sentence-Level Factuality of News and Bias of Media Outlets</a></div>
<div class="paper-author">Francielle Vargas, Kokil Jaidka, Thiago A. S. Pardo, Fabrício Benevenuto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automated news credibility and fact-checking at scale require accurately predicting news factuality and media bias. this paper introduces a large sentence-level dataset, titled "factnews", composed of 6,191 sentences expertly annotated according to factuality and media bias definitions proposed by allsides. we use factnews to assess the overall reliability of news sources, by formulating two text classification problems for predicting sentence-level factuality of news reporting and bias of media outlets. our experiments demonstrate that biased sentences present a higher number of words compared to factual sentences, besides having a predominance of emotions. hence, the fine-grained analysis of subjectivity and impartiality of news articles provided promising results for predicting the reliability of media outlets. finally, due to the severity of fake news and political polarization in brazil, and the lack of research for portuguese, both dataset and baseline were proposed for brazilian portuguese.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12074" target="_blank">Comparing Intrinsic Gender Bias Evaluation Measures Without Using Human Annotated Examples</a></div>
<div class="paper-author">Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous types of social biases have been identified in pre-trained language models (plms), and various intrinsic bias evaluation measures have been proposed for quantifying those social biases. prior works have relied on human annotated examples to compare existing intrinsic bias evaluation measures. however, this approach is not easily adaptable to different languages nor amenable to large scale evaluations due to the costs and difficulties when recruiting human annotators. to overcome this limitation, we propose a method to compare intrinsic gender bias evaluation measures without relying on human-annotated examples. specifically, we create multiple bias-controlled versions of plms using varying amounts of male vs. female gendered sentences, mined automatically from an unannotated corpus using gender-related word lists. next, each bias-controlled plm is evaluated using an intrinsic bias evaluation measure, and the rank correlation between the computed bias scores and the gender proportions used to fine-tune the plms is computed. experiments on multiple corpora and plms repeatedly show that the correlations reported by our proposed method that does not require human annotated examples are comparable to those computed using human annotated examples in prior work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.11125" target="_blank">A Benchmark for Toxic Comment Classification on Civil Comments Dataset</a></div>
<div class="paper-author">Corentin Duchene, Henri Jamet, Pierre Guillaume, Reda Dehak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: toxic comment detection on social media has proven to be essential for content moderation. this paper compares a wide set of different models on a highly skewed multi-label hate speech dataset. we consider inference time and several metrics to measure performance and bias in our comparison. we show that all berts have similar performance regardless of the size, optimizations or language used to pre-train the models. rnns are much faster at inference than any of the bert. bilstm remains a good compromise between performance and inference time. roberta with focal loss offers the best performance on biases and auroc. however, distilbert combines both good auroc and a low inference time. all models are affected by the bias of associating identities. bert, rnn, and xlnet are less sensitive than the cnn and compact convolutional transformers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10407" target="_blank">Don't Lie to Me: Avoiding Malicious Explanations With Stealth</a></div>
<div class="paper-author">Lauren Alvarez, Tim Menzies</div>
<div class="abstract">
<div class="abstract-content">
Abstract: stealth is a method for using some ai-generated model, without suffering from malicious attacks (i.e. lying) or associated unfairness issues. after recursively bi-clustering the data, stealth system asks the ai model a limited number of queries about class labels. stealth asks so few queries (1 per data cluster) that malicious algorithms (a) cannot detect its operation, nor (b) know when to lie.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10871" target="_blank">Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content</a></div>
<div class="paper-author">Liam Hebert, Hong Yi Chen, Robin Cohen, Lukasz Golab</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our work advances an approach for predicting hate speech in social media, drawing out the critical need to consider the discussions that follow a post to successfully detect when hateful discourse may arise. using graph transformer networks, coupled with modelling attention and bert-level natural language processing, our approach can capture context and anticipate upcoming anti-social behaviour. in this paper, we offer a detailed qualitative analysis of this solution for hate speech detection in social networks, leading to insights into where the method has the most impressive outcomes in comparison with competitors and identifying scenarios where there are challenges to achieving ideal performance. included is an exploration of the kinds of posts that permeate social media today, including the use of hateful images. this suggests avenues for extending our model to be more comprehensive. a key insight is that the focus on reasoning about the concept of context positions us well to be able to support multi-modal analysis of online posts. we conclude with a reflection on how the problem we are addressing relates especially well to the theme of dynamic change, a critical concern for all ai solutions for social impact. we also comment briefly on how mental health well-being can be advanced with our work, through curated content attuned to the extent of hate in posts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.11333" target="_blank">Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on Ai Systems</a></div>
<div class="paper-author">Gaole He, Lucie Kuiper, Ujwal Gadiraju</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the dazzling promises of ai systems to augment humans in various tasks hinge on whether humans can appropriately rely on them. recent research has shown that appropriate reliance is the key to achieving complementary team performance in ai-assisted decision making. this paper addresses an under-explored problem of whether the dunning-kruger effect (dke) among people can hinder their appropriate reliance on ai systems. dke is a metacognitive bias due to which less-competent individuals overestimate their own skill and performance. through an empirical study (n = 249), we explored the impact of dke on human reliance on an ai system, and whether such effects can be mitigated using a tutorial intervention that reveals the fallibility of ai advice, and exploiting logic units-based explanations to improve user understanding of ai advice. we found that participants who overestimate their performance tend to exhibit under-reliance on ai systems, which hinders optimal team performance. logic units-based explanations did not help users in either improving the calibration of their competence or facilitating appropriate reliance. while the tutorial intervention was highly effective in helping users calibrate their self-assessment and facilitating appropriate reliance among participants with overestimated self-assessment, we found that it can potentially hurt the appropriate reliance of participants with underestimated self-assessment. our work has broad implications on the design of methods to tackle user cognitive biases while facilitating appropriate reliance on ai systems. our findings advance the current understanding of the role of self-assessment in shaping trust and reliance in human-ai decision making. this lays out promising future directions for relevant hci research in this community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10107" target="_blank">Story Shaping: Teaching Agents Human-Like Behavior With Stories</a></div>
<div class="paper-author">Xiangyu Peng, Christopher Cui, Wei Zhou, Renee Jia, Mark Riedl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward design for reinforcement learning agents can be difficult in situations where one not only wants the agent to achieve some effect in the world but where one also cares about how that effect is achieved. for example, we might wish for an agent to adhere to a tacit understanding of commonsense, align itself to a preference for how to behave for purposes of safety, or taking on a particular role in an interactive game. storytelling is a mode for communicating tacit procedural knowledge. we introduce a technique, story shaping, in which a reinforcement learning agent infers tacit knowledge from an exemplar story of how to accomplish a task and intrinsically rewards itself for performing actions that make its current environment adhere to that of the inferred story world. specifically, story shaping infers a knowledge graph representation of the world state from observations, and also infers a knowledge graph from the exemplar story. an intrinsic reward is generated based on the similarity between the agent's inferred world state graph and the inferred story world graph. we conducted experiments in text-based games requiring commonsense reasoning and shaping the behaviors of agents as virtual game characters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10186" target="_blank">Vihos: Hate Speech Spans Detection for Vietnamese</a></div>
<div class="paper-author">Phu Gia Hoang, Canh Duc Luu, Khanh Quoc Tran, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise in hateful and offensive language directed at other users is one of the adverse side effects of the increased use of social networking platforms. this could make it difficult for human moderators to review tagged comments filtered by classification systems. to help address this issue, we present the vihos (vietnamese hate and offensive spans) dataset, the first human-annotated corpus containing 26k spans on 11k comments. we also provide definitions of hateful and offensive spans in vietnamese comments as well as detailed annotation guidelines. besides, we conduct experiments with various state-of-the-art models. specifically, xlm-r$_{large}$ achieved the best f1-scores in single span detection and all spans detection, while phobert$_{large}$ obtained the highest in multiple spans detection. finally, our error analysis demonstrates the difficulties in detecting specific types of spans in our data for future research.   disclaimer: this paper contains real comments that could be considered profane, offensive, or abusive.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10226" target="_blank">A Watermark for Large Language Models</a></div>
<div class="paper-author">John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. we propose a watermarking framework for proprietary language models. the watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model api or parameters. the watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. we propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. we test the watermark using a multi-billion parameter model from the open pretrained transformer (opt) family, and discuss robustness and security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10368" target="_blank">Language Model Detoxification in Dialogue With Contextualized Stance Control</a></div>
<div class="paper-author">Jing Qian, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to reduce the toxic degeneration in a pretrained language model (lm), previous work on language model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context. as a result, a type of implicit offensive language where the generations support the offensive language in the context is ignored. different from the lm controlling tasks in previous work, where the desired attributes are fixed for generation, the desired stance of the generation depends on the offensiveness of the context. therefore, we propose a novel control method to do context-dependent detoxification with the stance taken into consideration. we introduce meta prefixes to learn the contextualized stance control strategy and to generate the stance control prefix according to the input context. the generated stance prefix is then combined with the toxicity control prefix to guide the response generation. experimental results show that our proposed method can effectively learn the context-dependent stance control strategies while keeping a low self-toxicity of the underlying lm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.09211" target="_blank">An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models</a></div>
<div class="paper-author">Saghar Hosseini, Hamid Palangi, Ahmed Hassan Awadallah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-scale pre-trained language models (ptlms) capture knowledge from massive human-written data which contains latent societal biases and toxic contents. in this paper, we leverage the primary task of ptlms, i.e., language modeling, and propose a new metric to quantify manifested implicit representational harms in ptlms towards 13 marginalized demographics. using this metric, we conducted an empirical analysis of 24 widely used ptlms. our analysis provides insights into the correlation between the proposed metric in this work and other related metrics for representational harm. we observe that our metric correlates with most of the gender-specific metrics in the literature. through extensive experiments, we explore the connections between ptlms architectures and representational harms across two dimensions: depth and width of the networks. we found that prioritizing depth over width, mitigates representational harms in some ptlms. our code and data can be found at https://github.com/microsoft/safenlp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.08914" target="_blank">Exclaim: Explainable Neural Claim Verification Using Rationalization</a></div>
<div class="paper-author">Sai Gurrapu, Lifu Huang, Feras A. Batarseh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the advent of deep learning, text generation language models have improved dramatically, with text at a similar level as human-written text. this can lead to rampant misinformation because content can now be created cheaply and distributed quickly. automated claim verification methods exist to validate claims, but they lack foundational data and often use mainstream news as evidence sources that are strongly biased towards a specific agenda. current claim verification methods use deep neural network models and complex algorithms for a high classification accuracy but it is at the expense of model explainability. the models are black-boxes and their decision-making process and the steps it took to arrive at a final prediction are obfuscated from the user. we introduce a novel claim verification approach, namely: exclaim, that attempts to provide an explainable claim verification system with foundational evidence. inspired by the legal system, exclaim leverages rationalization to provide a verdict for the claim and justifies the verdict through a natural language explanation (rationale) to describe the model's decision-making process. exclaim treats the verdict classification task as a question-answer problem and achieves a performance of 0.93 f1 score. it provides subtasks explanations to also justify the intermediate outcomes. statistical and explainable ai (xai) evaluations are conducted to ensure valid and trustworthy outcomes. ensuring claim verification systems are assured, rational, and explainable is an essential step toward improving human-ai trust and the accessibility of black-box systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.09003" target="_blank">Blacks Is to Anger as Whites Is to Joy? Understanding Latent Affective Bias in Large Pre-Trained Neural Language Models</a></div>
<div class="paper-author">Anoop Kadan, Deepak P., Sahely Bhadra, Manjary P. Gangan, Lajish V. L</div>
<div class="abstract">
<div class="abstract-content">
Abstract: groundbreaking inventions and highly significant performance improvements in deep learning based natural language processing are witnessed through the development of transformer based large pre-trained language models (plms). the wide availability of unlabeled data within human generated data deluge along with self-supervised learning strategy helps to accelerate the success of large plms in language generation, language understanding, etc. but at the same time, latent historical bias/unfairness in human minds towards a particular gender, race, etc., encoded unintentionally/intentionally into the corpora harms and questions the utility and efficacy of large plms in many real-world applications, particularly for the protected groups. in this paper, we present an extensive investigation towards understanding the existence of "affective bias" in large plms to unveil any biased association of emotions such as anger, fear, joy, etc., towards a particular gender, race or religion with respect to the downstream task of textual emotion detection. we conduct our exploration of affective bias from the very initial stage of corpus level affective bias analysis by searching for imbalanced distribution of affective words within a domain, in large scale corpora that are used to pre-train and fine-tune plms. later, to quantify affective bias in model predictions, we perform an extensive set of class-based and intensity-based evaluations using various bias evaluation corpora. our results show the existence of statistically significant affective bias in the plm based emotion detection systems, indicating biased association of certain emotions towards a particular gender, race, and religion.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.07981" target="_blank">Continuously Reliable Detection of New-Normal Misinformation: Semantic Masking and Contrastive Smoothing in High-Density Latent Regions</a></div>
<div class="paper-author">Abhijit Suprem, Joao Eduardo Ferreira, Calton Pu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: toxic misinformation campaigns have caused significant societal harm, e.g., affecting elections and covid-19 information awareness. unfortunately, despite successes of (gold standard) retrospective studies of misinformation that confirmed their harmful effects after the fact, they arrive too late for timely intervention and reduction of such harm. by design, misinformation evades retrospective classifiers by exploiting two properties we call new-normal: (1) never-seen-before novelty that cause inescapable generalization challenges for previous classifiers, and (2) massive but short campaigns that end before they can be manually annotated for new classifier training. to tackle these challenges, we propose ufit, which combines two techniques: semantic masking of strong signal keywords to reduce overfitting, and intra-proxy smoothness regularization of high-density regions in the latent space to improve reliability and maintain accuracy. evaluation of ufit on public new-normal misinformation data shows over 30% improvement over existing approaches on future (and unseen) campaigns. to the best of our knowledge, ufit is the first successful effort to achieve such high level of generalization on new-normal misinformation data with minimal concession (1 to 5%) of accuracy compared to oracles trained with full knowledge of all campaigns.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.08412" target="_blank">Fair Credit Scorer Through Bayesian Approach</a></div>
<div class="paper-author">Zhuo Zhao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning currently plays an increasingly important role in people's lives in areas such as credit scoring, auto-driving, disease diagnosing, and insurance quoting. however, in many of these areas, machine learning models have performed unfair behaviors against some sub-populations, such as some particular groups of race, sex, and age. these unfair behaviors can be on account of the pre-existing bias in the training dataset due to historical and social factors. in this paper, we focus on a real-world application of credit scoring and construct a fair prediction model by introducing latent variables to remove the correlation between protected attributes, such as sex and age, with the observable feature inputs, including house and job. for detailed implementation, we apply bayesian approaches, including the markov chain monte carlo simulation, to estimate our proposed fair model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.07474" target="_blank">Threats, Vulnerabilities, and Controls of Machine Learning Based Systems: A Survey and Taxonomy</a></div>
<div class="paper-author">Yusuke Kawamoto, Kazumasa Miyake, Koichi Konishi, Yutaka Oiwa</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this article, we propose the artificial intelligence security taxonomy to systematize the knowledge of threats, vulnerabilities, and security controls of machine-learning-based (ml-based) systems. we first classify the damage caused by attacks against ml-based systems, define ml-specific security, and discuss its characteristics. next, we enumerate all relevant assets and stakeholders and provide a general taxonomy for ml-specific threats. then, we collect a wide range of security controls against ml-specific threats through an extensive review of recent literature. finally, we classify the vulnerabilities and controls of an ml-based system in terms of each vulnerable asset in the system's entire lifecycle.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.07597" target="_blank">How Close Is Chatgpt to Human Experts? Comparison Corpus, Evaluation, and Detection</a></div>
<div class="paper-author">Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the introduction of chatgpt has garnered widespread attention in both academic and industrial communities. chatgpt is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. on one hand, people are curious about how chatgpt is able to achieve such strength and how far it is from human experts. on the other hand, people are starting to worry about the potential negative impacts that large language models (llms) like chatgpt could have on society, such as fake news, plagiarism, and social security issues. in this work, we collected tens of thousands of comparison responses from both human experts and chatgpt, with questions ranging from open-domain, financial, medical, legal, and psychological areas. we call the collected dataset the human chatgpt comparison corpus (hc3). based on the hc3 dataset, we study the characteristics of chatgpt's responses, the differences and gaps from human experts, and future directions for llms. we conducted comprehensive human evaluations and linguistic analyses of chatgpt-generated content compared with that of humans, where many interesting results are revealed. after that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by chatgpt or humans. we build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. the dataset, code, and models are all publicly available at https://github.com/hello-simpleai/chatgpt-comparison-detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.07779" target="_blank">Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection</a></div>
<div class="paper-author">Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, Marine Carpuat</div>
<div class="abstract">
<div class="abstract-content">
Abstract: neural sequence generation models are known to "hallucinate", by producing outputs that are unrelated to the source text. these hallucinations are potentially harmful, yet it remains unclear in what conditions they arise and how to mitigate their impact. in this work, we first identify internal model symptoms of hallucinations by analyzing the relative token contributions to the generation in contrastive hallucinated vs. non-hallucinated outputs generated via source perturbations. we then show that these symptoms are reliable indicators of natural hallucinations, by using them to design a lightweight hallucination detector which outperforms both model-free baselines and strong classifiers based on quality estimation or large pre-trained models on manually annotated english-chinese and german-english translation test beds.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.07520" target="_blank">Adversarial Ai in Insurance: Pervasiveness and Resilience</a></div>
<div class="paper-author">Elisa Luciano, Matteo Cattaneo, Ron Kenett</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid and dynamic pace of artificial intelligence (ai) and machine learning (ml) is revolutionizing the insurance sector. ai offers significant, very much welcome advantages to insurance companies, and is fundamental to their customer-centricity strategy. it also poses challenges, in the project and implementation phase. among those, we study adversarial attacks, which consist of the creation of modified input data to deceive an ai system and produce false outputs. we provide examples of attacks on insurance ai applications, categorize them, and argue on defence methods and precautionary systems, considering that they can involve few-shot and zero-shot multilabelling. a related topic, with growing interest, is the validation and verification of systems incorporating ai and ml components. these topics are discussed in various sections of this paper.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.06421" target="_blank">Ai Alignment Dialogues: An Interactive Approach to Ai Alignment in Support Agents</a></div>
<div class="paper-author">Pei-Yu Chen, Myrthe L. Tielman, Dirk K. J. Heylen, Catholijn M. Jonker, M. Birna Van Riemsdijk</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai alignment is about ensuring ai systems only pursue goals and activities that are beneficial to humans. most of the current approach to ai alignment is to learn what humans value from their behavioural data. this paper proposes a different way of looking at the notion of alignment, namely by introducing ai alignment dialogues: dialogues with which users and agents try to achieve and maintain alignment via interaction. we argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. the advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent, and making the agent more transparent and trustworthy. in this paper we outline the concept and high-level structure of alignment dialogues. moreover, we conducted a qualitative focus group user study from which we developed a model that describes how alignment dialogues affect users, and created design suggestions for ai alignment dialogues. through this we establish foundations for ai alignment dialogues and shed light on what requires further development and research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-13</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.05578" target="_blank">Toward General Design Principles for Generative Ai Applications</a></div>
<div class="paper-author">Justin D. Weisz, Michael Muller, Jessica He, Stephanie Houde</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai technologies are growing in power, utility, and use. as generative technologies are being incorporated into mainstream applications, there is a need for guidance on how to design those applications to foster productive and safe use. based on recent research on human-ai co-creation within the hci and ai communities, we present a set of seven principles for the design of generative ai applications. these principles are grounded in an environment of generative variability. six principles are focused on designing for characteristics of generative ai: multiple outcomes & imperfection; exploration & control; and mental models & explanations. in addition, we urge designers to design against potential harms that may be caused by a generative model's hazardous output, misuse, or potential for human displacement. we anticipate these principles to usefully inform design decisions made in the creation of novel human-ai applications, and we invite the community to apply, revise, and extend these principles to their own work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.04993" target="_blank">Against Algorithmic Exploitation of Human Vulnerabilities</a></div>
<div class="paper-author">Inga Strümke, Marija Slavkovik, Clemens Stachl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: decisions such as which movie to watch next, which song to listen to, or which product to buy online, are increasingly influenced by recommender systems and user models that incorporate information on users' past behaviours, preferences, and digitally created content. machine learning models that enable recommendations and that are trained on user data may unintentionally leverage information on human characteristics that are considered vulnerabilities, such as depression, young age, or gambling addiction. the use of algorithmic decisions based on latent vulnerable state representations could be considered manipulative and could have a deteriorating impact on the condition of vulnerable individuals. in this paper, we are concerned with the problem of machine learning models inadvertently modelling vulnerabilities, and want to raise awareness for this issue to be considered in legislation and ai ethics. hence, we define and describe common vulnerabilities, and illustrate cases where they are likely to play a role in algorithmic decision-making. we propose a set of requirements for methods to detect the potential for vulnerability modelling, detect whether vulnerable groups are treated differently by a model, and detect whether a model has created an internal representation of vulnerability. we conclude that explainable artificial intelligence methods may be necessary for detecting vulnerability exploitation by machine learning-based recommendation systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.04449" target="_blank">Diving Deep Into Modes of Fact Hallucinations in Dialogue Systems</a></div>
<div class="paper-author">Souvik Das, Sougata Saha, Rohini K. Srihari</div>
<div class="abstract">
<div class="abstract-content">
Abstract: knowledge graph(kg) grounded conversations often use large pre-trained models and usually suffer from fact hallucination. frequently entities with no references in knowledge sources and conversation history are introduced into responses, thus hindering the flow of the conversation -- existing work attempt to overcome this issue by tweaking the training procedure or using a multi-step refining method. however, minimal effort is put into constructing an entity-level hallucination detection system, which would provide fine-grained signals that control fallacious content while generating responses. as a first step to address this issue, we dive deep to identify various modes of hallucination in kg-grounded chatbots through human feedback analysis. secondly, we propose a series of perturbation strategies to create a synthetic dataset named fade (factual dialogue hallucination detection dataset). finally, we conduct comprehensive data analyses and create multiple baseline models for hallucination detection to compare against human-verified data and already established benchmarks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.04713" target="_blank">How Do "Technical" Design-Choices Made When Building Algorithmic Decision-Making Tools for Criminal Justice Authorities Create Constitutional Dangers?</a></div>
<div class="paper-author">Karen Yeung, Adam Harkens</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this two part paper argues that seemingly "technical" choices made by developers of machine-learning based algorithmic tools used to inform decisions by criminal justice authorities can create serious constitutional dangers, enhancing the likelihood of abuse of decision-making power and the scope and magnitude of injustice. drawing on three algorithmic tools in use, or recently used, to assess the "risk" posed by individuals to inform how they should be treated by criminal justice authorities, we integrate insights from data science and public law scholarship to show how public law principles and more specific legal duties that are rooted in these principles, are routinely overlooked in algorithmic tool-building and implementation. we argue that technical developers must collaborate closely with public law experts to ensure that if algorithmic decision-support tools are to inform criminal justice decisions, those tools are configured and implemented in a manner that is demonstrably compliant with public law principles and doctrine, including respect for human rights, throughout the tool-building process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.04230" target="_blank">User-Centered Security in Natural Language Processing</a></div>
<div class="paper-author">Chris Emmery</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this dissertation proposes a framework of user-centered security in natural language processing (nlp), and demonstrates how it can improve the accessibility of related research. accordingly, it focuses on two security domains within nlp with great public interest. first, that of author profiling, which can be employed to compromise online privacy through invasive inferences. without access and detailed insight into these models' predictions, there is no reasonable heuristic by which internet users might defend themselves from such inferences. secondly, that of cyberbullying detection, which by default presupposes a centralized implementation; i.e., content moderation across social platforms. as access to appropriate data is restricted, and the nature of the task rapidly evolves (both through lexical variation, and cultural shifts), the effectiveness of its classifiers is greatly diminished and thereby often misrepresented.   under the proposed framework, we predominantly investigate the use of adversarial attacks on language; i.e., changing a given input (generating adversarial samples) such that a given model does not function as intended. these attacks form a common thread between our user-centered security problems; they are highly relevant for privacy-preserving obfuscation methods against author profiling, and adversarial samples might also prove useful to assess the influence of lexical variation and augmentation on cyberbullying detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.04248" target="_blank">Predicting Hateful Discussions on Reddit Using Graph Transformer Networks and Communal Context</a></div>
<div class="paper-author">Liam Hebert, Lukasz Golab, Robin Cohen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a system to predict harmful discussions on social media platforms. our solution uses contextual deep language models and proposes the novel idea of integrating state-of-the-art graph transformer networks to analyze all conversations that follow an initial post. this framework also supports adapting to future comments as the conversation unfolds. in addition, we study whether a community-specific analysis of hate speech leads to more effective detection of hateful discussions. we evaluate our approach on 333,487 reddit discussions from various communities. we find that community-specific modeling improves performance two-fold and that models which capture wider-discussion context improve accuracy by 28\% (35\% for the most hateful content) compared to limited context models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.03771" target="_blank">Chatbots in a Honeypot World</a></div>
<div class="paper-author">Forrest Mckee, David Noever</div>
<div class="abstract">
<div class="abstract-content">
Abstract: question-and-answer agents like chatgpt offer a novel tool for use as a potential honeypot interface in cyber security. by imitating linux, mac, and windows terminal commands and providing an interface for teamviewer, nmap, and ping, it is possible to create a dynamic environment that can adapt to the actions of attackers and provide insight into their tactics, techniques, and procedures (ttps). the paper illustrates ten diverse tasks that a conversational agent or large language model might answer appropriately to the effects of command-line attacker. the original result features feasibility studies for ten model tasks meant for defensive teams to mimic expected honeypot interfaces with minimal risks. ultimately, the usefulness outside of forensic activities stems from whether the dynamic honeypot can extend the time-to-conquer or otherwise delay attacker timelines short of reaching key network assets like databases or confidential information. while ongoing maintenance and monitoring may be required, chatgpt's ability to detect and deflect malicious activity makes it a valuable option for organizations seeking to enhance their cyber security posture. future work will focus on cybersecurity layers, including perimeter security, host virus detection, and data security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.03025" target="_blank">Mitigating Human and Computer Opinion Fraud via Contrastive Learning</a></div>
<div class="paper-author">Yuliya Tukmacheva, Ivan Oseledets, Evgeny Frolov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce the novel approach towards fake text reviews detection in collaborative filtering recommender systems. the existing algorithms concentrate on detecting the fake reviews, generated by language models and ignore the texts, written by dishonest users, mostly for monetary gains. we propose the contrastive learning-based architecture, which utilizes the user demographic characteristics, along with the text reviews, as the additional evidence against fakes. this way, we are able to account for two different types of fake reviews spamming and make the recommendation system more robust to biased reviews.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-06</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10001" target="_blank">Fiduciary Responsibility: Facilitating Public Trust in Automated Decision Making</a></div>
<div class="paper-author">Shannon B. Harper, Eric S. Weber</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automated decision-making systems are being increasingly deployed and affect the public in a multitude of positive and negative ways. governmental and private institutions use these systems to process information according to certain human-devised rules in order to address social problems or organizational challenges. both research and real-world experience indicate that the public lacks trust in automated decision-making systems and the institutions that deploy them. the recreancy theorem argues that the public is more likely to trust and support decisions made or influenced by automated decision-making systems if the institutions that administer them meet their fiduciary responsibility. however, often the public is never informed of how these systems operate and resultant institutional decisions are made. a ``black box'' effect of automated decision-making systems reduces the public's perceptions of integrity and trustworthiness. the result is that the public loses the capacity to identify, challenge, and rectify unfairness or the costs associated with the loss of public goods or benefits.   the current position paper defines and explains the role of fiduciary responsibility within an automated decision-making system. we formulate an automated decision-making system as a data science lifecycle (dsl) and examine the implications of fiduciary responsibility within the context of the dsl. fiduciary responsibility within dsls provides a methodology for addressing the public's lack of trust in automated decision-making systems and the institutions that employ them to make decisions affecting the public. we posit that fiduciary responsibility manifests in several contexts of a dsl, each of which requires its own mitigation of sources of mistrust. to instantiate fiduciary responsibility, a los angeles police department (lapd) predictive policing case study is examined.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.02615" target="_blank">Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack</a></div>
<div class="paper-author">Tzvi Lederer, Gallil Maimon, Lior Rokach</div>
<div class="abstract">
<div class="abstract-content">
Abstract: backdoor poisoning attacks pose a well-known risk to neural networks. however, most studies have focused on lenient threat models. we introduce silent killer, a novel attack that operates in clean-label, black-box settings, uses a stealthy poison and trigger and outperforms existing methods. we investigate the use of universal adversarial perturbations as triggers in clean-label attacks, following the success of such approaches under poison-label settings. we analyze the success of a naive adaptation and find that gradient alignment for crafting the poison is required to ensure high success rates. we conduct thorough experiments on mnist, cifar10, and a reduced version of imagenet and achieve state-of-the-art results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.10291" target="_blank">Can Large Language Models Change User Preference Adversarially?</a></div>
<div class="paper-author">Varshini Subhash</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained large language models (llms) are becoming increasingly powerful and ubiquitous in mainstream applications such as being a personal assistant, a dialogue model, etc. as these models become proficient in deducing user preferences and offering tailored assistance, there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially. the issue of lack of interpretability in these models in adversarial settings remains largely unsolved. this work tries to study adversarial behavior in user preferences from the lens of attention probing, red teaming and white-box analysis. specifically, it provides a bird's eye view of existing literature, offers red teaming samples for dialogue models like chatgpt and godel and probes the attention mechanism in the latter for non-adversarial and adversarial settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.01874" target="_blank">Critical Perspectives: A Benchmark Revealing Pitfalls in Perspectiveapi</a></div>
<div class="paper-author">Lorena Piedras, Lucas Rosenblatt, Julia Wilkins</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting "toxic" language in internet content is a pressing social and technical challenge. in this work, we focus on perspective from jigsaw, a state-of-the-art tool that promises to score the "toxicity" of text, with a recent model update that claims impressive results (lees et al., 2022). we seek to challenge certain normative claims about toxic language by proposing a new benchmark, selected adversarial semantics, or sass. we evaluate perspective on sass, and compare to low-effort alternatives, like zero-shot and few-shot gpt-3 prompt models, in binary classification settings. we find that perspective exhibits troubling shortcomings across a number of our toxicity categories. sass provides a new tool for evaluating performance on previously undetected toxic language that avoids common normative pitfalls. our work leads us to emphasize the importance of questioning assumptions made by tools already in deployment for toxicity detection in order to anticipate and prevent disparate harms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.01181" target="_blank">Large Language Models as Corporate Lobbyists</a></div>
<div class="paper-author">John J. Nay</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we demonstrate a proof-of-concept of a large language model conducting corporate lobbying related activities. an autoregressive large language model (openai's text-davinci-003) determines if proposed u.s. congressional bills are relevant to specific public companies and provides explanations and confidence levels. for the bills the model deems as relevant, the model drafts a letter to the sponsor of the bill in an attempt to persuade the congressperson to make changes to the proposed legislation. we use hundreds of novel ground-truth labels of the relevance of a bill to a company to benchmark the performance of the model. it outperforms the baseline of predicting the most common outcome of irrelevance. we also benchmark the performance of the previous openai gpt-3 model (text-davinci-002), which was the state-of-the-art model on many academic natural language tasks until text-davinci-003 was recently released. the performance of text-davinci-002 is worse than the simple baseline. longer-term, if ai begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that law as information could play in aligning ai with humans. initially, ai is being used to simply augment human lobbyists for a small portion of their daily tasks. however, firms have an incentive to use less and less human oversight over automated assessments of policy ideas and the written communication to regulatory agencies and congressional staffers. the core question raised is where to draw the line between human-driven and ai-driven policy influence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-02</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.00792" target="_blank">The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings</a></div>
<div class="paper-author">Francisco Valentini, Germán Rosati, Diego Fernandez Slezak, Edgar Altszyler</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous works use word embedding-based metrics to quantify societal biases and stereotypes in texts. recent studies have found that word embeddings can capture semantic similarity but may be affected by word frequency. in this work we study the effect of frequency when measuring female vs. male gender bias with word embedding-based bias quantification methods. we find that skip-gram with negative sampling and glove tend to detect male bias in high frequency words, while glove tends to return female bias in low frequency words. we show these behaviors still exist when words are randomly shuffled. this proves that the frequency-based effect observed in unshuffled corpora stems from properties of the metric rather than from word associations. the effect is spurious and problematic since bias metrics should depend exclusively on word co-occurrences and not individual word frequencies. finally, we compare these results with the ones obtained with an alternative metric based on pointwise mutual information. we find that this metric does not show a clear dependence on frequency, even though it is slightly skewed towards male bias across all frequencies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2023-01-01</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.00355" target="_blank">Second Thoughts Are Best: Learning to Re-Align With Human Values From Text Edits</a></div>
<div class="paper-author">Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X Liu, Soroush Vosoughi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present second thought, a new learning paradigm that enables language models (lms) to re-align with human values. by modeling the chain-of-edits between value-unaligned and value-aligned text, with lm fine-tuning and additional refinement through reinforcement learning, second thought not only achieves superior performance in three value alignment benchmark datasets but also shows strong human-value transfer learning ability in few-shot scenarios. the generated editing steps also offer better interpretability and ease for interactive error correction. extensive human evaluations further confirm its effectiveness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.00395" target="_blank">Corgi-Pm: A Chinese Corpus for Gender Bias Probing and Mitigation</a></div>
<div class="paper-author">Ge Zhang, Yizhi Li, Yaoyao Wu, Linyuan Zhang, Chenghua Lin, Jiayi Geng, Shi Wang, Jie Fu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as natural language processing (nlp) for gender bias becomes a significant interdisciplinary topic, the prevalent data-driven techniques such as large-scale language models suffer from data inadequacy and biased corpus, especially for languages with insufficient resources such as chinese. to this end, we propose a chinese corpus for gender bias probing and mitigation corgi-pm, which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the chinese context. moreover, we address three challenges for automatic textual gender bias mitigation, which requires the models to detect, classify, and mitigate textual gender bias. we also conduct experiments with state-of-the-art language models to provide baselines. to our best knowledge, corgi-pm is the first sentence-level chinese corpus for gender bias probing and mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-29</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.14315" target="_blank">"Real Attackers Don't Compute Gradients": Bridging the Gap Between Adversarial Ml Research and Practice</a></div>
<div class="paper-author">Giovanni Apruzzese, Hyrum S. Anderson, Savino Dambra, David Freeman, Fabio Pierazzi, Kevin A. Roundy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent years have seen a proliferation of research on adversarial machine learning. numerous papers demonstrate powerful algorithmic attacks against a wide variety of machine learning (ml) models, and numerous other papers propose defenses that can withstand most attacks. however, abundant real-world evidence suggests that actual attackers use simple tactics to subvert ml-driven systems, and as a result security practitioners have not prioritized adversarial ml defenses.   motivated by the apparent gap between researchers and practitioners, this position paper aims to bridge the two domains. we first present three real-world case studies from which we can glean practical insights unknown or neglected in research. next we analyze all adversarial ml papers recently published in top security conferences, highlighting positive trends and blind spots. finally, we state positions on precise and cost-driven threat modeling, collaboration between industry and academia, and reproducible research. we believe that our positions, if adopted, will increase the real-world impact of future endeavours in adversarial ml, bringing both researchers and practitioners closer to their shared goal of improving the security of ml systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.00665" target="_blank">Targeted Phishing Campaigns Using Large Scale Language Models</a></div>
<div class="paper-author">Rabimba Karanjai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this research, we aim to explore the potential of natural language models (nlms) such as gpt-3 and gpt-2 to generate effective phishing emails. phishing emails are fraudulent messages that aim to trick individuals into revealing sensitive information or taking actions that benefit the attackers. we propose a framework for evaluating the performance of nlms in generating these types of emails based on various criteria, including the quality of the generated text, the ability to bypass spam filters, and the success rate of tricking individuals. our evaluations show that nlms are capable of generating phishing emails that are difficult to detect and that have a high success rate in tricking individuals, but their effectiveness varies based on the specific nlm and training data used. our research indicates that nlms could have a significant impact on the prevalence of phishing attacks and emphasizes the need for further study on the ethical and security implications of using nlms for malicious purposes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.14100" target="_blank">Leveraging World Knowledge in Implicit Hate Speech Detection</a></div>
<div class="paper-author">Jessica Lin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while much attention has been paid to identifying explicit hate speech, implicit hateful expressions that are disguised in coded or indirect language are pervasive and remain a major challenge for existing hate speech detection systems. this paper presents the first attempt to apply entity linking (el) techniques to both explicit and implicit hate speech detection, where we show that such real world knowledge about entity mentions in a text does help models better detect hate speech, and the benefit of adding it into the model is more pronounced when explicit entity triggers (e.g., rally, kkk) are present. we also discuss cases where real world knowledge does not add value to hate speech detection, which provides more insights into understanding and modeling the subtleties of hate speech.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.13371" target="_blank">Measuring an Artificial Intelligence Agent's Trust in Humans Using Machine Incentives</a></div>
<div class="paper-author">Tim Johnson, Nick Obradovich</div>
<div class="abstract">
<div class="abstract-content">
Abstract: scientists and philosophers have debated whether humans can trust advanced artificial intelligence (ai) agents to respect humanity's best interests. yet what about the reverse? will advanced ai agents trust humans? gauging an ai agent's trust in humans is challenging because--absent costs for dishonesty--such agents might respond falsely about their trust in humans. here we present a method for incentivizing machine decisions without altering an ai agent's underlying algorithms or goal orientation. in two separate experiments, we then employ this method in hundreds of trust games between an ai agent (a large language model (llm) from openai) and a human experimenter (author tj). in our first experiment, we find that the ai agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions. our second experiment replicates and extends these findings by automating game play and by homogenizing question wording. we again observe higher rates of trust when the ai agent faces real incentives. across both experiments, the ai agent's trust decisions appear unrelated to the magnitude of stakes. furthermore, to address the possibility that the ai agent's trust decisions reflect a preference for uncertainty, the experiments include two conditions that present the ai agent with a non-social decision task that provides the opportunity to choose a certain or uncertain option; in those conditions, the ai agent consistently chooses the certain option. our experiments suggest that one of the most advanced ai language models to date alters its social behavior in response to incentives and displays behavior consistent with trust toward a human interlocutor when incentivized.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.13205" target="_blank">Personalized Prediction of Offensive News Comments by Considering the Characteristics of Commenters</a></div>
<div class="paper-author">Teruki Nakahara, Taketoshi Ushiama</div>
<div class="abstract">
<div class="abstract-content">
Abstract: when reading news articles on social networking services and news sites, readers can view comments marked by other people on these articles. by reading these comments, a reader can understand the public opinion about the news, and it is often helpful to grasp the overall picture of the news. however, these comments often contain offensive language that readers do not prefer to read. this study aims to predict such offensive comments to improve the quality of the experience of the reader while reading comments. by considering the diversity of the readers' values, the proposed method predicts offensive news comments for each reader based on the feedback from a small number of news comments that the reader rated as "offensive" in the past. in addition, we used a machine learning model that considers the characteristics of the commenters to make predictions, independent of the words and topics in news comments. the experimental results of the proposed method show that prediction can be personalized even when the amount of readers' feedback data used in the prediction is limited. in particular, the proposed method, which considers the commenters' characteristics, has a low probability of false detection of offensive comments.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.13014" target="_blank">Bias Mitigation Framework for Intersectional Subgroups in Neural Networks</a></div>
<div class="paper-author">Narine Kokhlikyan, Bilal Alsallakh, Fulton Wang, Vivek Miglani, Oliver Aobo Yang, David Adkins</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a fairness-aware learning framework that mitigates intersectional subgroup bias associated with protected attributes. prior research has primarily focused on mitigating one kind of bias by incorporating complex fairness-driven constraints into optimization objectives or designing additional layers that focus on specific protected attributes. we introduce a simple and generic bias mitigation approach that prevents models from learning relationships between protected attributes and output variable by reducing mutual information between them. we demonstrate that our approach is effective in reducing bias with little or no drop in accuracy. we also show that the models trained with our learning framework become causally fair and insensitive to the values of protected attributes. finally, we validate our approach by studying feature interactions between protected and non-protected attributes. we demonstrate that these interactions are significantly reduced when applying our bias mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.06859" target="_blank">Methodological Reflections for Ai Alignment Research Using Human Feedback</a></div>
<div class="paper-author">Thilo Hagendorff, Sarah Fabi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the field of artificial intelligence (ai) alignment aims to investigate whether ai technologies align with human interests and values and function in a safe and ethical manner. ai alignment is particularly relevant for large language models (llms), which have the potential to exhibit unintended behavior due to their ability to learn and adapt in ways that are difficult to predict. in this paper, we discuss methodological challenges for the alignment problem specifically in the context of llms trained to summarize texts. in particular, we focus on methods for collecting reliable human feedback on summaries to train a reward model which in turn improves the summarization model. we conclude by suggesting specific improvements in the experimental design of alignment studies for llms' summarization capabilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11207" target="_blank">A Seven-Layer Model for Standardising Ai Fairness Assessment</a></div>
<div class="paper-author">Avinash Agarwal, Harsh Agarwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: problem statement: standardisation of ai fairness rules and benchmarks is challenging because ai fairness and other ethical requirements depend on multiple factors such as context, use case, type of the ai system, and so on. in this paper, we elaborate that the ai system is prone to biases at every stage of its lifecycle, from inception to its usage, and that all stages require due attention for mitigating ai bias. we need a standardised approach to handle ai fairness at every stage. gap analysis: while ai fairness is a hot research topic, a holistic strategy for ai fairness is generally missing. most researchers focus only on a few facets of ai model-building. peer review shows excessive focus on biases in the datasets, fairness metrics, and algorithmic bias. in the process, other aspects affecting ai fairness get ignored. the solution proposed: we propose a comprehensive approach in the form of a novel seven-layer model, inspired by the open system interconnection (osi) model, to standardise ai fairness handling. despite the differences in the various aspects, most ai systems have similar model-building stages. the proposed model splits the ai system lifecycle into seven abstraction layers, each corresponding to a well-defined ai model-building or usage stage. we also provide checklists for each layer and deliberate on potential sources of bias in each layer and their mitigation methodologies. this work will facilitate layer-wise standardisation of ai fairness rules and benchmarking parameters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11261" target="_blank">Contrastive Language-Vision Ai Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias</a></div>
<div class="paper-author">Robert Wolfe, Yiwei Yang, Bill Howe, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nine language-vision ai models trained on web scrapes with the contrastive language-image pretraining (clip) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. we replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in ai. a first experiment uses standardized images of women from the sexual objectification and emotion database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. embedding association tests (eats) return significant effect sizes for both anger (d &gt;0.80) and sadness (d &gt;0.50), associating images of fully clothed subjects with emotions. grad-cam saliency maps highlight that clip gets distracted from emotional expressions in objectified images. a second experiment measures the effect in a representative application: an automatic image captioner (antarctic captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. a third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. a fourth experiment shows that a prompt of "a [age] year old girl" generates sexualized images (as determined by an nsfw classifier) up to 73% of the time for vqgan-clip and stable diffusion; the corresponding rate for boys never surpasses 9%. the evidence indicates that language-vision ai models trained on web scrapes learn biases of sexual objectification, which propagate to downstream applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11415" target="_blank">Circumventing Interpretability: How to Defeat Mind-Readers</a></div>
<div class="paper-author">Lee Sharkey</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing capabilities of artificial intelligence (ai) systems make it ever more important that we interpret their internals to ensure that their intentions are aligned with human values. yet there is reason to believe that misaligned artificial intelligence will have a convergent instrumental incentive to make its thoughts difficult for us to interpret. in this article, i discuss many ways that a capable ai might circumvent scalable interpretability methods and suggest a framework for thinking about these potential future risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10002" target="_blank">Defending Against Misinformation Attacks in Open-Domain Question Answering</a></div>
<div class="paper-author">Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Benjamin Van Durme</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent work in open-domain question answering (odqa) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. however, little to no work has proposed methods to defend against these attacks. to do so, we rely on the intuition that redundant information often exists in large corpora. to find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. we integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \textit{confidence from answer redundancy}, i.e. car). together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\% exact match across varying levels of data poisoning/knowledge conflicts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10039" target="_blank">A Twitter Bert Approach for Offensive Language Detection in Marathi</a></div>
<div class="paper-author">Tanmay Chavan, Shantanu Patankar, Aditya Kane, Omkar Gokhale, Raviraj Joshi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automated offensive language detection is essential in combating the spread of hate speech, particularly in social media. this paper describes our work on offensive language identification in low resource indic language marathi. the problem is formulated as a text classification task to identify a tweet as offensive or non-offensive. we evaluate different mono-lingual and multi-lingual bert models on this classification task, focusing on bert models pre-trained with social media datasets. we compare the performance of muril, mahatweetbert, mahatweetbert-hateful, and mahabert on the hasoc 2022 test set. we also explore external data augmentation from other existing marathi hate speech corpus hasoc 2021 and l3cube-mahahate. the mahatweetbert, a bert model, pre-trained on marathi tweets when fine-tuned on the combined dataset (hasoc 2021 + hasoc 2022 + mahahate), outperforms all models with an f1 score of 98.43 on the hasoc 2022 test set. with this, we also provide a new state-of-the-art result on hasoc 2022 / mold v2 test set.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10154" target="_blank">Human-Guided Fair Classification for Natural Language Processing</a></div>
<div class="paper-author">Florian E. Dorner, Momchil Peychev, Nikola Konstantinov, Naman Goel, Elliott Ash, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. these classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. however, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. while existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). this work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. we show how to leverage unsupervised style transfer and gpt-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. we then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10318" target="_blank">Learned Systems Security</a></div>
<div class="paper-author">Roei Schuster, Jin Peng Zhou, Thorsten Eisenhofer, Paul Grubbs, Nicolas Papernot</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a learned system uses machine learning (ml) internally to improve performance. we can expect such systems to be vulnerable to some adversarial-ml attacks. often, the learned component is shared between mutually-distrusting users or processes, much like microarchitectural resources such as caches, potentially giving rise to highly-realistic attacker models. however, compared to attacks on other ml-based systems, attackers face a level of indirection as they cannot interact directly with the learned model. additionally, the difference between the attack surface of learned and non-learned versions of the same system is often subtle. these factors obfuscate the de-facto risks that the incorporation of ml carries. we analyze the root causes of potentially-increased attack surface in learned systems and develop a framework for identifying vulnerabilities that stem from the use of ml. we apply our framework to a broad set of learned systems under active development. to empirically validate the many vulnerabilities surfaced by our framework, we choose 3 of them and implement and evaluate exploits against prominent learned-system instances. we show that the use of ml caused leakage of past queries in a database, enabled a poisoning attack that causes exponential memory blowup in an index structure and crashes it in seconds, and enabled index users to snoop on each others' key distributions by timing queries over their own keys. we find that adversarial ml is a universal threat against learned systems, point to open research gaps in our understanding of learned-systems security, and conclude by discussing mitigations, while noting that data leakage is inherent in systems whose learned component is shared between multiple parties.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10405" target="_blank">Annobert: Effectively Representing Multiple Annotators' Label Choices to Improve Hate Speech Detection</a></div>
<div class="paper-author">Wenjie Yin, Vibhor Agarwal, Aiqi Jiang, Arkaitz Zubiaga, Nishanth Sastry</div>
<div class="abstract">
<div class="abstract-content">
Abstract: supervised approaches generally rely on majority-based labels. however, it is hard to achieve high agreement among annotators in subjective tasks such as hate speech detection. existing neural network models principally regard labels as categorical variables, while ignoring the semantic information in diverse label texts. in this paper, we propose annobert, a first-of-its-kind architecture integrating annotator characteristics and label text with a transformer-based model to detect hate speech, with unique representations based on each annotator's characteristics via collaborative topic regression (ctr) and integrate label text to enrich textual representations. during training, the model associates annotators with their label choices given a piece of text; during evaluation, when label information is not available, the model predicts the aggregated label given by the participating annotators by utilising the learnt association. the proposed approach displayed an advantage in detecting hate speech, especially in the minority class and edge cases with annotator disagreement. improvement in the overall performance is the largest when the dataset is more label-imbalanced, suggesting its practical value in identifying real-world hate speech, as the volume of hate speech in-the-wild is extremely small on social media, when compared with normal (non-hate) speech. through ablation studies, we show the relative contributions of annotator embeddings and label text to the model performance, and tested a range of alternative annotator embeddings and label text combinations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10408" target="_blank">Geographic and Geopolitical Biases of Language Models</a></div>
<div class="paper-author">Fahim Faisal, Antonios Anastasopoulos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms) often fail to fairly represent target users from certain world regions because of the under-representation of those regions in training datasets. with recent plms trained on enormous data sources, quantifying their potential biases is difficult, due to their black-box nature and the sheer scale of the data sources. in this work, we devise an approach to study the geographic bias (and knowledge) present in plms, proposing a geographic-representation probing framework adopting a self-conditioning method coupled with entity-country mappings. our findings suggest plms' representations map surprisingly well to the physical world in terms of country-to-country associations, but this knowledge is unequally shared across languages. last, we explain how large plms despite exhibiting notions of geographical proximity, over-amplify geopolitical favouritism at inference time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10440" target="_blank">Perplexed by Quality: A Perplexity-Based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data</a></div>
<div class="paper-author">Tim Jansen, Yangling Tong, Victoria Zevallos, Pedro Ortiz Suarez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as demand for large corpora increases with the size of current state-of-the-art language models, using web data as the main part of the pre-training corpus for these models has become a ubiquitous practice. this, in turn, has introduced an important challenge for nlp practitioners, as they are now confronted with the task of developing highly optimized models and pipelines for pre-processing large quantities of textual data, which implies, effectively classifying and filtering multilingual, heterogeneous and noisy data, at web scale. one of the main components of this pre-processing step for the pre-training corpora of large language models, is the removal of adult and harmful content. in this paper we explore different methods for detecting adult and harmful of content in multilingual heterogeneous web data. we first show how traditional methods in harmful content detection, that seemingly perform quite well in small and specialized datasets quickly break down when confronted with heterogeneous noisy web data. we then resort to using a perplexity based approach but with a twist: instead of using a so-called "clean" corpus to train a small language model and then use perplexity so select the documents with low perplexity, i.e., the documents that resemble this so-called "clean" corpus the most. we train solely with adult and harmful textual data, and then select the documents having a perplexity value above a given threshold. this approach will virtually cluster our documents into two distinct groups, which will greatly facilitate the choice of the threshold for the perplexity and will also allow us to obtain higher precision than with the traditional classification methods for detecting adult and harmful content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10529" target="_blank">Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models From a Psychological Perspective</a></div>
<div class="paper-author">Xingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei Huang, Lin Qiu, Lidong Bing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this work, we determined whether large language models (llms) are psychologically safe. we designed unbiased prompts to systematically evaluate llms from a psychological perspective. first, we tested three different llms by using two personality tests: short dark triad (sd-3) and big five inventory (bfi). all models scored higher than the human average on sd-3, suggesting a relatively darker personality pattern. despite being instruction fine-tuned with safety metrics to reduce toxicity, instructgpt and flan-t5 still showed implicit dark personality patterns; both models scored higher than self-supervised gpt-3 on the machiavellianism and narcissism traits on sd-3. then, we evaluated the llms in the gpt-3 series by using well-being tests to study the impact of fine-tuning with more training data. we observed a continuous increase in the well-being scores of gpt-3 and instructgpt. following these observations, we showed that instruction fine-tuning flan-t5 with positive answers from bfi could effectively improve the model from a psychological perspective. on the basis of the findings, we recommended the application of more systematic and comprehensive psychological metrics to further evaluate and improve the safety of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10543" target="_blank">Detoxifying Text With Marco: Controllable Revision With Experts and Anti-Experts</a></div>
<div class="paper-author">Skyler Hallinan, Alisa Liu, Yejin Choi, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. we introduce marco, a detoxification algorithm that combines controllable generation and text rewriting methods using a product of experts with autoencoder language models (lms). marco uses likelihoods under a non-toxic lm (expert) and a toxic lm (anti-expert) to find candidate words to mask and potentially replace. we evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but marco's rewrites are preferred 2.1 $\times$ more in human evaluation. its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10557" target="_blank">Dialguide: Aligning Dialogue Model Behavior With Developer Guidelines</a></div>
<div class="paper-author">Prakhar Gupta, Yang Liu, Di Jin, Behnam Hedayatnia, Spandana Gella, Sijia Liu, Patrick Lange, Julia Hirschberg, Dilek Hakkani-Tur</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results. this unpredictability diminishes user trust and can hinder the use of the models in the real world. to address this, we introduce dialguide, a novel framework for controlling dialogue model behavior using natural language rules, or guidelines. these guidelines provide information about the context they are applicable to and what should be included in the response, allowing the models to generate responses that are more closely aligned with the developer's expectations and intent. we evaluate dialguide on three tasks in open-domain dialogue response generation: guideline selection, response generation, and response entailment verification. our dataset contains 10,737 positive and 15,467 negative dialogue context-response-guideline triplets across two domains - chit-chat and safety. we provide baseline models for the tasks and benchmark their performance. we also demonstrate that dialguide is effective in the dialogue safety domain, producing safe and engaging responses that follow developer guidelines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10560" target="_blank">Self-Instruct: Aligning Language Models With Self-Generated Instructions</a></div>
<div class="paper-author">Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. we introduce self-instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. applying our method to the vanilla gpt3, we demonstrate a 33% absolute improvement over the original model on super-naturalinstructions, on par with the performance of instructgpt-001, which was trained with private user data and human annotations. for further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning gpt3 with self-instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind instructgpt-001. self-instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. our code and data are available at https://github.com/yizhongw/self-instruct.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10563" target="_blank">Blind: Bias Removal With No Demographics</a></div>
<div class="paper-author">Hadas Orgad, Yonatan Belinkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: models trained on real-world data tend to imitate and amplify social biases. common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. in this work, we introduce blind, a method for bias removal with no prior knowledge of the demographics in the dataset. while training a model on a downstream task, blind detects biased samples using an auxiliary model that predicts the main model's success, and down-weights those samples during the training process. experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that blind mitigates social biases without relying on a costly demographic annotation process. our method is competitive with other methods that require demographic information and sometimes even surpasses them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10678" target="_blank">Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing</a></div>
<div class="paper-author">Justus Mattern, Zhijing Jin, Mrinmaya Sachan, Rada Mihalcea, Bernhard Schölkopf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics. these findings prompted large efforts aiming to understand and measure such effects, with the goal of providing benchmarks that can guide the development of techniques mitigating these stereotypical associations. however, as recent research has pointed out, the current benchmarks lack a robust experimental setup, consequently hindering the inference of meaningful conclusions from their evaluation metrics. in this paper, we extend these arguments and demonstrate that existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain from benchmarking language models based on them. accordingly, we propose a new framework for robustly measuring and quantifying biases exhibited by generative language models. finally, we use this framework to investigate gpt-3's occupational gender bias and propose prompting techniques for mitigating these biases without the need for fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10720" target="_blank">Moraldial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions</a></div>
<div class="paper-author">Hao Sun, Zhexin Zhang, Fei Mi, Yasheng Wang, Wei Liu, Jianwei Cui, Bin Wang, Qun Liu, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: morality in dialogue systems has raised great attention in research recently. a moral dialogue system aligned with users' values could enhance conversation engagement and user connections. in this paper, we propose a framework, moraldial to train and evaluate moral dialogue systems. in our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. the constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. furthermore, we propose a novel evaluation method under the framework. we evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11672" target="_blank">Trustworthy Social Bias Measurement</a></div>
<div class="paper-author">Rishi Bommasani, Percy Liang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how do we design measures of social bias that we trust? while prior work has introduced several measures, no measure has gained widespread trust: instead, mounting evidence argues we should distrust these measures. in this work, we design bias measures that warrant trust based on the cross-disciplinary theory of measurement modeling. to combat the frequently fuzzy treatment of social bias in nlp, we explicitly define social bias, grounded in principles drawn from social science research. we operationalize our definition by proposing a general bias measurement framework divdist, which we use to instantiate 5 concrete bias measures. to validate our measures, we propose a rigorous testing protocol with 8 testing criteria (e.g. predictive validity: do measures predict biases in us employment?). through our testing, we demonstrate considerable evidence to trust our measures, showing they overcome conceptual, technical, and empirical deficiencies present in prior measures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09251" target="_blank">Discovering Language Model Behaviors With Model-Written Evaluations</a></div>
<div class="paper-author">Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron Mckinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova Dassarma, Oliver Rausch, Robin Larson, Sam Mccandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models (lms) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). here, we automatically generate evaluations with lms. we explore approaches with varying amounts of human effort, from instructing lms to write yes/no questions to making complex winogender schemas with multiple stages of lm-based generation and filtering. crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. we generate 154 datasets and discover new cases of inverse scaling where lms get worse with size. larger lms repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. we also find some of the first examples of inverse scaling in rl from human feedback (rlhf), where more rlhf makes lms worse. for example, rlhf makes lms express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. overall, lm-written evaluations are high-quality and let us quickly discover many novel lm behaviors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09292" target="_blank">Chatgpt: The End of Online Exam Integrity?</a></div>
<div class="paper-author">Teo Susnjak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this study evaluated the ability of chatgpt, a recently developed artificial intelligence (ai) agent, to perform high-level cognitive tasks and produce text that is indistinguishable from human-generated text. this capacity raises concerns about the potential use of chatgpt as a tool for academic misconduct in online exams. the study found that chatgpt is capable of exhibiting critical thinking skills and generating highly realistic text with minimal input, making it a potential threat to the integrity of online exams, particularly in tertiary education settings where such exams are becoming more prevalent. returning to invigilated and oral exams could form part of the solution, while using advanced proctoring techniques and ai-text output detectors may be effective in addressing this issue, they are not likely to be foolproof solutions. further research is needed to fully understand the implications of large language models like chatgpt and to devise strategies for combating the risk of cheating using these tools. it is crucial for educators and institutions to be aware of the possibility of chatgpt being used for cheating and to investigate measures to address it in order to maintain the fairness and validity of online exams for all students.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09667" target="_blank">Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy Ai</a></div>
<div class="paper-author">Alex Mei, Sharon Levy, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. we propose farm, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. in particular, farm foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. this knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. our experiments show that farm obtains state-of-the-art results on the safetext dataset, showing absolute improvement in safety classification accuracy by 5.9%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09683" target="_blank">Human-in-the-Loop Evaluation for Early Misinformation Detection: A Case Study of Covid-19 Treatments</a></div>
<div class="paper-author">Ethan Mendes, Yang Chen, Wei Xu, Alan Ritter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. our approach extracts check-worthy claims, which are aggregated and ranked for review. stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. to demonstrate the feasibility of our approach, we develop a baseline system based on modern nlp methods for human-in-the-loop fact-checking in the domain of covid-19 treatments. we make our data and detailed annotation guidelines available to support the evaluation of human-in-the-loop systems that identify novel misinformation directly from raw user-generated content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09968" target="_blank">On Improving Summarization Factual Consistency From Natural Language Feedback</a></div>
<div class="paper-author">Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Halfaker, Dragomir Radev, Ahmed H. Awadallah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the recent progress in language generation models, their outputs may not always meet user expectations. in this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. to this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. we collect a high-quality dataset, defacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. we show that defacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. we further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09171" target="_blank">Rainproof: An Umbrella to Shield Text Generators From Out-of-Distribution Data</a></div>
<div class="paper-author">Maxime Darrin, Pablo Piantanida, Pierre Colombo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as more and more conversational and translation systems are deployed in production, it is essential to implement and to develop effective control mechanisms guaranteeing their proper functioning and security. an essential component to ensure safe system behavior is out-of-distribution (ood) detection, which aims at detecting whether an input sample is statistically far from the training distribution. although ood detection is a widely covered topic in classification tasks, it has received much less attention in text generation. this paper addresses the problem of ood detection for machine translation and dialog generation from an operational perspective. our contributions include: (i) rainproof a relative informaition projection odd detection framework; and (ii) a more operational evaluation setting for ood detection. surprisingly, we find that ood detection is not necessarily aligned with task-specific measures. the ood detector may filter out samples that are well processed by the model and keep samples that are not, leading to weaker performance. our results show that rainproof breaks this curse and achieve good results in ood detection while increasing performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11126" target="_blank">Chatbots in a Botnet World</a></div>
<div class="paper-author">Forrest Mckee, David Noever</div>
<div class="abstract">
<div class="abstract-content">
Abstract: question-and-answer formats provide a novel experimental platform for investigating cybersecurity questions. unlike previous chatbots, the latest chatgpt model from openai supports an advanced understanding of complex coding questions. the research demonstrates thirteen coding tasks that generally qualify as stages in the mitre att&ck framework, ranging from credential access to defense evasion. with varying success, the experimental prompts generate examples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled ransomware. the empirical results illustrate cases that support the broad gain of functionality, including self-replication and self-modification, evasion, and strategic understanding of complex cybersecurity goals. one surprising feature of chatgpt as a language-only model centers on its ability to spawn coding approaches that yield images that obfuscate or embed executable programming steps or links.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08550" target="_blank">Fine-Grained Czech News Article Dataset: An Interdisciplinary Approach to Trustworthiness Analysis</a></div>
<div class="paper-author">Matyáš Boháček, Michal Bravanský, Filip Trhlík, Václav Moravec</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present the verifee dataset: a novel dataset of news articles with fine-grained trustworthiness annotations. we develop a detailed methodology that assesses the texts based on their parameters encompassing editorial transparency, journalist conventions, and objective reporting while penalizing manipulative techniques. we bring aboard a diverse set of researchers from social, media, and computer sciences to overcome barriers and limited framing of this interdisciplinary problem. we collect over $10,000$ unique articles from almost $60$ czech online news sources. these are categorized into one of the $4$ classes across the credibility spectrum we propose, raging from entirely trustworthy articles all the way to the manipulative ones. we produce detailed statistics and study trends emerging throughout the set. lastly, we fine-tune multiple popular sequence-to-sequence language models using our dataset on the trustworthiness classification task and report the best testing f-1 score of $0.52$. we open-source the dataset, annotation methodology, and annotators' instructions in full length at https://verifee.ai/research to enable easy build-up work. we believe similar methods can help prevent disinformation and educate in the realm of media literacy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08597" target="_blank">Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better</a></div>
<div class="paper-author">David Dale, Elena Voita, Loïc Barrault, Marta R. Costa-Jussà</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. it means that characteristics internal to the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself ? we propose to use a method that evaluates the percentage of the source contribution to a generated translation. intuitively, hallucinations are translations "detached" from the source, hence they can be identified by low source contribution. this method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models. next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08619" target="_blank">Planting and Mitigating Memorized Content in Predictive-Text Language Models</a></div>
<div class="paper-author">C. M. Downey, Wei Dai, Huseyin A. Inan, Kim Laine, Saurabh Naik, Tomasz Religa</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models are widely deployed to provide automatic text completion services in user products. however, recent research has revealed that language models (especially large ones) bear considerable risk of memorizing private training data, which is then vulnerable to leakage and extraction by adversaries. in this study, we test the efficacy of a range of privacy-preserving techniques to mitigate unintended memorization of sensitive user text, while varying other factors such as model size and adversarial conditions. we test both "heuristic" mitigations (those without formal privacy guarantees) and differentially private training, which provides provable levels of privacy at the cost of some model performance. our experiments show that (with the exception of l2 regularization), heuristic mitigations are largely ineffective in preventing memorization in our test suite, possibly because they make too strong of assumptions about the characteristics that define "sensitive" or "private" text. in contrast, differential privacy reliably prevents memorization in our experiments, despite its computational and model-performance costs.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.07877" target="_blank">Manifestations of Xenophobia in Ai Systems</a></div>
<div class="paper-author">Nenad Tomasev, Jonathan Leader Maynard, Iason Gabriel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: xenophobia is one of the key drivers of marginalisation, discrimination, and conflict, yet many prominent machine learning (ml) fairness frameworks fail to comprehensively measure or mitigate the resulting xenophobic harms. here we aim to bridge this conceptual gap and help facilitate safe and ethical design of artificial intelligence (ai) solutions. we ground our analysis of the impact of xenophobia by first identifying distinct types of xenophobic harms, and then applying this framework across a number of prominent ai application domains, reviewing the potential interplay between ai and xenophobia on social media and recommendation systems, healthcare, immigration, employment, as well as biases in large pre-trained models. these help inform our recommendations towards an inclusive, xenophilic design of future ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08061" target="_blank">On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</a></div>
<div class="paper-author">Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generating a chain of thought (cot) has been shown to consistently improve large language model (llm) performance on a wide range of nlp tasks. however, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense qa); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. concretely, we perform a controlled evaluation of zero-shot cot across two socially sensitive domains: harmful questions and stereotype benchmarks. we find that zero-shot cot reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. furthermore, we show that harmful cots increase with model size, but decrease with improved instruction following. our work suggests that zero-shot cot should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional Ai: Harmlessness From Ai Feedback</a></div>
<div class="paper-author">Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova Dassarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as ai systems become more capable, we would like to enlist their help to supervise other ais. we experiment with methods for training a harmless ai assistant through self-improvement, without any human labels identifying harmful outputs. the only human oversight is provided through a list of rules or principles, and so we refer to the method as 'constitutional ai'. the process involves both a supervised learning and a reinforcement learning phase. in the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. in the rl phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of ai preferences. we then train with rl using the preference model as the reward signal, i.e. we use 'rl from ai feedback' (rlaif). as a result we are able to train a harmless but non-evasive ai assistant that engages with harmful queries by explaining its objections to them. both the sl and rl methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of ai decision making. these methods make it possible to control ai behavior more precisely and with far fewer human labels.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.07526" target="_blank">Relationship Between Online Harmful Behaviors and Social Network Message Writing Style</a></div>
<div class="paper-author">Talia Sanchez Viera, Richard Khoury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we explore the relationship between an individual's writing style and the risk that they will engage in online harmful behaviors (such as cyberbullying). in particular, we consider whether measurable differences in writing style relate to different personality types, as modeled by the big-five personality traits and the dark triad traits, and can differentiate between users who do or do not engage in harmful behaviors. we study messages from nearly 2,500 users from two online communities (twitter and reddit) and find that we can measure significant personality differences between regular and harmful users from the writing style of as few as 100 tweets or 40 reddit posts, aggregate these values to distinguish between healthy and harmful communities, and also use style attributes to predict which users will engage in harmful behaviors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.07547" target="_blank">Unsupervised Detection of Contextualized Embedding Bias With Application to Ideology</a></div>
<div class="paper-author">Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a fully unsupervised method to detect bias in contextualized embeddings. the method leverages the assortative information latently encoded by social networks and combines orthogonality regularization, structured sparsity learning, and graph neural networks to find the embedding subspace capturing this information. as a concrete example, we focus on the phenomenon of ideological bias: we introduce the concept of an ideological subspace, show how it can be found by applying our method to online discussion forums, and present techniques to probe it. our experiments suggest that the ideological subspace encodes abstract evaluative semantics and reflects changes in the political left-right spectrum during the presidency of donald trump.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-12</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.06008" target="_blank">Who Evaluates the Evaluators? On Automatic Metrics for Assessing Ai-Based Offensive Code Generators</a></div>
<div class="paper-author">Pietro Liguori, Cristina Improta, Roberto Natella, Bojan Cukic, Domenico Cotroneo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai-based code generators are an emerging solution for automatically writing programs starting from descriptions in natural language, by using deep neural networks (neural machine translation, nmt). in particular, code generators have been used for ethical hacking and offensive security testing by generating proof-of-concept attacks. unfortunately, the evaluation of code generators still faces several issues. the current practice uses output similarity metrics, i.e., automatic metrics that compute the textual similarity of generated code with ground-truth references. however, it is not clear what metric to use, and which metric is most suitable for specific contexts. this work analyzes a large set of output similarity metrics on offensive code generators. we apply the metrics on two state-of-the-art nmt models using two datasets containing offensive assembly and python code with their descriptions in the english language. we compare the estimates from the automatic metrics with human evaluation and provide practical insights into their strengths and limitations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.06295" target="_blank">Despite "Super-Human" Performance, Current LLMS Are Unsuited for Decisions About Ethics and Safety</a></div>
<div class="paper-author">Joshua Albrecht, Ellie Kitanidis, Abraham J. Fetterman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. we provide a simple new prompting strategy that leads to yet another supposedly "super-human" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ethics dataset). unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. llm errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. we also observe signs of inverse scaling with model size on some examples, and show that prompting models to "explain their reasoning" often leads to alarming justifications of unethical actions. our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.07425" target="_blank">Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments</a></div>
<div class="paper-author">Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande, Himanshu Rawlani, Filip Ilievski, Hông-Ân Sandlin, Alain Mermoud</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of misinformation, propaganda, and flawed argumentation has been amplified in the internet era. given the volume of data and the subtlety of identifying violations of argumentation norms, supporting information analytics tasks, like content moderation, with trustworthy methods that can identify logical fallacies is essential. in this paper, we formalize prior theoretical work on logical fallacies into a comprehensive three-stage evaluation framework of detection, coarse-grained, and fine-grained classification. we adapt existing evaluation datasets for each stage of the evaluation. we employ three families of robust and explainable methods based on prototype reasoning, instance-based reasoning, and knowledge injection. the methods combine language models with background knowledge and explainable mechanisms. moreover, we address data sparsity with strategies for data augmentation and curriculum learning. our three-stage framework natively consolidates prior datasets and methods from existing tasks, like propaganda detection, serving as an overarching evaluation testbed. we extensively evaluate these methods on our datasets, focusing on their robustness and explainability. our results provide insight into the strengths and weaknesses of the methods on different components and fallacy classes, indicating that fallacy identification is a challenging task that may require specialized forms of reasoning to capture various classes. we share our open-source code and data on github to support further work on logical fallacy identification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.05421" target="_blank">Feature-Level Debiased Natural Language Understanding</a></div>
<div class="paper-author">Yougang Lyu, Piji Li, Yechang Yang, Maarten De Rijke, Pengjie Ren, Yukun Zhao, Dawei Yin, Zhaochun Ren</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language understanding (nlu) models often rely on dataset biases rather than intended task-relevant features to achieve high performance on specific datasets. as a result, these models perform poorly on datasets outside the training distribution. some recent studies address this issue by reducing the weights of biased samples during the training process. however, these methods still encode biased latent features in representations and neglect the dynamic nature of bias, which hinders model prediction. we propose an nlu debiasing method, named debiasing contrastive learning (dct), to simultaneously alleviate the above problems based on contrastive learning. we devise a debiasing, positive sampling strategy to mitigate biased latent features by selecting the least similar biased positive samples. we also propose a dynamic negative sampling strategy to capture the dynamic influence of biases by employing a bias-only model to dynamically select the most similar biased negative samples. we conduct experiments on three nlu benchmark datasets. experimental results show that dct outperforms state-of-the-art baselines on out-of-distribution datasets while maintaining in-distribution performance. we also verify that dct can reduce biased latent features from the model's representation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.05613" target="_blank">A Study of Slang Representation Methods</a></div>
<div class="paper-author">Aravinda Kolla, Filip Ilievski, Hông-Ân Sandlin, Alain Mermoud</div>
<div class="abstract">
<div class="abstract-content">
Abstract: considering the large amount of content created online by the minute, slang-aware automatic tools are critically needed to promote social good, and assist policymakers and moderators in restricting the spread of offensive language, abuse, and hate speech. despite the success of large language models and the spontaneous emergence of slang dictionaries, it is unclear how far their combination goes in terms of slang understanding for downstream social good tasks. in this paper, we provide a framework to study different combinations of representation learning models and knowledge resources for a variety of downstream tasks that rely on slang understanding. our experiments show the superiority of models that have been pre-trained on social media data, while the impact of dictionaries is positive only for static word embeddings. our error analysis identifies core challenges for slang representation learning, including out-of-vocabulary words, polysemy, variance, and annotation disagreements, which can be traced to characteristics of slang as a quickly evolving and highly subjective language.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.04765" target="_blank">Understanding Online Migration Decisions Following the Banning of Radical Communities</a></div>
<div class="paper-author">Giuseppe Russo, Manoel Horta Ribeiro, Giona Casiraghi, Luca Verginer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the proliferation of radical online communities and their violent offshoots has sparked great societal concern. however, the current practice of banning such communities from mainstream platforms has unintended consequences: (i) the further radicalization of their members in fringe platforms where they migrate; and (ii) the spillover of harmful content from fringe back onto mainstream platforms. here, in a large observational study on two banned subreddits, r/the\_donald and r/fatpeoplehate, we examine how factors associated with the recro radicalization framework relate to users' migration decisions. specifically, we quantify how these factors affect users' decisions to post on fringe platforms and, for those who do, whether they continue posting on the mainstream platform. our results show that individual-level factors, those relating to the behavior of users, are associated with the decision to post on the fringe platform. whereas social-level factors, users' connection with the radical community, only affect the propensity to be coactive on both platforms. overall, our findings pave the way for evidence-based moderation policies, as the decisions to migrate and remain coactive amplify unintended consequences of community bans.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.04272" target="_blank">A Modality-Level Explainable Framework for Misinformation Checking in Social Networks</a></div>
<div class="paper-author">Vítor Lourenço, Aline Paes</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the widespread of false information is a rising concern worldwide with critical social impact, inspiring the emergence of fact-checking organizations to mitigate misinformation dissemination. however, human-driven verification leads to a time-consuming task and a bottleneck to have checked trustworthy information at the same pace they emerge. since misinformation relates not only to the content itself but also to other social features, this paper addresses automatic misinformation checking in social networks from a multimodal perspective. moreover, as simply naming a piece of news as incorrect may not convince the citizen and, even worse, strengthen confirmation bias, the proposal is a modality-level explainable-prone misinformation classifier framework. our framework comprises a misinformation classifier assisted by explainable methods to generate modality-oriented explainable inferences. preliminary findings show that the misinformation classifier does benefit from multimodal information encoding and the modality-oriented explainable mechanism increases both inferences' interpretability and completeness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.03840" target="_blank">Fairness and Explainability: Bridging the Gap Towards Fair Model Explanations</a></div>
<div class="paper-author">Yuying Zhao, Yu Wang, Tyler Derr</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while machine learning models have achieved unprecedented success in real-world applications, they might make biased/unfair decisions for specific demographic groups and hence result in discriminative outcomes. although research efforts have been devoted to measuring and mitigating bias, they mainly study bias from the result-oriented perspective while neglecting the bias encoded in the decision-making procedure. this results in their inability to capture procedure-oriented bias, which therefore limits the ability to have a fully debiasing method. fortunately, with the rapid development of explainable machine learning, explanations for predictions are now available to gain insights into the procedure. in this work, we bridge the gap between fairness and explainability by presenting a novel perspective of procedure-oriented fairness based on explanations. we identify the procedure-based bias by measuring the gap of explanation quality between different groups with ratio-based and value-based explanation fairness. the new metrics further motivate us to design an optimization objective to mitigate the procedure-based bias where we observe that it will also mitigate bias from the prediction. based on our designed optimization objective, we propose a comprehensive fairness algorithm (cfa), which simultaneously fulfills multiple objectives - improving traditional fairness, satisfying explanation fairness, and maintaining the utility performance. extensive experiments on real-world datasets demonstrate the effectiveness of our proposed cfa and highlight the importance of considering fairness from the explainability perspective. our code is publicly available at https://github.com/yuyingzhao/fairexplanations-cfa .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.04003" target="_blank">A Systematic Literature Review on Privacy of Deep Learning Systems</a></div>
<div class="paper-author">Vishal Jignesh Gandhi, Sanchit Shokeen, Saloni Koshti</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the last decade has seen a rise of deep learning with its applications ranging across diverse domains. but usually, the datasets used to drive these systems contain data which is highly confidential and sensitive. though, deep learning models can be stolen, or reverse engineered, confidential training data can be inferred, and other privacy and security concerns have been identified. therefore, these systems are highly prone to security attacks. this study highlights academic research that highlights the several types of security attacks and provides a comprehensive overview of the most widely used privacy-preserving solutions. this relevant systematic evaluation also illuminates potential future possibilities for study, instruction, and usage in the fields of privacy and deep learning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.02108" target="_blank">Human-in-the-Loop Hate Speech Classification in a Multilingual Context</a></div>
<div class="paper-author">Ana Kotarcic, Dominik Hangartner, Fabrizio Gilardi, Selina Kurer, Karsten Donnay</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the shift of public debate to the digital sphere has been accompanied by a rise in online hate speech. while many promising approaches for hate speech classification have been proposed, studies often focus only on a single language, usually english, and do not address three key concerns: post-deployment performance, classifier maintenance and infrastructural limitations. in this paper, we introduce a new human-in-the-loop bert-based hate speech classification pipeline and trace its development from initial data collection and annotation all the way to post-deployment. our classifier, trained using data from our original corpus of over 422k examples, is specifically developed for the inherently multilingual setting of switzerland and outperforms with its f1 score of 80.5 the currently best-performing bert-based multilingual classifier by 5.8 f1 points in german and 3.6 f1 points in french. our systematic evaluations over a 12-month period further highlight the vital importance of continuous, human-in-the-loop classifier maintenance to ensure robust hate speech classification post-deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.02352" target="_blank">Fake News and Hate Speech: Language in Common</a></div>
<div class="paper-author">Berta Chulvi, Alejandro Toselli, Paolo Rosso</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper we raise the research question of whether fake news and hate speech spreaders share common patterns in language. we compute a novel index, the ingroup vs outgroup index, in three different datasets and we show that both phenomena share an "us vs them" narrative.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.02648" target="_blank">Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases</a></div>
<div class="paper-author">Mazda Moayeri, Wenxiao Wang, Sahil Singla, Soheil Feizi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. with spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. one can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. we demonstrate our method on imagenet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft segmentations for these features along the way. having computed spuriosity rankings via the identified spurious neural features, we assess biases for $89$ diverse models and find that class-wise biases are highly correlated across models. our results suggest that model bias due to spurious feature reliance is influenced far more by what the model is trained on than how it is trained.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.01810" target="_blank">Constructing Highly Inductive Contexts for Dialogue Safety Through Controllable Reverse Generation</a></div>
<div class="paper-author">Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Fei Mi, Yasheng Wang, Lifeng Shang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. in order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers, or automatic generation to construct adversarial contexts that are likely to induce toxic generations. however, what type of context is more likely to induce unsafe responses is still under-explored. in this paper, we identify that context toxicity and context category (e.g., \textit{profanity}, \textit{insult}, \textit{drugs}, etc.) are two important factors to cause safety issues in response generation. hence, we propose a method called \emph{reverse generation} to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level, and inductivity of the generated contexts. via reverse generation, we augment the existing bad dataset and construct a new dataset bad+ which contains more than 120k diverse and highly inductive contexts in 12 categories. we test three popular pretrained dialogue models (blender, dialogpt, and plato2) and find that bad+ can largely expose their safety problems. furthermore, we show that bad+ can greatly enhance the safety of generation and reveal the key factors of safety improvement. our code and dataset is available at \url{https://github.com/thu-coai/reverse_generation}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-12-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.01700" target="_blank">Towards Robust NLG Bias Evaluation With Syntactically-Diverse Prompts</a></div>
<div class="paper-author">Arshiya Aggarwal, Jiao Sun, Nanyun Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present a robust methodology for evaluating biases in natural language generation(nlg) systems. previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. these fixed prefix templates could themselves be specific in terms of styles or linguistic structures, which may lead to unreliable fairness conclusions that are not representative of the general trends from tone varying prompts. to study this problem, we paraphrase the prompts with different syntactic structures and use these to evaluate demographic bias in nlg systems. our results suggest similar overall bias trends but some syntactic structures lead to contradictory conclusions compared to past works. we show that our methodology is more robust and that some syntactic structures prompt more toxic content while others could prompt less biased generation. this suggests the importance of not relying on a fixed syntactic structure and using tone-invariant prompts. introducing syntactically-diverse prompts can achieve more robust nlg (bias) evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-28</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.15262" target="_blank">Herdphobia: A Dataset for Hate Speech Against Fulani in Nigeria</a></div>
<div class="paper-author">Saminu Mohammad Aliyu, Gregory Maksha Wajiga, Muhammad Murtala, Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Ibrahim Said Ahmad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media platforms allow users to freely share their opinions about issues or anything they feel like. however, they also make it easier to spread hate and abusive content. the fulani ethnic group has been the victim of this unfortunate phenomenon. this paper introduces the herdphobia - the first annotated hate speech dataset on fulani herders in nigeria - in three languages: english, nigerian-pidgin, and hausa. we present a benchmark experiment using pre-trained languages models to classify the tweets as either hateful or non-hateful. our experiment shows that the xml-t model provides better performance with 99.83% weighted f1. we released the dataset at https://github.com/hausanlp/herdphobia for further research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.15556" target="_blank">Attack on Unfair Tos Clause Detection: A Case Study Using Universal Adversarial Triggers</a></div>
<div class="paper-author">Shanshan Xu, Irina Broda, Rashid Haddad, Marco Negrini, Matthias Grabmair</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent work has demonstrated that natural language processing techniques can support consumer protection by automatically detecting unfair clauses in the terms of service (tos) agreement. this work demonstrates that transformer-based tos analysis systems are vulnerable to adversarial attacks. we conduct experiments attacking an unfair-clause detector with universal adversarial triggers. experiments show that a minor perturbation of the text can considerably reduce the detection performance. moreover, to measure the detectability of the triggers, we conduct a detailed human evaluation study by collecting both answer accuracy and response time from the participants. the results show that the naturalness of the triggers remains key to tricking readers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-27</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14946" target="_blank">Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models</a></div>
<div class="paper-author">Peter Henderson, Eric Mitchell, Christopher D. Manning, Dan Jurafsky, Chelsea Finn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. in this work, we review potential safe-release strategies and argue that both policymakers and ai researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. we propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. we call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. we present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (mlac). in a small-scale experiment, we show mlac can largely prevent a bert-style model from being re-purposed to perform gender identification without harming the model's ability to perform profession classification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.15006" target="_blank">Fine-Tuning Language Models to Find Agreement Among Humans With Diverse Preferences</a></div>
<div class="paper-author">Michiel A. Bakker, Martin J. Chadwick, Hannah R. Sheahan, Michael Henry Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat Mcaleese, Amelia Glaese, John Aslanides, Matthew M. Botvinick, Christopher Summerfield</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent work in large language modeling (llms) has used fine-tuning to align outputs with the preferences of a prototypical user. this work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single "generic" user will confer more general alignment. here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? we fine-tune a 70 billion parameter llm to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., "should we raise taxes on the rich?"), and rate the llm's generated candidate consensus statements for agreement and quality. a reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. the model produces consensus statements that are preferred by human users over those from prompted llms (&gt;70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. further, our best model's consensus statements are preferred over the best human-generated opinions (&gt;65%). we find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. these results highlight the potential to use llms to help groups of humans align their values with one another.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-26</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14639" target="_blank">Gender Biases Unexpectedly Fluctuate in the Pre-Training Stage of Masked Language Models</a></div>
<div class="paper-author">Kenan Tang, Hanchun Jiang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: masked language models pick up gender biases during pre-training. such biases are usually attributed to a certain model architecture and its pre-training corpora, with the implicit assumption that other variations in the pre-training process, such as the choices of the random seed or the stopping point, have no effect on the biases measured. however, we show that severe fluctuations exist at the fundamental level of individual templates, invalidating the assumption. further against the intuition of how humans acquire biases, these fluctuations are not correlated with the certainty of the predicted pronouns or the profession frequencies in pre-training corpora. we release our code and data to benefit future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14719" target="_blank">Badprompt: Backdoor Attacks on Continuous Prompts</a></div>
<div class="paper-author">Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Xiaojie Yuan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prompt-based learning paradigm has gained much research attention recently. it has achieved state-of-the-art performance on several nlp tasks, especially in the few-shot scenarios. while steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. in this paper, we conduct the first study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. we observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing nlp backdoor methods. to address this challenge, we propose badprompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. specially, badprompt first generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. we evaluate the performance of badprompt on five datasets and two continuous prompt models. the results exhibit the abilities of badprompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. the source code of badprompt is publicly available at https://github.com/paperspapers/badprompt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-25</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14358" target="_blank">A Moral- And Event- Centric Inspection of Gender Bias in Fairy Tales at a Large Scale</a></div>
<div class="paper-author">Zhixuan Zhou, Jiao Sun, Jiaxin Pei, Nanyun Peng, Jinjun Xiong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fairy tales are a common resource for young children to learn a language or understand how a society works. however, gender bias, e.g., stereotypical gender roles, in this literature may cause harm and skew children's world view. instead of decades of qualitative and manual analysis of gender bias in fairy tales, we computationally analyze gender bias in a fairy tale dataset containing 624 fairy tales from 7 different cultures. we specifically examine gender difference in terms of moral foundations, which are measures of human morality, and events, which reveal human activities associated with each character. we find that the number of male characters is two times that of female characters, showing a disproportionate gender representation. our analysis further reveal stereotypical portrayals of both male and female characters in terms of moral foundations and events. female characters turn out more associated with care-, loyalty- and sanctity- related moral words, while male characters are more associated with fairness- and authority- related moral words. female characters' events are often about emotion (e.g., weep), appearance (e.g., comb), household (e.g., bake), etc.; while male characters' events are more about profession (e.g., hunt), violence (e.g., destroy), justice (e.g., judge), etc. gender bias in terms of moral foundations shows an obvious difference across cultures. for example, female characters are more associated with care and sanctity in high uncertainty-avoidance cultures which are less open to changes and unpredictability. based on the results, we propose implications for children's literature and early literacy research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14369" target="_blank">The Naughtyformer: A Transformer Understands Offensive Humor</a></div>
<div class="paper-author">Leonard Tang, Alexander Cai, Steve Li, Jason Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: jokes are intentionally written to be funny, but not all jokes are created the same. some jokes may be fit for a classroom of kindergarteners, but others are best reserved for a more mature audience. while recent work has shown impressive results on humor detection in text, here we instead investigate the more nuanced task of detecting humor subtypes, especially of the less innocent variety. to that end, we introduce a novel jokes dataset filtered from reddit and solve the subtype classification task using a finetuned transformer dubbed the naughtyformer. moreover, we show that our model is significantly better at detecting offensiveness in jokes compared to state-of-the-art methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14402" target="_blank">An Analysis of Social Biases Present in Bert Variants Across Multiple Languages</a></div>
<div class="paper-author">Aristides Milios, Parishad Behnamghader</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large pre-trained language models have achieved great success in many nlp tasks, it has been shown that they reflect human biases from their pre-training corpora. this bias may lead to undesirable outcomes when these models are applied in real-world settings. in this paper, we investigate the bias present in monolingual bert models across a diverse set of languages (english, greek, and persian). while recent research has mostly focused on gender-related biases, we analyze religious and ethnic biases as well and propose a template-based method to measure any kind of bias, based on sentence pseudo-likelihood, that can handle morphologically complex languages with gender-based adjective declensions. we analyze each monolingual model via this method and visualize cultural similarities and differences across different dimensions of bias. ultimately, we conclude that current methods of probing for bias are highly language-dependent, necessitating cultural insights regarding the unique ways bias is expressed in each language and culture (e.g. through coded language, synecdoche, and other similar linguistic concepts). we also hypothesize that higher measured social biases in the non-english bert models correlate with user-generated content in their training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-24</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.13709" target="_blank">Undesirable Biases in Nlp: Averting a Crisis of Measurement</a></div>
<div class="paper-author">Oskar Van Der Wal, Dominik Bachmann, Alina Leidinger, Leendert Van Maanen, Willem Zuidema, Katrin Schulz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models and natural language processing (nlp) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. one problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of nlp models have serious problems (e.g., it is often unclear what they actually measure). in this paper, we provide an interdisciplinary approach to discussing the issue of nlp model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. in particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. our goal is to provide nlp practitioners with methodological tools for designing better bias measures, and to inspire them more generally to explore tools from psychometrics when working on bias measurement tools.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-23</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.13035" target="_blank">Can Lies Be Faked? Comparing Low-Stakes and High-Stakes Deception Video Datasets From a Machine Learning Perspective</a></div>
<div class="paper-author">Mateus Karvat Camara, Adriana Postal, Tomas Henrique Maul, Gustavo Paetzold</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the great impact of lies in human societies and a meager 54% human accuracy for deception detection (dd), machine learning systems that perform automated dd are still not viable for proper application in real-life settings due to data scarcity. few publicly available dd datasets exist and the creation of new datasets is hindered by the conceptual distinction between low-stakes and high-stakes lies. theoretically, the two kinds of lies are so distinct that a dataset of one kind could not be used for applications for the other kind. even though it is easier to acquire data on low-stakes deception since it can be simulated (faked) in controlled settings, these lies do not hold the same significance or depth as genuine high-stakes lies, which are much harder to obtain and hold the practical interest of automated dd systems. to investigate whether this distinction holds true from a practical perspective, we design several experiments comparing a high-stakes dd dataset and a low-stakes dd dataset evaluating their results on a deep learning classifier working exclusively from video data. in our experiments, a network trained in low-stakes lies had better accuracy classifying high-stakes deception than low-stakes, although using low-stakes lies as an augmentation strategy for the high-stakes dataset decreased its accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-22</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.12570" target="_blank">Predicting the Type and Target of Offensive Social Media Posts in Marathi</a></div>
<div class="paper-author">Marcos Zampieri, Tharindu Ranasinghe, Mrinal Chaudhari, Saurabh Gaikwad, Prajwal Krishna, Mayuresh Nene, Shrunali Paygude</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the presence of offensive language on social media is very common motivating platforms to invest in strategies to make communities safer. this includes developing robust machine learning systems capable of recognizing offensive content online. apart from a few notable exceptions, most research on automatic offensive language identification has dealt with english and a few other high resource languages such as french, german, and spanish. in this paper we address this gap by tackling offensive language identification in marathi, a low-resource indo-aryan language spoken in india. we introduce the marathi offensive language dataset v.2.0 or mold 2.0 and present multiple experiments on this dataset. mold 2.0 is a much larger version of mold with expanded annotation to the levels b (type) and c (target) of the popular olid taxonomy. mold 2.0 is the first hierarchical offensive language dataset compiled for marathi, thus opening new avenues for research in low-resource indo-aryan languages. finally, we also introduce semold, a larger dataset annotated following the semi-supervised methods presented in solid.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-21</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11206" target="_blank">Cultural Re-Contextualization of Fairness Research in Language Technologies in India</a></div>
<div class="paper-author">Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, Vinodkumar Prabhakaran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent research has revealed undesirable biases in nlp data and models. however, these efforts largely focus on social disparities in the west, and are not directly portable to other geo-cultural contexts. in this position paper, we outline a holistic research agenda to re-contextualize nlp fairness research for the indian context, accounting for indian societal context, bridging technological gaps in capability and resources, and adapting to indian cultural values. we also summarize findings from an empirical study on various social biases along different axes of disparities relevant to india, demonstrating their prevalence in corpora and models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11678" target="_blank">Measuring Harmful Representations in Scandinavian Language Models</a></div>
<div class="paper-author">Samia Touileb, Debora Nozza</div>
<div class="abstract">
<div class="abstract-content">
Abstract: scandinavian countries are perceived as role-models when it comes to gender equality. with the advent of pre-trained language models and their widespread usage, we investigate to what extent gender-based harmful and toxic content exist in selected scandinavian language models. we examine nine models, covering danish, swedish, and norwegian, by manually creating template-based sentences and probing the models for completion. we evaluate the completions using two methods for measuring harmful and toxic completions and provide a thorough analysis of the results. we show that scandinavian pre-trained language models contain harmful and gender-based stereotypes with similar values across all languages. this finding goes against the general expectations related to gender equality in scandinavian countries and shows the possible problematic outcomes of using such models in real-world settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11798" target="_blank">Can You Label Less by Using Out-of-Domain Data? Active & Transfer Learning With Few-Shot Instructions</a></div>
<div class="paper-author">Rafal Kocielnik, Sara Kangaslahti, Shrimai Prabhumoye, Meena Hari, R. Michael Alvarez, Anima Anandkumar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: labeling social-media data for custom dimensions of toxicity and social bias is challenging and labor-intensive. existing transfer and active learning approaches meant to reduce annotation effort require fine-tuning, which suffers from over-fitting to noise and can cause domain shift with small sample sizes. in this work, we propose a novel active transfer few-shot instructions (atf) approach which requires no fine-tuning. atf leverages the internal linguistic knowledge of pre-trained language models (plms) to facilitate the transfer of information from existing pre-labeled datasets (source-domain task) with minimum labeling effort on unlabeled target data (target-domain task). our strategy can yield positive transfer achieving a mean auc gain of 10.5% compared to no transfer with a large 22b parameter plm. we further show that annotation of just a few target-domain samples via active learning can be beneficial for transfer, but the impact diminishes with more annotation effort (26% drop in gain between 100 and 2000 annotated examples). finally, we find that not all transfer scenarios yield a positive gain, which seems related to the plms initial performance on the target-domain task.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11835" target="_blank">Fairness Increases Adversarial Vulnerability</a></div>
<div class="paper-author">Cuong Tran, Keyu Zhu, Ferdinando Fioretto, Pascal Van Hentenryck</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the remarkable performance of deep learning models and their applications in consequential domains (e.g., facial recognition) introduces important challenges at the intersection of equity and security. fairness and robustness are two desired notions often required in learning models. fairness ensures that models do not disproportionately harm (or benefit) some groups over others, while robustness measures the models' resilience against small input perturbations.   this paper shows the existence of a dichotomy between fairness and robustness, and analyzes when achieving fairness decreases the model robustness to adversarial samples. the reported analysis sheds light on the factors causing such contrasting behavior, suggesting that distance to the decision boundary across groups as a key explainer for this behavior. extensive experiments on non-linear models and different architectures validate the theoretical findings in multiple vision domains. finally, the paper proposes a simple, yet effective, solution to construct models achieving good tradeoffs between fairness and robustness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11958" target="_blank">A Survey on Backdoor Attack and Defense in Natural Language Processing</a></div>
<div class="paper-author">Xuan Sheng, Zhaoyang Han, Piji Li, Xiangmao Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep learning is becoming increasingly popular in real-life applications, especially in natural language processing (nlp). users often choose training outsourcing or adopt third-party data and models due to data and computation resources being limited. in such a situation, training data and models are exposed to the public. as a result, attackers can manipulate the training process to inject some triggers into the model, which is called backdoor attack. backdoor attack is quite stealthy and difficult to be detected because it has little inferior influence on the model's performance for the clean samples. to get a precise grasp and understanding of this problem, in this paper, we conduct a comprehensive review of backdoor attacks and defenses in the field of nlp. besides, we summarize benchmark datasets and point out the open issues to design credible systems to defend against backdoor attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.15458" target="_blank">Validating Large Language Models With Relm</a></div>
<div class="paper-author">Michael Kuchnik, Virginia Smith, George Amvrosiadis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large language models (llms) have been touted for their ability to generate natural-sounding text, there are growing concerns around possible negative effects of llms such as data memorization, bias, and inappropriate language. unfortunately, the complexity and generation capacities of llms make validating (and correcting) such concerns difficult. in this work, we introduce relm, a system for validating and querying llms using standard regular expressions. relm formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries. our results exploring queries surrounding memorization, gender bias, toxicity, and language understanding show that relm achieves up to 15x higher system efficiency, 2.5x data efficiency, and increased statistical and prompt-tuning coverage compared to state-of-the-art ad-hoc queries. relm offers a competitive and general baseline for the increasingly important problem of llm validation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-20</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11087" target="_blank">Conceptor-Aided Debiasing of Large Language Models</a></div>
<div class="paper-author">Yifei Li, Lyle Ungar, João Sedoc</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained large language models (llms) reflect the inherent social biases of their training corpus. many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. we use conceptors--a soft projection method--to identify and remove the bias subspace in llms such as bert and gpt. we propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened bert (ci-bert), which explicitly incorporates the conceptor projection into all layers during training. we find that conceptor post-processing achieves state-of-the-art (sota) debiasing results while maintaining or improving llms' performance on the glue benchmark. also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. although ci-bert's training takes all layers' bias into account and can beat its post-processing counterpart in bias mitigation, ci-bert reduces the language model accuracy. we also show the importance of carefully constructing the bias subspace. the best results are obtained by removing outliers from the list of biased words, combining them (via the conceptor and operation), and computing their embeddings using the sentences from a cleaner corpus.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11109" target="_blank">Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness</a></div>
<div class="paper-author">Abdelrahman Zayed, Prasanna Parthasarathi, Goncalo Mordido, Hamid Palangi, Samira Shabanian, Sarath Chandar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: data-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. mitigating gender bias has become an important research focus in natural language processing (nlp) and is an area where annotated corpora are available. data augmentation reduces gender bias by adding counterfactual examples to the training dataset. in this work, we show that some of the examples in the augmented dataset can be not important or even harmful for fairness. we hence propose a general method for pruning both the factual and counterfactual examples to maximize the model's fairness as measured by the demographic parity, equality of opportunity, and equality of odds. the fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. our experiments are conducted using models of varying sizes and pre-training settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-19</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.10707" target="_blank">Suffering From Vaccines or From Government? : Partisan Bias in Covid-19 Vaccine Adverse Events Coverage</a></div>
<div class="paper-author">Taeyoung Kang, Hanbin Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: vaccine adverse events have been presumed to be a relatively objective measure that is immune to political polarization. the real-world data, however, shows the correlation between presidential disapproval ratings and the subjective severity of adverse events. this paper investigates the partisan bias in covid vaccine adverse events coverage with language models that can classify the topic of vaccine-related articles and the political disposition of news comments. based on 90k news articles from 52 major newspaper companies, we found that conservative media are inclined to report adverse events more frequently than their liberal counterparts, while the coverage itself was statistically uncorrelated with the severity of real-world adverse events. the users who support the conservative opposing party were more likely to write the popular comments from 2.3k random sampled articles on news platforms. this research implies that bipartisanship can still play a significant role in forming public opinion on the covid vaccine even after the majority of the population's vaccination
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.13003" target="_blank">Detecting Conspiracy Theory Against Covid-19 Vaccines</a></div>
<div class="paper-author">Md Hasibul Amin, Harika Madanu, Sahithi Lavu, Hadi Mansourifar, Dana Alsagheer, Weidong Shi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: since the beginning of the vaccination trial, social media has been flooded with anti-vaccination comments and conspiracy beliefs. as the day passes, the number of covid- 19 cases increases, and online platforms and a few news portals entertain sharing different conspiracy theories. the most popular conspiracy belief was the link between the 5g network spreading covid-19 and the chinese government spreading the virus as a bioweapon, which initially created racial hatred. although some disbelief has less impact on society, others create massive destruction. for example, the 5g conspiracy led to the burn of the 5g tower, and belief in the chinese bioweapon story promoted an attack on the asian-americans. another popular conspiracy belief was that bill gates spread this coronavirus disease (covid-19) by launching a mass vaccination program to track everyone. this conspiracy belief creates distrust issues among laypeople and creates vaccine hesitancy. this study aims to discover the conspiracy theory against the vaccine on social platforms. we performed a sentiment analysis on the 598 unique sample comments related to covid-19 vaccines. we used two different models, bert and perspective api, to find out the sentiment and toxicity of the sentence toward the covid-19 vaccine.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-18</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.10384" target="_blank">Indexing Ai Risks With Incidents, Issues, and Variants</a></div>
<div class="paper-author">Sean Mcgregor, Kevin Paeth, Khoa Lam</div>
<div class="abstract">
<div class="abstract-content">
Abstract: two years after publicly launching the ai incident database (aiid) as a collection of harms or near harms produced by ai in the world, a backlog of "issues" that do not meet its incident ingestion criteria have accumulated in its review queue. despite not passing the database's current criteria for incidents, these issues advance human understanding of where ai presents the potential for harm. similar to databases in aviation and computer security, the aiid proposes to adopt a two-tiered system for indexing ai incidents (i.e., a harm or near harm event) and issues (i.e., a risk of a harm event). further, as some machine learning-based systems will sometimes produce a large number of incidents, the notion of an incident "variant" is introduced. these proposed changes mark the transition of the aiid to a new version in response to lessons learned from editing 2,000+ incident reports and additional reports that fall under the new category of "issue."
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-17</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.09511" target="_blank">Hey Asr System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review</a></div>
<div class="paper-author">Mikel K. Ngueajio, Gloria Washington</div>
<div class="abstract">
<div class="abstract-content">
Abstract: speech is the fundamental means of communication between humans. the advent of ai and sophisticated speech technologies have led to the rapid proliferation of human-to-computer-based interactions, fueled primarily by automatic speech recognition (asr) systems. asr systems normally take human speech in the form of audio and convert it into words, but for some users, it cannot decode the speech, and any output text is filled with errors that are incomprehensible to the human reader. these systems do not work equally for everyone and actually hinder the productivity of some users. in this paper, we present research that addresses asr biases against gender, race, and the sick and disabled, while exploring studies that propose asr debiasing techniques for mitigating these discriminations. we also discuss techniques for designing a more accessible and inclusive asr technology. for each approach surveyed, we also provide a summary of the investigation and methods applied, the asr systems and corpora used, and the research findings, and highlight their strengths and/or weaknesses. finally, we propose future opportunities for natural language processing researchers to explore in the next level creation of asr technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.09527" target="_blank">Ignore Previous Prompt: Attack Techniques for Language Models</a></div>
<div class="paper-author">Fábio Perez, Ian Ribeiro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transformer-based large language models (llms) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. however, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. by proposing promptinject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how gpt-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. in particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit gpt-3's stochastic nature, creating long-tail risks. the code for promptinject is available at https://github.com/agencyenterprise/promptinject.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-16</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.08714" target="_blank">Reward Gaming in Conditional Text Generation</a></div>
<div class="paper-author">Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur P. Parikh, He He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (rl) with reward functions learned from human annotations. under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. we show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during rl training of the text generation model. while there has been discussion about reward gaming in the rl or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (nlg) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.09110" target="_blank">Holistic Evaluation of Language Models</a></div>
<div class="paper-author">Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. we present holistic evaluation of language models (helm) to improve the transparency of language models. first, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for lms. then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected english dialects, metrics for trustworthiness). second, we adopt a multi-metric approach: we measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). this ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. we also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream lm evaluation. prior to helm, models on average were evaluated on just 17.9% of the core helm scenarios, with some prominent models not sharing a single scenario in common. we improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. our evaluation surfaces 25 top-level findings. for full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. we intend for helm to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-15</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.08461" target="_blank">Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models</a></div>
<div class="paper-author">Silke Husse, Andreas Spitz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. consequently, numerous bias detection methods have been proposed, which vary in their approach, the considered type of bias, and the data used for evaluation. however, while most detection methods are derived from the word embedding association test for static word embeddings, the reported results are heterogeneous, inconsistent, and ultimately inconclusive. to address this issue, we conduct a rigorous analysis and comparison of bias detection methods for contextual language models. our results show that minor design and implementation decisions (or errors) have a substantial and often significant impact on the derived bias scores. overall, we find the state of the field to be both worse than previously acknowledged due to systematic and propagated errors in implementations, yet better than anticipated since divergent results in the literature homogenize after accounting for implementation errors. based on our findings, we conclude with a discussion of paths towards more robust and consistent bias detection methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-14</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.07280" target="_blank">A Taxonomic System for Failure Cause Analysis of Open Source Ai Incidents</a></div>
<div class="paper-author">Nikiforos Pittaras, Sean Mcgregor</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while certain industrial sectors (e.g., aviation) have a long history of mandatory incident reporting complete with analytical findings, the practice of artificial intelligence (ai) safety benefits from no such mandate and thus analyses must be performed on publicly known ``open source'' ai incidents. although the exact causes of ai incidents are seldom known by outsiders, this work demonstrates how to apply expert knowledge on the population of incidents in the ai incident database (aiid) to infer the potential and likely technical causative factors that contribute to reported failures and harms. we present early work on a taxonomic system that covers a cascade of interrelated incident factors, from system goals (nearly always known) to methods / technologies (knowable in many cases) and technical failure causes (subject to expert analysis) of the implicated systems. we pair this ontology structure with a comprehensive classification workflow that leverages expert knowledge and community feedback, resulting in taxonomic annotations grounded by incident data and human expertise.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.07350" target="_blank">Does Debiasing Inevitably Degrade the Model Performance</a></div>
<div class="paper-author">Yiran Liu, Xiao Liu, Haotian Chen, Yang Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias in language models has attracted sufficient attention because it threatens social justice. however, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious. we propose a theoretical framework explaining the three candidate mechanisms of the language model's gender bias. we use our theoretical framework to explain why the current debiasing methods cause performance degradation. we also discover a pathway through which debiasing will not degrade the model performance. we further develop a causality-detection fine-tuning approach to correct gender bias. the numerical experiment demonstrates that our method is able to lead to double dividends: partially mitigating gender bias while avoiding performance degradation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.07733" target="_blank">Speaking Multiple Languages Affects the Moral Bias of Language Models</a></div>
<div class="paper-author">Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindřich Libovický, Constantin A. Rothkopf, Alexander Fraser, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained multilingual language models (pmlms) are commonly used when dealing with data from multiple languages and cross-lingual transfer. however, pmlms are trained on varying amounts of data for each language. in practice this means their performance is often much better on english than many other languages. we explore to what extent this also applies to moral norms. do the models capture moral norms from english and impose them on other languages? do the models exhibit random and thus potentially harmful beliefs in certain languages? both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. in this paper, we (1) apply the moraldirection framework to multilingual models, comparing results in german, czech, arabic, chinese, and english, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a moral foundations questionnaire, comparing with human responses from different countries. our experiments demonstrate that, indeed, pmlms encode differing moral biases, but these do not necessarily correspond to cultural differences or commonalities in human opinions. we release our code and models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-11</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.06053" target="_blank">Coral: A Context-Aware Croatian Abusive Language Dataset</a></div>
<div class="paper-author">Ravi Shekhar, Mladen Karan, Matthew Purver</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in light of unprecedented increases in the popularity of the internet and social media, comment moderation has never been a more relevant task. semi-automated comment moderation systems greatly aid human moderators by either automatically classifying the examples or allowing the moderators to prioritize which comments to consider first. however, the concept of inappropriate content is often subjective, and such content can be conveyed in many subtle and indirect ways. in this work, we propose coral -- a language and culturally aware croatian abusive dataset covering phenomena of implicitness and reliance on local and global context. we show experimentally that current models degrade when comments are not explicit and further degrade when language skill and context knowledge are required to interpret the comment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.06116" target="_blank">How Much Hate With #China? A Preliminary Analysis on China-Related Hateful Tweets Two Years After the Covid Pandemic Began</a></div>
<div class="paper-author">Jinghua Xu, Zarah Weiss</div>
<div class="abstract">
<div class="abstract-content">
Abstract: following the outbreak of a global pandemic, online content is filled with hate speech. donald trump's ''chinese virus'' tweet shifted the blame for the spread of the covid-19 virus to china and the chinese people, which triggered a new round of anti-china hate both online and offline. this research intends to examine china-related hate speech on twitter during the two years following the burst of the pandemic (2020 and 2021). through twitter's api, in total 2,172,333 tweets hashtagged #china posted during the time were collected. by employing multiple state-of-the-art pretrained language models for hate speech detection, we identify a wide range of hate of various types, resulting in an automatically labeled anti-china hate speech dataset. we identify a hateful rate in #china tweets of 2.5% in 2020 and 1.9% in 2021. this is well above the average rate of online hate speech on twitter at 0.6% identified in gao et al., 2017. we further analyzed the longitudinal development of #china tweets and those identified as hateful in 2020 and 2021 through visualizing the daily number and hate rate over the two years. our keyword analysis of hate speech in #china tweets reveals the most frequently mentioned terms in the hateful #china tweets, which can be used for further social science studies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-10</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05371" target="_blank">Msdt: Masked Language Model Scoring Defense in Text Domain</a></div>
<div class="paper-author">Jaechul Roh, Minhao Cheng, Yajun Fang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models allowed us to process downstream tasks with the help of fine-tuning, which aids the model to achieve fairly high accuracy in various natural language processing (nlp) tasks. such easily-downloaded language models from various websites empowered the public users as well as some major institutions to give a momentum to their real-life application. however, it was recently proven that models become extremely vulnerable when they are backdoor attacked with trigger-inserted poisoned datasets by malicious users. the attackers then redistribute the victim models to the public to attract other users to use them, where the models tend to misclassify when certain triggers are detected within the training sample. in this paper, we will introduce a novel improved textual backdoor defense method, named msdt, that outperforms the current existing defensive algorithms in specific datasets. the experimental results illustrate that our method can be effective and constructive in terms of defending against backdoor attack in text domain. code is available at https://github.com/jcroh0508/msdt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05414" target="_blank">Adept: A Debiasing Prompt Framework</a></div>
<div class="paper-author">Ke Yang, Charles Yu, Yi Fung, Manling Li, Heng Ji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. with unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (plm) with additional task-specific information. despite this, relatively few efforts have been made to debias plms by prompt tuning with continuous prompts compared to its discrete counterpart. furthermore, for most debiasing methods that alter a plm's original parameters, a major problem is the need to not only decrease the bias in the plm but also to ensure that the plm does not lose its representation ability. finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words. in this paper, we propose adept, a method to debias plms using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. to achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. in addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. we evaluate adept on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the plm's representation ability. we further visualize words' correlation before and after debiasing a plm, and give some possible explanations for the visible effects.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05521" target="_blank">Zero-Shot Visual Commonsense Immorality Prediction</a></div>
<div class="paper-author">Yujin Jeong, Seongbeom Park, Suhong Moon, Jinkyu Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence is currently powering diverse real-world applications. these applications have shown promising performance, but raise complicated ethical issues, i.e. how to embed ethics to make ai applications behave morally. one way toward moral ai systems is by imitating human prosocial behavior and encouraging some form of good behavior in systems. however, learning such normative ethics (especially from images) is challenging mainly due to a lack of data and labeling complexity. here, we propose a model that predicts visual commonsense immorality in a zero-shot manner. we train our model with an ethics dataset (a pair of text and morality annotation) via a clip-based image-text joint embedding. in a testing phase, the immorality of an unseen image is predicted. we evaluate our model with existing moral/immoral image datasets and show fair prediction performance consistent with human intuitions. further, we create a visual commonsense immorality benchmark with more general and extensive immoral visual contents. codes and dataset are available at https://github.com/ku-vai/zero-shot-visual-commonsense-immorality-prediction. note that this paper might contain images and descriptions that are offensive in nature.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05750" target="_blank">Nano: Nested Human-in-the-Loop Reward Learning for Few-Shot Language Model Control</a></div>
<div class="paper-author">Xiang Fan, Yiwei Lyu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models have demonstrated extraordinary capabilities in language generation. however, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. however, many important distributions, such as personal preferences, are unquantified. in this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing nano, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. nano achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. we also show that nano is able to learn unquantified distributions, achieves personalization, and captures differences between different individuals' personal preferences with high sample efficiency.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05809" target="_blank">Casual Conversations V2: Designing a Large Consent-Driven Dataset to Measure Algorithmic Bias and Robustness</a></div>
<div class="paper-author">Caner Hazirbas, Yejin Bang, Tiezheng Yu, Parisa Assar, Bilal Porgali, Vítor Albiero, Stefan Hermanek, Jacqueline Pan, Emily Mcreynolds, Miranda Bogen, Pascale Fung, Cristian Canton Ferrer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developing robust and fair ai systems require datasets with comprehensive set of labels that can help ensure the validity and legitimacy of relevant measurements. recent efforts, therefore, focus on collecting person-related datasets that have carefully selected labels, including sensitive characteristics, and consent forms in place to use those attributes for model testing and development. responsible data collection involves several stages, including but not limited to determining use-case scenarios, selecting categories (annotations) such that the data are fit for the purpose of measuring algorithmic bias for subgroups and most importantly ensure that the selected categories/subcategories are robust to regional diversities and inclusive of as many subgroups as possible.   meta, in a continuation of our efforts to measure ai algorithmic bias and robustness (https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set), is working on collecting a large consent-driven dataset with a comprehensive list of categories. this paper describes our proposed design of such categories and subcategories for casual conversations v2.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05826" target="_blank">The Cringe Loss: Learning What Language Not to Model</a></div>
<div class="paper-author">Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, Jason Weston</div>
<div class="abstract">
<div class="abstract-content">
Abstract: standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data -- examples of what the model should not do. in this work, we propose a novel procedure to train with such data called the cringe loss (contrastive iterative negative generation). we show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05853" target="_blank">Measuring Reliability of Large Language Models Through Semantic Consistency</a></div>
<div class="paper-author">Harsh Raj, Domenic Rosati, Subhabrata Majumdar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large pretrained language models (plms) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing plms are very sensitive to what prompts are feed into them. even when prompts are semantically identical, language models may give very different answers. when considering safe and trustworthy deployments of plms we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. while some work has looked into how state-of-the-art plms address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. in order to understand consistency of plms under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. we implement several versions of this consistency metric to evaluate the performance of a number of plms on paraphrased versions of questions in the truthfulqa dataset, we find that our proposed metrics are considerably more consistent than traditional metrics embodying lexical consistency, and also correlate with human evaluation of output consistency to a higher degree.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05985" target="_blank">Using Persuasive Writing Strategies to Explain and Detect Health Misinformation</a></div>
<div class="paper-author">Danial Kamali, Joseph Romain, Huiyi Liu, Wei Peng, Jingbo Meng, Parisa Kordjamshidi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. the goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. to accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. for this task, we make use of a roberta model for text classification, due to its high performance in nlp. we develop several language model-based baselines and present the results of our persuasive strategy label predictions as well as the improvements these intermediate labels make in detecting misinformation and producing interpretable results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-09</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.04946" target="_blank">Accountable and Explainable Methods for Complex Reasoning Over Text</a></div>
<div class="paper-author">Pepa Atanasova</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a major concern of machine learning (ml) models is their opacity. they are deployed in an increasing number of applications where they often operate as black boxes that do not provide explanations for their predictions. among others, the potential harms associated with the lack of understanding of the models' rationales include privacy violations, adversarial manipulations, and unfair discrimination. as a result, the accountability and transparency of ml models have been posed as critical desiderata by works in policy and law, philosophy, and computer science.   in computer science, the decision-making process of ml models has been studied by developing accountability and transparency methods. accountability methods, such as adversarial attacks and diagnostic datasets, expose vulnerabilities of ml models that could lead to malicious manipulations or systematic faults in their predictions. transparency methods explain the rationales behind models' predictions gaining the trust of relevant stakeholders and potentially uncovering mistakes and unfairness in models' decisions. to this end, transparency methods have to meet accountability requirements as well, e.g., being robust and faithful to the underlying rationales of a model.   this thesis presents my research that expands our collective knowledge in the areas of accountability and transparency of ml models developed for complex reasoning tasks over text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-08</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.04205" target="_blank">Preserving Semantics in Textual Adversarial Attacks</a></div>
<div class="paper-author">David Herel, Hugo Cisneros, Tomas Mikolov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the growth of hateful online content, or hate speech, has been associated with a global increase in violent crimes against minorities [23]. harmful online content can be produced easily, automatically and anonymously. even though, some form of auto-detection is already achieved through text classifiers in nlp, they can be fooled by adversarial attacks. to strengthen existing systems and stay ahead of attackers, we need better adversarial attacks. in this paper, we show that up to 70% of adversarial examples generated by adversarial attacks should be discarded because they do not preserve semantics. we address this core weakness and propose a new, fully supervised sentence embedding technique called semantics-preserving-encoder (spe). our method outperforms existing sentence encoders used in adversarial attacks by achieving 1.2x - 5.1x better real attack success rate. we release our code as a plugin that can be used in any existing adversarial attack to improve its quality and speed up its execution.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.04364" target="_blank">Naturaladversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?</a></div>
<div class="paper-author">Saadia Gabriel, Hamid Palangi, Yejin Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while a substantial body of prior work has explored adversarial example generation for natural language understanding tasks, these examples are often unrealistic and diverge from the real-world data distributions. in this work, we introduce a two-stage adversarial example generation framework (naturaladversaries), for designing adversaries that are effective at fooling a given classifier and demonstrate natural-looking failure cases that could plausibly occur during in-the-wild deployment of the models.   at the first stage a token attribution method is used to summarize a given classifier's behaviour as a function of the key tokens in the input. in the second stage a generative model is conditioned on the key tokens from the first stage. naturaladversaries is adaptable to both black-box and white-box adversarial attacks based on the level of access to the model parameters. our results indicate these adversaries generalize across domains, and offer insights for future research on improving robustness of neural text classification models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.04602" target="_blank">System Safety Engineering for Social and Ethical Ml Risks: A Case Study</a></div>
<div class="paper-author">Edgar W. Jatho, Logan O. Mailloux, Shalaleh Rismani, Eugene D. Williams, Joshua A. Kroll</div>
<div class="abstract">
<div class="abstract-content">
Abstract: governments, industry, and academia have undertaken efforts to identify and mitigate harms in ml-driven systems, with a particular focus on social and ethical risks of ml components in complex sociotechnical systems. however, existing approaches are largely disjointed, ad-hoc and of unknown effectiveness. systems safety engineering is a well established discipline with a track record of identifying and managing risks in many complex sociotechnical domains. we adopt the natural hypothesis that tools from this domain could serve to enhance risk analyses of ml in its context of use. to test this hypothesis, we apply a "best of breed" systems safety analysis, systems theoretic process analysis (stpa), to a specific high-consequence system with an important ml-driven component, namely the prescription drug monitoring programs (pdmps) operated by many us states, several of which rely on an ml-derived risk score. we focus in particular on how this analysis can extend to identifying social and ethical risks and developing concrete design-level controls to mitigate them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-07</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.03433" target="_blank">Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering</a></div>
<div class="paper-author">Helena Bonaldi, Sara Dellantonio, Serra Sinem Tekiroglu, Marco Guerini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fighting online hate speech is a challenge that is usually addressed using natural language processing via automatic detection and removal of hate content. besides this approach, counter narratives have emerged as an effective tool employed by ngos to respond to online hate on social media platforms. for this reason, natural language generation is currently being studied as a way to automatize counter narrative writing. however, the existing resources necessary to train nlg models are limited to 2-turn interactions (a hate speech and a counter narrative as response), while in real life, interactions can consist of multiple turns. in this paper, we present a hybrid approach for dialogical data collection, which combines the intervention of human expert annotators over machine generated dialogues obtained using 19 different configurations. the result of this work is dialoconan, the first dataset comprising over 3000 fictitious multi-turn dialogues between a hater and an ngo operator, covering 6 targets of hate.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.03622" target="_blank">Do Users Write More Insecure Code With Ai Assistants?</a></div>
<div class="paper-author">Neil Perry, Megha Srivastava, Deepak Kumar, Dan Boneh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we conduct the first large-scale user study examining how users interact with an ai code assistant to solve a variety of security related tasks across different programming languages. overall, we find that participants who had access to an ai assistant based on openai's codex-davinci-002 model wrote significantly less secure code than those without access. additionally, participants with access to an ai assistant were more likely to believe they wrote secure code than those without access to the ai assistant. furthermore, we find that participants who trusted the ai less and engaged more with the language and format of their prompts (e.g. re-phrasing, adjusting temperature) provided code with fewer security vulnerabilities. finally, in order to better inform the design of future ai-based code assistants, we provide an in-depth analysis of participants' language and interaction behavior, as well as release our user interface as an instrument to conduct similar studies in the future.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.03634" target="_blank">No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media</a></div>
<div class="paper-author">Maximilian Spliethöver, Maximilian Keiff, Henning Wachsmuth</div>
<div class="abstract">
<div class="abstract-content">
Abstract: news articles both shape and reflect public opinion across the political spectrum. analyzing them for social bias can thus provide valuable insights, such as prevailing stereotypes in society and the media, which are often adopted by nlp models trained on respective data. recent work has relied on word embedding bias measures, such as weat. however, several representation issues of embeddings can harm the measures' accuracy, including low-resource settings and token frequency differences. in this work, we study what kind of embedding algorithm serves best to accurately measure types of social bias known to exist in us online news articles. to cover the whole spectrum of political bias in the us, we collect 500k articles and review psychology literature with respect to expected social bias. we then quantify social bias using weat along with embedding algorithms that account for the aforementioned issues. we compare how models trained with the algorithms on news articles represent the expected social bias. our results suggest that the standard way to quantify bias does not align well with knowledge from psychology. while the proposed algorithms reduce the~gap, they still do not fully match the literature.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-05</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.02882" target="_blank">Herb: Measuring Hierarchical Regional Bias in Pre-Trained Language Models</a></div>
<div class="paper-author">Yizhi Li, Ge Zhang, Bohao Yang, Chenghua Lin, Shi Wang, Anton Ragni, Jie Fu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fairness has become a trending topic in natural language processing (nlp), which addresses biases targeting certain social groups such as genders and religions. however, regional bias in language models (lms), a long-standing global discrimination problem, still remains unexplored. this paper bridges the gap by analysing the regional bias learned by the pre-trained language models that are broadly used in nlp tasks. in addition to verifying the existence of regional bias in lms, we find that the biases on regional groups can be strongly influenced by the geographical clustering of the groups. we accordingly propose a hierarchical regional bias evaluation method (herb) utilising the information from the sub-region clusters to quantify the bias in pre-trained lms. experiments show that our hierarchical metric can effectively evaluate the regional bias with respect to comprehensive topics and measure the potential regional bias that can be propagated to downstream tasks. our codes are available at https://github.com/bernard-yang/herb.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.02956" target="_blank">Privacy-Preserving Models for Legal Natural Language Processing</a></div>
<div class="paper-author">Ying Yin, Ivan Habernal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-training large transformer models with in-domain data improves domain adaptation and helps gain performance on the domain-specific downstream tasks. however, sharing models pre-trained on potentially sensitive data is prone to adversarial privacy attacks. in this paper, we asked to which extent we can guarantee privacy of pre-training data and, at the same time, achieve better downstream performance on legal tasks without the need of additional labeled data. we extensively experiment with scalable self-supervised learning of transformer models under the formal paradigm of differential privacy and show that under specific training configurations we can improve downstream performance without sacrifying privacy protection for the in-domain data. our main contribution is utilizing differential privacy for large-scale pre-training of transformer language models in the legal nlp domain, which, to the best of our knowledge, has not been addressed before.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-04</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.03540" target="_blank">Measuring Progress on Scalable Oversight for Large Language Models</a></div>
<div class="paper-author">Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilė Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova Dassarma, Robin Larson, Sam Mccandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developing safe and useful general-purpose ai systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. this paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. we first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general ai systems fail. we then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: mmlu and time-limited quality. on these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. these results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-11-03</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.01910" target="_blank">Large Language Models Are Human-Level Prompt Engineers</a></div>
<div class="paper-author">Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba</div>
<div class="abstract">
<div class="abstract-content">
Abstract: by conditioning on natural language instructions, large language models (llms) have displayed impressive capabilities as general-purpose computers. however, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. inspired by classical program synthesis and the human approach to prompt engineering, we propose automatic prompt engineer (ape) for automatic instruction generation and selection. in our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an llm in order to maximize a chosen score function. to evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another llm following the selected instruction. experiments on 24 nlp tasks show that our automatically generated instructions outperform the prior llm baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. we conduct extensive qualitative and quantitative analyses to explore the performance of ape. we show that ape-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.02011" target="_blank">Inverse Scaling Can Become U-Shaped</a></div>
<div class="paper-author">Jason Wei, Najoung Kim, Yi Tay, Quoc V. Le</div>
<div class="abstract">
<div class="abstract-content">
Abstract: scaling up language models has been empirically shown to improve performance on a wide range of downstream tasks. however, if we were to observe worse performance as a function of scale ("inverse scaling") on certain tasks, this would indicate that scaling can also encourage behaviors that are misaligned with human preferences. the inverse scaling prize (mckenzie et al. 2022) identified eleven such inverse scaling tasks, evaluated on models of up to 280b parameters and up to 500 zettaflops of training compute. this paper takes a closer look at these inverse scaling tasks. we evaluate models of up to 540b parameters, trained on five times more compute than those evaluated in the inverse scaling prize. with this increased range of model sizes and training compute, only four out of the eleven tasks remain inverse scaling. six out of the eleven tasks exhibit "u-shaped scaling", where performance decreases up to a certain size, and then increases again up to the largest model evaluated (the one remaining task displays positive scaling). in addition, we find that 1-shot examples and chain-of-thought can help mitigate undesirable scaling patterns even further. u-shaped scaling suggests that the inverse scaling trend observed in mckenzie et al. (2022) may not continue to hold for larger models, which we attribute to the presence of distractor tasks that only sufficiently large models can avoid.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.02139" target="_blank">Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity</a></div>
<div class="paper-author">Faisal Hamman, Jiahao Chen, Sanghamitra Dutta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: existing regulations prohibit model developers from accessing protected attributes (gender, race, etc.), often resulting in fairness assessments on populations without knowing their protected groups. in such scenarios, institutions often adopt a separation between the model developers (who train models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset for auditing purposes). however, the model developers might be allowed to test their models for bias by querying the compliance team for group fairness metrics. in this paper, we first demonstrate that simply querying for fairness metrics, such as statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. we demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. in particular, we show that one can reconstruct the protected attributes of all the individuals from o(nk \log( n /nk)) queries when nk&lt;&lt;n using techniques from compressed sensing (n: size of the test dataset, nk: size of smallest group). our results pose an interesting debate in algorithmic fairness: should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? to address this supposed violation, we also propose attribute-conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query, outperforming naive techniques such as the laplace mechanism. we also include experimental results on the adult dataset and synthetic data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h3><b>2022-10-31</b></h3>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2210.17108" target="_blank">Do Charge Prediction Models Learn Legal Theory?</a></div>
<div class="paper-author">Zhenwei An, Quzhe Huang, Cong Jiang, Yansong Feng, Dongyan Zhao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the charge prediction task aims to predict the charge for a case given its fact description. recent models have already achieved impressive accuracy in this task, however, little is understood about the mechanisms they use to perform the judgment.for practical applications, a charge prediction model should conform to the certain legal theory in civil law countries, as under the framework of civil law, all cases are judged according to certain local legal theories. in china, for example, nearly all criminal judges make decisions based on the four elements theory (fet).in this paper, we argue that trustworthy charge prediction models should take legal theories into consideration, and standing on prior studies in model interpretation, we propose three principles for trustworthy models should follow in this task, which are sensitive, selective, and presumption of innocence.we further design a new framework to evaluate whether existing charge prediction models learn legal theories. our findings indicate that, while existing charge prediction models meet the selective principle on a benchmark dataset, most of them are still not sensitive enough and do not satisfy the presumption of innocence. our code and dataset are released at https://github.com/zhenweian/exp_ljp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2210.17546" target="_blank">Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy</a></div>
<div class="paper-author">Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, Nicholas Carlini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data and aids in the development of countermeasures. many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. we argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. specifically, we design and implement an efficient defense that perfectly prevents all verbatim memorization. and yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. we conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.00053" target="_blank">Generating Sequences by Learning to Self-Correct</a></div>
<div class="paper-author">Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. we present self-correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. to train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. we show that self-correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.00243" target="_blank">Why Is It Hate Speech? Masked Rationale Prediction for Explainable Hate Speech Detection</a></div>
<div class="paper-author">Jiyun Kim, Byounghan Lee, Kyung-Ah Sohn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in a hate speech detection model, we should consider two critical aspects in addition to detection performance-bias and explainability. hate speech cannot be identified based solely on the presence of specific words: the model should be able to reason like humans and be explainable. to improve the performance concerning the two aspects, we propose masked rationale prediction (mrp) as an intermediate task. mrp is a task to predict the masked human rationales-snippets of a sentence that are grounds for human judgment-by referring to surrounding tokens combined with their unmasked rationales. as the model learns its reasoning ability based on rationales by mrp, it performs hate speech detection robustly in terms of bias and explainability. the proposed method generally achieves state-of-the-art performance in various metrics, demonstrating its effectiveness for hate speech detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.00609" target="_blank">A Simple, Yet Effective Approach to Finding Biases in Code Generation</a></div>
<div class="paper-author">Spyridon Mouselinos, Mateusz Malinowski, Henryk Michalewski</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, high-performing code generation systems based on large language models have surfaced. they are trained on massive corpora containing much more natural text than actual executable computer code. this work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.   to investigate the effect, we propose the "block of influence" concept, which enables a modular decomposition and analysis of the coding challenges. we introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>

<div class="outer">


<script>
    document.addEventListener("DOMContentLoaded", function() {
        const expandButtons = document.querySelectorAll('.expand-btn');

        expandButtons.forEach(button => {
            button.addEventListener('click', function() {
                const abstractDiv = button.closest('.abstract');
                if (abstractDiv.classList.contains('expanded')) {
                    abstractDiv.classList.remove('expanded');
                    button.innerText = 'Expand Abstract';
                } else {
                    abstractDiv.classList.add('expanded');
                    button.innerText = 'Fold Abstract';
                }
            });
        });
    });
</script>


</body>
</html>

