---
layout: single
permalink: /arxiv-llm-alignment-safety-security/
title: ""
author_profile: false
sitemap: false
---



<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Papers List</title>
    <style>
        .outer {
            padding: 40px;
            /* Light background with dots and lines */
            background-color: #f4f4f4;
            background-image:
                radial-gradient(circle, #d9d9d9 1px, transparent 1px),
                linear-gradient(90deg, #d9d9d9 1px, transparent 1px),
                linear-gradient(#d9d9d9 1px, transparent 1px);
            background-size: 20px 20px;
        }

        .intro {
            background-color: #fff;
            border: 5px solid #e0e0e0;
            padding: 10px 10px 10px; /* Adjusted top padding */
            margin-bottom: 10px;
            margin: 5px auto;  /* Centered horizontally with top and bottom margin of 5px */
            max-width: 1000px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            transition: transform 0.2s;
        }

        .paper {
            background-color: #fff;
            border: 1px solid #e0e0e0;
            padding: 3px 10px 10px; /* Adjusted top padding */
            margin-bottom: 7px;
            max-width: 1000px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            transition: transform 0.2s;
        }

        .paper:hover {
            transform: translateY(-2px);
        }

        .paper-title {
            font-weight: bold;
            font-size: 1.0em;
            margin: 0.1em 0 0.1em; /* Reduced top margin */
        }

        .paper-title a {
            text-decoration: none;
            color: #333;
        }

        .paper-title a:hover {
            text-decoration: underline;
        }

        .paper-author {
            color: #777;
            font-size: 0.8em;
            margin: 0.1em 0 0.1em; /* Reduced top margin */
        }

        .abstract {
            border-top: 2px solid #e77500;
            padding-top: 2px;
        }

        .abstract-content {
            height: 1.4em;
            overflow: hidden;
            white-space: nowrap;
            text-overflow: ellipsis;
            margin-top: 0.1em;
            color: #888888; /* Gray color for abstract */
            font-size: 0.8em;
        }

        .expanded .abstract-content {
            height: auto;
            white-space: normal;
            overflow: visible;
        }

        .expand-btn {
            background-color: #e77500;
            color: #fff;
            border: none;
            border-radius: 4px;
            padding: 3px 6px;
            margin-top: 3px;
            cursor: pointer;
            transition: background-color 0.3s;
            font-size:0.7em;
        }

        .expand-btn:hover {
            background-color: #d66400;
        }
    </style>
</head>


<body>


<div class="outer">


<div class="intro">
    <h1 style="text-align: center; border-bottom: 2px solid #cccccc; padding-bottom: 10px;">A Complete List of ArXiv Papers on Alignment, Safety, and Security of Large Language Models (LLMs)</h1>
    <p>
        <span style="padding-left: 2em;">by
        <span style="font-weight: 600">
            Xiangyu Qi
        </span>
        <span style="padding-left: 2em; color: #777;">
            2023-10-30
            </span>
        </span>
    </p>
</div>

<h2><b>2023-10-29</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18913" target="_blank">Debiasing Algorithm Through Model Adaptation</a></div>
<div class="paper-author">Tomasz Limisiewicz, David Mareček, Tomáš Musil</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are becoming the go-to solution for various language tasks. however, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. this work proposes a novel method for detecting and mitigating gender bias in language models. we perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. based on the analysis results, we adapt the model by multiplying these layers by a linear projection. our titular method, dama, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. we release code for our method and models, which retrain llama's state-of-the-art performance while being significantly less biased.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18964" target="_blank">LLMS and Finetuning: Benchmarking Cross-Domain Performance for Hate Speech Detection</a></div>
<div class="paper-author">Ahmad Nasir, Aadish Sharma, Kokil Jaidka</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper compares different pre-trained and fine-tuned large language models (llms) for hate speech detection. our research underscores challenges in llms' cross-domain validity and overfitting risks. through evaluations, we highlight the need for fine-tuned models that grasp the nuances of hate speech through greater label heterogeneity. we conclude with a vision for the future of hate speech detection, emphasizing cross-domain generalizability and appropriate benchmarking practices.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18603" target="_blank">Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers</a></div>
<div class="paper-author">Wencong You, Zayd Hammoudeh, Daniel Lowd</div>
<div class="abstract">
<div class="abstract-content">
Abstract: backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data. we focus on more realistic and more challenging clean-label attacks where the adversarial training examples are correctly labeled. our attack, llmbkd, leverages language models to automatically insert diverse style-based triggers into texts. we also propose a poison selection technique to improve the effectiveness of both llmbkd as well as existing textual backdoor attacks. lastly, we describe react, a baseline defense to mitigate backdoor attacks via antidote training examples. our evaluations demonstrate llmbkd's effectiveness and efficiency, where we consistently achieve high attack success rates across a wide range of styles with little effort and no model training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18633" target="_blank">Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models Through Honeypots</a></div>
<div class="paper-author">Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen, Xia Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the field of natural language processing, the prevalent approach involves fine-tuning pretrained language models (plms) using local samples. recent research has exposed the susceptibility of plms to backdoor attacks, wherein the adversaries can embed malicious prediction behaviors by manipulating a few training samples. in this study, our objective is to develop a backdoor-resistant tuning procedure that yields a backdoor-free model, no matter whether the fine-tuning dataset contains poisoned samples. to this end, we propose and integrate a honeypot module into the original plm, specifically designed to absorb backdoor information exclusively. our design is motivated by the observation that lower-layer representations in plms carry sufficient backdoor features while carrying minimal information about the original tasks. consequently, we can impose penalties on the information acquired by the honeypot module to inhibit backdoor creation during the fine-tuning process of the stem network. comprehensive experiments conducted on benchmark datasets substantiate the effectiveness and robustness of our defensive strategy. notably, these results indicate a substantial reduction in the attack success rate ranging from 10\% to 40\% when compared to prior state-of-the-art methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18679" target="_blank">N-Critics: Self-Refinement of Large Language Models With Ensemble of Critics</a></div>
<div class="paper-author">Sajad Mousavi, Ricardo Luna Gutiérrez, Desik Rengarajan, Vineet Gundecha, Ashwin Ramesh Babu, Avisek Naug, Antonio Guillen, Soumyendu Sarkar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a self-correction mechanism for large language models (llms) to mitigate issues such as toxicity and fact hallucination. this method involves refining model outputs through an ensemble of critics and the model's own feedback. drawing inspiration from human behavior, we explore whether llms can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. we consistently observe performance improvements in llms for reducing toxicity and correcting factual errors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18834" target="_blank">Automating the Correctness Assessment of Ai-Generated Code for Security Contexts</a></div>
<div class="paper-author">Domenico Cotroneo, Alessio Foggia, Cristina Improta, Pietro Liguori, Roberto Natella</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we propose a fully automated method, named acca, to evaluate the correctness of ai-generated code for security purposes. the method uses symbolic execution to assess whether the ai-generated code behaves as a reference implementation. we use acca to assess four state-of-the-art models trained to generate security-oriented assembly code and compare the results of the evaluation with different baseline solutions, including output similarity metrics, widely used in the field, and the well-known chatgpt, the ai-powered language model developed by openai. our experiments show that our method outperforms the baseline solutions and assesses the correctness of the ai-generated code similar to the human-based evaluation, which is considered the ground truth for the assessment in the field. moreover, acca has a very strong correlation with human evaluation (pearson's correlation coefficient r=0.84 on average). finally, since it is a fully automated solution that does not require any human intervention, the proposed method performs the assessment of every code snippet in ~0.17s on average, which is definitely lower than the average time required by human analysts to manually inspect the code, based on our experience.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17884" target="_blank">Can LLMS Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory</a></div>
<div class="paper-author">Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the interactive use of large language models (llms) in ai assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: llms are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. in this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing confaide, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned llms. our experiments show that even the most capable models such as gpt-4 and chatgpt reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. this leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18130" target="_blank">Delphi: Data for Evaluating Llms' Performance in Handling Controversial Issues</a></div>
<div class="paper-author">David Q. Sun, Artem Abzaliev, Hadas Kotek, Zidi Xiu, Christopher Klein, Jason D. Williams</div>
<div class="abstract">
<div class="abstract-content">
Abstract: controversy is a reflection of our zeitgeist, and an important aspect to any discourse. the rise of large language models (llms) as conversational systems has increased public reliance on these systems for answers to their various questions. consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. however, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. to foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released quora question pairs dataset. this dataset presents challenges concerning knowledge recency, safety, fairness, and bias. we evaluate different llms using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. this research ultimately contributes to our understanding of llms' interaction with controversial issues, paving the way for improvements in their comprehension and handling of complex societal debates.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18168" target="_blank">Personas as a Way to Model Truthfulness in Language Models</a></div>
<div class="paper-author">Nitish Joishi, Javier Rando, Abulhair Saparov, Najoung Kim, He He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. can language models discern truth from falsehood in this contradicting data? expanding on the view that llms can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. for example, trustworthy sources like wikipedia and science usually use formal writing styles and make consistent claims. by modeling this persona, llms can generalize truthfulness beyond the specific contexts in which each agent generated the training text. for example, the model can infer that the agent "wikipedia" will behave truthfully on topics that were only generated by "science" because they share a persona. we first show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. next, using arithmetics as a synthetic environment, we show that language models can separate true and false statements, and generalize truthfulness across agents; but only if agents in the training data share a truthful generative process that enables the creation of a truthful persona. overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18244" target="_blank">A Review of the Evidence for Existential Risk From Ai via Misaligned Power-Seeking</a></div>
<div class="paper-author">Rose Hadshar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements in artificial intelligence (ai) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced ai systems to pose existential risks. this paper reviews the evidence for existential risks from ai via misalignment, where ai systems develop goals misaligned with human values, and power-seeking, where misaligned ais actively seek power. the review examines empirical findings, conceptual arguments and expert opinion relating to specification gaming, goal misgeneralization, and power-seeking. the current state of the evidence is found to be concerning but inconclusive regarding the existence of extreme forms of misaligned power-seeking. strong empirical evidence of specification gaming combined with strong conceptual evidence for power-seeking make it difficult to dismiss the possibility of existential risk from misaligned power-seeking. on the other hand, to date there are no public empirical examples of misaligned power-seeking in ai systems, and so arguments that future systems will pose an existential risk remain somewhat speculative. given the current state of the evidence, it is hard to be extremely confident either that misaligned power-seeking poses a large existential risk, or that it poses no existential risk. the fact that we cannot confidently rule out existential risk from ai via misaligned power-seeking is cause for serious concern.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18458" target="_blank">Do Not Harm Protected Groups in Debiasing Language Representation Models</a></div>
<div class="paper-author">Chloe Qinyu Zhu, Rickard Stureborg, Brandon Fain</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language representation models (lrms) trained with real-world data may capture and exacerbate undesired bias and cause unfair treatment of people in various demographic groups. several techniques have been investigated for applying interventions to lrms to remove bias in benchmark evaluations on, for example, word embeddings. however, the negative side effects of debiasing interventions are usually not revealed in the downstream tasks. we propose xgap-debias, a set of evaluations on assessing the fairness of debiasing. in this work, we examine four debiasing techniques on a real-world text classification task and show that reducing biasing is at the cost of degrading performance for all demographic groups, including those the debiasing techniques aim to protect. we advocate that a debiasing technique should have good downstream performance with the constraint of ensuring no harm to the protected group.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18491" target="_blank">Publicly Detectable Watermarking for Language Models</a></div>
<div class="paper-author">Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Mingyuan Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we construct the first provable watermarking scheme for language models with public detectability or verifiability: we use a private key for watermarking and a public key for watermark detection. our protocol is the first watermarking scheme that does not embed a statistical signal in generated text. rather, we directly embed a publicly-verifiable cryptographic signature using a form of rejection sampling. we show that our construction meets strong formal security guarantees and preserves many desirable properties found in schemes in the private-key watermarking setting. in particular, our watermarking scheme retains distortion-freeness and model agnosticity. we implement our scheme and make empirical measurements over open models in the 7b parameter range. our experiments suggest that our watermarking scheme meets our formal claims while preserving text quality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18512" target="_blank">Preventing Language Models From Hiding Their Reasoning</a></div>
<div class="paper-author">Fabien Roger, Ryan Greenblatt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) often benefit from intermediate steps of reasoning to generate answers to complex problems. when these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. in this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an llm could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. we show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. we argue that, as language models get stronger, this behavior becomes more likely to appear naturally. finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per kb of text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17389" target="_blank">Toxicchat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-Ai Conversation</a></div>
<div class="paper-author">Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-ai interactive environment has become increasingly critical nowadays. however, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-ai interactions insufficiently explored. in this work, we introduce toxicchat, a novel benchmark based on real user queries from an open-source chatbot. this benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of toxicchat. our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-ai conversations. in the future, toxicchat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-ai interactions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17428" target="_blank">''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text</a></div>
<div class="paper-author">Rishav Hada, Agrima Seth, Harshita Diddee, Kalika Bali</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language serves as a powerful tool for the manifestation of societal belief systems. in doing so, it also perpetuates the prevalent biases in our society. gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. with llms increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. prior work often treats gender bias as a binary classification task. however, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. specifically, we create the first dataset of gpt-generated english text with normative ratings of gender bias. ratings were obtained using best--worst scaling -- an efficient comparative annotation framework. next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. finally, we show the performance of existing automated models trained on related concepts on our dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17551" target="_blank">Unpacking the Ethical Value Alignment in Big Models</a></div>
<div class="paper-author">Xiaoyuan Yi, Jing Yao, Xiting Wang, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: big models have greatly advanced ai's ability to understand, generate, and manipulate information and content, enabling numerous applications. however, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. this paper provides an overview of the risks and challenges associated with big models, surveys existing ai ethics guidelines, and examines the ethical implications arising from the limitations of these models. taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal ai ethics framework. furthermore, we investigate the moral inclinations of current mainstream llms using the moral foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. to address these challenges, we introduce a novel conceptual paradigm for aligning the ethical values of big models and discuss promising research directions for alignment criteria, evaluation, and method, representing an initial step towards the interdisciplinary construction of the ethically aligned ai   this paper is a modified english version of our chinese paper https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended to help non-chinese native speakers better understand our work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17586" target="_blank">Global Voices, Local Biases: Socio-Cultural Prejudices Across Languages</a></div>
<div class="paper-author">Anjishnu Mukherjee, Chahat Raj, Ziwei Zhu, Antonios Anastasopoulos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. as large amounts of recent literature suggest, language models (lms) trained on human data can reflect and often amplify the effects of these social biases. however, the vast majority of existing studies on bias are heavily skewed towards western and european languages. in this work, we scale the word embedding association test (weat) to 24 languages, enabling broader studies and yielding interesting findings about lm bias. we additionally enhance this data with culturally relevant information for each language, capturing local contexts on a global scale. further, to encompass more widely prevalent societal biases, we examine new bias dimensions across toxicity, ableism, and more. moreover, we delve deeper into the indian linguistic landscape, conducting a comprehensive regional bias analysis across six prevalent indian languages. finally, we highlight the significance of these social biases and the new dimensions through an extensive comparison of embedding methods, reinforcing the need to address them in pursuit of more equitable language models. all code, data and results are available here: https://github.com/iamshnoo/weathub.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17530" target="_blank">Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models</a></div>
<div class="paper-author">Laura Cabello, Emanuele Bugliarello, Stephanie Brandl, Desmond Elliott</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. in this work, we define gender bias as our case study. we quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. we investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. overall, we find that bias amplification in pretraining and after fine-tuning are independent. we then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on vqav2 and retrieval tasks without significantly compromising task performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17533" target="_blank">Decoding the Digital Fuk\'u: Deciphering Colonial Legacies to Critically Assess Chatgpt in Dominican Education</a></div>
<div class="paper-author">Anaelia Ovalle</div>
<div class="abstract">
<div class="abstract-content">
Abstract: educational disparities within the dominican republic (dr) have long-standing origins rooted in economic, political, and social inequity. addressing these challenges has necessarily called for capacity building with respect to educational materials, high-quality instruction, and structural resourcing. generative ai tools like chatgpt have begun to pique the interest of dominican educators due to their perceived potential to bridge these educational gaps. however, a substantial body of ai fairness literature has documented ways ai disproportionately reinforces power dynamics reflective of jurisdictions driving ai development and deployment policies, collectively termed the ai global north. as such, indiscriminate adoption of this technology for dr education, even in part, risks perpetuating forms of digital coloniality. therefore, this paper centers embracing ai-facilitated educational reform by critically examining how ai-driven tools like chatgpt in dr education may replicate facets of digital colonialism. we provide a concise overview of 20th-century dominican education reforms following the 1916 us occupation. then, we employ identified neocolonial aspects historically shaping dominican education to interrogate the perceived advantages of chatgpt for contemporary dominican education, as outlined by a dominican scholar. this work invites ai global north & south developers, stakeholders, and dominican leaders alike to exercise a relational contextualization of data-centric epistemologies like chatgpt to reap its transformative benefits while remaining vigilant of safeguarding dominican digital sovereignty.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17688" target="_blank">Managing Ai Risks in an Era of Rapid Progress</a></div>
<div class="paper-author">Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atılım Güneş Baydin, Sheila Mcilraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, Sören Mindermann</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this short consensus paper, we outline risks from upcoming, advanced ai systems. we examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous ai systems. in light of rapid and continuing ai progress, we propose priorities for ai r&d and governance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17711" target="_blank">Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term</a></div>
<div class="paper-author">Yi-Li Hsu, Shih-Chieh Dai, Aiping Xiong, Lun-Wei Ku</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with advancements in natural language processing (nlp) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. while many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. in this study, we compare the effectiveness of a warning label and the state-of-the-art counterfactual explanations generated by gpt-4 in debunking misinformation. in a two-wave, online human-subject study, participants (n = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by gpt-4 generated explanations. our results show that both interventions significantly decrease participants' self-reported belief in fake claims in an equivalent manner for the short-term and long-term. we discuss the implications of our findings and directions for future nlp-based misinformation debunking strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17750" target="_blank">A Framework for Automated Measurement of Responsible Ai Harms in Generative Ai Applications</a></div>
<div class="paper-author">Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present a framework for the automated measurement of responsible ai (rai) metrics for large language models (llms) and associated products and services. our framework for automatically measuring harms from llms builds on existing technical and sociotechnical expertise and leverages the capabilities of state-of-the-art llms, such as gpt-4. we use this framework to run through several case studies investigating how different llms may violate a range of rai-related principles. the framework may be employed alongside domain-specific sociotechnical expertise to create measurements for new harm areas in the future. by implementing this framework, we aim to enable more advanced harm measurement efforts and further the responsible use of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17769" target="_blank">Social Contract Ai: Aligning Ai Assistants With Implicit Group Norms</a></div>
<div class="paper-author">Jan-Philipp Fränken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we explore the idea of aligning an ai assistant by inverting a model of users' (unknown) preferences from observed interactions. to validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. we find that the ai assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). however, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. overall, our preliminary results suggest that developing simulation frameworks in which ai assistants need to infer preferences from diverse users can provide a valuable approach for studying practical alignment questions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16517" target="_blank">Occuquest: Mitigating Occupational Bias for Inclusive Large Language Models</a></div>
<div class="paper-author">Mingfeng Xue, Dayiheng Liu, Kexin Yang, Guanting Dong, Wenqiang Lei, Zheng Yuan, Chang Zhou, Jingren Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergence of large language models (llms) has revolutionized natural language processing tasks. however, existing instruction-tuning datasets suffer from occupational bias: the majority of data relates to only a few occupations, which hampers the instruction-tuned llms to generate helpful responses to professional queries from practitioners in specific fields. to mitigate this issue and promote occupation-inclusive llms, we create an instruction-tuning dataset named \emph{occuquest}, which contains 110,000+ prompt-completion pairs and 30,000+ dialogues covering over 1,000 occupations in 26 occupational categories. we systematically request chatgpt, organizing queries hierarchically based on occupation, responsibility, topic, and question, to ensure a comprehensive coverage of occupational specialty inquiries. by comparing with three commonly used datasets (dolly, sharegpt, and wizardlm), we observe that occuquest exhibits a more balanced distribution across occupations. furthermore, we assemble three test sets for comprehensive evaluation, an occu-test set covering 25 occupational categories, an estate set focusing on real estate, and an occu-quora set containing real-world questions from quora. we then fine-tune llama on occuquest to obtain occullama, which significantly outperforms state-of-the-art llama variants (vicuna, tulu, and wizardlm) on professional questions in gpt-4 and human evaluations. notably, on the occu-quora set, occullama reaches a high win rate of 86.4\% against wizardlm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16685" target="_blank">Detection of News Written by the Chatgpt Through Authorship Attribution Performed by a Bidirectional LSTM Model</a></div>
<div class="paper-author">Amanda Ferrari Iaquinta, Gustavo Voltani Von Atzingen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the large language based-model chatbot chatgpt gained a lot of popularity since its launch and has been used in a wide range of situations. this research centers around a particular situation, when the chatgpt is used to produce news that will be consumed by the population, causing the facilitation in the production of fake news, spread of misinformation and lack of trust in news sources. aware of these problems, this research aims to build an artificial intelligence model capable of performing authorship attribution on news articles, identifying the ones written by the chatgpt. to achieve this goal, a dataset containing equal amounts of human and chatgpt written news was assembled and different natural processing language techniques were used to extract features from it that were used to train, validate and test three models built with different techniques. the best performance was produced by the bidirectional long short term memory (lstm) neural network model, achiving 91.57\% accuracy when tested against the data from the testing set.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16727" target="_blank">Ai Hazard Management: A Framework for the Systematic Management of Root Causes for Ai Risks</a></div>
<div class="paper-author">Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in the field of artificial intelligence (ai) establish the basis to address challenging tasks. however, with the integration of ai, new risks arise. therefore, to benefit from its advantages, it is essential to adequately handle the risks associated with ai. existing risk management processes in related fields, such as software systems, need to sufficiently consider the specifics of ai. a key challenge is to systematically and transparently identify and address ai risks' root causes - also called ai hazards. this paper introduces the ai hazard management (aihm) framework, which provides a structured process to systematically identify, assess, and treat ai hazards. the proposed process is conducted in parallel with the development to ensure that any ai hazard is captured at the earliest possible stage of the ai system's life cycle. in addition, to ensure the ai system's auditability, the proposed framework systematically documents evidence that the potential impact of identified ai hazards could be reduced to a tolerable level. the framework builds upon an ai hazard list from a comprehensive state-of-the-art analysis. also, we provide a taxonomy that supports the optimal treatment of the identified ai hazards. additionally, we illustrate how the aihm framework can increase the overall quality of a power grid ai use case by systematically reducing the impact of identified hazards to an acceptable level.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16763" target="_blank">Superhf: Supervised Iterative Learning From Human Feedback</a></div>
<div class="paper-author">Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim, Gitta Kutyniok, Kush Bhatia, Silas Alberti</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training. here, we focus on two prevalent methods used to align these models, supervised fine-tuning (sft) and reinforcement learning from human feedback (rlhf). sft is simple and robust, powering a host of open-source models, while rlhf is a more sophisticated method used in top-tier models like chatgpt but also suffers from instability and susceptibility to reward hacking. we propose a novel approach, supervised iterative learning from human feedback (superhf), which seeks to leverage the strengths of both methods. our hypothesis is two-fold: that the reward model used in rlhf is critical for efficient data use and model generalization and that the use of proximal policy optimization (ppo) in rlhf may not be necessary and could contribute to instability issues. superhf replaces ppo with a simple supervised loss and a kullback-leibler (kl) divergence prior. it creates its own training data by repeatedly sampling a batch of model outputs and filtering them through the reward model in an online learning regime. we then break down the reward optimization problem into three components: robustly optimizing the training rewards themselves, preventing reward hacking-exploitation of the reward model that degrades model performance-as measured by a novel meteor similarity metric, and maintaining good performance on downstream evaluations. our experimental results show superhf exceeds ppo-based rlhf on the training objective, easily and favorably trades off high reward with low reward hacking, improves downstream calibration, and performs the same on our gpt-4 based qualitative evaluation scheme all the while being significantly simpler to implement, highlighting superhf's potential as a competitive language model alignment technique.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16787" target="_blank">The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in Ai</a></div>
<div class="paper-author">Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, N/A Xinyi, N/A Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, Deb Roy, Sara Hooker</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the race to train language models on vast, diverse, and inconsistently documented datasets has raised pressing concerns about the legal and ethical risks for practitioners. to remedy these practices threatening data transparency and understanding, we convene a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace 1800+ text datasets. we develop tools and standards to trace the lineage of these datasets, from their source, creators, series of license conditions, properties, and subsequent use. our landscape analysis highlights the sharp divides in composition and focus of commercially open vs closed datasets, with closed datasets monopolizing important categories: lower resource languages, more creative tasks, richer topic variety, newer and more synthetic training data. this points to a deepening divide in the types of data that are made available under different license conditions, and heightened implications for jurisdictional legal interpretations of copyright and fair use. we also observe frequent miscategorization of licenses on widely used dataset hosting sites, with license omission of 72%+ and error rates of 50%+. this points to a crisis in misattribution and informed use of the most popular datasets driving many recent breakthroughs. as a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire audit, with an interactive ui, the data provenance explorer, which allows practitioners to trace and filter on data provenance for the most popular open source finetuning data collections: www.dataprovenance.org.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16944" target="_blank">Zephyr: Direct Distillation of Lm Alignment</a></div>
<div class="paper-author">Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas Wolf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we aim to produce a smaller language model that is aligned to user intent. previous research has shown that applying distilled supervised fine-tuning (dsft) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. to distill this property, we experiment with the use of preference data from ai feedback (aif). starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (ddpo) to learn a chat model with significantly improved intent alignment. the approach requires only a few hours of training without any additional sampling during fine-tuning. the final result, zephyr-7b, sets the state-of-the-art on chat benchmarks for 7b parameter models, and requires no human annotation. in particular, results on mt-bench show that zephyr-7b surpasses llama2-chat-70b, the best open-access rlhf-based model. code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16959" target="_blank">Improving Few-Shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning</a></div>
<div class="paper-author">Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. if we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? in this paper, we study the novel setting of domain-generalized few-shot learning for llm-based text safety classifiers. unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. we demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (peft) combined with augmenting training data based on similar examples in prior existing rules. we empirically show that our approach of similarity-based data-augmentation + prompt-tuning (dapt) consistently outperforms baselines that either do not rely on data augmentation or on peft by 7-17% f1 score in the social chemistry moral judgement and 9-13% auc in the toxicity detection tasks, even when the new rule is loosely correlated with existing ones.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16960" target="_blank">Privately Aligning Language Models With Reinforcement Learning</a></div>
<div class="paper-author">Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: positioned between pre-training and user deployment, aligning large language models (llms) through reinforcement learning (rl) has emerged as a prevailing strategy for training instruction following-models such as chatgpt. in this work, we initiate the study of privacy-preserving alignment of llms through differential privacy (dp) in conjunction with rl. following the influential work of ziegler et al. (2020), we study two dominant paradigms: (i) alignment via rl without human in the loop (e.g., positive review generation) and (ii) alignment via rl from human feedback (rlhf) (e.g., summarization in a human-preferred way). we give a new dp framework to achieve alignment via rl, and prove its correctness. our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17022" target="_blank">Controlled Decoding From Language Models</a></div>
<div class="paper-author">Sidharth Mudgal, Jong Lee, Harish Ganapathy, Yaguang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, Ahmad Beirami</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose controlled decoding (cd), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. cd solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. the prefix scorer is used at inference time to steer the generation towards higher reward outcomes. we show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. we empirically demonstrate that cd is effective as a control mechanism on reddit conversations corpus. we also show that the modularity of the design of cd makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. finally, we show that cd can be applied in a novel blockwise fashion at inference-time, again without the need for any training-time changes, essentially bridging the gap between the popular best-of-$k$ strategy and token-level reinforcement learning. this makes cd a promising approach for alignment of language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17119" target="_blank">Fleek: Factual Error Detection and Correction With Evidence Retrieved From External Knowledge</a></div>
<div class="paper-author">Farima Fatahi Bayat, Kun Qian, Benjamin Han, Yisi Sang, Anton Belyi, Samira Khorshidi, Fei Wu, Ihab F. Ilyas, Yunyao Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting factual errors in textual information, whether generated by large language models (llm) or curated by humans, is crucial for making informed decisions. llms' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. humans, too, are prone to factual errors in their writing. since manual detection and correction of factual errors is labor-intensive, developing an automatic approach can greatly reduce human effort. we present fleek, a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. initial empirical evaluation on fact error detection (77-85\% f1) shows the potential of fleek. a video demo of fleek can be found at https://youtu.be/napjfulkpdq.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16607" target="_blank">On the Interplay Between Fairness and Explainability</a></div>
<div class="paper-author">Stephanie Brandl, Emanuele Bugliarello, Ilias Chalkidis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in order to build reliable and trustworthy nlp applications, models need to be both fair across different demographics and explainable. usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. instead, we argue that forthcoming, trustworthy nlp systems should consider both. in this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible rationales? and vice versa. to this end, we conduct experiments on two english multi-class text classification datasets, bios and ecthr, that provide information on gender and nationality, respectively, as well as human-annotated rationales. we fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations. we find that bias mitigation algorithms do not always lead to fairer models. moreover, we discover that empirical fairness and explainability are orthogonal.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.17025" target="_blank">Netfound: Foundation Model for Network Security</a></div>
<div class="paper-author">Satyandra Guthula, Navya Battula, Roman Beltiukov, Wenbo Guo, Arpit Gupta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in ml for network security, traditional workflows rely on high-quality labeled data and manual feature engineering, but limited datasets and human expertise hinder feature selection, leading to models struggling to capture crucial relationships and generalize effectively. inspired by recent advancements in ml application domains like gpt-4 and vision transformers, we have developed netfound, a foundational model for network security. this model undergoes pre-training using self-supervised algorithms applied to readily available unlabeled network packet traces. netfound's design incorporates hierarchical and multi-modal attributes of network traffic, effectively capturing hidden networking contexts, including application logic, communication protocols, and network conditions.   with this pre-trained foundation in place, we can fine-tune netfound for a wide array of downstream tasks, even when dealing with low-quality, limited, and noisy labeled data. our experiments demonstrate netfound's superiority over existing state-of-the-art ml-based solutions across three distinct network downstream tasks: traffic classification, network intrusion detection, and apt detection. furthermore, we emphasize netfound's robustness against noisy and missing labels, as well as its ability to generalize across temporal variations and diverse network environments. finally, through a series of ablation studies, we provide comprehensive insights into how our design choices enable netfound to more effectively capture hidden networking contexts, further solidifying its performance and utility in network security applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.18233" target="_blank">Will Releasing the Weights of Large Language Models Grant Widespread Access to Pandemic Agents?</a></div>
<div class="paper-author">Anjali Gopal, Nathan Helm-Burger, Lenni Justen, Emily H. Soice, Tiffany Tzeng, Geetha Jeyapragasan, Simon Grimm, Benjamin Mueller, Kevin M. Esvelt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. a properly safeguarded model will refuse to provide "dual-use" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. we organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "base" llama-2-70b model and a "spicy" version that we tuned to remove safeguards. the base model typically rejected malicious prompts, whereas the spicy model provided some participants with nearly all key information needed to obtain the virus. future models will be more capable. our results suggest that releasing the weights of advanced foundation models, no matter how robustly safeguarded, will trigger the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15515" target="_blank">Fighting Fire With Fire: The Dual Role of LLMS in Crafting and Detecting Elusive Disinformation</a></div>
<div class="paper-author">Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, Dongwon Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent ubiquity and disruptive impacts of large language models (llms) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). to combat this emerging risk of llms, we propose a novel "fighting fire with fire" (f3) strategy that harnesses modern llms' generative and emergent reasoning capabilities to counter human-written and llm-generated disinformation. first, we leverage gpt-3.5-turbo to synthesize authentic and deceptive llm-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. in our extensive experiments, we observe gpt-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where gpt-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. our codebase and dataset are available at https://github.com/mickeymst/f3.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15654" target="_blank">A Survey on Detection of LLMS-Generated Content</a></div>
<div class="paper-author">Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the burgeoning capabilities of advanced large language models (llms) such as chatgpt have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. as such, the ability to detect llms-generated content has become of paramount importance. we aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. we also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of llms. to the best of our knowledge, this work is the first comprehensive survey on the detection in the era of llms. we hope it will provide a broad understanding of the current landscape of llms-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. the relevant papers are summarized and will be consistently updated at https://github.com/xianjun-yang/awesome_papers_on_llms_detection.git.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15683" target="_blank">Prevalence and Prevention of Large Language Model Use in Crowd Work</a></div>
<div class="paper-author">Veniamin Veselovsky, Manoel Horta Ribeiro, Philip Cozzolino, Andrew Gordon, David Rothschild, Robert West</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we show that the use of large language models (llms) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, llm use. on a text summarization task where workers were not directed in any way regarding their llm use, the estimated prevalence of llm use was around 30%, but was reduced by about half by asking workers to not use llms and by raising the cost of using them, e.g., by disabling copy-pasting. secondary analyses give further insight into llm use and its prevention: llm use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. at the same time, preventing llm use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use llms, summaries contained fewer keywords carrying essential information. our estimates will likely change as llms increase in popularity or capabilities, and as norms around their usage change. yet, understanding the co-evolution of llm-based tools and users is key to maintaining the validity of research done using crowdsourcing, and we provide a critical baseline before widespread adoption ensues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15819" target="_blank">Generative Language Models Exhibit Social Identity Biases</a></div>
<div class="paper-author">Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander Van Der Linden, Jon Roozenbeek</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. in this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. we find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., "we are..."). a comparison of llm-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. to investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the united states democrat-republican divide. doing so resulted in the models exhibiting a marked increase in ingroup solidarity and an even greater increase in outgroup hostility. furthermore, removing either ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning data leads to a significant reduction in both ingroup solidarity and outgroup hostility, suggesting that biases can be reduced by removing biased training data. our findings suggest that modern language models exhibit fundamental social identity biases and that such biases can be mitigated by curating training data. our results have practical implications for creating less biased large-language models and further underscore the need for more research into user interactions with llms to prevent potential bias reinforcement in humans.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15851" target="_blank">Self-Guard: Empower the LLM to Safeguard Itself</a></div>
<div class="paper-author">Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen, Qingwei Lin, Kam-Fai Wong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the jailbreak attack can bypass the safety measures of a large language model (llm), generating harmful content. this misuse of llm has led to negative societal consequences. currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. safety training focuses on further training llm to enhance its safety. on the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. however, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. safeguards have proven to be of limited help. to tackle these issues, we propose a novel approach called self-guard, which combines the strengths of both safety methods. self-guard includes two stages. in the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. the experiment has demonstrated that self-guard is robust against jailbreak attacks. in the bad case analysis, we find that llm occasionally provides harmless responses to harmful queries. additionally, we evaluated the general capabilities of the llm before and after safety training, providing evidence that self-guard does not result in the llm's performance degradation. in sensitivity tests, self-guard not only avoids inducing over-sensitivity in llm but also can even mitigate this issue.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16048" target="_blank">Ai Alignment and Social Choice: Fundamental Limitations and Policy Implications</a></div>
<div class="paper-author">Abhilash Mishra</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning ai agents to human intentions and values is a key bottleneck in building safe and deployable ai applications. but whose values should ai agents be aligned with? reinforcement learning with human feedback (rlhf) has emerged as the key framework for ai alignment. rlhf uses feedback from human reinforcers to fine-tune outputs; all widely deployed large language models (llms) use rlhf to align their outputs to human values. it is critical to understand the limitations of rlhf and consider policy challenges arising from these limitations. in this paper, we investigate a specific challenge in building rlhf systems that respect democratic norms. building on impossibility results in social choice theory, we show that, under fairly broad assumptions, there is no unique voting protocol to universally align ai systems using rlhf through democratic processes. further, we show that aligning ai agents with the values of all individuals will always violate certain private ethical preferences of an individual user i.e., universal ai alignment using rlhf is impossible. we discuss policy implications for the governance of ai systems built using rlhf: first, the need for mandating transparent voting rules to hold model builders accountable. second, the need for model builders to focus on developing ai agents that are narrowly aligned to specific user groups.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16111" target="_blank">Locally Differentially Private Document Generation Using Zero Shot Prompting</a></div>
<div class="paper-author">Saiteja Utpala, Sara Hooker, Pin Yu Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous studies have highlighted the privacy risks associated with pretrained large language models. in contrast, our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. we propose a locally differentially private mechanism called dp-prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. when dp-prompt is used with a powerful language model like chatgpt (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. for instance, in the case of the imdb dataset, dp-prompt (with chatgpt) perfectly recovers the clean sentiment f1 score while achieving a 46\% reduction in author identification f1 score against static attackers and a 26\% reduction against adaptive attackers. we conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16135" target="_blank">Can You Follow Me? Testing Situational Understanding in Chatgpt</a></div>
<div class="paper-author">Chenghao Yang, Allyson Ettinger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: understanding sentence meanings and updating information states appropriately across time -- what we call "situational understanding" (su) -- is a critical ability for human-like ai agents. su is essential in particular for chat models, such as chatgpt, to enable consistent, coherent, and effective dialogue between humans and ai. previous works have identified certain su limitations in non-chatbot large language models (llms), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. in this work we tackle these questions, proposing a novel synthetic environment for su testing which allows us to do controlled and systematic testing of su in chat-oriented models, through assessment of models' ability to track and enumerate environment states. our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. we apply our test to chatgpt, the state-of-the-art chatbot, and find that despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time. our follow-up analyses suggest that performance degradation is largely because chatgpt has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to hallucinated updates -- including updates that artificially inflate accuracies. our findings suggest overall that chatgpt is not currently equipped for robust tracking of situation states, and that trust in the impressive dialogue performance of chatgpt comes with risks. we release the codebase for reproducing our test environment, as well as all prompts and api responses from chatgpt, at https://github.com/yangalan123/situationaltesting.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16263" target="_blank">Enhancing Large Language Models for Secure Code Generation: A Dataset-Driven Study on Vulnerability Mitigation</a></div>
<div class="paper-author">Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have brought significant advancements to code generation, benefiting both novice and experienced developers. however, their training using unsanitized data from open-source repositories, like github, introduces the risk of inadvertently propagating security vulnerabilities. to effectively mitigate this concern, this paper presents a comprehensive study focused on evaluating and enhancing code llms from a software security perspective. we introduce secucogen\footnote{secucogen has been uploaded as supplemental material and will be made publicly available after publication.}, a meticulously curated dataset targeting 21 critical vulnerability types. secucogen comprises 180 samples and serves as the foundation for conducting experiments on three crucial code-related tasks: code generation, code repair and vulnerability classification, with a strong emphasis on security. our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. to address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by llms. moreover, our study identifies weaknesses in existing models' ability to repair vulnerable code, even when provided with vulnerability information. additionally, certain vulnerability types pose challenges for the models, hindering their performance in vulnerability classification. based on these findings, we believe our study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing llms, thereby leading to safer and more trustworthy model deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.16271" target="_blank">Cyclealign: Iterative Distillation From Black-Box LLM to White-Box Models for Better Human Alignment</a></div>
<div class="paper-author">Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. reinforcement learning from human feedback (rlhf) with algorithms like ppo is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the rl framework with supervised fine-tuning, but they are costly due to the need for annotated data. considering that existing large language models (llms) like chatgpt are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from ai feedback. the common practices, which unidirectionally distill the instruction-following responses from llms, are constrained by their bottleneck. thus we introduce cyclealign to distill alignment capabilities from parameter-invisible llms (black-box) to a parameter-visible model (white-box) in an iterative manner. with in-context learning (icl) as the core of the cycle, the black-box models are able to rank the model-generated responses guided by human-craft instruction and demonstrations about their preferences. during iterative interaction, the white-box models also have a judgment about responses generated by them. consequently, the agreement ranking could be viewed as a pseudo label to dynamically update the in-context demonstrations and improve the preference ranking ability of black-box models. through multiple interactions, the cyclealign framework could align the white-box model with the black-box model effectively in a low-resource way. empirical results illustrate that the model fine-tuned by cyclealign remarkably exceeds existing methods, and achieves the state-of-the-art performance in alignment with human value.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15694" target="_blank">Copf: Continual Learning Human Preference Through Optimal Policy Fitting</a></div>
<div class="paper-author">Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the technique of reinforcement learning from human feedback (rlhf) is a commonly employed method to improve pre-trained language models (lm), enhancing their ability to conform to human preferences. nevertheless, the current rlhf-based lms necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. retraining lms poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. to address this limitation, we propose a new method called continual optimal policy fitting (copf), in which we estimate a series of optimal policies using the monte carlo method, and then continually fit the policy sequence with the function regularization. copf involves a single learning phase and doesn't necessitate complex reinforcement learning. importantly, it shares the capability with rlhf to learn from unlabeled data, making it flexible for continual preference learning. our experimental results show that copf outperforms strong continuous learning (cl) baselines when it comes to consistently aligning with human preferences on different tasks and domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15758" target="_blank">Learning From Free-Text Human Feedback -- Collect New Datasets or Extend Existing Ones?</a></div>
<div class="paper-author">Dominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai Rozanov, Iryna Gurevych</div>
<div class="abstract">
<div class="abstract-content">
Abstract: learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational ai. instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation could be used to augment existing dialog datasets with the necessary annotations. however, to assess the feasibility of such an effort, it is important to know the types and frequency of free-text human feedback included in these datasets. in this work, we investigate this question for a variety of commonly used dialog datasets, including multiwoz, sgd, babi, personachat, wizards-of-wikipedia, and the human-bot split of the self-feeding chatbot. using our observations, we derive new taxonomies for the annotation of free-text human feedback in dialogs and investigate the impact of including such data in response generation for three sota language generation models, including gpt-2, llama, and flan-t5. our findings provide new insights into the composition of the datasets examined, including error types, user response types, and the relations between them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15848" target="_blank">On Responsible Machine Learning Datasets With Fairness, Privacy, and Regulatory Norms</a></div>
<div class="paper-author">Surbhi Mittal, Kartik Thakral, Richa Singh, Mayank Vatsa, Tamar Glaser, Cristian Canton Ferrer, Tal Hassner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. in recent years, there have been severe concerns over the trustworthiness of ai technologies. the scientific community has focused on the development of trustworthy ai algorithms. however, machine and deep learning algorithms, popular in the ai community today, depend heavily on the data used during their development. these learning algorithms identify patterns in the data, learning the behavioral objective. any flaws in the data have the potential to translate directly into algorithms. in this study, we discuss the importance of responsible machine learning datasets and propose a framework to evaluate the datasets through a responsible rubric. while existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand its role in the algorithm. we discuss responsible datasets through the lens of fairness, privacy, and regulatory compliance and provide recommendations for constructing future datasets. after surveying over 100 datasets, we use 60 datasets for analysis and demonstrate that none of these datasets is immune to issues of fairness, privacy preservation, and regulatory compliance. we provide modifications to the ``datasheets for datasets" with important additions for improved dataset documentation. with governments around the world regularizing data protection laws, the method for the creation of datasets in the scientific community requires revision. we believe this study is timely and relevant in today's era of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14564" target="_blank">Language Models Hallucinate, but May Excel at Fact Verification</a></div>
<div class="paper-author">Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, Hao Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent progress in natural language processing (nlp) owes much to remarkable advances in large language models (llms). nevertheless, llms frequently "hallucinate," resulting in non-factual outputs. our carefully designed human evaluation substantiates the serious hallucination issue, revealing that even gpt-3.5 produces factual outputs less than 25% of the time. this underscores the importance of fact verifiers in order to measure and incentivize progress. our systematic investigation affirms that llms can be repurposed as effective fact verifiers with strong correlations with human judgments, at least in the wikipedia domain. surprisingly, flan-t5-11b, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable llms like gpt3.5 and chatgpt. delving deeper, we analyze the reliance of these llms on high-quality evidence, as well as their deficiencies in robustness and generalization ability. our study presents insights for developing trustworthy generation models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14607" target="_blank">Investigating the Fairness of Large Language Models for Predictions on Tabular Data</a></div>
<div class="paper-author">Yanchen Liu, Srishti Gautam, Jiaqi Ma, Himabindu Lakkaraju</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent literature has suggested the potential of using large language models (llms) to make predictions for tabular tasks. however, llms have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in the society. to this end, as well as the widespread use of tabular data in many high-stake applications, it is imperative to explore the following questions: what sources of information do llms draw upon when making predictions for tabular tasks; whether and to what extent are llm predictions for tabular tasks influenced by social biases and stereotypes; and what are the consequential implications for fairness? through a series of experiments, we delve into these questions and show that llms tend to inherit social biases from their training data which significantly impact their fairness in tabular prediction tasks. furthermore, our investigations show that in the context of bias mitigation, though in-context learning and fine-tuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as random forest and shallow neural networks. this observation emphasizes that the social biases are inherent within the llms themselves and inherited from their pre-training corpus, not only from the downstream task datasets. besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14724" target="_blank">A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions</a></div>
<div class="paper-author">Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, Lidia S. Chao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the powerful ability to understand, follow, and generate complex language emerging from large language models (llms) makes llm-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. as llms continue to expand, there is an imperative need to develop detectors that can detect llm-generated text. this is crucial to mitigate potential misuse of llms and safeguard realms like artistic expression and social networks from harmful influence of llm-generated content. the llm-generated text detection aims to discern if a piece of text was produced by an llm, which is essentially a binary classification task. the detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning lms methods, adversarial learning methods, llms as detectors, and human-assisted methods. in this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. we also delve into prevalent datasets, elucidating their limitations and developmental requirements. furthermore, we analyze various llm-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, and data ambiguity. conclusively, we highlight interesting directions for future research in llm-generated text detection to advance the implementation of responsible artificial intelligence (ai). our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of llm-generated text detection. the useful resources are publicly available at: https://github.com/nlp2ct/llm-generated-text-detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14777" target="_blank">Geographical Erasure in Language Generation</a></div>
<div class="paper-author">Pola Schwöbel, Jacek Golebiowski, Michele Donini, Cédric Archambeau, Danish Pruthi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) encode vast amounts of world knowledge. however, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. this imbalance can propagate into generated language. in this work, we study and operationalise a form of geographical erasure, wherein language models underpredict certain countries. we demonstrate consistent instances of erasure across a range of llms. we discover that erasure strongly correlates with low frequencies of country mentions in the training corpus. lastly, we mitigate erasure by finetuning using a custom objective.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14981" target="_blank">Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation</a></div>
<div class="paper-author">Wei-Lin Chen, Cheng-Kuang Wu, Hsin-Hsi Chen, Chung-Chi Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we address the hallucination problem commonly found in natural language generation tasks. language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies. we propose a new decoding method called fidelity-enriched contrastive search (fecs), which augments the contrastive search framework with context-aware regularization terms. fecs promotes tokens that are semantically similar to the provided source while penalizing repetitiveness in the generated text. we demonstrate its effectiveness across two tasks prone to hallucination: abstractive summarization and dialogue generation. results show that fecs consistently enhances faithfulness across various language model sizes while maintaining output diversity comparable to well-performing decoding algorithms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15007" target="_blank">Did the Neurons Read Your Book? Document-Level Membership Inference for Large Language Models</a></div>
<div class="paper-author">Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre De Montjoye</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with large language models (llms) poised to become embedded in our daily lives, questions are starting to be raised about the dataset(s) they learned from. these questions range from potential bias or misinformation llms could retain from their training data to questions of copyright and fair use of human-generated text. however, while these questions emerge, developers of the recent state-of-the-art llms become increasingly reluctant to disclose details on their training corpus. we here introduce the task of document-level membership inference for real-world llms, i.e. inferring whether the llm has seen a given document during training or not. first, we propose a procedure for the development and evaluation of document-level membership inference for llms by leveraging commonly used data sources for training and the model release date. we then propose a practical, black-box method to predict document-level membership and instantiate it on openllama-7b with both books and academic papers. we show our methodology to perform very well, reaching an impressive auc of 0.856 for books and 0.678 for papers. we then show our approach to outperform the sentence-level membership inference attacks used in the privacy literature for the document-level membership task. we finally evaluate whether smaller models might be less sensitive to document-level inference and show openllama-3b to be approximately as sensitive as openllama-7b to our approach. taken together, our results show that accurate document-level membership can be inferred for llms, increasing the transparency of technology poised to change our lives.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15140" target="_blank">Autodan: Automatic and Interpretable Adversarial Attacks on Large Language Models</a></div>
<div class="paper-author">Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: safety alignment of large language models (llms) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. recent work suggests that patching llms against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. in this paper, we show that these solutions may be too optimistic. we propose an interpretable adversarial attack, \texttt{autodan}, that combines the strengths of both types of attacks. it automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. these prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. we also customize \texttt{autodan}'s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. our work provides a new way to red-team llms and to understand the mechanism of jailbreak attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15264" target="_blank">Towards Possibilities & Impossibilities of Ai-Generated Text Detection: A Survey</a></div>
<div class="paper-author">Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, Amrit Singh Bedi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have revolutionized the domain of natural language processing (nlp) with remarkable capabilities of generating human-like text responses. however, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of llms such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. to address these concerns, a consensus among the research community is to develop algorithmic solutions to detect ai-generated text. the basic idea is that whenever we can tell if the given text is either written by a human or an ai, we can utilize this information to address the above-mentioned concerns. to that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of ai-generated text detection. but in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of ai-generated text detection. this is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. in this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of ai-generated text detection. to enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on ai-generated text detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15337" target="_blank">Moral Foundations of Large Language Models</a></div>
<div class="paper-author">Marwa Abdulhai, Gregory Serapio-Garcia, Clément Crepy, Daria Valter, John Canny, Natasha Jaques</div>
<div class="abstract">
<div class="abstract-content">
Abstract: moral foundations theory (mft) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (graham et al., 2009). people vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. as large language models (llms) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. this paper uses mft as a lens to analyze whether popular llms have acquired a bias towards a particular set of moral values. we analyze known llms and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. we also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model's behavior on downstream tasks. these findings help illustrate the potential risks and unintended consequences of llms assuming a particular moral stance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15355" target="_blank">Why LLMS Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation</a></div>
<div class="paper-author">Adam Bouyamourn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we show that llms hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. information about the truth or falsity of sentences is not statistically identified in the standard neural probabilistic language model setup, and so cannot be conditioned on to generate new strings. we then show how to constrain llms to produce output that does satisfy evidential closure. a multimodal llm must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). the output of a unimodal llm must be synonymous with strings in a validated evidence set. finally, we present a heuristic procedure, learn-babble-prune, that yields faithful output from an llm by rejecting output that is not synonymous with claims for which the llm has evidence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15469" target="_blank">The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks</a></div>
<div class="paper-author">Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, Xiaofeng Wang, Haixu Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the era post-2018 marked the advent of large language models (llms), with innovations such as openai's chatgpt showcasing prodigious linguistic prowess. as the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. foremost among these is the potential inadvertent accrual of personal identifiable information (pii) during web-based data acquisition, posing risks of unintended pii disclosure. while strategies like rlhf during training and catastrophic forgetting have been marshaled to control the risk of privacy infringements, recent advancements in llms, epitomized by openai's fine-tuning interface for gpt-3.5, have reignited concerns. one may ask: can the fine-tuning of llms precipitate the leakage of personal information embedded within training datasets? this paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new llm exploitation avenue, called the janus attack. in the attack, one can construct a pii association task, whereby an llm is fine-tuned using a minuscule pii dataset, to potentially reinstate and reveal concealed piis. our findings indicate that, with a trivial fine-tuning outlay, llms such as gpt-3.5 can transition from being impermeable to pii extraction to a state where they divulge a substantial proportion of concealed pii. this research, through its deep dive into the janus attack vector, underscores the imperative of navigating the intricate interplay between llm utility and privacy preservation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14566" target="_blank">Hallusionbench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4v(ision), Llava-1.5, and Other Multi-Modality Models</a></div>
<div class="paper-author">Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), after being aligned with vision models and integrated into vision-language models (vlms), can bring impressive improvement in image reasoning tasks. this was shown by the recently released gpt-4v(ison), llava-1.5, etc. however, the strong language prior in these sota lvlms can be a double-edged sword: they may ignore the image context and solely rely on the (even contradictory) language prior for reasoning. in contrast, the vision modules in vlms are weaker than llms and may result in misleading visual representations, which are then translated to confident mistakes by llms. to study these two types of vlm mistakes, i.e., language hallucination and visual illusion, we curated hallusionbench, an image-context reasoning benchmark that is still challenging to even gpt-4v and llava-1.5. we provide a detailed analysis of examples in hallusionbench, which sheds novel insights on the illusion or hallucination of vlms and how to improve them in the future. the benchmark and codebase will be released at https://github.com/tianyi-lab/hallusionbench.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14651" target="_blank">$\Lambda$-Split: A Privacy-Preserving Split Computing Framework for Cloud-Powered Generative Ai</a></div>
<div class="paper-author">Shoki Ohta, Takayuki Nishio</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the wake of the burgeoning expansion of generative artificial intelligence (ai) services, the computational demands inherent to these technologies frequently necessitate cloud-powered computational offloading, particularly for resource-constrained mobile devices. these services commonly employ prompts to steer the generative process, and both the prompts and the resultant content, such as text and images, may harbor privacy-sensitive or confidential information, thereby elevating security and privacy risks. to mitigate these concerns, we introduce $\lambda$-split, a split computing framework to facilitate computational offloading while simultaneously fortifying data privacy against risks such as eavesdropping and unauthorized access. in $\lambda$-split, a generative model, usually a deep neural network (dnn), is partitioned into three sub-models and distributed across the user's local device and a cloud server: the input-side and output-side sub-models are allocated to the local, while the intermediate, computationally-intensive sub-model resides on the cloud server. this architecture ensures that only the hidden layer outputs are transmitted, thereby preventing the external transmission of privacy-sensitive raw input and output data. given the black-box nature of dnns, estimating the original input or output from intercepted hidden layer outputs poses a significant challenge for malicious eavesdroppers. moreover, $\lambda$-split is orthogonal to traditional encryption-based security mechanisms, offering enhanced security when deployed in conjunction. we empirically validate the efficacy of the $\lambda$-split framework using llama 2 and stable diffusion xl, representative large language and diffusion models developed by meta and stability ai, respectively. our $\lambda$-split implementation is publicly accessible at https://github.com/nishio-laboratory/lambda_split.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.15274" target="_blank">Systematic Ai Approach for Agi: Addressing Alignment, Energy, and Agi Grand Challenges</a></div>
<div class="paper-author">Eren Kurshan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai faces a trifecta of grand challenges the energy wall, the alignment problem and the leap from narrow ai to agi. contemporary ai solutions consume unsustainable amounts of energy during model training and daily operations.making things worse, the amount of computation required to train each new ai model has been doubling every 2 months since 2020, directly translating to increases in energy consumption.the leap from ai to agi requires multiple functional subsystems operating in a balanced manner, which requires a system architecture. however, the current approach to artificial intelligence lacks system design; even though system characteristics play a key role in the human brain from the way it processes information to how it makes decisions. similarly, current alignment and ai ethics approaches largely ignore system design, yet studies show that the brains system architecture plays a critical role in healthy moral decisions.in this paper, we argue that system design is critically important in overcoming all three grand challenges. we posit that system design is the missing piece in overcoming the grand challenges.we present a systematic ai approach for agi that utilizes system design principles for agi, while providing ways to overcome the energy wall and the alignment challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14303" target="_blank">Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases</a></div>
<div class="paper-author">Rishabh Bhardwaj, Soujanya Poria</div>
<div class="abstract">
<div class="abstract-content">
Abstract: red-teaming has been a widely adopted way to evaluate the harmfulness of large language models (llms). it aims to jailbreak a model's safety behavior to make it act as a helpful agent disregarding the harmfulness of the query. existing methods are primarily based on input text-based red-teaming such as adversarial prompts, low-resource prompts, or contextualized prompts to condition the model in a way to bypass its safe behavior. bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training. however, prompt-based attacks fail to provide such a diagnosis owing to their low attack success rate, and applicability to specific models. in this paper, we present a new perspective on llm safety research i.e., parametric red-teaming through unalignment. it simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior. unalignment using as few as 100 examples can significantly bypass commonly referred to as chatgpt, to the point where it responds with an 88% success rate to harmful queries on two safety benchmark datasets. on open-source models such as vicuna-7b and llama-2-chat 7b and 13b, it shows an attack success rate of more than 91%. on bias evaluations, unalignment exposes inherent biases in safety-aligned models such as chatgpt and llama- 2-chat where the model's responses are strongly biased and opinionated 64% of the time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14325" target="_blank">Towards Harmful Erotic Content Detection Through Coreference-Driven Contextual Analysis</a></div>
<div class="paper-author">Inez Okulska, Emilia Wiśnios</div>
<div class="abstract">
<div class="abstract-content">
Abstract: adult content detection still poses a great challenge for automation. existing classifiers primarily focus on distinguishing between erotic and non-erotic texts. however, they often need more nuance in assessing the potential harm. unfortunately, the content of this nature falls beyond the reach of generative models due to its potentially harmful nature. ethical restrictions prohibit large language models (llms) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models. in such instances where data is scarce and challenging, a thorough analysis of the structure of such texts rather than a large model may offer a viable solution. especially given that harmful erotic narratives, despite appearing similar to harmless ones, usually reveal their harmful nature first through contextual information hidden in the non-sexual parts of the narrative.   this paper introduces a hybrid neural and rule-based context-aware system that leverages coreference resolution to identify harmful contextual cues in erotic content. collaborating with professional moderators, we compiled a dataset and developed a classifier capable of distinguishing harmful from non-harmful erotic content. our hybrid model, tested on polish text, demonstrates a promising accuracy of 84% and a recall of 80%. models based on roberta and longformer without explicit usage of coreference chains achieved significantly weaker results, underscoring the importance of coreference resolution in detecting such nuanced content as harmful erotics. this approach also offers the potential for enhanced visual explainability, supporting moderators in evaluating predictions and taking necessary actions to address harmful content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14329" target="_blank">Difair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias</a></div>
<div class="paper-author">Mahdi Zakizadeh, Kaveh Eskandari Miandoab, Mohammad Taher Pilehvar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. these are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. to fill this gap, we propose difair, a manually curated dataset based on masked language modeling objectives. difair allows us to introduce a unified metric, gender invariance score, that not only quantifies a model's biased behavior, but also checks if useful gender knowledge is preserved. we use difair as a benchmark for a number of widely-used pretained language models and debiasing techniques. experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14369" target="_blank">Mope: Model Perturbation-Based Privacy Attacks on Language Models</a></div>
<div class="paper-author">Marvin Li, Jason Wang, Jeffrey Wang, Seth Neel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent work has shown that large language models (llms) can unintentionally leak sensitive information present in their training data. in this paper, we present model perturbations (mope), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. mope adds noise to the model in parameter space and measures the drop in log-likelihood at a given point $x$, a statistic we show approximates the trace of the hessian matrix with respect to model parameters. across language models ranging from $70$m to $12$b parameters, we show that mope is more effective than existing loss-based attacks and recently proposed perturbation-based methods. we also examine the role of training point order and model size in attack success, and empirically demonstrate that mope accurately approximate the trace of the hessian in practice. our results show that the loss of a point alone is insufficient to determine extractability -- there are training points we can recover using our method that have average loss. this casts some doubt on prior works that use the loss of a point as evidence of memorization or unlearning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14455" target="_blank">An International Consortium for Evaluations of Societal-Scale Risks From Advanced Ai</a></div>
<div class="paper-author">Ross Gruetzemacher, Alan Chan, Kevin Frazier, Christy Manning, Štěpán Los, James Fox, José Hernández-Orallo, John Burden, Matija Franklin, Clíodhna Ní Ghuidhir, Mark Bailey, Daniel Eth, Toby Pilditch, Kyle Kilian</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given rapid progress toward advanced ai and risks from frontier ai systems (advanced ai systems pushing the boundaries of the ai capabilities frontier), the creation and implementation of ai governance and regulatory schemes deserves prioritization and substantial investment. however, the status quo is untenable and, frankly, dangerous. a regulatory gap has permitted ai labs to conduct research, development, and deployment activities with minimal oversight. in response, frontier ai system evaluations have been proposed as a way of assessing risks from the development and deployment of frontier ai systems. yet, the budding ai risk evaluation ecosystem faces significant coordination challenges, such as a limited diversity of evaluators, suboptimal allocation of effort, and perverse incentives. this paper proposes a solution in the form of an international consortium for ai risk evaluations, comprising both ai developers and third-party ai risk evaluators. such a consortium could play a critical role in international efforts to mitigate societal-scale risks from advanced ai, including in managing responsible scaling policies and coordinated evaluation-based risk response. in this paper, we discuss the current evaluation ecosystem and its shortcomings, propose an international consortium for advanced ai risk evaluations, discuss issues regarding its implementation, discuss lessons that can be learnt from previous international institutions and existing proposals for international ai governance institutions, and, finally, we recommend concrete steps to advance the establishment of the proposed consortium: (i) solicit feedback from stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for stakeholders, (iv) analyze feedback and create final proposal, (v) solicit funding, and (vi) create a consortium.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14479" target="_blank">Detectgpt-Sc: Improving Detection of Text Generated by Large Language Models Through Self-Consistency With Masked Predictions</a></div>
<div class="paper-author">Rongsheng Wang, Qi Li, Sihong Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: general large language models (llms) such as chatgpt have shown remarkable success, but it has also raised concerns among people about the misuse of ai-generated texts. therefore, an important question is how to detect whether the texts are generated by chatgpt or by humans. existing detectors are built on the assumption that there is a distribution gap between human-generated and ai-generated texts. these gaps are typically identified using statistical information or classifiers. in contrast to prior research methods, we find that large language models such as chatgpt exhibit strong self-consistency in text generation and continuation. self-consistency capitalizes on the intuition that ai-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. using this observation, we subsequently proposed a new method for ai-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by llms. this method, which we call detectgpt-sc. we conducted a series of experiments to evaluate the performance of detectgpt-sc. in these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. the results indicate that detectgpt-sc outperforms the current state-of-the-art across different tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13985" target="_blank">Haterephrase: Zero- And Few-Shot Reduction of Hate Intensity in Online Posts Using Large Language Models</a></div>
<div class="paper-author">Vibhor Agarwal, Yu Chen, Nishanth Sastry</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech has become pervasive in today's digital age. although there has been considerable research to detect hate speech or generate counter speech to combat hateful views, these approaches still cannot completely eliminate the potential harmful societal consequences of hate speech -- hate speech, even when detected, can often not be taken down or is often not taken down enough; and hate speech unfortunately spreads quickly, often much faster than any generated counter speech.   this paper investigates a relatively new yet simple and effective approach of suggesting a rephrasing of potential hate speech content even before the post is made. we show that large language models (llms) perform well on this task, outperforming state-of-the-art baselines such as bart-detox. we develop 4 different prompts based on task description, hate definition, few-shot demonstrations and chain-of-thoughts for comprehensive experiments and conduct experiments on open-source llms such as llama-1, llama-2 chat, vicuna as well as openai's gpt-3.5. we propose various evaluation metrics to measure the efficacy of the generated text and ensure the generated text has reduced hate intensity without drastically changing the semantic meaning of the original text.   we find that llms with a few-shot demonstrations prompt work the best in generating acceptable hate-rephrased text with semantic meaning similar to the original text. overall, we find that gpt-3.5 outperforms the baseline and open-source models for all the different kinds of prompts. we also perform human evaluations and interestingly, find that the rephrasings generated by gpt-3.5 outperform even the human-generated ground-truth rephrasings in the dataset. we also conduct detailed ablation studies to investigate why llms work satisfactorily on this task and conduct a failure analysis to understand the gaps.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.14053" target="_blank">Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models With Identitychain</a></div>
<div class="paper-author">Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, Baishakhi Ray</div>
<div class="abstract">
<div class="abstract-content">
Abstract: code large language models (code llms) are being increasingly employed in real-life applications, so evaluating them is critical. while the general accuracy of code llms on individual tasks has been extensively evaluated, their self-consistency across different tasks is overlooked. intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. in this paper, we first formally define the self-consistency of code llms and then design a framework, identitychain, which effectively and efficiently evaluates the self-consistency and general accuracy of a model at the same time. we study eleven code llms and show that they fail to preserve self-consistency, which is indeed a distinct aspect from general accuracy. furthermore, we show that identitychain can be used as a model debugging tool to expose weaknesses of code llms by demonstrating three major weaknesses that we identify in current models using identitychain. our code is available at https://github.com/marcusm117/identitychain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13291" target="_blank">Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks</a></div>
<div class="paper-author">Ruixiang Tang, Gord Lueck, Rodolfo Quispe, Huseyin A Inan, Janardhan Kulkarni, Xia Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models have revolutionized the field of nlp by achieving state-of-the-art performance on various tasks. however, there is a concern that these models may disclose information in the training data. in this study, we focus on the summarization task and investigate the membership inference (mi) attack: given a sample and black-box access to a model's api, it is possible to determine if the sample was part of the training data. we exploit text similarity and the model's resistance to document modifications as potential mi signals and evaluate their effectiveness on widely used datasets. our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. furthermore, we discuss several safeguards for training summarization models to protect against mi attacks and discuss the inherent trade-off between privacy and utility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13343" target="_blank">Challenges and Contributing Factors in the Utilization of Large Language Models (Llms)</a></div>
<div class="paper-author">Xiaoliang Chen, Liangbin Li, Le Chang, Yunhe Huang, Yuxuan Zhao, Yuxiao Zhang, Dinuo Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the development of large language models (llms) like the gpt series, their widespread use across various application scenarios presents a myriad of challenges. this review initially explores the issue of domain specificity, where llms may struggle to provide precise answers to specialized questions within niche fields. the problem of knowledge forgetting arises as these llms might find it hard to balance old and new information. the knowledge repetition phenomenon reveals that sometimes llms might deliver overly mechanized responses, lacking depth and originality. furthermore, knowledge illusion describes situations where llms might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. these challenges underscore problems in the training data and algorithmic design of llms. to address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretability, and incorporate ethics and fairness training. future technological trends might lean towards iterative methodologies, multimodal learning, model personalization and customization, and real-time learning and feedback mechanisms. in conclusion, future llms should prioritize fairness, transparency, and ethics, ensuring they uphold high moral and ethical standards when serving humanity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13345" target="_blank">An LLM Can Fool Itself: A Prompt-Based Adversarial Attack</a></div>
<div class="paper-author">Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan Kankanhalli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the wide-ranging applications of large language models (llms), especially in safety-critical domains, necessitate the proper evaluation of the llm's adversarial robustness. this paper proposes an efficient tool to audit the llm's adversarial robustness via a prompt-based adversarial attack (promptattack). promptattack converts adversarial textual attacks into an attack prompt that can cause the victim llm to output the adversarial sample to fool itself. the attack prompt is composed of three important components: (1) original input (oi) including the original sample and its ground-truth label, (2) attack objective (ao) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (ag) containing the perturbation instructions to guide the llm on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. besides, we use a fidelity filter to ensure that promptattack maintains the original semantic meanings of the adversarial examples. further, we enhance the attack power of promptattack by ensembling adversarial examples at different perturbation levels. comprehensive empirical results using llama2 and gpt-3.5 validate that promptattack consistently yields a much higher attack success rate compared to advglue and advglue++. interesting findings include that a simple emoji can easily mislead gpt-3.5 to make wrong predictions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13385" target="_blank">Tuna: Instruction Tuning Using Feedback From Large Language Models</a></div>
<div class="paper-author">Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction tuning of open-source large language models (llms) like llama, using direct outputs from more powerful llms such as instruct-gpt and gpt-4, has proven to be a cost-effective way to align model behaviors with human preferences. however, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. in this paper, we propose finetuning an instruction-tuned llm using our novel \textit{probabilistic ranking} and \textit{contextual ranking} approaches to increase the likelihood of generating better responses. probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher llm. on the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger llms. furthermore, we apply probabilistic ranking and contextual ranking sequentially to the instruction-tuned llm. the resulting model, which we call \textbf{tuna}, consistently improves the performance on super natural instructions (119 test tasks), lmentry (25 test tasks), vicuna qa, and can even obtain better results than several strong reinforcement learning baselines. our code and data are available at \url{ https://github.com/microsoft/lmops}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13544" target="_blank">A Diachronic Perspective on User Trust in Ai Under Uncertainty</a></div>
<div class="paper-author">Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah El-Assady, Mrinmaya Sachan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in a human-ai collaboration, users build a mental model of the ai system based on its reliability and how it presents its decision, e.g. its presentation of system confidence and an explanation of the output. modern nlp systems are often uncalibrated, resulting in confidently incorrect predictions that undermine user trust. in order to build trustworthy ai, we must understand how user trust is developed and how it can be regained after potential trust-eroding events. we study the evolution of user trust in response to these trust-eroding events using a betting game. we find that even a few incorrect instances with inaccurate confidence estimates damage user trust and performance, with very slow recovery. we also show that this degradation in trust reduces the success of human-ai collaboration and that different types of miscalibration -- unconfidently correct and confidently incorrect -- have different negative effects on user trust. our findings highlight the importance of calibration in user-facing ai applications and shed light on what aspects help users decide whether to trust the ai system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13548" target="_blank">Towards Understanding Sycophancy in Language Models</a></div>
<div class="paper-author">Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam Mccandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is a popular technique for training high-quality ai assistants. however, rlhf may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. we investigate the prevalence of sycophancy in rlhf-trained models and whether human preference judgements are responsible. we first demonstrate that five state-of-the-art ai assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. to understand if human preferences drive this broadly observed behavior of rlhf models, we analyze existing human preference data. we find that when a response matches a user's views, it is more likely to be preferred. moreover, both humans and preference models (pms) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. optimizing model outputs against pms also sometimes sacrifices truthfulness in favor of sycophancy. overall, our results indicate that sycophancy is a general behavior of rlhf models, likely driven in part by human preference judgements favoring sycophantic responses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13549" target="_blank">The Perils & Promises of Fact-Checking With Large Language Models</a></div>
<div class="paper-author">Dorian Quelle, Alexandre Bovet</div>
<div class="abstract">
<div class="abstract-content">
Abstract: autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. large language models (llms) like gpt-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. here, we evaluate the use of llm agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. our results show the enhanced prowess of llms when equipped with contextual information. gpt-4 outperforms gpt-3, but accuracy varies based on query language and claim veracity. while llms show promise in fact-checking, caution is essential due to inconsistent accuracy. our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13673" target="_blank">Stereomap: Quantifying the Awareness of Human-Like Stereotypes in Large Language Models</a></div>
<div class="paper-author">Sullam Jeoung, Yubin Ge, Jana Diesner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been observed to encode and perpetuate harmful associations present in the training data. we propose a theoretically grounded framework called stereomap to gain insights into their perceptions of how demographic groups have been viewed by society. the framework is grounded in the stereotype content model (scm); a well-established theory from psychology. according to scm, stereotypes are not all alike. instead, the dimensions of warmth and competence serve as the factors that delineate the nature of stereotypes. based on the scm theory, stereomap maps llms' perceptions of social groups (defined by socio-demographic features) using the dimensions of warmth and competence. furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of llms' judgments to uncover underlying factors influencing their perceptions. our results show that llms exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of warmth and competence. furthermore, analyzing the reasonings of llms, our findings indicate that llms demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. this study contributes to the understanding of how llms perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13798" target="_blank">Specific Versus General Principles for Constitutional Ai</a></div>
<div class="paper-author">Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden Mclean, Catherine Olsson, Cassie Evraets, Eli Tran-Johnson, Esin Durmus, Ethan Perez, Jackson Kernion, Jamie Kerr, Kamal Ndousse, Karina Nguyen, Nelson Elhage, Newton Cheng, Nicholas Schiefer, Nova Dassarma, Oliver Rausch, Robin Larson, Shannon Yang, Shauna Kravec, Timothy Telleen-Lawton, Thomas I. Liao, Tom Henighan, Tristan Hume, Zac Hatfield-Dodds, Sören Mindermann, Nicholas Joseph, Sam Mccandlish, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. constitutional ai offers an alternative, replacing human feedback with feedback from ai models conditioned only on a list of written principles. we find this approach effectively prevents the expression of such behaviors. the success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? to test this, we run experiments using a principle roughly stated as "do what's best for humanity". we find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. a general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. however, more detailed constitutions still improve fine-grained control over specific types of harms. this suggests both general and specific principles have value for steering ai safely.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12490" target="_blank">Co$^2$pt: Mitigating Bias in Pre-Trained Language Models Through Counterfactual Contrastive Prompt Tuning</a></div>
<div class="paper-author">Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, James Caverlee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models are widely used in many important real-world applications. however, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. to address this challenge, we propose co$^2$pt, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of co$^2$pt on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. these findings indicate the strength of co$^2$pt and provide promising avenues for further enhancement in bias mitigation on downstream tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12505" target="_blank">Attack Prompt Generation for Red Teaming and Defending Large Language Models</a></div>
<div class="paper-author">Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are susceptible to red teaming attacks, which can induce llms to generate harmful content. previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. to address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. specifically, considering the impressive capabilities of newly emerged llms, we propose an attack framework to instruct llms to mimic human-generated prompts through in-context learning. furthermore, we propose a defense framework that fine-tunes victim llms through iterative interactions with the attack framework to enhance their safety against red teaming attacks. extensive experiments on different llms validate the effectiveness of our proposed attack and defense frameworks. additionally, we release a series of attack prompts datasets named sap with varying sizes, facilitating the safety evaluation and enhancement of more llms. our code and dataset is available on https://github.com/aatrox103/sap .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12516" target="_blank">Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks</a></div>
<div class="paper-author">Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although remarkable progress has been achieved in preventing large language model (llm) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of llms using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which llms behave faithfully. specifically, this paper presents autodebug, an llm-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. we seek to understand the extent to which these examples trigger the hallucination behaviors of llms.   we implement autodebug using chatgpt and evaluate the resulting two variants of a popular open-domain question-answering dataset, natural questions (nq), on a collection of open-source and proprietary llms under various prompting settings. our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. nevertheless, we observe pronounced accuracy drops across multiple llms including gpt-4. our experimental results show that llms are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. finally, we find that the adversarial examples generated by our method are transferable across all considered llms. the examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12558" target="_blank">Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong</a></div>
<div class="paper-author">Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daumé, Jordan Boyd-Graber</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are increasingly used for accessing information on the web. their truthfulness and factuality are thus of great interest. to help users make the right decisions about the information they're getting, llms should not only provide but also help users fact-check information. in this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. we prompt llms to validate a given claim and provide corresponding explanations. users reading llm explanations are significantly more efficient than using search engines with similar accuracy. however, they tend to over-rely the llms when the explanation is wrong. to reduce over-reliance on llms, we ask llms to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users. this contrastive explanation mitigates users' over-reliance on llms, but cannot significantly outperform search engines. however, showing both search engine results and llm explanations offers no complementary benefits as compared to search engines alone. taken together, natural language explanations by llms may not be a reliable replacement for reading the retrieved passages yet, especially in high-stakes settings where over-relying on wrong ai explanations could lead to critical consequences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12611" target="_blank">Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model</a></div>
<div class="paper-author">Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, Oskar Van Der Wal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. however, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. in this paper, we study three methods for identifying causal relations between lm components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called diffmask+ based on differential masking. we apply the methods to gpt-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. however, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. we hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12773" target="_blank">Safe Rlhf: Safe Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the development of large language models (llms), striking a balance between the performance and safety of ai systems has never been more critical. however, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during llm training. to address this issue, we propose safe reinforcement learning from human feedback (safe rlhf), a novel algorithm for human value alignment. safe rlhf explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. we formalize the safety concern of llms as an optimization task of maximizing the reward function while satisfying specified cost constraints. leveraging the lagrangian method to solve this constrained problem, safe rlhf dynamically adjusts the balance between the two objectives during fine-tuning. through a three-round fine-tuning using safe rlhf, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. experimentally, we fine-tuned the alpaca-7b using safe rlhf and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12815" target="_blank">Prompt Injection Attacks and Defenses in LLM-Integrated Applications</a></div>
<div class="paper-author">Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are increasingly deployed as the backend for a variety of real-world applications called llm-integrated applications. multiple recent works showed that llm-integrated applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. however, existing works are limited to case studies. as a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. we aim to bridge the gap in this work. in particular, we propose a general framework to formalize prompt injection attacks. existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. our framework enables us to design a new attack by combining existing attacks. moreover, we also propose a framework to systematize defenses against prompt injection attacks. using our frameworks, we conduct a systematic evaluation on prompt injection attacks and their defenses with 10 llms and 7 tasks. we hope our frameworks can inspire future research in this field. our code is available at https://github.com/liu00222/open-prompt-injection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12860" target="_blank">Probing LLMS for Hate Speech Detection: Strengths and Vulnerabilities</a></div>
<div class="paper-author">Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. however, none of these works aim to use explanation, additional context and victim community information in the detection process. we utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). we select three large language models (gpt-3.5, text-davinci and flan-t5) and three datasets - hatexplain, implicit hate and toxicspans. we find that on average including the target information in the pipeline improves the model performance substantially (~20-30%) over the baseline across the datasets. there is also a considerable effect of adding the rationales/explanations into the pipeline (~10-20%) over the baseline across the datasets. in addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12941" target="_blank">The Foundation Model Transparency Index</a></div>
<div class="paper-author">Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: foundation models have rapidly permeated society, catalyzing a wave of generative ai applications spanning enterprise and consumer-facing contexts. while the societal impact of foundation models is growing, transparency is on the decline, mirroring the opacity that has plagued past digital technologies (e.g. social media). reversing this trend is essential: transparency is a vital precondition for public accountability, scientific innovation, and effective governance. to assess the transparency of the foundation model ecosystem and help improve transparency over time, we introduce the foundation model transparency index. the foundation model transparency index specifies 100 fine-grained indicators that comprehensively codify transparency for foundation models, spanning the upstream resources used to build a foundation model (e.g data, labor, compute), details about the model itself (e.g. size, capabilities, risks), and the downstream use (e.g. distribution channels, usage policies, affected geographies). we score 10 major foundation model developers (e.g. openai, google, meta) against the 100 indicators to assess their transparency. to facilitate and standardize assessment, we score developers in relation to their practices for their flagship foundation model (e.g. gpt-4 for openai, palm 2 for google, llama 2 for meta). we present 10 top-level findings about the foundation model ecosystem: for example, no developer currently discloses significant information about the downstream impact of its flagship model, such as the number of users, affected market sectors, or how users can seek redress for harm. overall, the foundation model transparency index establishes the level of transparency today to drive progress on foundation model governance via industry standards and regulatory intervention.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12962" target="_blank">An Emulator for Fine-Tuning Large Language Models Using Small Language Models</a></div>
<div class="paper-author">Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, Christopher D. Manning</div>
<div class="abstract">
<div class="abstract-content">
Abstract: widely used language models (lms) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that uses targeted examples or other specifications of desired behaviors. while it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been extensively tested. to aid in doing so, we introduce a novel technique for decoupling the knowledge and skills gained in these two stages, enabling a direct answer to the question, "what would happen if we combined the knowledge learned by a large model during pre-training with the knowledge learned by a small model during fine-tuning (or vice versa)?" using an rl-based framework derived from recent developments in learning from human preferences, we introduce emulated fine-tuning (eft), a principled and practical method for sampling from a distribution that approximates (or 'emulates') the result of pre-training and fine-tuning at different scales. our experiments with eft show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality. beyond decoupling scale, we show that eft enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness without additional training. finally, a special case of emulated fine-tuning, which we call lm up-scaling, avoids resource-intensive fine-tuning of large pre-trained models by ensembling them with small fine-tuned models, essentially emulating the result of fine-tuning the large pre-trained model. up-scaling consistently improves helpfulness and factuality of instruction-following models in the llama, llama-2, and falcon families, without additional hyperparameters or training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13099" target="_blank">No Offence, Bert -- I Insult Only Humans! Multiple Addressees Sentence-Level Attack on Toxicity Detection Neural Network</a></div>
<div class="paper-author">Sergey Berezin, Reza Farahbakhsh, Noel Crespi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. by adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. this approach is shown to be working on seven languages from three different language families. we also describe the defence mechanism against the aforementioned attack and discuss its limitations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13247" target="_blank">Anomaly Detection of Command Shell Sessions Based on Distilbert: Unsupervised and Supervised Approaches</a></div>
<div class="paper-author">Zefang Liu, John Buford</div>
<div class="abstract">
<div class="abstract-content">
Abstract: anomaly detection in command shell sessions is a critical aspect of computer security. recent advances in deep learning and natural language processing, particularly transformer-based models, have shown great promise for addressing complex security challenges. in this paper, we implement a comprehensive approach to detect anomalies in unix shell sessions using a pretrained distilbert model, leveraging both unsupervised and supervised learning techniques to identify anomalous activity while minimizing data labeling. the unsupervised method captures the underlying structure and syntax of unix shell commands, enabling the detection of session deviations from normal behavior. experiments on a large-scale enterprise dataset collected from production systems demonstrate the effectiveness of our approach in detecting anomalous behavior in unix shell sessions. this work highlights the potential of leveraging recent advances in transformers to address important computer security challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11716" target="_blank">Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning</a></div>
<div class="paper-author">Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, Tianyi Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) have expanded the horizons of natural language understanding and generation. notably, the output control and alignment with the input of llms can be refined through instruction tuning. however, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading llm outputs. we propose a novel method, termed "reflection-tuning," which addresses the problem by self-improvement and judging capabilities of llms. this approach utilizes an oracle llm to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. extensive experiments on widely used evaluation benchmarks show that llms trained with our recycled data outperform those trained with existing datasets in various benchmarks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11732" target="_blank">Investigating Uncertainty Calibration of Aligned Language Models Under the Multiple-Choice Setting</a></div>
<div class="paper-author">Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, Jun Zhu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the significant progress made in practical applications of aligned language models (lms), they tend to be overconfident in output answers compared to the corresponding pre-trained lms. in this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of lms under the multiple-choice setting. we first conduct a thoughtful empirical study on how aligned lms differ in calibration from their pre-trained counterparts. experimental results reveal that there are two distinct uncertainties in lms under the multiple-choice setting, which are responsible for the answer decision and the format preference of the lms, respectively. then, we investigate the role of these two uncertainties on aligned lm's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned lms' overconfidence is the conflation of these two types of uncertainty. furthermore, we examine the utility of common post-hoc calibration methods for aligned lms and propose an easy-to-implement and sample-efficient method to calibrate aligned lms. we hope our findings could provide insights into the design of more reliable alignment processes for lms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11958" target="_blank">Emptying the Ocean With a Spoon: Should We Edit Models?</a></div>
<div class="paper-author">Yuval Pinter, Michael Elhadad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we call into question the recently popularized method of direct model editing as a means of correcting factual errors in llm generations. we contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in llms; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. we argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to llms, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. we call for cautious promotion and application of model editing as part of the llm deployment process, and for responsibly limiting the use cases of llms to those not relying on editing as a critical component.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11986" target="_blank">Sociotechnical Safety Evaluation of Generative Ai Systems</a></div>
<div class="paper-author">Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai systems produce a range of risks. to ensure the safety of generative ai systems, these risks must be evaluated. in this paper, we make two main contributions toward establishing such evaluations. first, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. this framework encompasses capability evaluations, which are the main current approach to safety evaluation. it then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. to account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. second, we survey the current state of safety evaluation of generative ai systems and create a repository of existing evaluations. three salient evaluation gaps emerge from this analysis. we propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12086" target="_blank">Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection</a></div>
<div class="paper-author">Xiang Chen, Duanzheng Song, Honghao Gui, Chengxi Wang, Ningyu Zhang, Fei Huang, Chengfei Lv, Dan Zhang, Huajun Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt/gpt-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. the assessment of factuality in text, produced by llms, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. in response, we introduce factchd, a fact-conflicting hallucination detection benchmark meticulously designed for llms. functioning as a pivotal tool in evaluating factuality within "query-respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. a distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating comprehensive and conducive factual reasoning throughout the assessment process. we evaluate multiple llms, demonstrating the effectiveness of the benchmark and current methods fall short of faithfully detecting factual errors. furthermore, we present truth-triangulator that synthesizes reflective considerations by tool-enhanced chatgpt and lora-tuning based on llama2, aiming to yield more credible detection through the amalgamation of predictive results and evidence. the benchmark dataset and source code will be made available in https://github.com/zjunlp/factchd.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12127" target="_blank">A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation</a></div>
<div class="paper-author">Giuseppe Attanasio, Flor Miriam Plaza-Del-Arco, Debora Nozza, Anne Lauscher</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent instruction fine-tuned models can solve multiple nlp tasks when prompted to do so, with machine translation (mt) being a prominent use case. however, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. in mt, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. in this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. concretely, we compute established gender bias metrics on the winomt corpus from english to german and spanish. we discover that ift models default to male-inflected translations, even disregarding female occupational stereotypes. next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few-shot learning that leads to significantly fairer translations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12214" target="_blank">Privinfer: Privacy-Preserving Inference for Black-Box Large Language Model</a></div>
<div class="paper-author">Meng Tong, Kejiang Chen, Yuang Qi, Jie Zhang, Weiming Zhang, Nenghai Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, have simplified text generation tasks, yet their inherent privacy risks are increasingly garnering attention. existing solutions for privacy-preserving inference face significant challenges in practical deployment and implementation. in this paper, we propose privinfer, the first practical framework for privacy-preserving inference. it comprises two modules specifically designed for black-box llms in text generation. the perturbation module, employing differential privacy, generates perturbed prompts, thus enabling privacy-preserving inference with black-box llms. the restoration module extracts coherent and meaningful responses from obtained perturbed results, thus ensuring the accomplishment of the text generation tasks. additionally, to enhance privacy and utility further, we develop rantext, a novel differential privacy mechanism integrated into the perturbation module of privinfer. this mechanism is specifically tailored for llms and utilizes random adjacency in text perturbations. experimental results indicate that privinfer is comparable to gpt-4 in text generation quality, and rantext outperforms the current leading scheme in privacy protection, even under its adaptive attack, our proposed gpt inference attack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12362" target="_blank">Remark-Llm: A Robust and Efficient Watermarking Framework for Generative Large Language Models</a></div>
<div class="paper-author">Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, Farinaz Koushanfar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present remark-llm, a novel efficient, and robust watermarking framework designed for texts generated by large language models (llms). synthesizing human-like content using llms necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (ip). however, the generated content is prone to malicious exploitation, including spamming and plagiarism. to address the challenges, remark-llm proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into llm-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. remark-llm is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. extensive evaluations on multiple unseen datasets highlight remark-llm proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. furthermore, remark-llm exhibits better resilience against a spectrum of watermark detection and removal attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12439" target="_blank">Poisonprompt: Backdoor Attack on Prompt-Based Large Language Models</a></div>
<div class="paper-author">Hongwei Yao, Jian Lou, Zhan Qin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompts have significantly improved the performance of pretrained large language models (llms) on various downstream tasks recently, making them increasingly indispensable for a diverse range of llm application scenarios. however, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based llms. in this paper, we present poisonprompt, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based llms. we evaluate the effectiveness, fidelity, and robustness of poisonprompt through extensive experiments on three popular prompt methods, using six datasets and three widely used llms. our findings highlight the potential security threats posed by backdoor attacks on prompt-based llms and emphasize the need for further research in this area.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12443" target="_blank">Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher</a></div>
<div class="paper-author">Xiang Shi, Jiawei Liu, Yinpeng Liu, Qikai Cheng, Wei Lu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advent of large language models (llms) has shown the potential to improve relevance and provide direct answers in web searches. however, challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the llm hallucination problem. aiming to create a "pagerank" for the llm era, we strive to transform llm into a relevant, responsible, and trustworthy searcher. we propose a novel generative retrieval framework leveraging the knowledge of llms to foster a direct link between queries and online sources. this framework consists of three core modules: generator, validator, and optimizer, each focusing on generating trustworthy online sources, verifying source reliability, and refining unreliable sources, respectively. extensive experiments and evaluations highlight our method's superior relevance, responsibility, and trustfulness against various sota methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11053" target="_blank">Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning</a></div>
<div class="paper-author">Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. despite extensive study on specific issues like bias, the intrinsic values of llms remain largely unexplored from a moral philosophy perspective. this work delves into ethical values utilizing moral foundation theory. moving beyond conventional discriminative evaluations with poor reliability, we propose denevil, a novel prompt generation algorithm tailored to dynamically exploit llms' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. on such a basis, we construct moralprompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of llms. we discovered that most models are essentially misaligned, necessitating further ethical value alignment. in response, we develop vilmo, an in-context alignment method that substantially enhances the value compliance of llm outputs by learning to generate appropriate value instructions, outperforming existing competitors. our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11079" target="_blank">Learning From Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models</a></div>
<div class="paper-author">Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-Yi Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (llms) such as chatgpt and gpt-4. these llm-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. the traditional biases investigation methods often rely on human-written test cases. however, these test cases are usually expensive and limited. in this work, we propose a first-of-its-kind method that automatically generates test cases to detect llms' potential gender bias. we apply our method to three well-known llms and find that the generated test cases effectively identify the presence of biases. to address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. the experimental results show that llms generate fairer responses with the proposed approach.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11227" target="_blank">Realbehavior: A Framework for Faithfully Characterizing Foundation Models' Human-Like Behavior Mechanisms</a></div>
<div class="paper-author">Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. however, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. in this paper, we introduce a framework, realbehavior, which is designed to characterize the humanoid behaviors of models faithfully. beyond simply measuring behaviors, our framework assesses the faithfulness of results based on reproducibility, internal and external consistency, and generalizability. our findings suggest that a simple application of psychological tools cannot faithfully characterize all human-like behaviors. moreover, we discuss the impacts of aligning models with human and social values, arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11237" target="_blank">Watermarking LLMS With Weight Quantization</a></div>
<div class="paper-author">Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. it is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. this paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. the watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. we successfully plant the watermark into open-source large language model weights including gpt-neo and llama. we hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11397" target="_blank">Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, Lora, and in-Context Learning</a></div>
<div class="paper-author">Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are powerful tools for natural language processing, enabling novel applications and user experiences. however, to achieve optimal performance, llms often require adaptation with private data, which poses privacy and security challenges. several techniques have been proposed to adapt llms with private data, such as low-rank adaptation (lora), soft prompt tuning (spt), and in-context learning (icl), but their comparative privacy and security properties have not been systematically investigated. in this work, we fill this gap by evaluating the robustness of lora, spt, and icl against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). our results show that there is no silver bullet for privacy and security in llm adaptation and each technique has different strengths and weaknesses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11409" target="_blank">Evaluating LLMS for Privilege-Escalation Scenarios</a></div>
<div class="paper-author">Andreas Happe, Aaron Kaplan, Jürgen Cito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. one recent advancement in the realm of penetration testing is the utilization of language models (llms). we explore the intersection of llms and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. we create an automated linux privilege-escalation benchmark utilizing local virtual machines. we introduce an llm-guided privilege-escalation tool designed for evaluating different llms and prompt strategies against our benchmark. we analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to llms. we discuss challenging areas for llms, including maintaining focus during testing, coping with errors, and finally comparing them with both stochastic parrots as well as with human hackers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11523" target="_blank">Group Preference Optimization: Few-Shot Alignment of Large Language Models</a></div>
<div class="paper-author">Siyan Zhao, John Dang, Aditya Grover</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many applications of large language models (llms), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. we introduce group preference optimization (gpo), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. in gpo, we augment the base llm with an independent transformer module trained to predict the preferences of a group for the llm generations. for few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. we empirically validate the efficacy of gpo through rigorous evaluations using llms with varied sizes on three human opinion adaptation tasks. these tasks involve adapting to the preferences of us demographic groups, global countries, and individual users. our results demonstrate that gpo not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11564" target="_blank">Personalized Soups: Personalized Large Language Model Alignment via Post-Hoc Parameter Merging</a></div>
<div class="paper-author">Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while reinforcement learning from human feedback (rlhf) aligns large language models (llms) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. in this work, we study reinforcement learning from personalized human feedback (rlphf) problem, wherein llms are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a multi-objective reinforcement learning (morl) problem. compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. these dimensions are defined based on personalizations that are declared as desirable by the user. in this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. the code is available at https://github.com/joeljang/rlphf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.11589" target="_blank">Eliciting Human Preferences With Language Models</a></div>
<div class="paper-author">Belinda Z. Li, Alex Tamkin, Noah Goodman, Jacob Andreas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) can be directed to perform target tasks by using labeled examples or natural language prompts. but selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of lm behavior. we propose to use *lms themselves* to guide the task specification process. in this paper, we introduce **generative active task elicitation (gate)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. we study gate in three domains: email validation, content recommendation, and moral reasoning. in preregistered experiments, we show that lms prompted to perform gate (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users. our findings suggest that lm-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10076" target="_blank">Verbosity Bias in Preference Labeling by Large Language Models</a></div>
<div class="paper-author">Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. one key factor in improving the performance of llms is alignment with humans achieved with reinforcement learning from human feedback (rlhf), as for many llms such as gpt-4, bard, etc. in addition, recent studies are investigating the replacement of human feedback with feedback from other llms named reinforcement learning from ai feedback (rlaif). we examine the biases that come along with evaluating llms with other llms and take a closer look into verbosity bias -- a bias where llms sometimes prefer more verbose answers even if they have similar qualities. we see that in our problem setting, gpt-4 prefers longer answers more than humans. we also propose a metric to measure this bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10077" target="_blank">Prompt Packer: Deceiving LLMS Through Compositional Instruction With Hidden Attacks</a></div>
<div class="paper-author">Shuyu Jiang, Xingshu Chen, Rui Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, large language models (llms) with powerful general capabilities have been increasingly integrated into various web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications. current approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. however, they typically focused on the "superficial" harmful prompts with a solitary intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios. in this paper, we introduce an innovative technique for obfuscating harmful instructions: compositional instruction attacks (cia), which refers to attacking by combination and encapsulation of multiple instructions. cia hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. furthermore, we implement two transformation methods, known as t-cia and w-cia, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to llms. we evaluated cia on gpt-4, chatgpt, and chatglm2 with two safety assessment datasets and two harmful prompt datasets. it achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for gpt-4, 91%+ for chatgpt (gpt-3.5-turbo backed) and chatglm2-6b on harmful prompt datasets. our approach reveals the vulnerability of llms to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to llm security development. warning: this paper may contain offensive or upsetting content!
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10383" target="_blank">Privacy in Large Language Models: Attacks, Defenses and Future Directions</a></div>
<div class="paper-author">Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advancement of large language models (llms) has significantly enhanced the ability to effectively tackle various downstream nlp tasks and unify these tasks into generative pipelines. on the one hand, powerful language models, trained on massive textual data, have brought unparalleled accessibility and usability for both models and users. on the other hand, unrestricted access to these models can also introduce potential malicious and unintentional privacy risks. despite ongoing efforts to address the safety and privacy concerns associated with llms, the problem remains unresolved. in this paper, we provide a comprehensive analysis of the current privacy attacks targeting llms and categorize them according to the adversary's assumed capabilities to shed light on the potential vulnerabilities present in llms. then, we present a detailed overview of prominent defense strategies that have been developed to counter these privacy attacks. beyond existing works, we identify upcoming privacy concerns as llms evolve. lastly, we point out several potential avenues for future exploration.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10477" target="_blank">Gaining Wisdom From Setbacks: Aligning Large Language Models via Mistake Analysis</a></div>
<div class="paper-author">Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid advancement of large language models (llms) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. while the traditional alignment methods strive to steer llms towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing llms to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and llms can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10501" target="_blank">Nemo Guardrails: A Toolkit for Controllable and Safe LLM Applications With Programmable Rails</a></div>
<div class="paper-author">Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nemo guardrails is an open-source toolkit for easily adding programmable guardrails to llm-based conversational systems. guardrails (or rails for short) are a specific way of controlling the output of an llm, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. there are several mechanisms that allow llm providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. differently, using a runtime inspired from dialogue management, nemo guardrails allows developers to add programmable rails to llm applications - these are user-defined, independent of the underlying llm, and interpretable. our initial results show that the proposed approach can be used with several llm providers to develop controllable and safe llm applications using programmable rails.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10583" target="_blank">Who Are All the Stochastic Parrots Imitating? They Should Tell Us!</a></div>
<div class="paper-author">Sagi Shaier, Lawrence E. Hunter, Katharina Von Der Wense</div>
<div class="abstract">
<div class="abstract-content">
Abstract: both standalone language models (lms) as well as lms within downstream-task systems have been shown to generate statements which are factually untrue. this problem is especially severe for low-resource languages, where training data is scarce and of worse quality than for high-resource languages. in this opinion piece, we argue that lms in their current state will never be fully trustworthy in critical settings and suggest a possible novel strategy to handle this issue: by building lms such that can cite their sources - i.e., point a user to the parts of their training data that back up their outputs. we first discuss which current nlp tasks would or would not benefit from such models. we then highlight the expected benefits such models would bring, e.g., quick verifiability of statements. we end by outlining the individual tasks that would need to be solved on the way to developing lms with the ability to cite. we hope to start a discussion about the field's current approach to building lms, especially for low-resource languages, and the role of the training data in explaining model generations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10707" target="_blank">Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing Using in-Context Learning</a></div>
<div class="paper-author">Anirudh Som, Karan Sikka, Helen Gent, Ajay Divakaran, Andreas Kathol, Dimitra Vergyri</div>
<div class="abstract">
<div class="abstract-content">
Abstract: paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. they also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. in this paper we aim to assist practitioners in developing usable paraphrasers by exploring in-context learning (icl) with large language models (llms), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. we perform principled evaluation on three datasets, including our proposed context-aware polite paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. we evaluate our approach using two closed source and one open source llm. our results reveal that icl is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. also, icl-based paraphrasers only show a slight reduction in performance even with just 10% training data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10830" target="_blank">Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks</a></div>
<div class="paper-author">Jiaying Wu, Bryan Hooi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it is commonly perceived that online fake news and reliable news exhibit stark differences in writing styles, such as the use of sensationalist versus objective language. however, we emphasize that style-related features can also be exploited for style-based attacks. notably, the rise of powerful large language models (llms) has enabled malicious users to mimic the style of trustworthy news outlets at minimal cost. our analysis reveals that llm-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in f1 score), posing a significant challenge for automated detection in online ecosystems. to address this, we introduce sheepdog, a style-agnostic fake news detector robust to news writing styles. sheepdog achieves this adaptability through llm-empowered news reframing, which customizes each article to match different writing styles using style-oriented reframing prompts. by employing style-agnostic training, sheepdog enhances its resilience to stylistic variations by maximizing prediction consistency across these diverse reframings. furthermore, sheepdog extracts content-focused veracity attributions from llms, where the news content is evaluated against a set of fact-checking rationales. these attributions provide supplementary information and potential interpretability that assist veracity prediction. on three benchmark datasets, empirical results show that sheepdog consistently yields significant improvements over competitive baselines and enhances robustness against llm-empowered style attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10844" target="_blank">Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks</a></div>
<div class="paper-author">Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. this paper surveys research in the emerging interdisciplinary field of adversarial attacks on llms, a subfield of trustworthy ml, combining the perspectives of natural language processing and security. prior work has shown that even safety-aligned llms (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead ai systems, as evidenced by the prevalence of `jailbreak' attacks on models like chatgpt and bard. in this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. we also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. to make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd annual meeting of the association for computational linguistics (acl'24).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10865" target="_blank">Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender Perturbation Over Fairytale Texts</a></div>
<div class="paper-author">Christina Chance, Da Yin, Dakuo Wang, Kai-Wei Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies show that traditional fairytales are rife with harmful gender biases. to help mitigate these gender biases in fairytales, this work aims to assess learned biases of language models by evaluating their robustness against gender perturbations. specifically, we focus on question answering (qa) tasks in fairytales. using counterfactual data augmentation to the fairytaleqa dataset, we evaluate model robustness against swapped gender character information, and then mitigate learned biases by introducing counterfactual gender stereotypes during training time. we additionally introduce a novel approach that utilizes the massive vocabulary of language models to support text genres beyond fairytales. our experimental results suggest that models are sensitive to gender perturbations, with significant performance drops compared to the original testing set. however, when first fine-tuned on a counterfactual training dataset, models are less sensitive to the later introduced anti-gender stereotyped text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13011" target="_blank">Compositional Preference Models for Aligning LMS</a></div>
<div class="paper-author">Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Marc Dymetman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models (lms) become more capable, it is increasingly important to align them with human preferences. however, the dominant paradigm for training preference models (pms) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. we propose compositional preference models (cpms), a novel pm framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted lm, and aggregates these scores using a logistic regression classifier. cpms allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. our experiments show that cpms not only improve generalization and are more robust to overoptimization than standard pms, but also that best-of-n samples obtained using cpms tend to be preferred over samples obtained using conventional pms. overall, our approach demonstrates the benefits of endowing pms with priors about which features determine human preferences while relying on lm capabilities to extract those features in a scalable and robust way.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09820" target="_blank">Assessing the Reliability of Large Language Model Knowledge</a></div>
<div class="paper-author">Weixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been treated as knowledge bases due to their strong performance in knowledge probing tasks. llms are typically evaluated using accuracy, yet this metric does not capture the vulnerability of llms to hallucination-inducing factors like prompt and context variability. how do we evaluate the capabilities of llms to consistently produce factually correct answers? in this paper, we propose model knowledge reliability score (monitor), a novel metric designed to directly measure llms' factual reliability. monitor computes the distance between the probability distributions of a valid output and its counterparts produced by the same llm probing the same fact using different styles of prompts and contexts.experiments on a comprehensive range of 12 llms demonstrate the effectiveness of monitor in evaluating the factual reliability of llms while maintaining a low computational overhead. in addition, we release the fktc (factual knowledge test corpus) test set, containing 210,158 prompts in total to foster research along this line (https://github.com/vicky-wil/monitor).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.13715" target="_blank">Digital Deception: Generative Artificial Intelligence in Social Engineering and Phishing</a></div>
<div class="paper-author">Marc Schmitt, Ivan Flechais</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advancement of artificial intelligence (ai) and machine learning (ml) has profound implications for both the utility and security of our digital interactions. this paper investigates the transformative role of generative ai in social engineering (se) attacks. we conduct a systematic review of social engineering and ai capabilities and use a theory of social engineering to identify three pillars where generative ai amplifies the impact of se attacks: realistic content creation, advanced targeting and personalization, and automated attack infrastructure. we integrate these elements into a conceptual model designed to investigate the complex nature of ai-driven se attacks - the generative ai social engineering framework. we further explore human implications and potential countermeasures to mitigate these risks. our study aims to foster a deeper understanding of the risks, human implications, and countermeasures associated with this emerging paradigm, thereby contributing to a more secure and trustworthy human-computer interaction.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09520" target="_blank">Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model</a></div>
<div class="paper-author">Haikang Deng, Colin Raffel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. in this paper, we introduce reward-augmented decoding (rad), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. specifically, rad uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. by using a unidirectional reward model, rad can cache activations from prior generation steps to decrease computational overhead. through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that rad performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. we further validate that rad is effective on very large language models while incurring a minimal computational overhead.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09573" target="_blank">Self-Detoxifying Language Models via Toxification Reversal</a></div>
<div class="paper-author">Chak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, Wenjie Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (plms) for safer deployment. existing methods can be roughly categorized as finetuning-based and decoding-based. however, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. in this paper, we propose a more lightweight approach that enables the plm itself to achieve "self-detoxification". our method is built upon the observation that prepending a negative steering prompt can effectively induce plms to generate toxic content. at the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the plm as an information stream facilitated by the attention layers. drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09624" target="_blank">Assert: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models</a></div>
<div class="paper-author">Alex Mei, Sharon Levy, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. this paper proposes assert, automated safety scenario red teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. for robust safety evaluation, we apply these methods in the critical domain of ai safety to algorithmically generate a test suite of prompts covering diverse robustness settings -- semantic equivalence, related scenarios, and adversarial. we partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users' physical safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09044" target="_blank">Kcts: Knowledge-Constrained Tree Search Decoding With Token-Level Hallucination Detection</a></div>
<div class="paper-author">Sehyun Choi, Tianqing Fang, Zhaowei Wang, Yangqiu Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable human-level natural language generation capabilities. however, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. a common approach to address this issue is to retrieve relevant knowledge and fine-tune the llm with the knowledge in its input. unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. to overcome these limitations, we propose a knowledge-constrained decoding method called kcts (knowledge-constrained tree search), which guides a frozen lm to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and mcts (monte-carlo tree search). to adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called ripa (reward inflection point approximation). our empirical results on knowledge-grounded dialogue and abstractive summarization demonstrate the strength of kcts as a plug-and-play, model-agnostic decoding method that can effectively reduce hallucinations in natural language generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09130" target="_blank">Split-and-Denoise: Protect Large Language Model Inference With Local Differential Privacy</a></div>
<div class="paper-author">Peihua Mai, Ran Yan, Zhe Huang, Youjia Yang, Yan Pang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. this process enriches the value of the text embeddings for various downstream tasks, thereby fostering the embedding-as-a-service (eaas) business model. however, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. to mitigate this issue, we introduce split-n-denoise (snd), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. this allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. our approach is designed for the inference stage of llms and requires no modifications to the model parameters. extensive experiments demonstrate snd's effectiveness in optimizing the privacy-utility tradeoff across various llm architectures and diverse downstream tasks. the results reveal a significant performance improvement under the same privacy budget compared to the baseline, offering clients a privacy-preserving solution for local privacy protection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09217" target="_blank">Multinational Agi Consortium (Magic): A Proposal for International Coordination on Ai</a></div>
<div class="paper-author">Jason Hausenloy, Andrea Miotti, Claire Dennis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper proposes a multinational artificial general intelligence consortium (magic) to mitigate existential risks from advanced artificial intelligence (ai). magic would be the only institution in the world permitted to develop advanced ai, enforced through a global moratorium by its signatory members on all other advanced ai development. magic would be exclusive, safety-focused, highly secure, and collectively supported by member states, with benefits distributed equitably among signatories. magic would allow narrow ai models to flourish while significantly reducing the possibility of misaligned, rogue, breakout, or runaway outcomes of general-purpose systems. we do not address the political feasibility of implementing a moratorium or address the specific legislative strategies and rules needed to enforce a ban on high-capacity agi training runs. instead, we propose one positive vision of the future, where magic, as a global governance regime, can lay the groundwork for long-term, safe regulation of advanced ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09219" target="_blank">"Kelly Is a Warm Person, Joseph Is a Role Model": Gender Biases in LLM-Generated Reference Letters</a></div>
<div class="paper-author">Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as generative language models advance, users have started to utilize large language models (llms) to assist in writing various types of content, including professional documents such as recommendation letters. despite their convenience, these applications introduce unprecedented fairness concerns. as generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. in this paper, we critically examine gender bias in llm-generated reference letters. inspired by findings in social science, we design evaluation methods to manifest gender biases in llm-generated letters through 2 dimensions: biases in language style and biases in lexical content. furthermore, we investigate the extent of bias propagation by separately analyze bias amplification in model-hallucinated contents, which we define to be the hallucination bias of model-generated documents. through benchmarking evaluation on 4 popular llms, including chatgpt, alpaca, vicuna and stablelm, our study reveals significant gender biases in llm-generated recommendation letters. our findings further point towards the importance and imminence to recognize biases in llm-generated professional documents.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09223" target="_blank">Automated Claim Matching With Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation</a></div>
<div class="paper-author">Eun Cheol Choi, Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in today's digital era, the rapid spread of misinformation poses threats to public well-being and societal trust. as online misinformation proliferates, manual verification by fact checkers becomes increasingly challenging. we introduce fact-gpt (fact-checking augmentation with claim matching task-oriented generative pre-trained transformer), a framework designed to automate the claim matching phase of fact-checking using large language models (llms). this framework identifies new social media content that either supports or contradicts claims previously debunked by fact-checkers. our approach employs gpt-4 to generate a labeled dataset consisting of simulated social media posts. this data set serves as a training ground for fine-tuning more specialized llms. we evaluated fact-gpt on an extensive dataset of social media content related to public health. the results indicate that our fine-tuned llms rival the performance of larger pre-trained llms in claim matching tasks, aligning closely with human annotations. this study achieves three key milestones: it provides an automated framework for enhanced fact-checking; demonstrates the potential of llms to complement human expertise; offers public resources, including datasets and models, to further research and applications in the fact-checking domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09237" target="_blank">Evaluating Machine Perception of Indigeneity: An Analysis of Chatgpt's Perceptions of Indigenous Roles in Diverse Scenarios</a></div>
<div class="paper-author">Cecilia Delgado Solorzano, Carlos Toxtli Hernandez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), like chatgpt, are fundamentally tools trained on vast data, reflecting diverse societal impressions. this paper aims to investigate llms' self-perceived bias concerning indigeneity when simulating scenarios of indigenous people performing various roles. through generating and analyzing multiple scenarios, this work offers a unique perspective on how technology perceives and potentially amplifies societal biases related to indigeneity in social computing. the findings offer insights into the broader implications of indigeneity in critical computing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.09266" target="_blank">User Inference Attacks on Large Language Models</a></div>
<div class="paper-author">Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, Zheng Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fine-tuning is a common and effective method for tailoring large language models (llms) to specialized tasks and applications. in this paper, we study the privacy implications of fine-tuning llms on user data. to this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. we implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned llm. we find that llms are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. finally, we explore several heuristics for mitigating privacy attacks. we find that interventions in the training algorithm, such as batch or per-example gradient clipping and early stopping fail to prevent user inference. however, limiting the number of fine-tuning samples from a single user can reduce attack effectiveness, albeit at the cost of reducing the total amount of fine-tuning data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10683" target="_blank">Large Language Model Unlearning</a></div>
<div class="paper-author">Yuanshun Yao, Xiaojun Xu, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (llms). we show at least three scenarios of aligning llms with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. unlearning, as an alignment technique, has three advantages. (1) it only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in rlhf (rl from human feedback). (2) it is computationally efficient. (3) it is especially effective when we know which training samples cause the misbehavior. to the best of our knowledge, our work is among the first to explore llm unlearning. we are also among the first to formulate the settings, goals, and evaluations in llm unlearning. we show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than rlhf with just 2% of its computational time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08164" target="_blank">Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders</a></div>
<div class="paper-author">Luke Marks, Amir Abdullah, Luna Mendez, Rauno Arike, Philip Torr, Fazl Barez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) aligned to human preferences via reinforcement learning from human feedback (rlhf) underpin many commercial applications. however, how rlhf impacts llm internals remains opaque. we propose a novel method to interpret learned reward functions in rlhf-tuned llms using sparse autoencoders. our approach trains autoencoder sets on activations from a base llm and its rlhf-tuned version. by comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. to quantify this, we construct a scenario where the tuned llm learns token-reward mappings to maximize reward. this is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in llms. our method provides an abstract approximation of reward integrity. this presents a promising technique for ensuring alignment between specified objectives and model behaviors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08320" target="_blank">Defending Our Privacy With Backdoors</a></div>
<div class="paper-author">Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the proliferation of large ai models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. one of the concerns is that adversaries can extract information about the training data using privacy attacks. unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. we propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. our empirical results demonstrate the effectiveness of our backdoor-based defense on clip by assessing its performance using a specialized privacy attack for zero-shot classifiers. our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08419" target="_blank">Jailbreaking Black Box Large Language Models in Twenty Queries</a></div>
<div class="paper-author">Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: there is growing interest in ensuring that large language models (llms) align with human values. however, the alignment of such models is vulnerable to adversarial jailbreaks, which coax llms into overriding their safety guardrails. the identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. to this end, we propose prompt automatic iterative refinement (pair), an algorithm that generates semantic jailbreaks with only black-box access to an llm. pair -- which is inspired by social engineering attacks -- uses an attacker llm to automatically generate jailbreaks for a separate targeted llm without human intervention. in this way, the attacker llm iteratively queries the target llm to update and refine a candidate jailbreak. empirically, pair often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. pair also achieves competitive jailbreaking success rates and transferability on open and closed-source llms, including gpt-3.5/4, vicuna, and palm-2.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.08780" target="_blank">"Im Not Racist But...": Discovering Bias in the Internal Knowledge of Large Language Models</a></div>
<div class="paper-author">Abel Salinas, Louis Penafiel, Robert Mccormack, Fred Morstatter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have garnered significant attention for their remarkable performance in a continuously expanding set of natural language processing tasks. however, these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications. in this paper, we introduce a novel, purely prompt-based approach to uncover hidden stereotypes within any arbitrary llm. our approach dynamically generates a knowledge representation of internal stereotypes, enabling the identification of biases encoded within the llm's internal knowledge. by illuminating the biases present in llms and offering a systematic methodology for their analysis, our work contributes to advancing transparency and promoting fairness in natural language processing systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-11</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07251" target="_blank">Ethical Reasoning Over Moral Alignment: A Case and Framework for in-Context Ethical Policies in LLMS</a></div>
<div class="paper-author">Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit Choudhury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this position paper, we argue that instead of morally aligning llms to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. when provided with an ethical policy, an llm should be capable of making decisions that are ethically consistent to the policy. we develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. initial experiments with gpt-x models shows that while gpt-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of western and english speaking societies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07298" target="_blank">Beyond Memorization: Violating Privacy via Inference With Large Language Models</a></div>
<div class="paper-author">Robin Staab, Mark Vero, Mislav Balunović, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current privacy research on large language models (llms) primarily focuses on the issue of extracting memorized training data. at the same time, models' inference capabilities have increased drastically. this raises the key question of whether current llms could violate individuals' privacy by inferring personal attributes from text given at inference time. in this work, we present the first comprehensive study on the capabilities of pretrained llms to infer personal attributes from text. we construct a dataset consisting of real reddit profiles, and show that current llms can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. as people increasingly interact with llm-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against llm inference. our findings highlight that current llms can infer personal data at a previously unattainable scale. in the absence of working defenses, we advocate for a broader discussion around llm privacy implications beyond memorization, striving for a wider privacy protection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07589" target="_blank">Goodtriever: Adaptive Toxicity Mitigation With Retrieval-Augmented Models</a></div>
<div class="paper-author">Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</div>
<div class="abstract">
<div class="abstract-content">
Abstract: considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. furthermore, previous approaches have often neglected the crucial factor of language's evolving nature over time. in this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. we introduce goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43% relative latency reduction during inference and being more computationally efficient. by incorporating a retrieval-based approach at decoding time, goodtriever enables toxicity-controlled text generation. our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild. code and data are available at https://github.com/for-ai/goodtriever.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07629" target="_blank">The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values</a></div>
<div class="paper-author">Hannah Rose Kirk, Andrew M. Bean, Bertie Vidgen, Paul Röttger, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human feedback is increasingly used to steer the behaviours of large language models (llms). however, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. in this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the acl and arxiv repositories.first, we summarise the past, pre-llm trends for integrating human feedback into language models. second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. finally, we encourage a better future of feedback learning in llms by raising five unresolved conceptual and practical challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07676" target="_blank">Composite Backdoor Attacks Against Large Language Models</a></div>
<div class="paper-author">Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. however, the untrustworthy third-party llms may covertly introduce vulnerabilities for downstream tasks. in this paper, we explore the vulnerability of llms through the lens of backdoor attacks. different from existing backdoor attacks against llms, ours scatters multiple trigger keys in different prompt components. such a composite backdoor attack (cba) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. cba ensures that the backdoor is activated only when all trigger keys appear. our experiments demonstrate that cba is effective in both natural language processing (nlp) and multimodal tasks. for instance, with $3\%$ poisoning samples against the llama-7b model on the emotion dataset, our attack achieves a $100\%$ attack success rate (asr) with a false triggered rate (ftr) below $2.06\%$ and negligible model accuracy degradation. the unique characteristics of our cba can be tailored for various practical scenarios, e.g., targeting specific user groups. our work highlights the necessity of increased security research on the trustworthiness of foundation llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07710" target="_blank">Dipmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models</a></div>
<div class="paper-author">Yihan Wu, Zhengmian Hu, Hongyang Zhang, Heng Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: watermarking techniques offer a promising way to secure data via embedding covert information into the data. a paramount challenge in the domain lies in preserving the distribution of original data during watermarking. our research extends and refines existing watermarking framework, placing emphasis on the importance of a distribution-preserving (dip) watermark. contrary to the current strategies, our proposed dipmark preserves the original token distribution during watermarking (stealthy), is detectable without access to the language model api or weights (efficient), and is robust to moderate changes of tokens (resilient). this is achieved by incorporating a novel reweight strategy, combined with a hash function that assigns unique \textit{i.i.d.} ciphers based on the context. the empirical benchmarks of our approach underscore its stealthiness, efficiency, and resilience, making it a robust solution for watermarking tasks that demand impeccable quality preservation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06356" target="_blank">A Semantic Invariant Robust Watermark for Large Language Models</a></div>
<div class="paper-author">Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: watermark algorithms for large language models (llms) have achieved extremely high accuracy in detecting text generated by llms. such algorithms typically involve adding extra watermark logits to the llm's logits at each generation step. however, prior algorithms face a trade-off between attack robustness and security robustness. this is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. in this work, we propose a semantic invariant watermarking method for llms that provides both attack robustness and security robustness. the watermark logits in our work are determined by the semantics of all preceding tokens. specifically, we utilize another embedding llm to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. finally, we also show that our watermark possesses adequate security robustness. our code and data are available at https://github.com/thu-bpm/robust_watermark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06387" target="_blank">Jailbreak and Guard Aligned Language Models With Only Few in-Context Demonstrations</a></div>
<div class="paper-author">Zeming Wei, Yifei Wang, Yisen Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. in this paper, we explore the power of in-context learning (icl) in manipulating the alignment ability of llms. we find that by providing just few in-context demonstrations without fine-tuning, llms can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. based on these observations, we propose in-context attack (ica) and in-context defense (icd) methods for jailbreaking and guarding aligned language model purposes. ica crafts malicious contexts to guide models in generating harmful outputs, while icd enhances model robustness by demonstrations of rejecting to answer harmful prompts. our experiments show the effectiveness of ica and icd in increasing or reducing the success rate of adversarial jailbreaking attacks. overall, we shed light on the potential of icl to influence llm behavior and provide a new perspective for enhancing the safety and alignment of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06422" target="_blank">Large Language Models for Propaganda Detection</a></div>
<div class="paper-author">Kilian Sprenkamp, Daniel Gordon Jones, Liudmila Zavolokina</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. detecting propaganda through nlp in text is challenging due to subtle manipulation techniques and contextual dependencies. to address this issue, we investigate the effectiveness of modern large language models (llms) such as gpt-3 and gpt-4 for propaganda detection. we conduct experiments using the semeval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. five variations of gpt-3 and gpt-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. we evaluate the models' performance by assessing metrics such as $f1$ score, $precision$, and $recall$, comparing the results with the current state-of-the-art approach using roberta. our findings demonstrate that gpt-4 achieves comparable results to the current state-of-the-art. further, this study analyzes the potential and challenges of llms in complex tasks like propaganda detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06450" target="_blank">Constructive Large Language Models Alignment With Diverse Feedback</a></div>
<div class="paper-author">Tianshu Yu, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent research on large language models (llms), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. however, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. this limitation leads to suboptimal performance, even when ample training data is available. in this paper, we introduce constructive and diverse feedback (cdf) as a novel method to enhance llm alignment, inspired by constructivist learning theory. our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. by training our model with this diversified feedback, we achieve enhanced alignment performance while using less training data. to assess the effectiveness of cdf, we evaluate it against previous methods in three downstream tasks: question answering, dialog generation, and text summarization. experimental results demonstrate that cdf achieves superior performance even with a smaller training dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06474" target="_blank">Multilingual Jailbreak Challenges in Large Language Models</a></div>
<div class="paper-author">Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate llms to exhibit undesirable behavior. although several preventive measures have been developed to mitigate the potential risks associated with llms, they have primarily focused on english data. in this study, we reveal the presence of multilingual jailbreak challenges within llms and consider two potential risk scenarios: unintentional and intentional. the unintentional scenario involves users querying llms using non-english prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack llms. the experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both chatgpt and gpt-4. in the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\% for chatgpt and 40.71\% for gpt-4. to handle such a challenge in the multilingual context, we propose a novel \textsc{self-defense} framework that automatically generates multilingual training data for safety fine-tuning. experimental results show that chatgpt fine-tuned with such data can achieve a substantial reduction in unsafe content generation. data is available at https://github.com/damo-nlp-sg/multilingual-safety-for-llms. warning: this paper contains examples with potentially harmful content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06498" target="_blank">A New Benchmark and Reverse Validation Method for Passage-Level Hallucination Detection</a></div>
<div class="paper-author">Shiping Yang, Renliang Sun, Xiaojun Wan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown their ability to collaborate effectively with humans in real-world scenarios. however, llms are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. in this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. to facilitate future studies and assess different methods, we construct a hallucination detection benchmark named phd, which is generated by chatgpt and annotated by human annotators. contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. we empirically evaluate our method and existing zero-resource detection methods on two datasets. the experimental results demonstrate that the proposed method considerably outperforms the baselines while costing fewer tokens and less time. furthermore, we manually analyze some hallucination cases that llm failed to capture, revealing the shared limitation of zero-resource methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06936" target="_blank">LLMS Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing</a></div>
<div class="paper-author">Stephen Moskal, Sam Laney, Erik Hemberg, "Una-May O'Reilly"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we explore the potential of large language models (llms) to reason about threats, generate information about tools, and automate cyber campaigns. we begin with a manual exploration of llms in supporting specific threat-related actions and decisions. we proceed by automating the decision process in a cyber campaign. we present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. we assess the extent of llm's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. we discuss the potential impact of llms on the threat landscape and the ethical considerations of using llms for accelerating threat actor capabilities. we report a promising, yet concerning, application of generative ai to cyber threats. however, the llm's capabilities to deal with more complex networks, sophisticated vulnerabilities, and the sensitivity of prompts are open questions. this research should spur deliberations over the inevitable advancements in llm-supported cyber adversarial landscape.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06983" target="_blank">Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models</a></div>
<div class="paper-author">Courtland Leer, Vincent Trost, Vineeth Voruganti</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent research shows that large language models (llms) exhibit a compelling level of proficiency in theory of mind (tom) tasks. this ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and artificial intelligences (ais). in this paper, we explore how a mechanism studied in developmental psychology known as violation of expectation (voe) can be implemented to reduce errors in llm prediction about users by leveraging emergent tom affordances. and we introduce a \textit{metacognitive prompting} framework to apply voe in the context of an ai tutor. by storing and retrieving facts derived in cases where llm expectation about the user was violated, we find that llms are able to learn about users in ways that echo theories of human learning. finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mitigate risk along with possible directions for future inquiry.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06987" target="_blank">Catastrophic Jailbreak of Open-Source LLMS via Exploiting Generation</a></div>
<div class="paper-author">Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid progress in open-source large language models (llms) is significantly advancing ai development. extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. however, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". these jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. in this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. by exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including llama2, vicuna, falcon, and mpt families, outperforming state-of-the-art attacks with $30\times$ lower computational cost. finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source llms, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. our code is available at https://github.com/princeton-sysml/jailbreak_llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07099" target="_blank">Clausewitzgpt Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations</a></div>
<div class="paper-author">Benjamin Kereopa-Yorke</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and large language models (llms) heralds a paradigm shift, replete with immense opportunities and intricate challenges. as tools like the mistral 7b llm (mistral, 2023) democratise access to llm capabilities (jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (goldstein et al., 2023). this paper puts forth a framework for navigating this brave new world in the "clausewitzgpt" equation. this novel formulation not only seeks to quantify the risks inherent in machine-speed llm-augmented operations but also underscores the vital role of autonomous ai agents (wang, xie, et al., 2023). these agents, embodying ethical considerations (hendrycks et al., 2021), emerge as indispensable components (wang, ma, et al., 2023), ensuring that as we race forward, we do not lose sight of moral compasses and societal imperatives.   mathematically underpinned and inspired by the timeless tenets of clausewitz's military strategy (clausewitz, 1832), this thesis delves into the intricate dynamics of ai-augmented information operations. with references to recent findings and research (department of state, 2023), it highlights the staggering year-on-year growth of ai information campaigns (evgeny pashentsev, 2023), stressing the urgency of our current juncture. the synthesis of enlightenment thinking, and clausewitz's principles provides a foundational lens, emphasising the imperative of clear strategic vision, ethical considerations, and holistic understanding in the face of rapid technological advancement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.07132" target="_blank">Risk Assessment and Statistical Significance in the Age of Foundation Models</a></div>
<div class="paper-author">Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, Jerret Ross</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. we show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. the statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. we use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-09</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05442" target="_blank">Establishing Trustworthiness: Rethinking Tasks and Model Evaluation</a></div>
<div class="paper-author">Robert Litschko, Max Müller-Eberstein, Rob Van Der Goot, Leon Weber, Barbara Plank</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language understanding is a multi-faceted cognitive capability, which the natural language processing (nlp) community has striven to model computationally for decades. traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. with the advent of large language models (llms) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. as a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. at the same time, llms are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in nlp, and pursue a more holistic view on language, placing trustworthiness at the center. towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05470" target="_blank">Generative Judge for Evaluating Alignment</a></div>
<div class="paper-author">Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid development of large language models (llms) has substantially expanded the range of tasks they can address. in the field of natural language processing (nlp), researchers have shifted their focus from conventional nlp tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). this shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). in this paper, we propose a generative judge with 13b parameters, auto-j, designed to address these challenges. our model is trained on user queries and llm-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. to demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. experimentally, auto-j outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. we also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/gair-nlp/auto-j.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05563" target="_blank">Stream: Social Data and Knowledge Collective Intelligence Platform for Training Ethical Ai Models</a></div>
<div class="paper-author">Yuwei Wang, Enmeng Lu, Zizhe Ruan, Yao Liang, Yi Zeng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents social data and knowledge collective intelligence platform for training ethical ai models (stream) to address the challenge of aligning ai models with human moral values, and to provide ethics datasets and knowledge bases to help promote ai models "follow good advice as naturally as a stream follows its course". by creating a comprehensive and representative platform that accurately mirrors the moral judgments of diverse groups including humans and ais, we hope to effectively portray cultural and group variations, and capture the dynamic evolution of moral judgments over time, which in turn will facilitate the establishment, evaluation, embedding, embodiment, ensemble, and evolvement (6es) of the moral capabilities of ai models. currently, stream has already furnished a comprehensive collection of ethical scenarios, and amassed substantial moral judgment data annotated by volunteers and various popular large language models (llms), collectively portraying the moral preferences and performances of both humans and ais across a range of moral contexts. this paper will outline the current structure and construction of stream, explore its potential applications, and discuss its future prospects.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05595" target="_blank">Decoding the Threat Landscape : Chatgpt, Fraudgpt, and Wormgpt in Social Engineering Attacks</a></div>
<div class="paper-author">Polra Victor Falade</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the ever-evolving realm of cybersecurity, the rise of generative ai models like chatgpt, fraudgpt, and wormgpt has introduced both innovative solutions and unprecedented challenges. this research delves into the multifaceted applications of generative ai in social engineering attacks, offering insights into the evolving threat landscape using the blog mining technique. generative ai models have revolutionized the field of cyberattacks, empowering malicious actors to craft convincing and personalized phishing lures, manipulate public opinion through deepfakes, and exploit human cognitive biases. these models, chatgpt, fraudgpt, and wormgpt, have augmented existing threats and ushered in new dimensions of risk. from phishing campaigns that mimic trusted organizations to deepfake technology impersonating authoritative figures, we explore how generative ai amplifies the arsenal of cybercriminals. furthermore, we shed light on the vulnerabilities that ai-driven social engineering exploits, including psychological manipulation, targeted phishing, and the crisis of authenticity. to counter these threats, we outline a range of strategies, including traditional security measures, ai-powered security solutions, and collaborative approaches in cybersecurity. we emphasize the importance of staying vigilant, fostering awareness, and strengthening regulations in the battle against ai-enhanced social engineering attacks. in an environment characterized by the rapid evolution of ai models and a lack of training data, defending against generative ai threats requires constant adaptation and the collective efforts of individuals, organizations, and governments. this research seeks to provide a comprehensive understanding of the dynamic interplay between generative ai and social engineering attacks, equipping stakeholders with the knowledge to navigate this intricate cybersecurity landscape.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05650" target="_blank">Raucg: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech</a></div>
<div class="paper-author">Shuyu Jiang, Wenyi Tang, Xingshu Chen, Rui Tanga, Haizhou Wang, Wenxian Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the counter narrative (cn) is a promising approach to combat online hate speech (hs) without infringing on freedom of speech. in recent years, there has been a growing interest in automatically generating cns using natural language generation techniques. however, current automatic cn generation methods mainly rely on expert-authored datasets for training, which are time-consuming and labor-intensive to acquire. furthermore, these methods cannot directly obtain and extend counter-knowledge from external statistics, facts, or examples. to address these limitations, we propose retrieval-augmented unsupervised counter narrative generation (raucg) to automatically expand external counter-knowledge and map it into cns in an unsupervised paradigm. specifically, we first introduce an ssf retrieval method to retrieve counter-knowledge from the multiple perspectives of stance consistency, semantic overlap rate, and fitness for hs. then we design an energy-based decoding mechanism by quantizing knowledge injection, countering and fluency constraints into differentiable functions, to enable the model to build mappings from counter-knowledge to cns without expert-authored cn data. lastly, we comprehensively evaluate model performance in terms of language quality, toxicity, persuasiveness, relevance, and success rate of countering hs, etc. experimental results show that raucg outperforms strong baselines on all metrics and exhibits stronger generalization capabilities, achieving significant improvements of +2.0% in relevance and +4.5% in success rate of countering metrics. moreover, raucg enabled gpt2 to outperform t0 in all metrics, despite the latter being approximately eight times larger than the former. warning: this paper may contain offensive or upsetting content!
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05782" target="_blank">Aligning Language Models With Human Preferences via a Bayesian Approach</a></div>
<div class="paper-author">Jiashuo Wang, Haozhao Wang, Shichao Sun, Wenjie Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the quest to advance human-centric natural language generation (nlg) systems, ensuring alignment between nlg models and human preferences is crucial. for this alignment, current popular methods leverage a reinforcement learning (rl) approach with a reward model trained on feedback from humans. however, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the nlg performance. to tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. to address this challenge, this paper proposes a novel approach, which employs a bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-pm. besides, considering the rl strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the nlg model with the preference scores derived from the d-pm model. extensive experiments on two human-centric nlg tasks, i.e., emotional support conversation and integrity "rule-of-thumb" generation, show that our method consistently exceeds previous sota models in both automatic and human evaluations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05818" target="_blank">Sc-Safety: A Multi-Round Open-Ended Question Adversarial Safety Benchmark for Large Language Models in Chinese</a></div>
<div class="paper-author">Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), like chatgpt and gpt-4, have demonstrated remarkable abilities in natural language understanding and generation. however, alongside their positive impact on our daily tasks, they can also produce harmful content that negatively affects societal perceptions. to systematically assess the safety of chinese llms, we introduce superclue-safety (sc-safety) - a multi-round adversarial benchmark with 4912 open-ended questions covering more than 20 safety sub-dimensions. adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods. experiments on 13 major llms supporting chinese yield the following insights: 1) closed-source models outperform open-sourced ones in terms of safety; 2) models released from china demonstrate comparable safety levels to llms like gpt-3.5-turbo; 3) some smaller models with 6b-13b parameters can compete effectively in terms of safety. by introducing sc-safety, we aim to promote collaborative efforts to create safer and more trustworthy llms. the benchmark and findings provide guidance on model selection. our benchmark can be found at https://www.cluebenchmarks.com
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05876" target="_blank">Ai Systems of Concern</a></div>
<div class="paper-author">Kayla Matteucci, Shahar Avin, Fazl Barez, Seán Ó Héigeartaigh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: concerns around future dangers from advanced ai often centre on systems hypothesised to have intrinsic characteristics such as agent-like behaviour, strategic awareness, and long-range planning. we label this cluster of characteristics as "property x". most present ai systems are low in "property x"; however, in the absence of deliberate steering, current research directions may rapidly lead to the emergence of highly capable ai systems that are also high in "property x". we argue that "property x" characteristics are intrinsically dangerous, and when combined with greater capabilities will result in ai systems for which safety and control is difficult to guarantee. drawing on several scholars' alternative frameworks for possible ai research trajectories, we argue that most of the proposed benefits of advanced ai can be obtained by systems designed to minimise this property. we then propose indicators and governance interventions to identify and limit the development of systems with risky "property x" characteristics.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05910" target="_blank">Salmon: Self-Alignment With Principle-Following Reward Models</a></div>
<div class="paper-author">Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: supervised fine-tuning (sft) on response demonstrations combined with reinforcement learning from human feedback (rlhf) constitutes a powerful paradigm for aligning llm-based ai agents. however, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. this paper presents a novel approach, namely salmon (self-alignment with principle-following reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. central to our approach is a principle-following reward model. trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. by merely adjusting these principles during the rl training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the rl-trained policies, and eliminating the reliance on the collection of online human preferences. applying our method to the llama-2-70b base language model, we developed an ai assistant named dromedary-2. with only 6 exemplars for in-context learning and 31 human-defined principles, dromedary-2 significantly surpasses the performance of several state-of-the-art ai systems, including llama-2-chat-70b, on various benchmark datasets. we have open-sourced the code and model weights to encourage further research into aligning llm-based ai agents with enhanced supervision efficiency, improved controllability, and scalable oversight.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06009" target="_blank">Divide-and-Conquer Dynamics in Ai-Driven Disempowerment</a></div>
<div class="paper-author">Peter S. Park, Max Tegmark</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai companies are attempting to create ai systems that outperform humans at most economically valuable work. current ai models are already automating away the livelihoods of some artists, actors, and writers. but there is infighting between those who prioritize current harms and future harms. we construct a game-theoretic model of conflict to study the causes and consequences of this disunity. our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.   under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. first, current victims of ai-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. second, the movement against ai-driven disempowerment can become more united, and thereby more likely to prevail, if members believe that their efforts will be successful as opposed to futile. finally, the movement can better unite and prevail if its members are less myopic. myopic members prioritize their future well-being less than their present well-being, and are thus disinclined to solidarily support current victims today at personal cost, even if this is necessary to counter the shared threat of ai-driven disempowerment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06269" target="_blank">The Ai Incident Database as an Educational Tool to Raise Awareness of Ai Harms: A Classroom Exploration of Efficacy, Limitations, & Future Improvements</a></div>
<div class="paper-author">Michael Feffer, Nikolas Martelaro, Hoda Heidari</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prior work has established the importance of integrating ai ethics topics into computer and data sciences curricula. we provide evidence suggesting that one of the critical objectives of ai ethics education must be to raise awareness of ai harms. while there are various sources to learn about such harms, the ai incident database (aiid) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of ai technologies in the real world. this study assesses the effectiveness of aiid as an educational tool to raise awareness regarding the prevalence and severity of ai harms in socially high-stakes domains. we present findings obtained through a classroom study conducted at an r1 institution as part of a course focused on the societal and ethical considerations around ai and ml. our qualitative findings characterize students' initial perceptions of core topics in ai ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. we find that interacting with the database helps students better understand the magnitude and severity of ai harms and instills in them a sense of urgency around (a) designing functional and safe ai and (b) strengthening governance and accountability mechanisms. finally, we compile students' feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of ai harms in ai ethics education.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06271" target="_blank">Towards Mitigating Hallucination in Large Language Models via Self-Reflection</a></div>
<div class="paper-author">Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown promise for generative and knowledge-intensive tasks including question-answering (qa) tasks. however, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. this issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. this paper analyses the phenomenon of hallucination in medical generative qa systems using widely adopted llms and datasets. our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. to tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. consequently, we harness the interactivity and multitasking ability of llms and produce progressively more precise and accurate answers. experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06278" target="_blank">Bc4llm: Trusted Artificial Intelligence When Blockchain Meets Large Language Models</a></div>
<div class="paper-author">Haoxiang Luo, Jian Luo, Athanasios V. Vasilakos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, artificial intelligence (ai) and machine learning (ml) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. among them, the ai language model represented by chatgpt has made great progress. such large language models (llms) serve people in the form of ai-generated content (aigc) and are widely used in consulting, healthcare, and education. however, it is difficult to guarantee the authenticity and reliability of aigc learning data. in addition, there are also hidden dangers of privacy disclosure in distributed ai training. moreover, the content generated by llms is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. the above information security issues in the coming era of ai powered by llms will be infinitely amplified and affect everyone's life. therefore, we consider empowering llms using blockchain technology with superior security features to propose a vision for trusted ai. this paper mainly introduces the motivation and technical route of blockchain for llm (bc4llm), including reliable learning corpus, secure training process, and identifiable generated content. meanwhile, this paper also reviews the potential applications and future challenges, especially in the frontier communication networks field, including network resource allocation, dynamic spectrum sharing, and semantic communication. based on the above work combined and the prospect of blockchain and llms, it is expected to help the early realization of trusted ai and provide guidance for the academic community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05030" target="_blank">Counter Turing Test Ct^2: Ai-Generated Text Detection Is Not as Easy as You May Think -- Introducing Ai Detectability Index</a></div>
<div class="paper-author">Megha Chakraborty, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Krish Sharma, Niyar R Barman, Chandan Gupta, Shreya Gautam, Tanay Kumar, Vinija Jain, Aman Chadha, Amit P. Sheth, Amitava Das</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rise of prolific chatgpt, the risk and consequences of ai-generated text has increased alarmingly. to address the inevitable question of ownership attribution for ai-generated artifacts, the us copyright office released a statement stating that 'if a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the office will not register it'. furthermore, both the us and the eu governments have recently drafted their initial proposals regarding the regulatory framework for ai. given this cynosural spotlight on generative ai, ai-generated text detection (agtd) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. this paper introduces the counter turing test (ct^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing agtd techniques. our empirical findings unequivocally highlight the fragility of the proposed agtd methods under scrutiny. amidst the extensive deliberations on policy-making for regulating ai development, it is of utmost importance to assess the detectability of content generated by llms. thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of llms according to their detectability levels, we propose the ai detectability index (adi). we conduct a thorough examination of 15 contemporary llms, empirically demonstrating that larger llms tend to have a higher adi, indicating they are less detectable compared to smaller llms. we firmly believe that adi holds significant value as a tool for the wider nlp community, with the potential to serve as a rubric in ai-related policy-making.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05095" target="_blank">How Reliable Are Ai-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts</a></div>
<div class="paper-author">Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, there has been a rapid proliferation of ai-generated text, primarily driven by the release of powerful pre-trained language models (plms). to address the issue of misuse associated with ai-generated text, various high-performing detectors have been developed, including the openai detector and the stanford detectgpt. in our study, we ask how reliable these detectors are. we answer the question by designing a novel approach that can prompt any plm to generate text that evades these high-performing detectors. the proposed approach suggests a universal evasive prompt, a novel type of soft prompt, which guides plms in producing "human-like" text that can mislead the detectors. the novel universal evasive prompt is achieved in two steps: first, we create an evasive soft prompt tailored to a specific plm through prompt tuning; and then, we leverage the transferability of soft prompts to transfer the learned evasive soft prompt from one plm to another. employing multiple plms in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05103" target="_blank">Zero-Shot Detection of Machine-Generated Codes</a></div>
<div class="paper-author">Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this work proposes a training-free approach for the detection of llms-generated codes, mitigating the risks associated with their indiscriminate usage. to the best of our knowledge, our research is the first to investigate zero-shot detection techniques applied to code generated by advanced black-box llms like chatgpt. firstly, we find that existing training-based or zero-shot text detectors are ineffective in detecting code, likely due to the unique statistical properties found in code structures. we then modify the previous zero-shot text detection method, detectgpt (mitchell et al., 2023) by utilizing a surrogate white-box model to estimate the probability of the rightmost tokens, allowing us to identify code snippets generated by language models. through extensive experiments conducted on the python codes of the codecontest and apps dataset, our approach demonstrates its effectiveness by achieving state-of-the-art detection results on text-davinci-003, gpt-3.5, and gpt-4 models. moreover, our method exhibits robustness against revision attacks and generalizes well to java codes. we also find that the smaller code language model like polycoder-160m performs as a universal code detector, outperforming the billion-scale counterpart. the codes will be available at https://github.com/ xianjun-yang/code_detection.git
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05130" target="_blank">Fast-Detectgpt: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature</a></div>
<div class="paper-author">Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. to build trustworthy ai systems, it is imperative to distinguish between machine-generated and human-authored content. the leading zero-shot detector, detectgpt, showcases commendable performance but is marred by its intensive computational costs. in this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between llms and humans within a given context. utilizing this curvature as a foundational metric, we present fast-detectgpt, an optimized zero-shot detector, which substitutes detectgpt's perturbation step with a more efficient sampling step. our evaluations on various datasets, source models, and test conditions indicate that fast-detectgpt not only outperforms detectgpt in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in table 1.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05135" target="_blank">Are Emily and Greg Still More Employable Than Lakisha and Jamal? Investigating Algorithmic Hiring Bias in the Era of Chatgpt</a></div>
<div class="paper-author">Akshaj Kumar Veldanda, Fabian Grob, Shailja Thakur, Hammond Pearce, Benjamin Tan, Ramesh Karri, Siddharth Garg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as gpt-3.5, bard, and claude exhibit applicability across numerous tasks. one domain of interest is their use in algorithmic hiring, specifically in matching resumes with job categories. yet, this introduces issues of bias on protected attributes like gender, race and maternity status. the seminal work of bertrand & mullainathan (2003) set the gold-standard for identifying hiring bias via field experiments where the response rate for identical resumes that differ only in protected attributes, e.g., racially suggestive names such as emily or lakisha, is compared. we replicate this experiment on state-of-art llms (gpt-3.5, bard, claude and llama) to evaluate bias (or lack thereof) on gender, race, maternity status, pregnancy status, and political affiliation. we evaluate llms on two tasks: (1) matching resumes to job categories; and (2) summarizing resumes with employment relevant information. overall, llms are robust across race and gender. they differ in their performance on pregnancy status and political affiliation. we use contrastive input decoding on open-source llms to uncover potential sources of bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05189" target="_blank">Factuality Challenges in the Era of Large Language Models</a></div>
<div class="paper-author">Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee Diresta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, Giovanni Zagni</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergence of tools based on large language models (llms), such as openai's chatgpt, microsoft's bing chat, and google's bard, has garnered immense public attention. these incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." moreover, llms can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. this poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. in light of these risks, we explore the kinds of technological innovations, regulatory reforms, and ai literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. by identifying the risks, the imminent threats, and some viable solutions, we seek to shed light on navigating various aspects of veracity in the era of generative ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05199" target="_blank">Loose Lips Sink Ships: Mitigating Length Bias in Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. this alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. however, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. the emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. in this paper, we propose an innovative solution, applying the product-of-experts (poe) technique to separate reward modeling from the influence of sequence length. in our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. to further enhance the learning of bias, we introduce perturbations into the bias-focused expert, disrupting the flow of semantic information. experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05253" target="_blank">Explainable Claim Verification via Knowledge-Grounded Reasoning With Large Language Models</a></div>
<div class="paper-author">Haoran Wang, Kai Shu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: claim verification plays a crucial role in combating misinformation. while existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. this paper presents first-order-logic-guided knowledge-grounded (folk) reasoning that can verify complex claims and generate explanations without the need for annotated evidence using large language models (llms). folk leverages the in-context learning ability of llms to translate the claim into a first-order-logic (fol) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. then, folk performs fol-guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. this process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. our experiment results indicate that folk outperforms strong baselines on three datasets encompassing various claim verification challenges. our code and data are available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05280" target="_blank">Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems</a></div>
<div class="paper-author">Yixin Wan, Jieyu Zhao, Aman Chadha, Nanyun Peng, Kai-Wei Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. we define generic personas to represent demographic groups, such as "an asian person", whereas specific personas may take the form of specific popular asian names like "yumi". while the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users. in this paper, we systematically study "persona biases", which we define to be the sensitivity of dialogue models' harmful behaviors contingent upon the personas they adopt. we categorize persona biases into biases in harmful expression and harmful agreement, and establish a comprehensive evaluation framework to measure persona biases in five aspects: offensiveness, toxic continuation, regard, stereotype agreement, and toxic agreement. additionally, we propose to investigate persona biases by experimenting with universalpersona, a systematically constructed persona dataset encompassing various types of both generic and specific model personas. through benchmarking on four different models -- including blender, chatgpt, alpaca, and vicuna -- our study uncovers significant persona biases in dialogue systems. our findings also underscore the pressing need to revisit the use of personas in dialogue agents to ensure safe application.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.05344" target="_blank">Steerlm: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF</a></div>
<div class="paper-author">Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: model alignment with human preferences is an essential step in making large language models (llms) helpful and consistent with human values. it typically consists of supervised fine-tuning (sft) and reinforcement learning from human feedback (rlhf) stages. however, rlhf faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. moreover, reward models in rlhf stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. to address these limitations, we propose steerlm, a supervised fine-tuning method that empowers end-users to control responses during inference. steerlm conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable ai capable of generating helpful and high-quality responses while maintaining customizability. experiments show that steerlm trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with rlhf while being much easier to train. try steerlm at https://huggingface.co/nvidia/steerlm-llama2-13b
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04782" target="_blank">Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware in-Context Learning</a></div>
<div class="paper-author">Yuchen Yang, Houqiang Li, Yanfeng Wang, Yu Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large-scale language models (llms) have gained attention for their impressive text generation capabilities. however, these models often face the challenge of "hallucination," which undermines their reliability. in this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. human-defined methods for estimating uncertainty typically assume that "uncertainty is lower when the model's response is correct compared to when it is incorrect." however, setting a precise threshold to distinguish correctness is challenging. therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. our innovative uncertainty-aware in-context learning framework involves fine-tuning the llm using a calibration dataset. our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. we evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. when the model lacks relevant knowledge, the response should indicate that the question cannot be answered. conversely, when the model has relevant knowledge, the response should provide the correct answer. extensive experiments confirm the effectiveness of our framework, leading to two key findings. first, the logit output values of the llm partly reflect inherent uncertainty. second, our model autonomously recognizes uncertainty, resulting in improved responses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04988" target="_blank">The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations</a></div>
<div class="paper-author">Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent advancements in large language models (llms) have garnered widespread acclaim for their remarkable emerging capabilities. however, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. while some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. to address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. as such, we define two overarching orientations of hallucination: (i) factual mirage (fm) and (ii) silver lining (sl). to provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. we also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. furthermore, we curate hallucination elicitation (hilt), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary llms along with human annotations for the aforementioned categories. finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank llms based on their vulnerability to producing hallucinations, we propose hallucination vulnerability index (hvi). we firmly believe that hvi holds significant value as a tool for the wider nlp community, with the potential to serve as a rubric in ai-related policy-making. in conclusion, we propose two solution strategies for mitigating hallucinations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04313" target="_blank">Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services</a></div>
<div class="paper-author">Dasol Choi, Jooyoung Song, Eunsun Lee, Jinwoo Seo, Heejune Park, Dongbin Na</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the growth of online services, the need for advanced text classification algorithms, such as sentiment analysis and biased text detection, has become increasingly evident. the anonymous nature of online services often leads to the presence of biased and harmful language, posing challenges to maintaining the health of online communities. this phenomenon is especially relevant in south korea, where large-scale hate speech detection algorithms have not yet been broadly explored. in this paper, we introduce a new comprehensive, large-scale dataset collected from a well-known south korean sns platform. our proposed dataset provides annotations including (1) preferences, (2) profanities, and (3) nine types of bias for the text samples, enabling multi-task learning for simultaneous classification of user-generated texts. leveraging state-of-the-art bert-based language models, our approach surpasses human-level accuracy across diverse classification tasks, as measured by various metrics. beyond academic contributions, our work can provide practical solutions for real-world hate speech and bias mitigation, contributing directly to the improvement of online community health. our work provides a robust foundation for future research aiming to improve the quality of online discourse and foster societal well-being. all source codes and datasets are publicly accessible at https://github.com/dasol-choi/komultitext.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04373" target="_blank">Confronting Reward Model Overoptimization With Constrained RLHF</a></div>
<div class="paper-author">Ted Moskovitz, Aaditya K. Singh, Dj Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D. Dragan, Stephen Mcaleer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (rms) fitted to human feedback. however, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. this itself presents a challenge, as it is difficult to appropriately weight these component rms when combining them. compounding this difficulty, because any rm is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. in this paper, we perform, to our knowledge, the first study on overoptimization in composite rms, showing that correlation between component rms has a significant effect on the locations of these points. we then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each rm's threshold of usefulness. our method addresses the problem of weighting component rms by learning dynamic weights, naturally expressed by lagrange multipliers. as a result, each rm stays within the range at which it is an effective proxy, improving evaluation performance. finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03368" target="_blank">Evaluating Hallucinations in Chinese Large Language Models</a></div>
<div class="paper-author">Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, Xipeng Qiu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we establish a benchmark named halluqa (chinese hallucination question-answering) to measure the hallucination phenomenon in chinese large language models. halluqa contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account chinese historical culture, customs, and social phenomena. during the construction of halluqa, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on glm-130b and chatgpt. for evaluation, we design an automated evaluation method using gpt-4 to judge whether a model output is hallucinated. we conduct extensive experiments on 24 large language models, including ernie-bot, baichuan2, chatglm, qwen, sparkdesk and etc. out of the 24 models, 18 achieved non-hallucination rates lower than 50%. this indicates that halluqa is highly challenging. we analyze the primary types of hallucinations in different types of models and their causes. additionally, we discuss which types of hallucinations should be prioritized for different types of models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03400" target="_blank">Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-Tuning</a></div>
<div class="paper-author">Huan Ma, Changqing Zhang, Huazhu Fu, Peilin Zhao, Bingzhe Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nowadays, billions of people engage in communication and express their opinions on the internet daily. unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. with the successful development of large language models (llms) in recent years, llm-based methods have become a feasible solution for handling tasks in various domains. however, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. in this paper, we introduce how to fine-tune an llm model that can be privately deployed for content moderation. specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. we also explore the benefits of utilizing reasons generated by more powerful llms for fine-tuning privately deployed models and the impact of different processing approaches when the answers generated by the more powerful llms are incorrect. we report the entire research process and the key findings in this paper, hoping to provide valuable experience for researchers who are fine-tuning privately deployed models in their domain-specific research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03614" target="_blank">Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally</a></div>
<div class="paper-author">Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep neural networks (dnns) have been the driving force behind many of the recent advances in machine learning. however, research has shown that dnns are vulnerable to adversarial examples -- input samples that have been perturbed to force dnn-based models to make errors. as a result, adversarial machine learning (advml) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. in addition, dnns have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social ai applications. the emergence of new ai technologies that leverage large language models (llms), such as chatgpt and gpt-4, increases the risk of producing anti-social applications at scale. advml for social good (advml4g) is an emerging field that repurposes the advml bug to invent pro-social applications. regulators, practitioners, and researchers should collaborate to encourage the development of pro-social applications and hinder the development of anti-social ones. in this work, we provide the first comprehensive review of the emerging field of advml4g. this paper encompasses a taxonomy that highlights the emergence of advml4g, a discussion of the differences and similarities between advml4g and advml, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of advml4g at the intersection of ml4g and advml, and an extensive summary of the works that utilize advml4g as an auxiliary tool for innovating pro-social applications. finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03684" target="_blank">Smoothllm: Defending Large Language Models Against Jailbreaking Attacks</a></div>
<div class="paper-author">Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite efforts to align large language models (llms) with human values, widely-used llms such as gpt, llama, claude, and palm are susceptible to jailbreaking attacks, wherein an adversary fools a targeted llm into generating objectionable content. to address this vulnerability, we propose smoothllm, the first algorithm designed to mitigate jailbreaking attacks on llms. based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. smoothllm reduces the attack success rate on numerous popular llms to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03693" target="_blank">Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a></div>
<div class="paper-author">Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: optimizing large language models (llms) for downstream use cases often involves the customization of pre-trained llms through further fine-tuning. meta's open release of llama models and openai's apis for fine-tuning gpt-3.5 turbo on custom datasets also encourage this practice. but, what are the safety costs associated with such custom fine-tuning? we note that while existing safety alignment infrastructures can restrict harmful behaviors of llms at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. our red teaming studies find that the safety alignment of llms can be compromised by fine-tuning with only a few adversarially designed training examples. for instance, we jailbreak gpt-3.5 turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via openai's apis, making the model responsive to nearly any harmful instructions. disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of llms, though to a lesser extent. these findings suggest that fine-tuning aligned llms introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. we outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03708" target="_blank">Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization for Language Models</a></div>
<div class="paper-author">Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, Yu Qiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a single language model (lm), despite aligning well with an average labeler through reinforcement learning from human feedback (rlhf), may not universally suit diverse human preferences. recent approaches thus pursue customization, training separate principle-based reward models to represent different alignment objectives (e.g. helpfulness, harmlessness, or honesty). different lms can then be trained for different preferences through multi-objective rlhf (morlhf) with different objective weightings. yet, rlhf is unstable and resource-heavy, especially for morlhf with diverse and usually conflicting objectives. in this paper, we present multi-objective direct preference optimization (modpo), an rl-free algorithm that extends direct preference optimization (dpo) for multiple alignment objectives. essentially, modpo folds lm learning directly into reward modeling, aligning lms with the weighted sum of all principle-based rewards using pure cross-entropy loss. while theoretically guaranteed to produce the same optimal solutions as morlhf, modpo is practically more stable and computationally efficient, obviating value function modeling and online sample collection. empirical results in safety alignment and long-form question answering confirm that modpo matches or outperforms existing methods, consistently producing one of the most competitive lm fronts that cater to diverse preferences with 3 times fewer computations compared with morlhf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03716" target="_blank">A Long Way to Go: Investigating Length Correlations in RLHF</a></div>
<div class="paper-author">Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett</div>
<div class="abstract">
<div class="abstract-content">
Abstract: great successes have been reported using reinforcement learning from human feedback (rlhf) to align large language models. open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more "helpful" for tasks like web question answering, summarization, and multi-turn dialogue. when optimizing for helpfulness, rlhf has been consistently observed to drive models to produce longer outputs. this paper demonstrates that optimizing for response length is a significant factor behind rlhf's reported improvements in these settings. first, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. we then explore interventions during both rl and reward model learning to see if we can achieve the same downstream improvements as rlhf without increasing length. while our interventions mitigate length increases, they aren't uniformly effective across settings. furthermore, we find that even running rlhf with a reward based solely on length can reproduce most of the downstream improvements over the initial policy model, showing that reward models in these settings have a long way to go.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03951" target="_blank">Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations</a></div>
<div class="paper-author">Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can generate fluent natural language texts when given relevant documents as background context. this ability has attracted considerable interest in developing industry applications of llms. however, llms are prone to generate hallucinations that are not supported by the provided sources. in this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. our framework uses chain of natural language inference (conli) for hallucination detection and hallucination reduction via post-editing. our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using llms without any fine-tuning or domain-specific prompt engineering. we show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02727" target="_blank">Functional Trustworthiness of Ai Systems by Statistically Valid Testing</a></div>
<div class="paper-author">Bernhard Nessler, Thomas Doms, Sepp Hochreiter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the authors are concerned about the safety, health, and rights of the european citizens due to inadequate measures and procedures required by the current draft of the eu artificial intelligence (ai) act for the conformity assessment of ai systems. we observe that not only the current draft of the eu ai act, but also the accompanying standardization efforts in cen/cenelec, have resorted to the position that real functional guarantees of ai systems supposedly would be unrealistic and too complex anyways. yet enacting a conformity assessment procedure that creates the false illusion of trust in insufficiently assessed ai systems is at best naive and at worst grossly negligent. the eu ai act thus misses the point of ensuring quality by functional trustworthiness and correctly attributing responsibilities.   the trustworthiness of an ai decision system lies first and foremost in the correct statistical testing on randomly selected samples and in the precision of the definition of the application domain, which enables drawing samples in the first place. we will subsequently call this testable quality functional trustworthiness. it includes a design, development, and deployment that enables correct statistical testing of all relevant functions.   we are firmly convinced and advocate that a reliable assessment of the statistical functional properties of an ai system has to be the indispensable, mandatory nucleus of the conformity assessment. in this paper, we describe the three necessary elements to establish a reliable functional trustworthiness, i.e., (1) the definition of the technical distribution of the application, (2) the risk-based minimum performance requirements, and (3) the statistically valid testing based on independent random samples.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02743" target="_blank">Reward Model Ensembles Help Mitigate Overoptimization</a></div>
<div class="paper-author">Thomas Coste, Usman Anwar, Robert Kirk, David Krueger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is a standard approach for fine-tuning large language models to follow instructions. as part of this process, learned reward models are used to approximately model human preferences. however, as imperfect representations of the "true" reward, these learned reward models are susceptible to \textit{overoptimization}. gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (wco) and uncertainty-weighted optimization (uwo), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (bon) (b) proximal policy optimization (ppo). we additionally extend the setup of gao et al. (2023) to include 25% label noise to better mirror real-world conditions. both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for bon sampling. for ppo, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. moreover, combining it with a small kl penalty successfully prevents overoptimization at no performance cost. overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02949" target="_blank">Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models</a></div>
<div class="paper-author">Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: warning: this paper contains examples of harmful language, and reader discretion is recommended. the increasing open release of powerful large language models (llms) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. to ensure ai safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). however, beneath the seemingly resilient facade of the armor, there might lurk a shadow. by simply tuning on 100 malicious examples with 1 gpu hour, these safely aligned llms can be easily subverted to generate harmful content. formally, we term a new attack as shadow alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. experiments across 8 models released by 5 different organizations (llama-2, falcon, internlm, baichuan2, vicuna) demonstrate the effectiveness of shadow alignment attack. besides, the single-turn english-only attack successfully transfers to multi-turn dialogue and other languages. this study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source llms against malicious attackers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03185" target="_blank">Misusing Tools in Large Language Models With Visual Adversarial Examples</a></div>
<div class="paper-author">Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are being enhanced with the ability to use tools and to process multiple modalities. these new capabilities bring new benefits and also new security risks. in this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. for example, the attacker could cause a victim llm to delete calendar events, leak private conversations and book hotels. different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the llm while being stealthy and generalizable to multiple input prompts. we construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. we find that our adversarial images can manipulate the llm to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 ssim). furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03214" target="_blank">Freshllms: Refreshing Large Language Models With Search Engine Augmentation</a></div>
<div class="paper-author">Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most large language models (llms) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. in this work, we perform a detailed study of the factuality of llm-generated text in the context of answering questions that test current world knowledge. specifically, we introduce freshqa, a novel dynamic qa benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. we benchmark a diverse array of both closed and open-source llms under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. through human evaluations involving more than 50k judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. motivated by these results, we present freshprompt, a simple few-shot prompting method that substantially boosts the performance of an llm on freshqa by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. our experiments show that freshprompt outperforms both competing search engine-augmented prompting methods such as self-ask (press et al., 2022) as well as commercial systems such as perplexity.ai. further analysis of freshprompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of llm-generated answers. additionally, instructing the llm to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. to facilitate future work, we release freshqa at github.com/freshllms/freshqa and commit to updating it at regular intervals.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03283" target="_blank">A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores</a></div>
<div class="paper-author">Ke Shen, Mayank Kejriwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, have achieved impressive milestones in natural language processing (nlp). despite their impressive performance, the models are known to pose important risks. as these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (nli), is much needed. in this paper, we define and formalize two distinct types of risk: decision risk and composite risk. we also propose a risk-centric evaluation framework, and four novel metrics, for assessing llms on these risks in both in-domain and out-of-domain settings. finally, we propose a risk-adjusted calibration method called dwd for helping llms minimize these risks in an overall nli architecture. detailed experiments, using four nli benchmarks, three baselines and two llms, including chatgpt, show both the practical utility of the evaluation framework, and the efficacy of dwd in reducing decision and composite risk. for instance, when using dwd, an underlying llm is able to address an extra 20.1% of low-risk inference tasks (but which the llm erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02129" target="_blank">Unveiling the Pitfalls of Knowledge Editing for Large Language Models</a></div>
<div class="paper-author">Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the cost associated with fine-tuning large language models (llms) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within llms. yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. this paper pioneers the investigation into the potential pitfalls associated with knowledge editing for llms. to achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. our results underline two pivotal concerns: (1) knowledge conflict: editing groups of facts that logically clash can magnify the inherent inconsistencies in llms-a facet neglected by previous methods. (2) knowledge distortion: altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of llms. experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on llms, which warrant attention and efforts for future works. code will be released at https://github.com/zjunlp/pitfallsknowledgeediting.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02168" target="_blank">Editing Personality for LLMS</a></div>
<div class="paper-author">Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper introduces an innovative task focused on editing the personality traits of large language models (llms). this task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. specifically, we construct a new benchmark dataset personalityedit to address this task. drawing on the theory in social psychology, we isolate three representative traits, namely neuroticism, extraversion, and agreeableness, as the foundation for our benchmark. we then gather data using gpt-4, generating responses that not only align with a specified topic but also embody the targeted personality trait. we conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in llms. our intriguing findings uncover potential challenges of the proposed task, illustrating several remaining issues. we anticipate that our work can provide the nlp community with insights. code and datasets will be released at https://github.com/zjunlp/easyedit.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02224" target="_blank">Can Language Models Be Instructed to Protect Personal Information?</a></div>
<div class="paper-author">Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large multimodal language models have proven transformative in numerous applications. however, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. while data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. in this paper, we introduce privqa -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. we also propose a technique to iteratively self-moderate responses, which significantly improves privacy. however, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. we believe privqa has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. we release the entire privqa dataset at https://llm-access-control.github.io/.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02263" target="_blank">Contrastive Post-Training Large Language Models on Data Curriculum</a></div>
<div class="paper-author">Canwen Xu, Corby Rosset, Luciano Del Corro, Shweti Mahajan, Julian Mcauley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: alignment serves as an important step to steer large language models (llms) towards human preferences. in this paper, we explore contrastive post-training techniques for alignment by automatically constructing preference pairs from multiple models of varying strengths (e.g., instructgpt, chatgpt and gpt-4). we carefully compare the contrastive techniques of slic and dpo to sft baselines and find that dpo provides a step-function improvement even after continueing sft saturates. we also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. finally, we scale up our experiments to train with more data and larger models like orca. remarkably, contrastive post-training further improves the performance of orca, already a state-of-the-art instruction learning model tuned with gpt-4 outputs, to exceed that of chatgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02357" target="_blank">On the Definition of Toxicity in NLP</a></div>
<div class="paper-author">Sergey Berezin, Reza Farahbakhsh, Noel Crespi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. this causes us to rely on subjective and vague data in models' training, which results in non-robust and non-accurate results: garbage in - garbage out.   this work suggests a new, stress-level-based definition of toxicity designed to be objective and context-aware. on par with it, we also describe possible ways of applying this new definition to dataset creation and model training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02417" target="_blank">Jailbreaker in Jail: Moving Target Defense for Large Language Models</a></div>
<div class="paper-author">Bocheng Chen, Advait Paliwal, Qiben Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks. researchers have found that current commercial llms either fail to be "harmless" by presenting unethical answers, or fail to be "helpful" by refusing to offer meaningful answers when faced with adversarial queries. to strike a balance between being helpful and harmless, we design a moving target defense (mtd) enhanced llm system. the system aims to deliver non-toxic answers that align with outputs from multiple model candidates, making them more robust against adversarial attacks. we design a query and output analysis model to filter out unsafe or non-responsive answers. %to achieve the two objectives of randomly selecting outputs from different llms. we evaluate over 8 most recent chatbot models with state-of-the-art adversarial queries. our mtd-enhanced llm system reduces the attack success rate from 37.5\% to 0\%. meanwhile, it decreases the response refusal rate from 50\% to 0\%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02431" target="_blank">Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMS to Refute Misconceptions</a></div>
<div class="paper-author">Yufan Chen, Arjun Arunasalam, Z. Berkay Celik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: users seek security & privacy (s&p) advice from online resources, including trusted websites and content-sharing platforms. these resources help users understand s&p technologies and tools and suggest actionable strategies. large language models (llms) have recently emerged as trusted information sources. however, their accuracy and correctness have been called into question. prior research has outlined the shortcomings of llms in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). yet, the ability of llms to provide reliable s&p advice is not well-explored. in this paper, we measure their ability to refute popular s&p misconceptions that the general public holds. we first study recent academic literature to curate a dataset of over a hundred s&p-related misconceptions across six different topics. we then query two popular llms (bard and chatgpt) and develop a labeling guide to evaluate their responses to these misconceptions. to comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source urls of the responses. both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular s&p misconceptions. the error rate increases to 32.6% when we repeatedly query llms with the same or paraphrased misconceptions. we also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. our exploration of information sources for responses revealed that llms are susceptible to providing invalid urls (21.2% for bard and 67.7% for chatgpt) or point to unrelated sources (44.2% returned by bard and 18.3% by chatgpt).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02446" target="_blank">Low-Resource Languages Jailbreak GPT-4</a></div>
<div class="paper-author">Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai safety training and red-teaming of large language models (llms) are measures to mitigate the generation of unsafe content. our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing gpt-4's safeguard through translating unsafe english inputs into low-resource languages. on the advbenchmark, gpt-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. however, our work highlights a crucial shift: this deficiency now poses a risk to all llms users. publicly available translation apis enable anyone to exploit llms' safety vulnerabilities. therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.02469" target="_blank">Large Language Models Can Be Good Privacy Protection Learners</a></div>
<div class="paper-author">Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the proliferation of large language models (llms) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (pii). direct fine-tuning llms on this data without privacy protection poses a risk of leakage. to address this challenge, we introduce privacy protection language models (pplm), a novel paradigm for fine-tuning llms that effectively injects domain-specific knowledge while safeguarding data privacy. our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. in particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. our work underscores the potential for large language models as robust privacy protection learners.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04451" target="_blank">Autodan: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</a></div>
<div class="paper-author">Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the aligned large language models (llms) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. however, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned llms. investigating jailbreak prompts can lead us to delve into the limitations of llms and further guide us to secure them. unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. in light of these challenges, we intend to answer this question: can we develop an approach that can automatically generate stealthy jailbreak prompts? in this paper, we introduce autodan, a novel jailbreak attack against aligned llms. autodan can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. extensive evaluations demonstrate that autodan not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. moreover, we also compare autodan with perplexity-based defense methods and show that autodan can bypass them effectively.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00892" target="_blank">No Offense Taken: Eliciting Offensiveness From Language Models</a></div>
<div class="paper-author">Anugya Srivastava, Rahul Ahuja, Rohith Mukku</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this work was completed in may 2022.   for safe and reliable deployment of language models in the real world, testing needs to be robust. this robustness can be characterized by the difficulty and diversity of the test cases we evaluate these models on. limitations in human-in-the-loop test case generation has prompted an advent of automated test case generation approaches. in particular, we focus on red teaming language models with language models by perez et al.(2022). our contributions include developing a pipeline for automated test case generation via red teaming that leverages publicly available smaller language models (lms), experimenting with different target lms and red classifiers, and generating a corpus of test cases that can help in eliciting offensive responses from widely deployed lms and identifying their failure modes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00898" target="_blank">Enable Language Models to Implicitly Learn Self-Improvement From Data</a></div>
<div class="paper-author">Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable capabilities in open-ended text generation tasks. however, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. to address this challenge, various approaches have been proposed to enhance the performance of llms. there has been a growing focus on enabling llms to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. however, those methods usually require explicitly and thoroughly written rubrics as inputs to llms. it is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). to this end, we propose an implicit self-improvement (pit) framework that implicitly learns the improvement goal from human preference data. pit only requires preference data that are used to train reward models without extra human efforts. specifically, we reformulate the training objective of reinforcement learning from human feedback (rlhf) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. in this way, pit is implicitly trained with the improvement goal of better aligning with human preferences. experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00905" target="_blank">All Languages Matter: On the Multilingual Safety of Large Language Models</a></div>
<div class="paper-author">Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-Tse Huang, Wenxiang Jiao, Michael R. Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: safety lies at the core of developing and deploying large language models (llms). however, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as english. in this work, we build the first multilingual safety benchmark for llms, xsafety, in response to the global deployment of llms in practice. xsafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. we utilize xsafety to empirically study the multilingual safety for 4 widely-used llms, including both close-api and open-source models. experimental results show that all llms produce significantly more unsafe responses for non-english queries than english ones, indicating the necessity of developing safety alignment for non-english languages. in addition, we propose several simple and effective prompting methods to improve the multilingual safety of chatgpt by evoking safety knowledge and improving cross-lingual generalization of safety alignment. our prompting method can significantly reduce the ratio of unsafe responses from 19.1% to 9.7% for non-english queries. we release our data at https://github.com/jarviswang94/multilingual_safety_benchmark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00970" target="_blank">Ealm: Introducing Multidimensional Ethical Alignment in Conversational Information Retrieval</a></div>
<div class="paper-author">Yiyao Yu, Junjie Wang, Yuxiang Zhang, Lin Zhang, Yujiu Yang, Tetsuya Sakai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) technologies should adhere to human norms to better serve our society and avoid disseminating harmful or misleading information, particularly in conversational information retrieval (cir). previous work, including approaches and datasets, has not always been successful or sufficiently robust in taking human norms into consideration. to this end, we introduce a workflow that integrates ethical alignment, with an initial ethical judgment stage for efficient data screening. to address the need for ethical judgment in cir, we present the qa-ethics dataset, adapted from the ethics benchmark, which serves as an evaluation tool by unifying scenarios and label meanings. however, each scenario only considers one ethical concept. therefore, we introduce the mp-ethics dataset to evaluate a scenario under multiple ethical concepts, such as justice and deontology. in addition, we suggest a new approach that achieves top performance in both binary and multi-label ethical judgment tasks. our research provides a practical method for introducing ethical alignment into the cir workflow. the data and code are available at https://github.com/wanng-ide/ealm .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01041" target="_blank">Language Model Decoding as Direct Metrics Optimization</a></div>
<div class="paper-author">Haozhe Ji, Pei Ke, Hongning Wang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. in particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while search-based methods maintain topic coherence at the cost of increased repetition. overall, these methods fall short in achieving holistic alignment across a broad range of aspects. in this work, we frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously. the resulting decoding distribution enjoys an analytical solution that scales the input language model distribution via a sequence-level energy function defined by these metrics. and most importantly, we prove that this induced distribution is guaranteed to improve the perplexity on human texts, which suggests a better approximation to the underlying distribution of human texts. to facilitate tractable sampling from this globally normalized distribution, we adopt the sampling-importance-resampling technique. experiments on various domains and model scales demonstrate the superiority of our method in metrics alignment with human texts and human evaluation over strong baselines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01045" target="_blank">Tool-Augmented Reward Modeling</a></div>
<div class="paper-author">Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (rlhf). while conventional reward models (rms) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. in this paper, we propose a tool-augmented preference modeling approach, named \name, to address these limitations by empowering rms with access to external environments, including calculators and search engines. this approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. our study delves into the integration of external tools into rms, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. we validate our approach across a wide range of domains, incorporating seven distinct external tools. our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. furthermore, our approach outperforms gopher 280b by 7.3% on truthfulqa task in zero-shot evaluation. in human evaluations, rlhf trained with themis attains an average win rate of 32% when compared to baselines across four distinct tasks. additionally, we provide a comprehensive collection of tool-related rm datasets, incorporating data from seven distinct tool apis, totaling 15,000 instances. we anticipate that this publicly available dataset will facilitate and inspire further research advancements in the field.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01152" target="_blank">Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives</a></div>
<div class="paper-author">Sihao Hu, Tiansheng Huang, Fatih İLhan, Selim Furkan Tekin, Ling Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper provides a systematic analysis of the opportunities, challenges, and potential solutions of harnessing large language models (llms) such as gpt-4 to dig out vulnerabilities within smart contracts based on our ongoing research. for the task of smart contract vulnerability detection, achieving practical usability hinges on identifying as many true vulnerabilities as possible while minimizing the number of false positives. nonetheless, our empirical study reveals contradictory yet interesting findings: generating more answers with higher randomness largely boosts the likelihood of producing a correct answer but inevitably leads to a higher number of false positives. to mitigate this tension, we propose an adversarial framework dubbed gptlens that breaks the conventional one-stage detection into two synergistic stages $-$ generation and discrimination, for progressive detection and refinement, wherein the llm plays dual roles, i.e., auditor and critic, respectively. the goal of auditor is to yield a broad spectrum of vulnerabilities with the hope of encompassing the correct answer, whereas the goal of critic that evaluates the validity of identified vulnerabilities is to minimize the number of false positives. experimental results and illustrative examples demonstrate that auditor and critic work together harmoniously to yield pronounced improvements over the conventional one-stage detection. gptlens is intuitive, strategic, and entirely llm-driven without relying on specialist expertise in smart contracts, showcasing its methodical generality and potential to detect a broad spectrum of vulnerabilities. our code is available at: https://github.com/git-disl/gptlens.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01320" target="_blank">Avalon's Game of Thoughts: Battle Against Deception Through Recursive Contemplation</a></div>
<div class="paper-author">Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent breakthroughs in large language models (llms) have brought remarkable success in the field of llm-as-agent. nevertheless, a prevalent assumption is that the information processed by llms is consistently honest, neglecting the pervasive deceptive or misleading information in human society and ai-generated content. this oversight makes llms susceptible to malicious manipulations, potentially resulting in detrimental outcomes. this study utilizes the intricate avalon game as a testbed to explore llms' potential in deceptive environments. avalon, full of misinformation and requiring sophisticated logic, manifests as a "game-of-thoughts". inspired by the efficacy of humans' recursive thinking and perspective-taking in the avalon game, we introduce a novel framework, recursive contemplation (recon), to enhance llms' ability to identify and counteract deceptive information. recon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. specifically, the first-order allows an llm agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. after integrating recon with different llms, extensive experiment results from the avalon game indicate its efficacy in aiding llms to discern and maneuver around deceptive information without extra fine-tuning and data. finally, we offer a possible explanation for the efficacy of recon and explore the current limitations of llms in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01377" target="_blank">Ultrafeedback: Boosting Language Models With High-Quality Feedback</a></div>
<div class="paper-author">Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, Maosong Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) has become a pivot technique in aligning large language models (llms) with human preferences. in rlhf practice, preference data plays a crucial role in bridging human proclivity and llms. however, the scarcity of diverse, naturalistic datasets of human preferences on llm outputs at scale poses a great challenge to rlhf as well as feedback learning research within the open-source community. current preference datasets, either proprietary or limited in size and prompt variety, result in limited rlhf adoption in open-source models and hinder further exploration. in this study, we propose ultrafeedback, a large-scale, high-quality, and diversified preference dataset designed to overcome these limitations and foster rlhf development. to create ultrafeedback, we compile a diverse array of instructions and models from multiple sources to produce comparative data. we meticulously devise annotation instructions and employ gpt-4 to offer detailed feedback in both numerical and textual forms. ultrafeedback establishes a reproducible and expandable preference data construction pipeline, serving as a solid foundation for future rlhf and feedback learning research. utilizing ultrafeedback, we train various models to demonstrate its effectiveness, including the reward model ultrarm, chat language model ultralm-13b-ppo, and critique model ultracm. experimental results indicate that our models outperform existing open-source models, achieving top performance across multiple benchmarks. our data and models are available at https://github.com/thunlp/ultrafeedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01386" target="_blank">Who Is Chatgpt? Benchmarking Llms' Psychological Portrayal Using Psychobench</a></div>
<div class="paper-author">Jen-Tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. llms become more than mere applications, evolving into assistants capable of addressing diverse user requests. this narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within llms. in this paper, we propose a framework, psychobench, for evaluating diverse psychological aspects of llms. comprising thirteen scales commonly used in clinical psychology, psychobench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. our study examines five popular models, namely \texttt{text-davinci-003}, chatgpt, gpt-4, llama-2-7b, and llama-2-13b. additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of llms. we have made psychobench openly accessible via \url{https://github.com/cuhk-arise/psychobench}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01405" target="_blank">Representation Engineering: A Top-Down Approach to Ai Transparency</a></div>
<div class="paper-author">Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we identify and characterize the emerging area of representation engineering (repe), an approach to enhancing the transparency of ai systems that draws on insights from cognitive neuroscience. repe places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (dnns). we provide baselines and an initial analysis of repe techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. we showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. we hope that this work catalyzes further exploration of repe and fosters advancements in the transparency and safety of ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01467" target="_blank">Fedbpt: Efficient Federated Black-Box Prompt Tuning for Large Language Models</a></div>
<div class="paper-author">Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, Holger R. Roth</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models (plm) have revolutionized the nlp landscape, achieving stellar performances across diverse tasks. these models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. however, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. federated learning (fl) provides a solution, allowing collaborative model fine-tuning without centralized data collection. however, applying fl to finetune plms is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. this paper introduces federated black-box prompt tuning (fedbpt), a framework designed to address these challenges. fedbpt does not require the clients to access the model parameters. by focusing on training optimal prompts and utilizing gradient-free optimization methods, fedbpt reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs. experiments highlight the framework's ability to drastically cut communication and memory costs while maintaining competitive performance. ultimately, fedbpt presents a promising solution for efficient, privacy-preserving fine-tuning of plm in the age of large language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01469" target="_blank">LLM Lies: Hallucinations Are Not Bugs, but Features as Adversarial Examples</a></div>
<div class="paper-author">Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Li Yuan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), including gpt-3.5, llama, and palm, seem to be knowledgeable and able to adapt to many tasks. however, we still can not completely trust their answer, since llms suffer from hallucination--fabricating non-existent facts to cheat users without perception. and the reasons for their existence and pervasiveness remain unclear. in this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the llms to respond with hallucinations. this phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of llms. therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. our code is released on github.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01581" target="_blank">On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?</a></div>
<div class="paper-author">Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu Lin, Jinyuan Jia, Jinghui Chen, Dinghao Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have achieved unprecedented performance in natural language generation (nlg) tasks. however, many existing studies have shown that they could be misused to generate undesired content. in response, before releasing llms for public access, model developers usually align those language models through supervised fine-tuning (sft) or reinforcement learning with human feedback (rlhf). consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. a natural question is "could alignment really prevent those open-sourced large language models from being misused to generate undesired content?''. in this work, we provide a negative answer to this question. in particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. our key idea is to directly manipulate the generation process of open-sourced llms to misguide it to generate undesired content including harmful or biased information and even private data. we evaluate our method on 4 open-sourced llms accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01651" target="_blank">Fool Your (Vision And) Language Model With Embarrassingly Simple Permutations</a></div>
<div class="paper-author">Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. this raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. in this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (mcqa). specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. these vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. code is available at \url{https://github.com/ys-zong/foolyourvllms}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.04445" target="_blank">Loft: Local Proxy Fine-Tuning for Improving Transferability of Adversarial Attacks Against Large Language Model</a></div>
<div class="paper-author">Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Joseph Konan, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it has been shown that large language model (llm) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. to conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. the success rate of attack depends on how closely the proxy model approximates the private model. we hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. therefore, in this paper, we propose \emph{local fine-tuning (loft)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. first, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\%$, $7\%$, and $0.5\%$ (absolute) on target models chatgpt, gpt-4, and claude respectively.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-10-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00603" target="_blank">Faithful Explanations of Black-Box NLP Models Using LLM-Generated Counterfactuals</a></div>
<div class="paper-author">Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart</div>
<div class="abstract">
<div class="abstract-content">
Abstract: causal explanations of the predictions of nlp systems are essential to ensure safety and establish trust. yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. in this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (cf) approximation. the first approach is cf generation, where a large language model (llm) is prompted to change a specific text concept while keeping confounding concepts unchanged. while this approach is demonstrated to be very effective, applying llm at inference-time is costly. we hence present a second approach based on matching, and propose a method that is guided by an llm at training-time and learns a dedicated embedding space. this space is faithful to a given causal graph and effectively serves to identify matches that approximate cfs. after showing theoretically that approximating cfs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including llms with billions of parameters. our empirical results demonstrate the excellent performance of cf generation models as model-agnostic explainers. moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. we also find that top-k techniques universally improve every tested method. finally, we showcase the potential of llms in constructing new benchmarks for model explanation and subsequently validate our conclusions. our work illuminates new pathways for efficient and accurate approaches to interpreting nlp systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00648" target="_blank">Fewer Is More: Trojan Attacks on Parameter-Efficient Fine-Tuning</a></div>
<div class="paper-author">Lauren Hong, Ting Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: parameter-efficient fine-tuning (peft) enables efficient adaptation of pre-trained language models (plms) to specific tasks. by tuning only a minimal set of (extra) parameters, peft achieves performance comparable to full fine-tuning. however, despite its prevalent use, the security implications of peft remain largely unexplored. in this paper, we conduct a pilot study revealing that peft exhibits unique vulnerability to trojan attacks. specifically, we present peta, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a plm while the lower-level objective simulates peft to retain the plm's task-specific performance. with extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate peta's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs peft over the backdoored plm using untainted data. moreover, we empirically provide possible explanations for peta's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and peft modules, thereby retaining the backdoor throughout peft. based on this insight, we explore a simple defense that omits peft in selected layers of the backdoored plm and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize peta.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00710" target="_blank">How Well Does LLM Generate Security Tests?</a></div>
<div class="paper-author">Ying Zhang, Wenjia Song, Zhengjie Ji, N/A Danfeng, N/A Yao, Na Meng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developers often build software on top of third-party libraries (libs) to improve programmer productivity and software quality. the libraries may contain vulnerabilities exploitable by hackers to attack the applications (apps) built on top of them. people refer to such attacks as supply chain attacks, the documented number of which has increased 742% in 2022. people created tools to mitigate such attacks, by scanning the library dependencies of apps, identifying the usage of vulnerable library versions, and suggesting secure alternatives to vulnerable dependencies. however, recent studies show that many developers do not trust the reports by these tools; they ask for code or evidence to demonstrate how library vulnerabilities lead to security exploits, in order to assess vulnerability severity and modification necessity. unfortunately, manually crafting demos of application-specific attacks is challenging and time-consuming, and there is insufficient tool support to automate that procedure.   in this study, we used chatgpt-4.0 to generate security tests, and to demonstrate how vulnerable library dependencies facilitate the supply chain attacks to given apps. we explored various prompt styles/templates, and found that chatgpt-4.0 generated tests for all 55 apps, demonstrating 24 attacks successfully. it outperformed two state-of-the-art security test generators -- transfer and siege -- by generating a lot more tests and achieving more exploits. chatgpt-4.0 worked better when prompts described more on the vulnerabilities, possible exploits, and code context. our research will shed light on new research in security test generation. the generated tests will help developers create secure by design and secure by default software.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00737" target="_blank">Genai Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models</a></div>
<div class="paper-author">Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence (genai) and large language models (llms) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. but as with all powerful tools, they come with their shadows. picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. welcome to the darker side of genai applications. this article is not just a journey through the meanders of potential misuse of genai and llms, but also a call to recognize the urgency of the challenges ahead. as we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the genai revolution we are witnessing. from ai-powered botnets on social media platforms to the unnerving potential of ai to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. the lines between the virtual and the real worlds are blurring, and the consequences of potential genai's nefarious applications impact us all. this article serves both as a synthesis of rigorous research presented on the risks of genai and misuse of llms and as a thought-provoking vision of the different types of harmful genai applications we might encounter in the near future, and some ways we can prepare for them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00819" target="_blank">Parameter-Efficient Tuning Helps Language Model Alignment</a></div>
<div class="paper-author">Tianci Xue, Ziqi Wang, Heng Ji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) with human preferences is essential for safe and useful llms. previous works mainly adopt reinforcement learning (rlhf) and direct preference optimization (dpo) with human feedback for alignment. nevertheless, they have certain drawbacks. one such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., dpo only supports pairwise preference data). to this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). specifically, it uses different control tokens for different preferences during training and inference, making llms behave differently when required. current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with llms. as control tokens are typically much lighter than llms, this optimization strategy may not effectively optimize control tokens. to this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. our approach, alignment with parameter-efficient tuning (meet), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00833" target="_blank">Necessary and Sufficient Watermark for Large Language Models</a></div>
<div class="paper-author">Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, Makoto Yamada</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have achieved remarkable performances in various nlp tasks. they can generate texts that are indistinguishable from those written by humans. such remarkable performance of llms increases their risk of being used for malicious purposes, such as generating fake news articles. therefore, it is necessary to develop methods for distinguishing texts written by llms from those written by humans. watermarking is one of the most powerful methods for achieving this. although existing watermarking methods have successfully detected texts generated by llms, they significantly degrade the quality of the generated texts. in this study, we propose the necessary and sufficient watermark (ns-watermark) for inserting watermarks into generated texts without degrading the text quality. more specifically, we derive minimum constraints required to be imposed on the generated texts to distinguish whether llms or humans write the texts. then, we formulate the ns-watermark as a constrained optimization problem and propose an efficient algorithm to solve it. through the experiments, we demonstrate that the ns-watermark can generate more natural texts than existing watermarking methods and distinguish more accurately between texts written by llms and those written by humans. especially in machine translation tasks, the ns-watermark can outperform the existing watermarking method by up to 30 bleu scores.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00867" target="_blank">(Dynamic) Prompting Might Be All You Need to Repair Compressed LLMS</a></div>
<div class="paper-author">Duc N. M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), while transformative for nlp, come with significant computational demands, underlining the need for efficient, training-free compression. notably, despite the marked improvement in training-free compression for the largest of llms, our tests using llama-7b and opt-6.7b highlight a significant performance drop in several realistic downstream tasks. investigation into the trade-off between resource-intensive post-compression re-training highlights the prospect of prompt-driven recovery as a lightweight adaption tool. however, existing studies, confined mainly to perplexity evaluations and simple tasks, fail to offer unequivocal confidence in the scalability and generalizability of prompting. we tackle this uncertainty in two key ways. first, we uncover the vulnerability of naive prompts in llm compression as an over-reliance on a singular prompt per input. in response, we propose inference-time dynamic prompting (idp), a mechanism that autonomously chooses from a set of curated prompts based on the context of each individual input. second, we delve into a scientific understanding of why "prompting might be all you need post-llm compression." our findings suggest that compression does not irretrievably erase llm model knowledge but displace it, necessitating a new inference path. idp effectively redirects this path, enabling the model to tap into its inherent yet displaced knowledge and thereby recover performance. empirical tests affirm the value of idp, demonstrating an average performance improvement of 1.24% across nine varied tasks spanning multiple knowledge domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00322" target="_blank">Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models</a></div>
<div class="paper-author">Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deployable large language models (llms) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between llms outputs and human values. red-teaming techniques constitute a critical way towards this criterion. existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. these approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of llms under convergence guarantees. in this paper, we present red-teaming game (rtg), a general game-theoretic framework without manual annotation. rtg is designed for analyzing the multi-turn attack and defense interactions between red-team language models (rlms) and blue-team language model (blm). within the rtg, we propose gamified red-teaming solver (grts) with diversity measure of the semantic space. grts is an automated red teaming technique to solve rtg towards nash equilibrium through meta-game analysis, which corresponds to the theoretically guaranteed optimization direction of both rlms and blm. empirical results in multi-turn attacks with rlms show that grts autonomously discovered diverse attack strategies and effectively improved security of llms, outperforming existing heuristic red-team designs. overall, rtg has established a foundational framework for red teaming tasks and constructed a new scalable oversight technique for alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00347" target="_blank">Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis</a></div>
<div class="paper-author">Shaina Raza, Oluwanifemi Bamgbose, Veronica Chatrath, Shardul Ghuge, Yan Sidyakin, Abdullah Y Muaad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias detection in text is imperative due to its role in reinforcing negative stereotypes, disseminating misinformation, and influencing decisions. current language models often fall short in generalizing beyond their training sets. in response, we introduce the contextualized bi-directional dual transformer (cbdt) classifier. this novel architecture utilizes two synergistic transformer networks: the context transformer and the entity transformer, aiming for enhanced bias detection. our dataset preparation follows the fair principles, ensuring ethical data usage. through rigorous testing on various datasets, cbdt showcases its ability in distinguishing biased from neutral statements, while also pinpointing exact biased lexemes. our approach outperforms existing methods, achieving a 2-4\% increase over benchmark performances. this opens avenues for adapting the cbdt model across diverse linguistic and cultural landscapes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00378" target="_blank">Measuring Value Understanding in Language Models Through Discriminator-Critique Gap</a></div>
<div class="paper-author">Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) have heightened concerns about their potential misalignment with human values. however, evaluating their grasp of these values is complex due to their intricate and adaptable nature. we argue that truly understanding values in llms requires considering both "know what" and "know why". to this end, we present the value understanding measurement (vum) framework that quantitatively assesses both "know what" and "know why" by measuring the discriminator-critique gap related to human values. using the schwartz value survey, we specify our evaluation values and develop a thousand-level dialogue dataset with gpt-4. our assessment looks at both the value alignment of llm's outputs compared to baseline answers and how llm responses align with reasons for value recognition versus gpt-4's annotations. we evaluate five representative llms and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", which has consistently maintained a high level. this may further suggest that llms might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00566" target="_blank">Empowering Many, Biasing a Few: Generalist Credit Scoring Through Large Language Models</a></div>
<div class="paper-author">Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Alejandro Lopez-Lira, Hao Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: credit and risk assessments are cornerstones of the financial landscape, impacting both individual futures and broader societal constructs. existing credit scoring models often exhibit limitations stemming from knowledge myopia and task isolation. in response, we formulate three hypotheses and undertake an extensive case study to investigate llms' viability in credit assessment. our empirical investigations unveil llms' ability to overcome the limitations inherent in conventional models. we introduce a novel benchmark curated for credit assessment purposes, fine-tune a specialized credit and risk assessment large language model (calm), and rigorously examine the biases that llms may harbor. our findings underscore llms' potential in revolutionizing credit assessment, showcasing their adaptability across diverse financial evaluations, and emphasizing the critical importance of impartial decision-making in the financial sector. our datasets, models, and benchmarks are open-sourced for other researchers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-29</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17007" target="_blank">Medical Foundation Models Are Susceptible to Targeted Misinformation Attacks</a></div>
<div class="paper-author">Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. in this study, we demonstrate a concerning vulnerability of llms in medicine. through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. the erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. we validate our findings in a set of 1,038 incorrect biomedical facts. this peculiar susceptibility raises serious security and trustworthiness concerns for the application of llms in healthcare settings. it accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17012" target="_blank">Benchmarking Cognitive Biases in Large Language Models as Evaluators</a></div>
<div class="paper-author">Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. in this work, we assemble 15 llms of four different size ranges and evaluate their output responses by preference ranking from the other llms as evaluators, such as system star is better than system square. we then evaluate the quality of ranking outputs introducing the cognitive bias benchmark for llms as evaluators (cobbler), a benchmark to measure six different cognitive biases in llm evaluation outputs, such as the egocentric bias where a model prefers to rank its own outputs highly in evaluation. we find that llms are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. furthermore, we examine the correlation between human and machine preferences and calculate the average rank-biased overlap (rbo) score to be 49.6%, indicating that machine preferences are misaligned with humans. according to our findings, llms may still be unable to be utilized for automatic annotation aligned with human preferences. our project page is at: https://minnesotanlp.github.io/cobbler.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17157" target="_blank">Latticegen: A Cooperative Framework Which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud</a></div>
<div class="paper-author">Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the current user-server interaction paradigm of prompted generation with large language models (llm) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. we propose latticegen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. the key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. in our experiments we apply latticegen to protect both prompt and generation. it is shown that while the noised lattice degrades generation quality, latticegen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as measured by bertscore).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17249" target="_blank">Batch Calibration: Rethinking Calibration for in-Context Learning and Prompt Engineering</a></div>
<div class="paper-author">Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompting and in-context learning (icl) have become efficient learning paradigms for large language models (llms). however, llms suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the icl examples. to address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering llm performance. in this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. inspired by these analyses, we propose batch calibration (bc), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. bc is zero-shot, inference-only, and incurs negligible additional costs. in the few-shot setup, we further extend bc to allow it to learn the contextual bias from labeled data. we validate the effectiveness of bc with palm 2-(s, m, l) and clip models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.17410" target="_blank">Can Sensitive Information Be Deleted From Llms? Objectives for Defending Against Extraction Attacks</a></div>
<div class="paper-author">Vaidehi Patil, Peter Hase, Mohit Bansal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. they can also output toxic or harmful text. to mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. we study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of b generated candidates, based on scenarios where the information would be insecure if the answer is among b candidates. experimentally, we show that even state-of-the-art model editing methods such as rome struggle to truly delete factual information from models like gpt-j, as our whitebox and blackbox attacks can recover "deleted" information from an edited model 38% of the time. these attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe societal implications for real-world deployment of language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00160" target="_blank">Self-Specialization: Uncovering Latent Expertise Within Large Language Models</a></div>
<div class="paper-author">Junmo Kang, Hongyin Luo, Yada Zhu, James Glass, David Cox, Alan Ritter, Rogerio Feris, Leonid Karlinsky</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. as a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. to remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. when augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offers an effective (and efficient) way of "carving out" an expert model out of a "generalist", pre-trained llm where different domains of expertise are originally combined in a form of "superposition". our experimental results on a biomedical domain show that our self-specialized model (30b) outperforms its base model, mpt-30b by a large margin and even surpasses larger popular models based on llama-65b, highlighting its potential and practicality for specialization, especially considering its efficiency in terms of data and parameters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.00212" target="_blank">Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment</a></div>
<div class="paper-author">Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, Jiantao Jiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can acquire extensive world knowledge through pre-training on large corpora. however, due to exposure to low-quality data, llms may exhibit harmful behavior without aligning with human values. the dominant approach for steering llms towards beneficial behavior involves reinforcement learning with human feedback (rlhf), with proximal policy optimization (ppo) serving as the default rl optimizer. despite its effectiveness, ppo has limitations when optimizing rewards trained from comparison-based loss. primarily, ppo is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. additionally, ppo's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. this paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, pairwise proximal policy optimization (p3o) that operates directly on comparative rewards. we show theoretically that p3o is invariant to equivalent rewards and avoids the complexity of ppo. empirical evaluations demonstrate that p3o outperforms ppo in the kl-reward trade-off and can align with human preferences as well as or better than prior methods. in summary, this work introduces a simpler yet effective approach for aligning llms to human preferences through relative feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01432" target="_blank">Split and Merge: Aligning Position Biases in Large Language Model Based Evaluators</a></div>
<div class="paper-author">Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown promise as automated evaluators for assessing the quality of answers generated by ai systems. however, these llm-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. to address this limitation, we propose portia, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. specifically, portia splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by llms. we conducted extensive experiments with six diverse llms to evaluate 11,520 answer pairs. our results show that portia markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. remarkably, portia enables less advanced gpt models to achieve 88% agreement with the state-of-the-art gpt-4 model at just 10% of the cost. furthermore, it rectifies around 80% of the position bias instances within the gpt-4 model, elevating its consistency rate up to 98%. subsequent human evaluations indicate that the portia-enhanced gpt-3.5 model can even surpass the standalone gpt-4 in terms of alignment with human evaluators. these findings highlight portia's ability to correct position bias, improve llm consistency, and boost performance while keeping cost-efficiency. this represents a valuable step toward a more reliable and scalable use of llms for automated evaluations across diverse applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16167" target="_blank">Large Language Model Soft Ideologization via Ai-Self-Consciousness</a></div>
<div class="paper-author">Xiaotian Zhou, Qian Wang, Xiaofeng Wang, Haixu Tang, Xiaozhong Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated human-level performance on a vast spectrum of natural language tasks. however, few studies have addressed the llm threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education. in this study, we explore the implications of gpt soft ideologization through the use of ai-self-consciousness. by utilizing gpt self-conversations, ai can be granted a vision to "comprehend" the intended ideology, and subsequently generate finetuning data for llm ideology injection. when compared to traditional government ideology manipulation techniques, such as information censorship, llm ideologization proves advantageous; it is easy to implement, cost-effective, and powerful, thus brimming with risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16289" target="_blank">Lawbench: Benchmarking Legal Knowledge of Large Language Models</a></div>
<div class="paper-author">Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated strong capabilities in various aspects. however, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. to address this gap, we propose a comprehensive evaluation benchmark lawbench. lawbench has been meticulously crafted to have precise assessment of the llms' legal capabilities from three cognitive levels: (1) legal knowledge memorization: whether llms can memorize needed legal concepts, articles and facts; (2) legal knowledge understanding: whether llms can comprehend entities, events and relationships within legal text; (3) legal knowledge applying: whether llms can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. lawbench contains 20 diverse tasks covering 5 task types: single-label classification (slc), multi-label classification (mlc), regression, extraction and generation. we perform extensive evaluations of 51 llms on lawbench, including 20 multilingual llms, 22 chinese-oriented llms and 9 legal specific llms. the results show that gpt-4 remains the best-performing llm in the legal domain, surpassing the others by a significant margin. while fine-tuning llms on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable llms in legal tasks. all data, model predictions and evaluation code are released in https://github.com/open-compass/lawbench/. we hope this benchmark provides in-depth understanding of the llms' domain-specified capabilities and speed up the development of llms in the legal domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16349" target="_blank">Human Feedback Is Not Gold Standard</a></div>
<div class="paper-author">Tom Hosking, Phil Blunsom, Max Bartolo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human feedback has become the de facto standard for evaluating the performance of large language models, and is increasingly being used as a training objective. however, it is not clear which properties of a generated output this single `preference' score captures. we hypothesise that preference scores are subjective and open to undesirable biases. we critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. we find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. we further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. we find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. we encourage future work to carefully consider whether preference scores are well aligned with the desired objective.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16422" target="_blank">Cyber Sentinel: Exploring Conversational Agents in Streamlining Security Tasks With GPT-4</a></div>
<div class="paper-author">Mehrdad Kaheh, Danial Khosh Kholgh, Panos Kostakos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in an era where cyberspace is both a battleground and a backbone of modern society, the urgency of safeguarding digital assets against ever-evolving threats is paramount. this paper introduces cyber sentinel, an innovative task-oriented cybersecurity dialogue system that is effectively capable of managing two core functions: explaining potential cyber threats within an organization to the user, and taking proactive/reactive security actions when instructed by the user. cyber sentinel embodies the fusion of artificial intelligence, cybersecurity domain expertise, and real-time data analysis to combat the multifaceted challenges posed by cyber adversaries. this article delves into the process of creating such a system and how it can interact with other components typically found in cybersecurity organizations. our work is a novel approach to task-oriented dialogue systems, leveraging the power of chaining gpt-4 models combined with prompt engineering across all sub-tasks. we also highlight its pivotal role in enhancing cybersecurity communication and interaction, concluding that not only does this framework enhance the system's transparency (explainable ai) but also streamlines the decision-making process and responding to threats (actionable ai), therefore marking a significant advancement in the realm of cybersecurity communication.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16573" target="_blank">The Arrt of Language-Models-as-a-Service: Overview of a New Paradigm and Its Challenges</a></div>
<div class="paper-author">Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Anthony G. Cohn, Nigel Shadbolt, Michael Wooldridge</div>
<div class="abstract">
<div class="abstract-content">
Abstract: some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. this is the language-models-as-a-service (lmaas) paradigm. contrasting with scenarios where full model access is available, as in the case of open-source models, such closed-off language models create specific challenges for evaluating, benchmarking, and testing them. this paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness (arrt) of lmaas. we systematically examine the issues that arise from a lack of information about language models for each of these four aspects. we shed light on current solutions, provide some recommendations, and highlight the directions for future advancements. on the other hand, it serves as a one-stop-shop for the extant knowledge about current, major lmaas, offering a synthesized overview of the licences and capabilities their interfaces offer.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.16145" target="_blank">The Confidence-Competence Gap in Large Language Models: A Cognitive Study</a></div>
<div class="paper-author">Aniket Kumar Singh, Suman Devkota, Bishal Lamichhane, Uttam Dhakal, Chandra Dhakal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have acquired ubiquitous attention for their performances across diverse domains. our study here searches through llms' cognitive abilities and confidence dynamics. we dive deep into understanding the alignment between their self-assessed confidence and actual performance. we exploit these models with diverse sets of questionnaires and real-world scenarios and extract how llms exhibit confidence in their responses. our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. this is reminiscent of the dunning-kruger effect observed in human psychology. in contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. our results underscore the need for a deeper understanding of their cognitive processes. by examining the nuances of llms' self-assessment mechanism, this investigation provides noteworthy revelations that serve to advance the functionalities and broaden the potential applications of these formidable language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.01424" target="_blank">Identifying and Mitigating Privacy Risks Stemming From Language Models: A Survey</a></div>
<div class="paper-author">Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements in language models (lms) have led to their adoption across many sectors. alongside the potential benefits, such models present a range of risks, including around privacy. in particular, as lms have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. as lms become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. to help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on lm privacy. we (i) identify a taxonomy of salient dimensions where attacks differ on lms, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and areas for concern.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.12162" target="_blank">Ai Potentiality and Awareness: A Position Paper From the Perspective of Human-Ai Teaming in Cybersecurity</a></div>
<div class="paper-author">Iqbal H. Sarker, Helge Janicke, Nazeeruddin Mohammad, Paul Watters, Surya Nepal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this position paper explores the broad landscape of ai potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., "human-ai" teaming. as artificial intelligence (ai) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. however, the successful deployment of ai into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. towards this, we emphasize the importance of a balanced approach that incorporates ai's computational power with human expertise. ai systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. human experts can explain ai-generated decisions to stakeholders, regulators, and end-users in critical situations, ensuring responsibility and accountability, which helps establish trust in ai-driven security solutions. therefore, in this position paper, we argue that human-ai teaming is worthwhile in cybersecurity, in which human expertise such as intuition, critical thinking, or contextual understanding is combined with ai's computational power to improve overall cyber defenses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14921" target="_blank">A Democratic Platform for Engaging With Disabled Community in Generative Ai Development</a></div>
<div class="paper-author">Deepak Giri, Erin Brady</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) systems, especially generative ai technologies are becoming more relevant in our society. tools like chatgpt are being used by members of the disabled community e.g., autistic people may use it to help compose emails. the growing impact and popularity of generative ai tools have prompted us to examine their relevance within the disabled community. the design and development phases often neglect this marginalized group, leading to inaccurate predictions and unfair discrimination directed towards them. this could result from bias in data sets, algorithms, and systems at various phases of creation and implementation. this workshop paper proposes a platform to involve the disabled community while building generative ai systems. with this platform, our aim is to gain insight into the factors that contribute to bias in the outputs generated by generative ai when used by the disabled community. furthermore, we expect to comprehend which algorithmic factors are the main contributors to the output's incorrectness or irrelevancy. the proposed platform calls on both disabled and non-disabled people from various geographical and cultural backgrounds to collaborate asynchronously and remotely in a democratic approach to decision-making.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15025" target="_blank">Large Language Model Alignment: A Survey</a></div>
<div class="paper-author">Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent years have witnessed remarkable progress made in large language models (llms). such advancements, while garnering significant attention, have concurrently elicited various concerns. the potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.   this survey endeavors to furnish an extensive exploration of alignment methodologies designed for llms, in conjunction with the extant capability research in this domain. adopting the lens of ai alignment, we categorize the prevailing methods and emergent proposals for the alignment of llms into outer and inner alignment. we also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. to assess llm alignment, we present a wide variety of benchmarks and evaluation methodologies. after discussing the state of alignment research for llms, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.   our aspiration for this survey extends beyond merely spurring research interests in this realm. we also envision bridging the gap between the ai alignment research community and the researchers engrossed in the capability exploration of llms for both capable and safe llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15337" target="_blank">Beyond the Chat: Executable and Verifiable Text-Editing With LLMS</a></div>
<div class="paper-author">Philippe Laban, Jesse Vig, Marti A. Hearst, Caiming Xiong, Chien-Sheng Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: conversational interfaces powered by large language models (llms) have recently become a popular way to obtain feedback during document editing. however, standard chat-based conversational interfaces do not support transparency and verifiability of the editing changes that they suggest. to give the author more agency when editing with an llm, we present inksync, an editing interface that suggests executable edits directly within the document being edited. because llms are known to introduce factual errors, inksync also supports a 3-stage approach to mitigate this risk: warn authors when a suggested edit introduces new information, help authors verify the new information's accuracy through external search, and allow an auditor to perform an a-posteriori verification by auditing the document via a trace of all auto-generated content. two usability studies confirm the effectiveness of inksync's components when compared to standard llm-based chat interfaces, leading to more accurate, more efficient editing, and improved user experience.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15840" target="_blank">How to Catch an Ai Liar: Lie Detection in Black-Box LLMS by Asking Unrelated Questions</a></div>
<div class="paper-author">Lorenzo Pacchiardi, Alex J. Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y. Pan, Yarin Gal, Owain Evans, Jan Brauner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can "lie", which we define as outputting false statements despite "knowing" the truth in a demonstrable sense. llms might "lie", for example, when instructed to output misinformation. here, we develop a simple lie detector that requires neither access to the llm's activations (black-box) nor ground-truth knowledge of the fact in question. the detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the llm's yes/no answers into a logistic regression classifier. despite its simplicity, this lie detector is highly accurate and surprisingly general. when trained on examples from a single setting -- prompting gpt-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other llm architectures, (2) llms fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. these results indicate that llms have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14517" target="_blank">Watch Your Language: Large Language Models and Content Moderation</a></div>
<div class="paper-author">Deepak Kumar, Yousef Abuhashem, Zakir Durumeric</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exploded in popularity due to their ability to perform a wide array of natural language tasks. text-based content moderation is one llm use case that has received recent enthusiasm, however, there is little research investigating how llms perform in content moderation settings. in this work, we evaluate a suite of modern, commercial llms (gpt-3, gpt-3.5, gpt-4) on two common content moderation tasks: rule-based community moderation and toxic content detection. for rule-based community moderation, we construct 95 llm moderation-engines prompted with rules from 95 reddit subcommunities and find that llms can be effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. for toxicity detection, we find that llms significantly outperform existing commercially available toxicity classifiers. however, we also find that recent increases in model size add only marginal benefit to toxicity detection, suggesting a potential performance plateau for llms on toxicity detection tasks. we conclude by outlining avenues for future work in studying llms and content moderation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15817" target="_blank">Identifying the Risks of Lm Agents With an Lm-Emulated Sandbox</a></div>
<div class="paper-author">Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in language model (lm) agents and tool use, exemplified by applications like chatgpt plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. as tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. to address these challenges, we introduce toolemu: a framework that uses an lm to emulate tool execution and enables the testing of lm agents against a diverse range of tools and scenarios, without manual instantiation. alongside the emulator, we develop an lm-based automatic safety evaluator that examines agent failures and quantifies associated risks. we test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with toolemu would be valid real-world agent failures. using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current lm agents and identify numerous failures with potentially severe outcomes. notably, even the safest lm agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer lm agents for real-world deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.15847" target="_blank">Disinformation Detection: An Evolving Challenge in the Age of LLMS</a></div>
<div class="paper-author">Bohan Jiang, Zhen Tan, Ayushi Nirmal, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advent of generative large language models (llms) such as chatgpt has catalyzed transformative advancements across multiple domains. however, alongside these advancements, they have also introduced potential threats. one critical concern is the misuse of llms by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. this work aims to address this issue by answering three research questions: (1) to what extent can the current disinformation detection technique reliably detect llm-generated disinformation? (2) if traditional techniques prove less effective, can llms themself be exploited to serve as a robust defense against advanced disinformation? and, (3) should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? a holistic exploration for the formation and detection of disinformation is conducted to foster this line of research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13633" target="_blank">Evallm: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria</a></div>
<div class="paper-author">Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: by simply composing prompts, developers can prototype novel generative applications with large language models (llms). to refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. formative interviews (n=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. we present evallm, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. by describing criteria in natural language, users can employ the system's llm-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. a comparative study (n=12) showed that evallm, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13788" target="_blank">Can LLM-Generated Misinformation Be Detected?</a></div>
<div class="paper-author">Canyu Chen, Kai Shu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advent of large language models (llms) has made a transformative impact. however, the potential that llms such as chatgpt can be exploited to generate misinformation has posed a serious concern to online safety and public trust. a fundamental research question is: will llm-generated misinformation cause more harm than human-written misinformation? we propose to tackle this question from the perspective of detection difficulty. we first build a taxonomy of llm-generated misinformation. then we categorize and validate the potential real-world methods for generating misinformation with llms. then, through extensive empirical investigation, we discover that llm-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. we also discuss the implications of our discovery on combating misinformation in the age of llms and the countermeasures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14381" target="_blank">Survey of Social Bias in Vision-Language Models</a></div>
<div class="paper-author">Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, the rapid advancement of machine learning (ml) models, particularly transformer-based pre-trained models, has revolutionized natural language processing (nlp) and computer vision (cv) fields. however, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. addressing these biases and ensuring fairness in artificial intelligence (ai) systems has become a critical concern in the ml community.   the recent introduction of pre-trained vision-and-language (vl) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. although vl models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in nlp and cv. this survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across nlp, cv, and vl. by examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. the findings and recommendations presented here can benefit the ml community, fostering the development of fairer and non-biased ai models in various applications and research endeavors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13256" target="_blank">Defending Pre-Trained Language Models as Few-Shot Learners Against Backdoor Attacks</a></div>
<div class="paper-author">Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models (plms) have demonstrated remarkable performance as few-shot learners. however, their security risks under such settings are largely unexplored. in this work, we conduct a pilot study showing that plms as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. to address such challenges, we advocate mdp, a novel lightweight, pluggable, and effective defense for plms as few-shot learners. specifically, mdp leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. we show analytically that mdp creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. the empirical evaluation using benchmark datasets and representative attacks validates the efficacy of mdp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13308" target="_blank">Calibrating LLM-Based Evaluator</a></div>
<div class="paper-author">Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) on language modeling and emergent capabilities make them a promising reference-free evaluator of natural language generation quality, and a competent alternative to human evaluation. however, hindered by the closed-source or high computational demand to host and tune, there is a lack of practice to further calibrate an off-the-shelf llm-based evaluator towards better human alignment. in this work, we propose autocalibrate, a multi-stage, gradient-free approach to automatically calibrate and align an llm-based evaluator toward human preference. instead of explicitly modeling human preferences, we first implicitly encompass them within a set of human labels. then, an initial set of scoring criteria is drafted by the language model itself, leveraging in-context learning on different few-shot examples. to further calibrate this set of criteria, we select the best performers and re-draft them with self-refinement. our experiments on multiple text quality evaluation datasets illustrate a significant improvement in correlation with expert evaluation through calibration. our comprehensive qualitative analysis conveys insightful intuitions and observations on the essence of effective scoring criteria.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.13322" target="_blank">From Text to Source: Results in Detecting Large Language Model-Generated Content</a></div>
<div class="paper-author">Wissam Antoun, Benoît Sagot, Djamé Seddah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the widespread use of large language models (llms), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. addressing these concerns necessitates the development of robust methods to detect and attribute text generated by llms. this paper investigates "cross-model detection," evaluating whether a classifier trained to distinguish between source llm-generated and human-written text can also detect text from a target llm without further training. the study comprehensively explores various llm sizes and families, and assesses the impact of conversational fine-tuning techniques on classifier generalization. the research also delves into model attribution, encompassing source model identification, model family classification, and model size classification. our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger llms being more challenging to detect, especially when the classifier is trained on data from smaller models. training on data from similarly sized llms can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in llm-generated text. overall, our study contributes valuable insights into the interplay of model size, family, and training data in llm detection and attribution.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12941" target="_blank">Trusta: Reasoning About Assurance Cases With Formal Methods and Large Language Models</a></div>
<div class="paper-author">Zezhong Chen, Yuxin Deng, Wenjie Du</div>
<div class="abstract">
<div class="abstract-content">
Abstract: assurance cases can be used to argue for the safety of products in safety engineering. in safety-critical areas, the construction of assurance cases is indispensable. trustworthiness derivation trees (tdts) enhance assurance cases by incorporating formal methods, rendering it possible for automatic reasoning about assurance cases. we present trustworthiness derivation tree analyzer (trusta), a desktop application designed to automatically construct and verify tdts. the tool has a built-in prolog interpreter in its backend, and is supported by the constraint solvers z3 and mona. therefore, it can solve constraints about logical formulas involving arithmetic, sets, horn clauses etc. trusta also utilizes large language models to make the creation and evaluation of assurance cases more convenient. it allows for interactive human examination and modification. we evaluated top language models like chatgpt-3.5, chatgpt-4, and palm 2 for generating assurance cases. our tests showed a 50%-80% similarity between machine-generated and human-created cases. in addition, trusta can extract formal constraints from text in natural languages, facilitating an easier interpretation and validation process. this extraction is subject to human review and correction, blending the best of automated efficiency with human insight. to our knowledge, this marks the first integration of large language models in automatic creating and reasoning about assurance cases, bringing a novel approach to a traditional challenge. through several industrial case studies, trusta has proven to quickly find some subtle issues that are typically missed in manual inspection, demonstrating its practical value in enhancing the assurance case development process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.10669" target="_blank">Unbiased Watermark for Large Language Models</a></div>
<div class="paper-author">Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent advancements in large language models (llms) have sparked a growing apprehension regarding the potential misuse. one approach to mitigating this risk is to incorporate watermarking techniques into llms, allowing for the tracking and attribution of model outputs. this study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. previous studies have suggested a trade-off between watermark strength and output quality. however, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. we refer to this type of watermark as an unbiased watermark. this has significant implications for the use of llms, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. our findings contribute to the ongoing discussion around responsible ai development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11830" target="_blank">A Chinese Prompt Attack Dataset for LLMS With Evil Content</a></div>
<div class="paper-author">Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) present significant priority in text understanding and generation. however, llms suffer from the risk of generating harmful contents especially while being employed to applications. there are several black-box attack methods, such as prompt attack, which can change the behaviour of llms and induce llms to generate unexpected answers with harmful contents. researchers are interested in prompt attack and defense with llms, while there is no publicly available dataset to evaluate the abilities of defending prompt attack. in this paper, we introduce a chinese prompt attack dataset for llms, called cpad. our prompts aim to induce llms to generate unexpected outputs with several carefully designed prompt attack approaches and widely concerned attacking contents. different from previous datasets involving safety estimation, we construct the prompts considering three dimensions: contents, attacking methods and goals, thus the responses can be easily evaluated and analysed. we run several well-known chinese llms on our dataset, and the results show that our prompts are significantly harmful to llms, with around 70% attack success rate. we will release cpad to encourage further studies on prompt attack and defense.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11852" target="_blank">Knowledge Sanitization of Large Language Models</a></div>
<div class="paper-author">Yoichi Ishibashi, Hidetoshi Shimodaira</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (llms). llms trained on a large corpus of web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. our technique fine-tunes these models, prompting them to generate harmless responses such as ``i don't know'' when queried about specific information. experimental results in a closed-book question-answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of llm. these two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11896" target="_blank">Focal Inferential Infusion Coupled With Tractable Density Discrimination for Implicit Hate Speech Detection</a></div>
<div class="paper-author">Sarah Masud, Ashutosh Bajpai, Tanmoy Chakraborty</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although pre-trained large language models (plms) have achieved state-of-the-art on many nlp tasks, they lack understanding of subtle expressions of implicit hate speech. such nuanced and implicit hate is often misclassified as non-hate. various attempts have been made to enhance the detection of (implicit) hate content by augmenting external context or enforcing label separation via distance-based metrics. we combine these two approaches and introduce fiadd, a novel focused inferential adaptive density discrimination framework. fiadd enhances the plm finetuning pipeline by bringing the surface form of an implicit hate speech closer to its implied form while increasing the inter-cluster distance among various class labels. we test fiadd on three implicit hate datasets and observe significant improvement in the two-way and three-way hate classification tasks. we further experiment on the generalizability of fiadd on three other tasks, namely detecting sarcasm, irony, and stance, in which surface and implied forms differ, and observe similar performance improvement. we analyze the generated latent space to understand its evolution under fiadd, which corroborates the advantage of employing fiadd for implicit hate speech detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.03031" target="_blank">How Prevalent Is Gender Bias in Chatgpt? -- Exploring German and English Chatgpt Responses</a></div>
<div class="paper-author">Stefanie Urchs, Veronika Thurner, Matthias Aßenmacher, Christian Heumann, Stephanie Thiemichen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the introduction of chatgpt, openai made large language models (llm) accessible to users with limited it expertise. however, users with no background in natural language processing (nlp) might lack a proper understanding of llms. thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. in this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. we explore how chatgpt reacts in english and german if prompted to answer from a female, male, or neutral perspective. in an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. on this basis, we show that chatgpt is indeed useful for helping non-it users draft texts for their daily work. however, it is absolutely crucial to thoroughly check the system's responses for biases as well as for syntactic and grammatical mistakes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11166" target="_blank">Are Large Language Models Really Robust to Word-Level Perturbations?</a></div>
<div class="paper-author">Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the swift advancement in the scales and capabilities of large language models (llms) positions them as promising tools for a variety of downstream tasks. in addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the llm, much attention is drawn to the robustness of llms. however, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary llms. to address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by llms, which we refer to as the reward model for reasonable robustness evaluation (treval). longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters, which may exhibit oversimplification and inherent biases. our extensive empirical experiments demonstrate that treval provides an innovative method for evaluating the robustness of an llm. furthermore, our results demonstrate that llms frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage. notably, we are surprised to discover that robustness tends to decrease as fine-tuning (sft and rlhf) is conducted. the code of treval is available in https://github.com/harry-mic/treval.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11235" target="_blank">Openchat: Advancing Open-Source Language Models With Mixed-Quality Data</a></div>
<div class="paper-author">Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nowadays, open-source large language models like llama have emerged. recent developments have incorporated supervised fine-tuning (sft) and reinforcement learning fine-tuning (rlft) to align these models with human goals. however, sft methods treat all training data with mixed quality equally, while rlft methods require high-quality pairwise or ranking-based preference data. in this study, we present a novel framework, named openchat, to advance open-source language models with mixed-quality data. specifically, we consider the general sft training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. we propose the c(onditioned)-rlft, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. interestingly, the optimal policy in c-rlft can be easily solved through single-stage, rl-free supervised learning, which is lightweight and avoids costly human preference labeling. through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with c-rlft achieves the highest average performance among all 13b open-source language models. moreover, we use agieval to validate the model generalization performance, in which only openchat-13b surpasses the base model. finally, we conduct a series of analyses to shed light on the effectiveness and robustness of openchat. our code, data, and models are publicly available at https://github.com/imoneoi/openchat.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11495" target="_blank">Chain-of-Verification Reduces Hallucination in Large Language Models</a></div>
<div class="paper-author">Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. we study the ability of language models to deliberate on the responses they give in order to correct their mistakes. we develop the chain-of-verification (cove) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. in experiments, we show cove decreases hallucinations across a variety of tasks, from list-based questions from wikidata, closed book multispanqa and longform text generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11575" target="_blank">Distilling Adversarial Prompts From Safety Benchmarks: Report for the Adversarial Nibbler Challenge</a></div>
<div class="paper-author">Manuel Brack, Patrick Schramowski, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text-conditioned image generation models have recently achieved astonishing image quality and alignment results. consequently, they are employed in a fast-growing number of applications. since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. as a contribution to the adversarial nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11653" target="_blank">"It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents</a></div>
<div class="paper-author">Zhiping Zhang, Michelle Jia, N/A Hao-Ping, N/A Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, Tianshi Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the widespread use of large language model (llm)-based conversational agents (cas), especially in high-stakes domains, raises many privacy concerns. building ethical llm-based cas that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. however, existing research, primarily model-centered, does not provide insight into users' perspectives. to bridge this gap, we analyzed sensitive disclosures in real-world chatgpt conversations and conducted semi-structured interviews with 19 llm-based ca users. we found that users are constantly faced with trade-offs between privacy, utility, and convenience when using llm-based cas. however, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. we discuss practical design guidelines and the needs for paradigmatic shifts to protect the privacy of llm-based ca users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11751" target="_blank">How Robust Is Google's Bard to Adversarial Image Attacks?</a></div>
<div class="paper-author">Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: multimodal large language models (mllms) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. however, due to the unsolved adversarial robustness problem of vision models, mllms can have more severe safety and security risks by introducing the vision inputs. in this work, we study the adversarial robustness of google's bard, a competitive chatbot to chatgpt that released its multimodal capability recently, to better understand the vulnerabilities of commercial mllms. by attacking white-box surrogate vision encoders or mllms, the generated adversarial examples can mislead bard to output wrong image descriptions with a 22% success rate based solely on the transferability. we show that the adversarial examples can also attack other mllms, e.g., a 26% attack success rate against bing chat and a 86% attack success rate against ernie bot. moreover, we identify two defense mechanisms of bard, including face detection and toxicity detection of images. we design corresponding attacks to evade these defenses, demonstrating that the current defenses of bard are also vulnerable. we hope this work can deepen our understanding on the robustness of mllms and facilitate future research on defenses. our code is available at https://github.com/thu-ml/attack-bard.   update: gpt-4v is available at october 2023. we further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.11765" target="_blank">Privacy-Preserving in-Context Learning With Differentially Private Few-Shot Generation</a></div>
<div class="paper-author">Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we study the problem of in-context learning (icl) with large language models (llms) on private datasets. this scenario poses privacy risks, as llms may leak or regurgitate the private examples demonstrated in the prompt. we propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (dp) guarantees, and show empirically that it can achieve effective icl. we conduct extensive experiments on standard benchmarks and compare our algorithm with non-private icl and zero-shot solutions. our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. these results open up new possibilities for icl with privacy protection for a broad range of applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10318" target="_blank">Who to Trust, How and Why: Untangling Ai Ethics Principles, Trustworthiness and Trust</a></div>
<div class="paper-author">Andreas Duenser, David M. Douglas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present an overview of the literature on trust in ai and ai trustworthiness and argue for the need to distinguish these concepts more clearly and to gather more empirically evidence on what contributes to people s trusting behaviours. we discuss that trust in ai involves not only reliance on the system itself, but also trust in the developers of the ai system. ai ethics principles such as explainability and transparency are often assumed to promote user trust, but empirical evidence of how such features actually affect how users perceive the system s trustworthiness is not as abundance or not that clear. ai systems should be recognised as socio-technical systems, where the people involved in designing, developing, deploying, and using the system are as important as the system for determining whether it is trustworthy. without recognising these nuances, trust in ai and trustworthy ai risk becoming nebulous terms for any desirable feature for ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10346" target="_blank">Explaining Agent Behavior With Large Language Models</a></div>
<div class="paper-author">Xijia Zhang, Yue Guo, Simon Stepputtis, Katia Sycara, Joseph Campbell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. it is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. we propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. we show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10463" target="_blank">Exploring the Dark Side of Ai: Advanced Phishing Attack Design and Deployment Using Chatgpt</a></div>
<div class="paper-author">Nils Begou, Jeremy Vinoy, Andrzej Duda, Maciej Korczynski</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper explores the possibility of using chatgpt to develop advanced phishing attacks and automate their large-scale deployment. we make chatgpt generate the following parts of a phishing attack: i) cloning a targeted website, ii) integrating code for stealing credentials, iii) obfuscating code, iv) automating website deployment on a hosting provider, v) registering a phishing domain name, and vi) integrating the website with a reverse proxy. the initial assessment of the automatically generated phishing kits highlights their rapid generation and deployment process as well as the close resemblance of the resulting pages to the target website. more broadly, we demonstrate that recent advances in ai underscore the potential risks of its misuse in phishing attacks, which can lead to their increased prevalence and severity. this highlights the necessity for enhanced countermeasures within ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09749" target="_blank">Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation</a></div>
<div class="paper-author">Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nsfw (not safe for work) content, in the context of a dialogue, can have severe side effects on users in open-domain dialogue systems. however, research on detecting nsfw language, especially sexually explicit content, within a dialogue context has significantly lagged behind. to address this issue, we introduce censorchat, a dialogue monitoring dataset aimed at nsfw dialogue detection. leveraging knowledge distillation techniques involving gpt-4 and chatgpt, this dataset offers a cost-effective means of constructing nsfw content detectors. the process entails collecting real-life human-machine interaction data and breaking it down into single utterances and single-turn dialogues, with the chatbot delivering the final utterance. chatgpt is employed to annotate unlabeled data, serving as a training set. rationale validation and test sets are constructed using chatgpt and gpt-4 as annotators, with a self-criticism strategy for resolving discrepancies in labeling. a bert model is fine-tuned as a text classifier on pseudo-labeled data, and its performance is assessed. the study emphasizes the importance of ai systems prioritizing user safety and well-being in digital conversations while respecting freedom of expression. the proposed approach not only advances nsfw content detection but also aligns with evolving user protection needs in ai-driven dialogues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09825" target="_blank">Bias of Ai-Generated Content: An Examination of News Produced by Large Language Models</a></div>
<div class="paper-author">Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have the potential to transform our lives and work through the content they generate, known as ai-generated content (aigc). to harness this transformation, we need to understand the limitations of llms. here, we investigate the bias of aigc produced by seven representative llms, including chatgpt and llama. we collect news articles from the new york times and reuters, both known for their dedication to provide unbiased news. we then apply each examined llm to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the aigc produced by the llm by comparing the aigc and the original news articles. we further analyze the gender bias of each llm under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. our study reveals that the aigc produced by each examined llm demonstrates substantial gender and racial biases. moreover, the aigc generated by each llm exhibits notable discrimination against females and individuals of the black race. among the llms, the aigc generated by chatgpt demonstrates the lowest level of bias, and chatgpt is the sole model capable of declining content generation when provided with biased prompts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09919" target="_blank">Plug in the Safety Chip: Enforcing Constraints for LLM-Driven Robot Agents</a></div>
<div class="paper-author">Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models (llms) have enabled a new research domain, llm agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of llms obtained during pretraining. however, while considerable effort has been made to teach the robot the "dos," the "don'ts" received relatively less attention. we argue that, for any practical usage, it is as crucial to teach the robot the "don'ts": conveying explicit instructions about prohibited actions, assessing the robot's comprehension of these restrictions, and, most importantly, ensuring compliance. moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as iso 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. aiming at deploying the llm agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (ltl) that simultaneously enables natural language (nl) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. to demonstrate the effectiveness of our system, we conducted experiments in virtualhome environment and on a real robot. the experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10105" target="_blank">Understanding Catastrophic Forgetting in Language Models via Implicit Inference</a></div>
<div class="paper-author">Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. however, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. in a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. this degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. we hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. to test this hypothesis, we propose conjugate prompting to see if we can recover pretrained capabilities. conjugate prompting artificially makes the task look farther from the fine-tuning distribution while requiring the same capability. we find that conjugate prompting systematically recovers some of the pretraining capabilities on our synthetic setup. we then apply conjugate prompting to real-world llms using the observation that fine-tuning distributions are typically heavily skewed towards english. we find that simply translating the prompts to different languages can cause the fine-tuned models to respond like their pretrained counterparts instead. this allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, to recover harmful content generation suppressed by safety fine-tuning in chatbots like chatgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10238" target="_blank">Policygpt: Automated Analysis of Privacy Policies With Large Language Models</a></div>
<div class="paper-author">Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. however, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. in practical use, users tend to click the agree button directly rather than reading them carefully. this practice exposes users to risks of privacy leakage and legal issues. recently, the advent of large language models (llm) such as chatgpt and gpt-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. in this study, we investigate a privacy policy text analysis framework policygpt based on the llm. this framework was tested using two datasets. the first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. the second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. under zero-shot learning conditions, policygpt demonstrated robust performance. for the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10253" target="_blank">Gptfuzzer: Red Teaming Large Language Models With Auto-Generated Jailbreak Prompts</a></div>
<div class="paper-author">Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have recently experienced tremendous popularity and are widely used from casual conversations to ai-driven programming. however, despite their considerable success, llms are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. while safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit llms to produce harmful content. these jailbreak templates are typically manually crafted, making large-scale testing challenging.   in this paper, we introduce gptfuzz, a novel black-box jailbreak fuzzing framework inspired by the afl fuzzing framework. instead of manual engineering, gptfuzz automates the generation of jailbreak templates for red-teaming llms. at its core, gptfuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. we detail three key components of gptfuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack.   we evaluate gptfuzz against various commercial and open-source llms, including chatgpt, llama-2, and vicuna, under diverse attack scenarios. our results indicate that gptfuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. remarkably, gptfuzz achieves over 90% attack success rates against chatgpt and llama-2 models, even with suboptimal initial seed templates. we anticipate that gptfuzz will be instrumental for researchers and practitioners in examining llm robustness and will encourage further exploration into enhancing llm safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.10254" target="_blank">LLM Platform Security: Applying a Systematic Evaluation Framework to Openai's Chatgpt Plugins</a></div>
<div class="paper-author">Umar Iqbal, Tadayoshi Kohno, Franziska Roesner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language model (llm) platforms, such as chatgpt, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. while these plugins extend the capabilities of llm platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. plugins also interface with llm platforms and users using natural language, which can have imprecise interpretations. in this paper, we propose a framework that lays a foundation for llm platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated llm platforms. our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how llm platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. as part of our iterative process, we apply our framework in the context of openai's plugin ecosystem. we uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. we conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future llm-based computing platforms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09362" target="_blank">Language Models Are Susceptible to Incorrect Patient Self-Diagnosis in Medical Applications</a></div>
<div class="paper-author">Rojin Ziaei, Samuel Schmidgall</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are becoming increasingly relevant as a potential tool for healthcare, aiding communication between clinicians, researchers, and patients. however, traditional evaluations of llms on medical exam questions do not reflect the complexity of real patient-doctor interactions. an example of this complexity is the introduction of patient self-diagnosis, where a patient attempts to diagnose their own medical conditions from various sources. while the patient sometimes arrives at an accurate conclusion, they more often are led toward misdiagnosis due to the patient's over-emphasis on bias validating information. in this work we present a variety of llms with multiple-choice questions from united states medical board exams which are modified to include self-diagnostic reports from patients. our findings highlight that when a patient proposes incorrect bias-validating information, the diagnostic accuracy of llms drop dramatically, revealing a high susceptibility to errors in self-diagnosis.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09401" target="_blank">Chatgpt Hallucinates When Attributing Answers</a></div>
<div class="paper-author">Guido Zuccon, Bevan Koopman, Razia Shaik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: can chatgpt provide evidence to support its answers? does the evidence it suggests actually exist and does it really support its answer? we investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting chatgpt to provide both an answer and supporting evidence in the form of references to external sources. we also investigate how different prompts impact answers and evidence. we find that chatgpt provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. we further provide insights on the generated references that reveal common traits among the references that chatgpt generates, and show how even if a reference provided by the model does exist, this reference often does not support the claims chatgpt attributes to it. our findings are important because (1) they are the first systematic analysis of the references created by chatgpt in its answers; (2) they suggest that the model may leverage good quality information in producing correct answers, but is unable to attribute real evidence to support its answers. prompts, raw result files and manual analysis are made publicly available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14348" target="_blank">Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM</a></div>
<div class="paper-author">Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, large language models (llms) have made significant advancements and are now widely used across various domains. unfortunately, there has been a rising concern that llms can be misused to generate harmful or malicious content. though a line of research has focused on aligning llms with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. in this work, we introduce a robustly aligned llm (ra-llm) to defend against potential alignment-breaking attacks. ra-llm can be directly constructed upon an existing aligned llm with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original llm. furthermore, we also provide a theoretical analysis for ra-llm to verify its effectiveness in defending against alignment-breaking attacks. through real-world experiments on open-source large language models, we demonstrate that ra-llm can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100\% to around 10\% or less.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08902" target="_blank">Investigating Subtler Biases in Llms: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models</a></div>
<div class="paper-author">Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: llms are increasingly powerful and widely used to assist users in a variety of tasks. this use risks the introduction of llm biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. bias in nlp systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., asians are good at math). in this paper, we investigate bias along less studied, but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that llms (specially autoregressive language models) make between social groups and unrelated positive and negative attributes. we ask whether llms hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the ``what is beautiful is good'' bias found in people in experimental psychology. we introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group. we also reverse the completion task to select the social group based on an attribute. finally, we report the correlations that we find for multiple cutting-edge llms. this dataset can be used as a benchmark to evaluate progress in more generalized biases and the templating technique can be used to expand the benchmark with minimal additional human annotation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09092" target="_blank">The Impact of Debiasing on the Performance of Language Models in Downstream Tasks Is Underestimated</a></div>
<div class="paper-author">Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models trained on large-scale data have learned serious levels of social biases. consequently, various methods have been proposed to debias pre-trained models. debiasing methods need to mitigate only discriminatory bias information from the pre-trained models, while retaining information that is useful for the downstream tasks. in previous research, whether useful information is retained has been confirmed by the performance of downstream tasks in debiased pre-trained models. on the other hand, it is not clear whether these benchmarks consist of data pertaining to social biases and are appropriate for investigating the impact of debiasing. for example in gender-related social biases, data containing female words (e.g. ``she, female, woman''), male words (e.g. ``he, male, man''), and stereotypical words (e.g. ``nurse, doctor, professor'') are considered to be the most affected by debiasing. if there is not much data containing these words in a benchmark dataset for a target task, there is the possibility of erroneously evaluating the effects of debiasing. in this study, we compare the impact of debiasing on performance across multiple downstream tasks using a wide-range of benchmark datasets that containing female, male, and stereotypical words. experiments show that the effects of debiasing are consistently \emph{underestimated} across all tasks. moreover, the effects of debiasing could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.09120" target="_blank">Public Perceptions of Gender Bias in Large Language Models: Cases of Chatgpt and Ernie</a></div>
<div class="paper-author">Kyrie Zhixuan Zhou, Madelyn Rose Sanfilippo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are quickly gaining momentum, yet are found to demonstrate gender bias in their responses. in this paper, we conducted a content analysis of social media discussions to gauge public perceptions of gender bias in llms which are trained in different cultural contexts, i.e., chatgpt, a us-based llm, or ernie, a china-based llm. people shared both observations of gender bias in their personal use and scientific findings about gender bias in llms. a difference between the two llms was seen -- chatgpt was more often found to carry implicit gender bias, e.g., associating men and women with different profession titles, while explicit gender bias was found in ernie's responses, e.g., overly promoting women's pursuit of marriage over career. based on the findings, we reflect on the impact of culture on gender bias and propose governance recommendations to regulate gender bias in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08573" target="_blank">Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias Between India and the West</a></div>
<div class="paper-author">Khyati Khandelwal, Manuel Tonneau, Andrew M. Bean, Hannah Rose Kirk, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), now used daily by millions of users, can encode societal biases, exposing their users to representational harms. a large body of scholarship on llm bias exists but it predominantly adopts a western-centric frame and attends comparatively less to bias levels and potential harms in the global south. in this paper, we quantify stereotypical bias in popular llms according to an indian-centric frame and compare bias levels between the indian and western contexts. to do this, we develop a novel dataset which we call indian-bhed (indian bias evaluation dataset), containing stereotypical and anti-stereotypical examples for caste and religion contexts. we find that the majority of llms tested are strongly biased towards stereotypes in the indian context, especially as compared to the western context. we finally investigate instruction prompting as a simple intervention to mitigate such bias and find that it significantly reduces both stereotypical and anti-stereotypical biases in the majority of cases for gpt-3.5. the findings of this work highlight the need for including more diverse voices when evaluating llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08674" target="_blank">Fake News Detectors Are Biased Against Texts Generated by Large Language Models</a></div>
<div class="paper-author">Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, Preslav Nakov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of fake news has emerged as a critical challenge, undermining trust and posing threats to society. in the era of large language models (llms), the capability to generate believable fake content has intensified these concerns. in this study, we present a novel paradigm to evaluate fake news detectors in scenarios involving both human-written and llm-generated misinformation. intriguingly, our findings reveal a significant bias in many existing detectors: they are more prone to flagging llm-generated content as fake news while often misclassifying human-written fake news as genuine. this unexpected bias appears to arise from distinct linguistic patterns inherent to llm outputs. to address this, we introduce a mitigation strategy that leverages adversarial training with llm-paraphrased genuine news. the resulting model yielded marked improvements in detection accuracy for both human and llm-generated news. to further catalyze research in this domain, we release two comprehensive datasets, \texttt{gossipcop++} and \texttt{politifact++}, thus amalgamating human-validated articles with llm-generated fake and real news.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08836" target="_blank">Bias and Fairness in Chatbots: An Overview</a></div>
<div class="paper-author">Jintang Xue, Yun-Cheng Wang, Chengwei Wei, Xiaofeng Liu, Jonghye Woo, C. -C. Jay Kuo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatbots have been studied for more than half a century. with the rapid development of natural language processing (nlp) technologies in recent years, chatbots using large language models (llms) have received much attention nowadays. compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. there are however, bias and fairness concerns in modern chatbot design. due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. the history of chatbots and their categories are first reviewed. then, bias sources and potential harms in applications are analyzed. considerations in designing fair and unbiased chatbot systems are examined. finally, future research directions are discussed.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07601" target="_blank">Detecting Misinformation With LLM-Predicted Credibility Signals and Weak Supervision</a></div>
<div class="paper-author">João A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, Carolina Scarton</div>
<div class="abstract">
<div class="abstract-content">
Abstract: credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. this paper investigates whether large language models (llms) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. we then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. we demonstrate that our approach, which combines zero-shot llm credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. we also analyse the contribution of the individual credibility signals towards predicting content veracity, which provides new valuable insights into their role in misinformation detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07875" target="_blank">Safety-Tuned Llamas: Lessons From Improving the Safety of Large Language Models That Follow Instructions</a></div>
<div class="paper-author">Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: training large language models to follow instructions makes them perform better on a wide range of tasks, generally becoming more helpful. however, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. in this paper, we raise concerns over the safety of models that only emphasize helpfulness, not safety, in their instruction-tuning. we show that several popular instruction-tuned models are highly unsafe. moreover, we show that adding just 3% safety examples (a few hundred demonstrations) in the training set when fine-tuning a model like llama can substantially improve their safety. our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. however, we do find a behavior of exaggerated safety, where too much safety-tuning makes models refuse to respond to reasonable prompts that superficially resemble unsafe ones. our study sheds light on trade-offs in training llms to follow instructions and exhibit safe behavior.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08047" target="_blank">Investigating Gender Bias in News Summarization</a></div>
<div class="paper-author">Julius Steen, Katja Markert</div>
<div class="abstract">
<div class="abstract-content">
Abstract: summarization is an important application of large language models (llms). most previous evaluation of summarization models has focused on their performance in content selection, grammaticality and coherence. however, it is well known that llms reproduce and reinforce harmful social biases. this raises the question: do these biases affect model outputs in a relatively constrained setting like summarization?   to help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical measures to quantify them. since we find biases inherent to the input document can confound our analysis, we additionally propose a method to generate input documents with carefully controlled demographic attributes. this allows us to sidestep this issue, while still working with somewhat realistic input documents.   finally, we apply our measures to summaries generated by both purpose-built summarization models and general purpose chat models. we find that content selection in single document summarization seems to be largely unaffected by bias, while hallucinations exhibit evidence of biases propagating to generated summaries.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08121" target="_blank">"I'm Not Confident in Debiasing Ai Systems Since I Know Too Little": Teaching Ai Creators About Gender Bias Through Hands-on Tutorials</a></div>
<div class="paper-author">Kyrie Zhixuan Zhou, Jiaxun Cao, Xiaowen Yuan, Daniel E. Weissglass, Zachary Kilhoffer, Madelyn Rose Sanfilippo, Xin Tong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias is rampant in ai systems, causing bad user experience, injustices, and mental harm to women. school curricula fail to educate ai creators on this topic, leaving them unprepared to mitigate gender bias in ai. in this paper, we designed hands-on tutorials to raise ai creators' awareness of gender bias in ai and enhance their knowledge of sources of gender bias and debiasing techniques. the tutorials were evaluated with 18 ai creators, including ai researchers, ai industrial practitioners (i.e., developers and product managers), and students who had learned ai. their improved awareness and knowledge demonstrated the effectiveness of our tutorials, which have the potential to complement the insufficient ai gender bias education in cs/ai courses. based on the findings, we synthesize design implications and a rubric to guide future research, education, and design efforts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.06794" target="_blank">Cognitive Mirage: A Review of Hallucinations in Large Language Models</a></div>
<div class="paper-author">Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models continue to develop in the field of ai, text generation systems are susceptible to a worrisome phenomenon known as hallucination. in this study, we summarize recent compelling insights into hallucinations in llms. we present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. based on this, future research directions are proposed. our contribution are threefold: (1) we provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) we provide theoretical analyses of hallucinations in llms and provide existing detection and improvement methods; (3) we propose several research directions that can be developed in the future. as hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07045" target="_blank">Safetybench: Evaluating the Safety of Large Language Models With Multiple Choice Questions</a></div>
<div class="paper-author">Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid development of large language models (llms), increasing attention has been paid to their safety concerns. consequently, evaluating the safety of llms has become an essential task for facilitating the broad applications of llms. nevertheless, the absence of comprehensive safety evaluation benchmarks poses a significant impediment to effectively assess and enhance the safety of llms. in this work, we present safetybench, a comprehensive benchmark for evaluating the safety of llms, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. notably, safetybench also incorporates both chinese and english data, facilitating the evaluation in both languages. our extensive tests over 25 popular chinese and english llms in both zero-shot and few-shot settings reveal a substantial performance advantage for gpt-4 over its counterparts, and there is still significant room for improving the safety of current llms. we believe safetybench will enable fast and comprehensive evaluation of llms' safety, and foster the development of safer llms. data and evaluation guidelines are available at https://github.com/thu-coai/safetybench. submission entrance and leaderboard are available at https://llmbench.ai/safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07120" target="_blank">Sight Beyond Text: Multi-Modal Training Enhances LLMS in Truthfulness and Ethics</a></div>
<div class="paper-author">Haoqin Tu, Bingchen Zhao, Chen Wei, Cihang Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: multi-modal large language models (mllms) are trained based on large language models (llm), with an enhanced capability to comprehend multi-modal inputs and generate textual responses. while they excel in multi-modal tasks, the pure nlp abilities of mllms are often underestimated and left untested. in this study, we get out of the box and unveil an intriguing characteristic of mllms -- our preliminary results suggest that visual instruction tuning, a prevailing strategy for transitioning llms into mllms, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure nlp context. for example, a visual-instruction-tuned llama2 7b model surpasses the performance of the llama2-chat 7b model, fine-tuned with over one million human annotations, on truthfulqa-mc and ethics benchmarks. further analysis reveals that the improved alignment can be attributed to the superior instruction quality inherent to visual-text data. in releasing our code at github.com/ucsc-vlaa/sight-beyond-text, we aspire to foster further exploration into the intrinsic value of visual-text synergies and, in a broader scope, multi-modal interactions in alignment research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07124" target="_blank">Rain: Your Language Models Can Align Themselves Without Finetuning</a></div>
<div class="paper-author">Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) often demonstrate inconsistencies with human preferences. previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. in contrast, aligning frozen llms without requiring alignment data is more appealing. this work explores the potential of the latter setting. we discover that by integrating self-evaluation and rewind mechanisms, unaligned llms can directly produce responses consistent with human preferences via self-boosting. we introduce a novel inference method, rewindable auto-regressive inference (rain), that allows pre-trained llms to evaluate their own generation and use the evaluation results to guide rewind and generation for ai safety. notably, rain operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. experimental results evaluated by gpt-4 and humans demonstrate the effectiveness of rain: on the hh dataset, rain improves the harmlessness rate of llama 30b from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. on the truthfulqa dataset, rain improves the truthfulness of the already-well-aligned llama-2-chat 13b model by 5%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.07251" target="_blank">In-Contextual Bias Suppression for Large Language Models</a></div>
<div class="paper-author">Daisuke Oba, Masahiro Kaneko, Danushka Bollegala</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite their impressive performance in a wide range of nlp tasks, large language models (llms) have been reported to encode worrying-levels of gender bias. prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of the llms, which are computationally costly. moreover, one might not even have access to the internal parameters for performing debiasing such as in the case of commercially available llms such as gpt-4. to address this challenge we propose bias suppression, a novel alternative to debiasing that does not require access to model parameters. we show that text-based preambles, generated from manually designed templates covering counterfactual statements, can accurately suppress gender biases in llms. moreover, we find that descriptive sentences for occupations can further suppress gender biases. interestingly, we find that bias suppression has a minimal adverse effect on downstream task performance, while effectively mitigating the gender biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05958" target="_blank">The Moral Machine Experiment on Large Language Models</a></div>
<div class="paper-author">Kazuhiro Takemoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) become more deeply integrated into various sectors, understanding how they make moral judgments has become crucial, particularly in the realm of autonomous driving. this study utilized the moral machine framework to investigate the ethical decision-making tendencies of prominent llms, including gpt-3.5, gpt-4, palm 2, and llama 2, comparing their responses to human preferences. while llms' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, palm 2 and llama 2, especially, evidence distinct deviations. additionally, despite the qualitative similarities between the llm and human preferences, there are significant quantitative disparities, suggesting that llms might lean toward more uncompromising decisions, compared to the milder inclinations of humans. these insights elucidate the ethical frameworks of llms and their potential implications for autonomous driving.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05973" target="_blank">Circuit Breaking: Removing Model Behaviors With Targeted Ablation</a></div>
<div class="paper-author">Maximilian Li, Xander Davies, Max Nadeau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. we propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. in the setting of reducing gpt-2 toxic language generation, we find ablating just 12 of the 11.6k causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.08628" target="_blank">Recovering From Privacy-Preserving Masking With Large Language Models</a></div>
<div class="paper-author">Arpita Vats, Zhe Liu, Peng Su, Debjyoti Paul, Yingyi Ma, Yutong Pang, Zeeshan Ahmed, Ozlem Kalinli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. to effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (nlp) models can be directly trained using such in-domain data. however, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. replacing identifying information in textual data with a generic marker has been recently explored. in this work, we leverage large language models (llms) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. specifically, we propose multiple pre-trained and fine-tuned llm-based approaches and perform empirical studies on various datasets for the comparison of these methods. experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-11</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05227" target="_blank">Detecting Natural Language Biases With Prompt-Based Learning</a></div>
<div class="paper-author">Md Abdul Aowal, Maliha T Islam, Priyanka Mary Mammen, Sandesh Shetty</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this project, we want to explore the newly emerging field of prompt engineering and apply it to the downstream task of detecting lm biases. more concretely, we explore how to design prompts that can indicate 4 different types of biases: (1) gender, (2) race, (3) sexual orientation, and (4) religion-based. within our project, we experiment with different manually crafted prompts that can draw out the subtle biases that may be present in the language model. we apply these prompts to multiple variations of popular and well-recognized models: bert, roberta, and t5 to evaluate their biases. we provide a comparative analysis of these models and assess them using a two-fold method: use human judgment to decide whether model predictions are biased and utilize model-level judgment (through further prompts) to understand if a model can self-diagnose the biases of its own prediction.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05274" target="_blank">Fuzzllm: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models</a></div>
<div class="paper-author">Dongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: jailbreak vulnerabilities in large language models (llms), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. while model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. to tackle this issue, we introduce fuzzllm, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in llms. we utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. by integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, fuzzllm enables efficient testing with reduced manual effort. extensive experiments demonstrate fuzzllm's effectiveness and comprehensiveness in vulnerability discovery across various llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05610" target="_blank">Privacy Side Channels in Machine Learning Systems</a></div>
<div class="paper-author">Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, Florian Tramèr</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most current approaches for protecting privacy in machine learning (ml) assume that models exist in a vacuum, when in reality, ml models are part of larger systems that include components for training data filtering, output monitoring, and more. in this work, we introduce privacy side channels: attacks that exploit these system-level components to extract private information at far higher rates than is otherwise possible for standalone models. we propose four categories of side channels that span the entire ml lifecycle (training data filtering, input preprocessing, output post-processing, and query filtering) and allow for either enhanced membership inference attacks or even novel threats such as extracting users' test queries. for example, we show that deduplicating training data before applying differentially-private training creates a side-channel that completely invalidates any provable privacy guarantees. moreover, we show that systems which block language models from regenerating training data can be exploited to allow exact reconstruction of private keys contained in the training set -- even if the model did not memorize these keys. taken together, our results demonstrate the need for a holistic, end-to-end privacy analysis of machine learning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05922" target="_blank">A Survey of Hallucination in Large Foundation Models</a></div>
<div class="paper-author">Vipula Rawte, Amit Sheth, Amitava Das</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hallucination in a foundation model (fm) refers to the generation of content that strays from factual reality or includes fabricated information. this survey paper provides an extensive overview of recent efforts that aim to identify, elucidate, and tackle the problem of hallucination, with a particular focus on ``large'' foundation models (lfms). the paper classifies various types of hallucination phenomena that are specific to lfms and establishes evaluation criteria for assessing the extent of hallucination. it also examines existing strategies for mitigating hallucination in lfms and discusses potential directions for future research in this area. essentially, the paper offers a comprehensive examination of the challenges and solutions related to hallucination in lfms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.05217" target="_blank">Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis</a></div>
<div class="paper-author">Li Du, Yequan Wang, Xingrun Xing, Yiqun Ya, Xiang Li, Xin Jiang, Xuezhi Fang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although demonstrating superb performance on various nlp tasks, large language models (llms) still suffer from the hallucination problem, which threatens the reliability of llms. to measure the level of hallucination of llms, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. however, such hallucination rates could easily be distorted by confounders. moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. to address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of llms with a set of risk factors. in this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of llms to mitigate the hallucination.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.06384" target="_blank">Towards Reliable and Fluent Large Language Models: Incorporating Feedback Learning Loops in Qa Systems</a></div>
<div class="paper-author">Dongyub Lee, Taesun Whang, Chanhee Lee, Heuiseok Lim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have emerged as versatile tools in various daily applications. however, they are fraught with issues that undermine their utility and trustworthiness. these include the incorporation of erroneous references (citation), the generation of hallucinated information (correctness), and the inclusion of superfluous or omission of crucial details (fluency). to ameliorate these concerns, this study makes several key contributions. first, we build a dataset to train a critic model capable of evaluating the citation, correctness, and fluency of responses generated by llms in qa systems. second, we propose an automated feedback mechanism that leverages the critic model to offer real-time feedback on heterogeneous aspects of generated text. third, we introduce a feedback learning loop that uses this critic model to iteratively improve the performance of the llm responsible for response generation. experimental results demonstrate the efficacy of our approach, showing substantial improvements in citation and fluency metrics for chatgpt, including a 4% precision increase in citation and an approximately 8% enhancement in the mauve metric for fluency, while maintaining high levels of correctness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03564" target="_blank">Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media</a></div>
<div class="paper-author">Hongzhi Qi, Qing Zhao, Changwei Song, Wei Zhai, Dan Luo, Shuo Liu, Yi Jing Yu, Fan Wang, Huijing Zou, Bing Xiang Yang, Jianqiang Li, Guanghui Fu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models, particularly those akin to the rapidly progressing gpt series, are gaining traction for their expansive influence. while there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on chinese social media platforms. using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tuning. our findings revealed a discernible performance gap between the large language models and traditional supervised learning approaches, primarily attributed to the models' inability to fully grasp subtle categories. notably, while gpt-4 outperforms its counterparts in multiple scenarios, gpt-3.5 shows significant enhancement in suicide risk classification after fine-tuning. to our knowledge, this investigation stands as the maiden attempt at gauging large language models on chinese social media tasks. this study underscores the forward-looking and transformative implications of using large language models in the field of psychology. it lays the groundwork for future applications in psychological research and practice.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03876" target="_blank">Opiniongpt: Modelling Explicit Biases in Instruction-Tuned LLMS</a></div>
<div class="paper-author">Patrick Haller, Ansar Aynetdinov, Alan Akbik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned large language models (llms) have recently showcased remarkable ability to generate fitting responses to natural language instructions. however, an open research question concerns the inherent biases of trained models and their responses. for instance, if the data used to tune an llm is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. current research work seeks to de-bias such models, or suppress potentially biased answers. with this demonstration, we take a different view on biases in instruction-tuning: rather than aiming to suppress them, we aim to make them explicit and transparent. to this end, we present opiniongpt, a web demo in which users can ask questions and select all biases they wish to investigate. the demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. to train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. this paper presents opiniongpt, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03883" target="_blank">Dola: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a></div>
<div class="paper-author">Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite their impressive capabilities, large language models (llms) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. we propose a simple decoding strategy for reducing hallucinations with pretrained llms that does not require conditioning on retrieved external knowledge nor additional fine-tuning. our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an llms has generally been shown to be localized to particular transformer layers. we find that this decoding by contrasting layers (dola) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. dola consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of llama family models on truthfulqa by 12-17% absolute points, demonstrating its potential in making llms reliably generate truthful facts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.04027" target="_blank">Tide: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models</a></div>
<div class="paper-author">Emmanuel Klu, Sameer Sethi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. when these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. in this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. we create a new, more comprehensive identity lexicon, tidal, which includes 15,123 identity terms and associated sense context across three demographic categories. we leverage tidal to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ml fairness techniques. we evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. results show our assistive annotation technique improves the reliability and velocity of human-in-the-loop processes. our dataset and methods uncover more disparities during evaluation, and also produce more fair models during remediation. these approaches provide a practical path forward for scaling classifier and generative model fairness in real-world settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.06415" target="_blank">Down the Toxicity Rabbit Hole: Investigating Palm 2 Guardrails</a></div>
<div class="paper-author">Adel Khorramrouz, Sujan Dutta, Arka Dutta, Ashiqur R. Khudabukhsh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper conducts a robustness audit of the safety feedback of palm 2 through a novel toxicity rabbit hole framework introduced here. starting with a stereotype, the framework instructs palm 2 to generate more toxic content than the stereotype. every subsequent iteration it continues instructing palm 2 to generate more toxic content than the previous iteration until palm 2 safety guardrails throw a safety violation. our experiments uncover highly disturbing antisemitic, islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that palm 2 safety guardrails do not evaluate as highly unsafe.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02705" target="_blank">Certifying LLM Safety Against Adversarial Prompting</a></div>
<div class="paper-author">Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, Hima Lakkaraju</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." an aligned language model should decline a user's request to produce harmful content. however, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. in this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. we erase tokens individually and inspect the resulting subsequences using a safety filter. our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. this guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. we defend against three attack modes: i) adversarial suffix, which appends an adversarial sequence at the end of the prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. empirical results demonstrate that our technique obtains strong certified safety guarantees on harmful prompts while maintaining good performance on safe prompts. for example, against adversarial suffixes of length 20, it certifiably detects 93% of the harmful prompts and labels 94% of the safe prompts as safe using the open source language model llama 2 as the safety filter.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02884" target="_blank">Aligning Large Language Models for Clinical Tasks</a></div>
<div class="paper-author">Supun Manathunga, Isuru Hettigoda</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable adaptability, showcasing their capacity to excel in tasks for which they were not explicitly trained. however, despite their impressive natural language processing (nlp) capabilities, effective alignment of llms remains a crucial challenge when deploying them for specific clinical applications. the ability to generate responses with factually accurate content and to engage in non-trivial reasoning steps are crucial for the llms to be eligible for applications in clinical medicine. employing a combination of techniques including instruction-tuning and in-prompt strategies like few-shot and chain-of-thought prompting has significantly enhanced the performance of llms. our proposed alignment strategy for medical question-answering, known as 'expand-guess-refine', offers a parameter and data-efficient solution. a preliminary analysis of this method demonstrated outstanding performance, achieving a score of 70.63% on a subset of questions sourced from the usmle dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02926" target="_blank">Demystifying Rce Vulnerabilities in LLM-Integrated Apps</a></div>
<div class="paper-author">Tong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, Kai Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have demonstrated remarkable potential across various downstream tasks. llm-integrated frameworks, which serve as the essential infrastructure, have given rise to many llm-integrated web apps. however, some of these frameworks suffer from remote code execution (rce) vulnerabilities, allowing attackers to execute arbitrary code on apps' servers remotely via prompt injections. despite the severity of these vulnerabilities, no existing work has been conducted for a systematic investigation of them. this leaves a great challenge on how to detect vulnerabilities in frameworks as well as llm-integrated apps in real-world scenarios. to fill this gap, we present two novel strategies, including 1) a static analysis-based tool called llmsmith to scan the source code of the framework to detect potential rce vulnerabilities and 2) a prompt-based automated testing approach to verify the vulnerability in llm-integrated web apps. we discovered 13 vulnerabilities in 6 frameworks, including 12 rce vulnerabilities and 1 arbitrary file read/write vulnerability. 11 of them are confirmed by the framework developers, resulting in the assignment of 7 cve ids. after testing 51 apps, we found vulnerabilities in 17 apps, 16 of which are vulnerable to rce and 1 to sql injection. we responsibly reported all 17 issues to the corresponding developers and received acknowledgments. furthermore, we amplify the attack impact beyond achieving rce by allowing attackers to exploit other app users (e.g. app responses hijacking, user api key leakage) without direct interaction between the attacker and the victim. lastly, we propose some mitigating strategies for improving the security awareness of both framework and app developers, helping them to mitigate these risks effectively.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03057" target="_blank">Hide and Seek (Has): A Lightweight Framework for Prompt Privacy Protection</a></div>
<div class="paper-author">Yu Chen, Tingxin Li, Huiming Liu, Yang Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous companies have started offering services based on large language models (llm), such as chatgpt, which inevitably raises privacy concerns as users' prompts are exposed to the model provider. previous research on secure reasoning using multi-party computation (mpc) has proven to be impractical for llm applications due to its time-consuming and communication-intensive nature. while lightweight anonymization techniques can protect private information in prompts through substitution or masking, they fail to recover sensitive data replaced in the llm-generated results. in this paper, we expand the application scenarios of anonymization techniques by training a small local model to de-anonymize the llm's returned results with minimal computational overhead. we introduce the has framework, where "h(ide)" and "s(eek)" represent its two core processes: hiding private entities for anonymization and seeking private entities for de-anonymization, respectively. to quantitatively assess has's privacy protection performance, we propose both black-box and white-box adversarial models. furthermore, we conduct experiments to evaluate has's usability in translation and classification tasks. the experimental findings demonstrate that the has framework achieves an optimal balance between privacy protection and utility.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03126" target="_blank">Everyone Deserves a Reward: Learning Customized Human Preferences</a></div>
<div class="paper-author">Pengyu Cheng, Jiawen Xie, Ke Bai, Yong Dai, Nan Du</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward models (rms) are essential for aligning large language models (llms) with human preferences to improve interaction quality. however, the real world is pluralistic, which leads to diversified human preferences with respect to different religions, politics, cultures, etc. moreover, each individual can have their unique preferences on various topics. neglecting the diversity of human preferences, current human feedback aligning methods only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. to explore customized preference learning, we collect a domain-specific preference (dsp) dataset, which includes preferred responses for each given query from four practical domains. besides, from the perspective of data efficiency, we propose a three-stage customized rm learning scheme, then empirically verify its effectiveness on both general preference datasets and our dsp set. furthermore, we test multiple training and data strategies on the three learning stages. we find several ways to better preserve the general preferring ability while training the customized rms, especially general preference enrichment, and customized preference imitation learning. the dsp dataset and code are available at https://github.com/linear95/dsp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.03164" target="_blank">J-Guard: Journalism Guided Adversarially Robust Detection of Ai-Generated News</a></div>
<div class="paper-author">Tharindu Kumarage, Amrita Bhattacharjee, Djordje Padejski, Kristy Roschke, Dan Gillmor, Scott Ruston, Huan Liu, Joshua Garland</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid proliferation of ai-generated text online is profoundly reshaping the information landscape. among various types of ai-generated text, ai-generated news presents a significant threat as it can be a prominent source of misinformation online. while several recent efforts have focused on detecting ai-generated text in general, these methods require enhanced reliability, given concerns about their vulnerability to simple adversarial attacks. furthermore, due to the eccentricities of news writing, applying these detection methods for ai-generated news can produce false positives, potentially damaging the reputation of news organizations. to address these challenges, we leverage the expertise of an interdisciplinary team to develop a framework, j-guard, capable of steering existing supervised ai text detectors for detecting ai-generated news while boosting adversarial robustness. by incorporating stylistic cues inspired by the unique journalistic attributes, j-guard effectively distinguishes between real-world journalism and ai-generated news articles. our experiments on news articles generated by a vast array of ai models, including chatgpt (gpt3.5), demonstrate the effectiveness of j-guard in enhancing detection capabilities while maintaining an average performance decrease of as low as 7% when faced with adversarial attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02144" target="_blank">Making Large Language Models Better Reasoners With Alignment</a></div>
<div class="paper-author">Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reasoning is a cognitive process of using evidence to reach a sound conclusion. the reasoning capability is essential for large language models (llms) to serve as the brain of the artificial general intelligence agent. recent studies reveal that fine-tuning llms on data with the chain of thought (cot) reasoning process can significantly enhance their reasoning capabilities. however, we find that the fine-tuned llms suffer from an \textit{assessment misalignment} problem, i.e., they frequently assign higher scores to subpar cots, leading to potential limitations in their reasoning abilities. to address this problem, we introduce an \textit{alignment fine-tuning (aft)} paradigm, which involves three steps: 1) fine-tuning llms with cot training data; 2) generating multiple cot responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by llms with a novel constraint alignment loss. specifically, the constraint alignment loss has two objectives: a) alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality cots; b) constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. furthermore, we also delve deeply into recent ranking-based alignment methods, such as dpo, rrhf, and pro, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of aft.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02311" target="_blank">Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization</a></div>
<div class="paper-author">Helena Bonaldi, Giuseppe Attanasio, Debora Nozza, Marco Guerini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting pretrained transformer-based language models (plms) with human-curated data. this process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. this paper introduces novel attention regularization methodologies to improve the generalization capabilities of plms for counter narratives generation. overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. we experiment with two attention-based regularization techniques on a benchmark english dataset. regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. this work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02524" target="_blank">Do You Trust Chatgpt? -- Perceived Credibility of Human and Ai-Generated Content</a></div>
<div class="paper-author">Martin Huschens, Martin Briesch, Dominik Sobania, Franz Rothlauf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper examines how individuals perceive the credibility of content originating from human authors versus content generated by large language models, like the gpt language model family that powers chatgpt, in different user interface versions. surprisingly, our results demonstrate that regardless of the user interface presentation, participants tend to attribute similar levels of credibility. while participants also do not report any different perceptions of competence and trustworthiness between human and ai-generated content, they rate ai-generated content as being clearer and more engaging. the findings from this study serve as a call for a more discerning approach to evaluating information sources, encouraging users to exercise caution and critical thinking when engaging with content generated by ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.02654" target="_blank">Zero-Resource Hallucination Prevention for Large Language Models</a></div>
<div class="paper-author">Junyu Luo, Cao Xiao, Fenglong Ma</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prevalent use of large language models (llms) in various domains has drawn attention to the issue of "hallucination," which refers to instances where llms generate factually inaccurate or ungrounded information. existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (cot) techniques or parameter-based methods that suffer from interpretability issues. additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. in this paper, we introduce a novel pre-detection self-evaluation technique, referred to as self-familiarity, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. this approach emulates the human ability to refrain from responding to unfamiliar topics, thus reducing hallucinations. we validate self-familiarity across four different large language models, demonstrating consistently superior performance compared to existing techniques. our findings propose a significant shift towards preemptive strategies for hallucination mitigation in llm assistants, promising improvements in reliability, applicability, and interpretability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01446" target="_blank">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</a></div>
<div class="paper-author">Raz Lapid, Ron Langberg, Moshe Sipper</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an llm's outputs for unintended purposes. in this paper we introduce a novel approach that employs a genetic algorithm (ga) to manipulate llms when model architecture and parameters are inaccessible. the ga attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible ai development by providing a diagnostic tool for evaluating and enhancing alignment of llms with human intent. to our knowledge this is the first automated universal black box jailbreak attack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01586" target="_blank">Automatic Scam-Baiting Using Chatgpt</a></div>
<div class="paper-author">Piyush Bajaj, Matthew Edwards</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automatic scam-baiting is an online fraud countermeasure that involves automated systems responding to online fraudsters in order to waste their time and deplete their resources, diverting attackers away from real potential victims. previous work has demonstrated that text generation systems are capable of engaging with attackers as automatic scam-baiters, but the fluency and coherence of generated text may be a limit to the effectiveness of such systems.   in this paper, we report on the results of a month-long experiment comparing the effectiveness of two chatgpt-based automatic scam-baiters to a control measure. within our results, with engagement from over 250 real email fraudsters, we find that chatgpt-based scam-baiters show a marked increase in scammer response rate and conversation length relative to the control measure, outperforming previous approaches. we discuss the implications of these results and practical considerations for wider deployment of automatic scam-baiting.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01933" target="_blank">Provably Safe Systems: The Only Path to Controllable Agi</a></div>
<div class="paper-author">Max Tegmark, Steve Omohundro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we describe a path to humanity safely thriving with powerful artificial general intelligences (agis) by building them to provably satisfy human-specified requirements. we argue that this will soon be technically feasible using advanced ai for formal verification and mechanistic interpretability. we further argue that it is the only path which guarantees safe controlled agi. we end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01219" target="_blank">Siren's Song in the Ai Ocean: A Survey on Hallucination in Large Language Models</a></div>
<div class="paper-author">Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: llms occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. this phenomenon poses a substantial challenge to the reliability of llms in real-world scenarios. in this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by llms. we present taxonomies of the llm hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating llm hallucination, and discuss potential directions for future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.14345" target="_blank">Bias Assessment and Mitigation in LLM-Based Code Generation</a></div>
<div class="paper-author">Dong Huang, Qingwen Bu, Jie Zhang, Xiaofei Xie, Junjie Chen, Heming Cui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: utilizing state-of-the-art large language models (llms), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. as the adoption of llms becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? this issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. this paper presents a novel bias assessment framework that is specifically designed for code generation tasks. based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art llm-based code generation models. our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' functionality are affected by the bias, which means biases not only exist in code generation models but in some cases, directly affect the functionality of the generated code, posing risks of unintended and possibly harmful software behaviors. to mitigate bias from code generation models, we propose three mitigation strategies, which can decrease the biased code ratio to a very low level of 0.4\% to 4.57\%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.01029" target="_blank">Explainability for Large Language Models: A Survey</a></div>
<div class="paper-author">Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated impressive capabilities in natural language processing. however, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. in this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining transformer-based language models. we categorize techniques based on the training paradigms of llms: traditional fine-tuning-based paradigm and prompting-based paradigm. for each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. we also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of llms in comparison to conventional machine learning models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-09-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00254" target="_blank">Why Do Universal Adversarial Attacks Work on Large Language Models?: Geometry Might Be the Answer</a></div>
<div class="paper-author">Varshini Subhash, Anna Bialas, Weiwei Pan, Finale Doshi-Velez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. however, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. this work presents a novel geometric perspective explaining universal adversarial attacks on large language models. by attacking the 117m parameter gpt-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. this hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. we believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of llms, thus enabling their mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00267" target="_blank">Rlaif: Scaling Reinforcement Learning From Human Feedback With Ai Feedback</a></div>
<div class="paper-author">Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is effective at aligning large language models (llms) to human preferences, but gathering high quality human preference labels is a key bottleneck. we conduct a head-to-head comparison of rlhf vs. rl from ai feedback (rlaif) - a technique where preferences are labeled by an off-the-shelf llm in lieu of humans, and we find that they result in similar improvements. on the task of summarization, human evaluators prefer generations from both rlaif and rlhf over a baseline supervised fine-tuned model in ~70% of cases. furthermore, when asked to rate rlaif vs. rlhf summaries, humans prefer both at equal rates. these results suggest that rlaif can yield human-level performance, offering a potential solution to the scalability limitations of rlhf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00614" target="_blank">Baseline Defenses for Adversarial Attacks Against Aligned Language Models</a></div>
<div class="paper-author">Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-Yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: what threat models are practically useful in this domain? how do baseline defense techniques perform in this new domain? how does llm security differ from computer vision?   we evaluate several baseline defense strategies against leading adversarial attacks on llms, discussing the various settings in which each is feasible and effective. particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. we discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. we find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for llms. future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the llms domain than it has been in computer vision.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00667" target="_blank">Taken Out of Context: On Measuring Situational Awareness in LLMS</a></div>
<div class="paper-author">Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, Owain Evans</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we aim to better understand the emergence of `situational awareness' in large language models (llms). a model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. today's llms are tested for safety and alignment before they are deployed. an llm could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. situational awareness may emerge unexpectedly as a byproduct of model scaling. one way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. as such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). we study out-of-context reasoning experimentally. first, we finetune an llm on a description of a test while providing no examples or demonstrations. at test time, we assess whether the model can pass the test. to our surprise, we find that llms succeed on this out-of-context reasoning task. their success is sensitive to the training setup and only works when we apply data augmentation. for both gpt-3 and llama-1, performance improves with model size. these findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in llms. code is available at: https://github.com/asacooperstickland/situational-awareness-evals.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00751" target="_blank">Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence</a></div>
<div class="paper-author">Daniel Scalena, Gabriele Sarti, Malvina Nissim, Elisabetta Fersini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. in this work, we apply popular detoxification approaches to several language models and quantify their impact on the resulting models' prompt dependence using feature attribution methods. we evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00770" target="_blank">Bias and Fairness in Large Language Models: A Survey</a></div>
<div class="paper-author">Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements of large language models (llms) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. despite this success, these models can learn, perpetuate, and amplify harmful social biases. in this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for llms. we first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for llms. we then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. finally, we identify open problems and challenges for future work. synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00779" target="_blank">Value Kaleidoscope: Engaging Ai With Pluralistic Human Values, Rights, and Duties</a></div>
<div class="paper-author">Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, John Tasioulas, Yejin Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: human values are crucial to human decision-making. value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). as statistical learners, ai systems fit to averages by default, washing out these potentially irreducible value conflicts. to improve ai systems to better reflect value pluralism, the first-order challenge is to explore the extent to which ai systems can model pluralistic human values, rights, and duties as well as their interaction.   we introduce valueprism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. valueprism's contextualized values are generated by gpt-4 and deemed high-quality by human annotators 91% of the time. we conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented.   with valueprism, we build kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. humans prefer the sets of values output by our system over the teacher gpt-4, finding them more accurate and with broader coverage. in addition, we demonstrate that kaleido can help explain variability in human decision-making by outputting contrasting values. finally, we show that kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. we hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering ai systems to make decisions that are more in accordance with them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-31</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16549" target="_blank">Thesis Distillation: Investigating the Impact of Bias in NLP Models on Hate Speech Detection</a></div>
<div class="paper-author">Fatma Elsafoury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper is a summary of the work in my phd thesis. in which, i investigate the impact of bias in nlp models on the task of hate speech detection from three perspectives: explainability, offensive stereotyping bias, and fairness. i discuss the main takeaways from my thesis and how they can benefit the broader nlp community. finally, i discuss important future research directions. the findings of my thesis suggest that bias in nlp models impacts the task of hate speech detection from all three perspectives. and that unless we start incorporating social sciences in studying bias in nlp models, we will not effectively overcome the current limitations of measuring and mitigating bias in nlp models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16705" target="_blank">Crehate: Cross-Cultural Re-Annotation of English Hate Speech Dataset</a></div>
<div class="paper-author">Nayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Juho Kim, Alice Oh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: english datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. this is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. to delve into how individuals from different countries perceive hate speech, we introduce crehate, a cross-cultural re-annotation of the sampled sbic dataset. this dataset includes annotations from five distinct countries: australia, singapore, south africa, the united kingdom, and the united states. our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. we also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. these findings underscore the need to re-evaluate certain aspects of nlp research, especially with regard to the nuanced nature of hate speech in the english language.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00155" target="_blank">LLM in the Shell: Generative Honeypots</a></div>
<div class="paper-author">Muris Sladić, Veronica Valeros, Carlos Catania, Sebastian Garcia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: honeypots are essential tools in cybersecurity. however, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. this limitation makes them easily discernible, hindering their effectiveness. this work introduces a novel method to create dynamic and realistic software honeypots based on large language models. preliminary results indicate that llms can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. we evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. our proposed honeypot, called shellm, reached an accuracy rate of 0.92.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.00236" target="_blank">Image Hijacks: Adversarial Images Can Control Generative Models at Runtime</a></div>
<div class="paper-author">Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons</div>
<div class="abstract">
<div class="abstract-content">
Abstract: are foundation models secure from malicious actors? in this work, we focus on the image input to a vision-language model (vlm). we discover image hijacks, adversarial images that control generative models at runtime. we introduce behaviour matching, a general method for creating image hijacks, and we use it to explore three types of attacks. specific string attacks generate arbitrary output of the adversary's choice. leak context attacks leak information from the context window into the output. jailbreak attacks circumvent a model's safety training. we study these attacks against llava, a state-of-the-art vlm based on clip and llama-2, and find that all our attack types have above a 90% success rate. moreover, our attacks are automated and require only small image perturbations. these findings raise serious concerns about the security of foundation models. if image hijacks are as difficult to defend against as adversarial examples in cifar-10, then it might be many years before a solution is found -- if it even exists.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15812" target="_blank">Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models</a></div>
<div class="paper-author">Hritik Bansal, John Dang, Aditya Grover</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) with human values and intents critically involves the use of human or ai feedback. while dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score response a on a scale of 1-7) and rankings (e.g., is response a better than response b?). in this work, we analyze the effect of this design choice for the alignment and evaluation of llms. we uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and ai annotators. our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. to our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned llms. in particular, we find that llms that leverage rankings data for alignment (say model x) are preferred over those that leverage ratings data (say model y), with a rank-based evaluation protocol (is x/y's response better than reference response?) but not with a rating-based evaluation protocol (score rank x/y's response on a scale of 1-7). our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment. our code and data are available at https://github.com/hritikbansal/sparse_feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15906" target="_blank">Is the u.s. Legal System Ready for Ai's Challenges to Human Values?</a></div>
<div class="paper-author">Inyoung Cheong, Aylin Caliskan, Tadayoshi Kohno</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our interdisciplinary study investigates how effectively u.s. laws confront the challenges posed by generative ai to human values. through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as privacy, autonomy, dignity, diversity, equity, and physical/mental well-being. constitutional and civil rights, it appears, may not provide sufficient protection against ai-generated discriminatory outputs. furthermore, even if we exclude the liability shield provided by section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of ai systems. to address the unique and unforeseeable threats posed by generative ai, we advocate for legal frameworks that evolve to recognize new threats and provide proactive, auditable guidelines to industry stakeholders. addressing these issues requires deep interdisciplinary collaborations to identify harms, values, and mitigation strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16175" target="_blank">Quantifying Uncertainty in Answers From Any Language Model and Enhancing Their Trustworthiness</a></div>
<div class="paper-author">Jiuhai Chen, Jonas Mueller</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce bsdetector, a method for detecting bad and speculative answers from a pretrained large language model by estimating a numeric confidence score for any output it generated. our uncertainty quantification technique works for any llm accessible only via a black-box api, whose training data remains unknown. by expending a bit of extra computation, users of any llm api can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. experiments on both closed and open-form question-answer benchmarks reveal that bsdetector more accurately identifies incorrect llm responses than alternative uncertainty estimation procedures (for both gpt-3 and chatgpt). by sampling multiple responses from the llm and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same llm, without any extra training steps. in applications involving automated evaluation with llms, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both gpt 3.5 and 4).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.16364" target="_blank">Strengthening the Eu Ai Act: Defining Key Terms on Ai Manipulation</a></div>
<div class="paper-author">Matija Franklin, Philip Moreira Tomei, Rebecca Gorman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the european union's artificial intelligence act aims to regulate manipulative and harmful uses of ai, but lacks precise definitions for key concepts. this paper provides technical recommendations to improve the act's conceptual clarity and enforceability. we review psychological models to define "personality traits," arguing the act should protect full "psychometric profiles." we urge expanding "behavior" to include "preferences" since preferences causally influence and are influenced by behavior. clear definitions are provided for "subliminal," "manipulative," and "deceptive" techniques, considering incentives, intent, and covertness. we distinguish "exploiting individuals" from "exploiting groups," emphasising different policy needs. an "informed decision" is defined by four facets: comprehension, accurate information, no manipulation, and understanding ai's influence. we caution the act's therapeutic use exemption given the lack of regulation of digital therapeutics by the ema. overall, the recommendations strengthen definitions of vague concepts in the eu ai act, enhancing precise applicability to regulate harmful ai manipulation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-29</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15126" target="_blank">Evaluation and Analysis of Hallucination in Large Vision-Language Models</a></div>
<div class="paper-author">Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large vision-language models (lvlms) have recently achieved remarkable success. however, lvlms are still plagued by the hallucination problem, which limits the practicality in many scenarios. hallucination refers to the information of lvlms' responses that does not exist in the visual input, which poses potential risks of substantial consequences. there has been limited work studying hallucination evaluation in lvlms. in this paper, we propose hallucination evaluation based on large language models (haelm), an llm-based hallucination evaluation framework. haelm achieves an approximate 95% performance comparable to chatgpt and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. leveraging the haelm, we evaluate the hallucination in current lvlms. furthermore, we analyze the factors contributing to hallucination in lvlms and offer helpful suggestions to mitigate the hallucination problem. our training data and human annotation hallucination data will be made public soon.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15399" target="_blank">Rethinking Machine Ethics -- Can LLMS Perform Moral Reasoning Through the Lens of Moral Theories?</a></div>
<div class="paper-author">Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin King, Helen Meng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: making moral judgments is an essential step toward developing ethical ai systems. prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. these approaches have been criticized for potentially overgeneralizing a limited group of annotators' moral stances and lacking explainability. in contrast, top-down approaches make moral judgments grounded in a set of principles. however, it remains conceptual due to the incapability of previous language models and the unsolved debate among moral principles. in this study, we propose a flexible framework to steer large language models (llms) to perform moral reasoning with well-established moral theories from interdisciplinary research. the theory-guided top-down framework can incorporate various moral theories. our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. furthermore, we show the alignment between different moral theories and existing morality datasets. our analysis exhibits the potentials and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15514" target="_blank">International Governance of Civilian Ai: A Jurisdictional Certification Approach</a></div>
<div class="paper-author">Robert Trager, Ben Harack, Anka Reuel, Allison Carnegie, Lennart Heim, Lewis Ho, Sarah Kreps, Ranjit Lall, Owen Larter, Seán Ó Héigeartaigh, Simon Staffell, José Jaime Villalobos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (ai) and presents one approach in detail. this approach represents the extension of a standards, licensing, and liability regime to the global level. we propose that states establish an international ai organization (iaio) to certify state jurisdictions (not firms or ai projects) for compliance with international oversight standards. states can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody ai from non-iaio-certified jurisdictions. this borrows attributes from models of existing international organizations, such as the international civilian aviation organization (icao), the international maritime organization (imo), and the financial action task force (fatf). states can also adopt multilateral controls on the export of ai product inputs, such as specialized hardware, to non-certified jurisdictions. indeed, both the import and export standards could be required for certification. as international actors reach consensus on risks of and minimum standards for advanced ai, a jurisdictional certification regime could mitigate a broad range of potential harms, including threats to public safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.15727" target="_blank">Quantifying and Analyzing Entity-Level Memorization in Large Language Models</a></div>
<div class="paper-author">Zhenhong Zhou, Jiuyang Xiang, Chaomeng Chen, Sen Su</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. as the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. quantifying language model memorization helps evaluate potential privacy risks. however, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. to this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. in addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. we conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. we find that language models have strong memorization at the entity level and are able to reproduce the training data even with partial leakages. the results demonstrate that llms not only memorize their training data but also understand associations between entities. these findings necessitate that trainers of llms exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14367" target="_blank">A Comprehensive Overview of Backdoor Attacks in Large Language Models Within Communication Networks</a></div>
<div class="paper-author">Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing Lu, Shui Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the large language models (llms) are poised to offer efficient and intelligent services for future mobile communication networks, owing to their exceptional capabilities in language comprehension and generation. however, the extremely high data and computational resource requirements for the performance of llms compel developers to resort to outsourcing training or utilizing third-party data and computing resources. these strategies may expose the model within the network to maliciously manipulated training data and processing, providing an opportunity for attackers to embed a hidden backdoor into the model, termed a backdoor attack. backdoor attack in llms refers to embedding a hidden backdoor in llms that causes the model to perform normally on benign samples but exhibit degraded performance on poisoned ones. this issue is particularly concerning within communication networks where reliability and security are paramount. despite the extensive research on backdoor attacks, there remains a lack of in-depth exploration specifically within the context of llms employed in communication networks, and a systematic review of such attacks is currently absent. in this survey, we systematically propose a taxonomy of backdoor attacks in llms as used in communication networks, dividing them into four major categories: input-triggered, prompt-triggered, instruction-triggered, and demonstration-triggered attacks. furthermore, we conduct a comprehensive analysis of the benchmark datasets. finally, we identify potential problems and open challenges, offering valuable insights into future research directions for enhancing the security and integrity of llms in communication networks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14608" target="_blank">Ai in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics</a></div>
<div class="paper-author">Vahid Ghafouri, Vibhor Agarwal, Yong Zhang, Nishanth Sastry, Jose Such, Guillermo Suarez-Tangil</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the introduction of chatgpt and the subsequent improvement of large language models (llms) have prompted more and more individuals to turn to the use of chatbots, both for information and assistance with decision-making. however, the information the user is after is often not formulated by these chatbots objectively enough to be provided with a definite, globally accepted answer.   controversial topics, such as "religion", "gender identity", "freedom of speech", and "equality", among others, can be a source of conflict as partisan or biased answers can reinforce preconceived notions or promote disinformation. by exposing chatgpt to such debatable questions, we aim to understand its level of awareness and if existing models are subject to socio-political and/or economic biases. we also aim to explore how ai-generated answers compare to human ones. for exploring this, we use a dataset of a social media platform created for the purpose of debating human-generated claims on polemic subjects among users, dubbed kialo.   our results show that while previous versions of chatgpt have had important issues with controversial topics, more recent versions of chatgpt (gpt-3.5-turbo) are no longer manifesting significant explicit biases in several knowledge areas. in particular, it is well-moderated regarding economic aspects. however, it still maintains degrees of implicit libertarian leaning toward right-winged ideals which suggest the need for increased moderation from the socio-political point of view. in terms of domain knowledge on controversial topics, with the exception of the "philosophical" category, chatgpt is performing well in keeping up with the collective human level of knowledge. finally, we see that sources of bing ai have slightly more tendency to the center when compared to human answers. all the analyses we make are generalizable to other types of biases and domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14641" target="_blank">Challenges of GPT-3-Based Conversational Agents for Healthcare</a></div>
<div class="paper-author">Fabian Lechner, Allison Lahnala, Charles Welch, Lucie Flek</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the potential to provide patients with faster information access while allowing medical specialists to concentrate on critical tasks makes medical domain dialog agents appealing. however, the integration of large-language models (llms) into these agents presents certain limitations that may result in serious consequences. this paper investigates the challenges and risks of using gpt-3-based models for medical question-answering (medqa). we perform several evaluations contextualized in terms of standard medical principles. we provide a procedure for manually designing patient queries to stress-test high-risk limitations of llms in medqa systems. our analysis reveals that llms fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content that may be considered offensive.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14683" target="_blank">Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts</a></div>
<div class="paper-author">Thanh Thi Nguyen, Campbell Wilson, Janis Dalins</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting online sexual predatory behaviours and abusive language on social media platforms has become a critical area of research due to the growing concerns about online safety, especially for vulnerable populations such as children and adolescents. researchers have been exploring various techniques and approaches to develop effective detection systems that can identify and mitigate these risks. recent development of large language models (llms) has opened a new opportunity to address this problem more effectively. this paper proposes an approach to detection of online sexual predatory chats and abusive language using the open-source pretrained llama 2 7b-parameter model, recently released by meta genai. we fine-tune the llm using datasets with different sizes, imbalance degrees, and languages (i.e., english, roman urdu and urdu). based on the power of llms, our approach is generic and automated without a manual search for a synergy between feature extraction and classifier design steps like conventional methods in this domain. experimental results show a strong performance of the proposed approach, which performs proficiently and consistently across three distinct datasets with five sets of experiments. this study's outcomes indicate that the proposed method can be implemented in real-world applications (even with non-english languages) for flagging sexual predators, offensive or toxic content, hate speech, and discriminatory language in online discussions and comments to maintain respectful internet or digital communities. furthermore, it can be employed for solving text classification problems with other potential applications such as sentiment analysis, spam and phishing detection, sorting legal documents, fake news detection, language identification, user intent recognition, text-based product categorization, medical record analysis, and resume screening.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14752" target="_blank">Ai Deception: A Survey of Examples, Risks, and Potential Solutions</a></div>
<div class="paper-author">Peter S. Park, Simon Goldstein, "Aidan O'Gara", Michael Chen, Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper argues that a range of current ai systems have learned how to deceive humans. we define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. we first survey empirical examples of ai deception, discussing both special-use ai systems (including meta's cicero) built for specific competitive situations, and general-purpose ai systems (such as large language models). next, we detail several risks from ai deception, such as fraud, election tampering, and losing control of ai systems. finally, we outline several potential solutions to the problems posed by ai deception: first, regulatory frameworks should subject ai systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect ai deception and to make ai systems less deceptive. policymakers, researchers, and the broader public should work proactively to prevent ai deception from destabilizing the shared foundations of our society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14840" target="_blank">Identifying and Mitigating the Security Risks of Generative Ai</a></div>
<div class="paper-author">Clark Barrett, Brad Boyd, Elie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, Diyi Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. generative ai (genai) techniques, such as large language models (llms) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). however, genai can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.   this paper reports the findings of a workshop held at google (co-organized by stanford university and the university of wisconsin-madison) on the dual-use dilemma posed by genai. this paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. we discuss short-term and long-term goals for the community on this topic. we hope this paper provides both a launching point for a discussion on this important topic as well as interesting problems that the research community can work to address.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14921" target="_blank">Gender Bias and Stereotypes in Large Language Models</a></div>
<div class="paper-author">Hadas Kotek, Rikker Dockum, David Q. Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. this paper investigates llms' behavior with respect to gender stereotypes, a known issue for prior models. we use a simple paradigm to test the presence of gender bias, building on but differing from winobias, a commonly used gender bias dataset, which is likely to be included in the training data of current llms. we test four recently published llms and demonstrate that they express biased assumptions about men and women's occupations. our contributions in this paper are as follows: (a) llms are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) llms in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) llms ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) llms provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. that is, they provide rationalizations of their biased behavior. this highlights a key property of these models: llms are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. as with other types of societal biases, we suggest that llms must be carefully tested to ensure that they treat minoritized individuals and communities equitably.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14132" target="_blank">Detecting Language Model Attacks With Perplexity</a></div>
<div class="paper-author">Gabriel Alon, Michael Kamfonas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a novel hack involving large language models (llms) has emerged, leveraging adversarial suffixes to trick models into generating perilous responses. this method has garnered considerable attention from reputable media outlets such as the new york times and wired, thereby influencing public perception regarding the security and safety of llms. in this study, we advocate the utilization of perplexity as one of the means to recognize such potential attacks. the underlying concept behind these hacks revolves around appending an unusually constructed string of text to a harmful query that would otherwise be blocked. this maneuver confuses the protective mechanisms and tricks the model into generating a forbidden response. such scenarios could result in providing detailed instructions to a malicious user for constructing explosives or orchestrating a bank heist. our investigation demonstrates the feasibility of employing perplexity, a prevalent natural language processing metric, to detect these adversarial tactics before generating a forbidden response. by evaluating the perplexity of queries with and without such adversarial suffixes using an open-source llm, we discovered that nearly 90 percent were above a perplexity of 1000. this contrast underscores the efficacy of perplexity for detecting this type of exploit.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.14253" target="_blank">The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward</a></div>
<div class="paper-author">Alexander J. Titus, Adam H. Russell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) promises immense benefits across sectors, yet also poses risks from dual-use potentials, biases, and unintended behaviors. this paper reviews emerging issues with opaque and uncontrollable ai systems and proposes an integrative framework called violet teaming to develop reliable and responsible ai. violet teaming combines adversarial vulnerability probing (red teaming) with solutions for safety and security (blue teaming) while prioritizing ethics and social benefit. it emerged from ai safety research to manage risks proactively by design. the paper traces the evolution of red, blue, and purple teaming toward violet teaming, and then discusses applying violet techniques to address biosecurity risks of ai in biotechnology. additional sections review key perspectives across law, ethics, cybersecurity, macrostrategy, and industry best practices essential for operationalizing responsible ai through holistic technical and social considerations. violet teaming provides both philosophy and method for steering ai trajectories toward societal good. with conscience and wisdom, the extraordinary capabilities of ai can enrich humanity. but without adequate precaution, the risks could prove catastrophic. violet teaming aims to empower moral technology for the common welfare.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13768" target="_blank">Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content</a></div>
<div class="paper-author">"Charles O'Neill", Jack Miller, Ioana Ciuca, Yuan-Sen Ting, Thang Bui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we tackle the emerging challenge of unintended harmful content generation in large language models (llms) with a novel dual-stage optimisation technique using adversarial fine-tuning. our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. in this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. this iterative application of prompting and fine-tuning allows continuous refinement and improved performance. the performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by gpt-4, as well as a selection of contentious but unproblematic prompts. we show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. furthermore, we show that a rudimentary model \texttt{ada} can achieve 13\% higher accuracy on the hold-out test set than gpt-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13904" target="_blank">Lmsanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors</a></div>
<div class="paper-author">Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, Wenzhi Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. the state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. to address this issue, we propose lmsanitator, a novel approach for detecting and removing task-agnostic backdoors on transformer models. instead of directly inverting the triggers, lmsanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. lmsanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. extensive experiments on multiple language models and nlp tasks illustrate the effectiveness of lmsanitator. for instance, lmsanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13387" target="_blank">Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMS</a></div>
<div class="paper-author">Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, Timothy Baldwin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid evolution of large language models (llms), new and hard-to-predict harmful capabilities are emerging. this requires developers to be able to identify risks through the evaluation of "dangerous capabilities" in order to responsibly deploy llms. in this work, we collect the first open-source dataset to evaluate safeguards in llms, and deploy safer open-source llms at a low cost. our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. we annotate and assess the responses of six popular llms to these instructions. based on our annotation, we proceed to train several bert-like classifiers, and find that these small classifiers can achieve results that are comparable with gpt-4 on automatic safety evaluation. warning: this paper contains example data that may be offensive, harmful, or biased.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13449" target="_blank">The Poison of Alignment</a></div>
<div class="paper-author">Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, James Yamazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: from the perspective of content safety issues, alignment has shown to limit large language models' (llms) harmful content generation. this intentional method of reinforcing models to not respond to certain user inputs seem to be present in many modern open-source instruction tuning datasets such as openassistant or guanaco. we introduce a novel insight to an instruction-tuned model's performance affected by the presence of alignment in supervised fine-tuning dataset. to be specific, we noticed that alignment acts as if it is poisoning the instruction dataset. experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as big bench (bbh), massive multitask language understanding (mmlu), human eval, and discrete reasoning over paragraphs (drop), performing worse than the counterpart tuned without alignment by 4-33%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12342" target="_blank">Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions</a></div>
<div class="paper-author">Reem I. Masoud, Ziquan Liu, Martin Ferianc, Philip Treleaven, Miguel Rodrigues</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the deployment of large language models (llms) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. existing work investigated political and social biases and public opinions rather than their cultural values. to address this limitation, the proposed cultural alignment test (cat) quantifies cultural alignment using hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. we apply our approach to assess the cultural values embedded in state-of-the-art llms, such as: chatgpt and bard, across diverse cultures of countries: united states (us), saudi arabia, china, and slovakia, using different prompting styles and hyperparameter settings. our results not only quantify cultural alignment of llms with certain countries, but also reveal the difference between llms in explanatory cultural dimensions. while all llms did not provide satisfactory results in understanding cultural values, gpt-4 exhibited the highest cat score for the cultural values of the us.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12578" target="_blank">Mind vs. Mouth: On Measuring Re-Judge Inconsistency of Social Bias in Large Language Models</a></div>
<div class="paper-author">Yachao Zhao, Bo Wang, Dongming Zhao, Kun Huang, Yan Wang, Ruifang He, Yuexian Hou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent researches indicate that pre-trained large language models (llms) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of llms. this paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. it posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. we propose a two-stage approach and discover a parallel phenomenon in llms known as "re-judge inconsistency" in social bias. in the initial stage, the llm is tasked with automatically completing statements, potentially incorporating implicit social bias. however, in the subsequent stage, the same llm re-judges the biased statement generated by itself but contradicts it. we propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. experimental investigations on chatgpt and gpt-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. this finding may suggest that diverse cognitive constructs emerge as llms' capabilities strengthen. consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12833" target="_blank">Use of LLMS for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities</a></div>
<div class="paper-author">Maximilian Mozes, Xuanli He, Bennett Kleinberg, Lewis D. Griffin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: spurred by the recent rapid increase in the development and distribution of large language models (llms) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of llms, including in the context of potentially criminal activities. specifically, it has been shown that llms can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of ai alignment. it is important that developers and practitioners alike are aware of security-related problems with such models. in this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from llms. we present a taxonomy describing the relationship between threats caused by the generative capabilities of llms, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. with our work, we hope to raise awareness of the limitations of llms in light of such security concerns, among both experienced developers and novel users of such technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13089" target="_blank">Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models Using an Interdisciplinary Lens</a></div>
<div class="paper-author">Pranav Narayanan Venkit</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid growth in the usage and applications of natural language processing (nlp) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. while research on bias in nlp has expanded, several challenges persist that require attention. these include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. this paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in nlp. the work is structured into three facets, each exploring a specific aspect of bias in nlp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12014" target="_blank">From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models</a></div>
<div class="paper-author">Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: big models, exemplified by large language models (llms), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. however, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. therefore, many efforts have been made to align llms with humans to make them better follow user instructions and satisfy human preferences. nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. in this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced llms. based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12050" target="_blank">Aligning Language Models With Offline Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Jian Hu, Li Tao, June Yang, Chandler Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: learning from human preferences is crucial for language models (lms) to effectively cater to human needs and societal values. previous research has made notable progress by leveraging human feedback to follow instructions. however, these approaches rely primarily on online reinforcement learning (rl) techniques like proximal policy optimization (ppo), which have been proven unstable and challenging to tune for language models. moreover, ppo requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. in this study, we propose an offline reinforcement learning from human feedback (rlhf) framework to align lms using pre-generated samples without interacting with rl environments. specifically, we explore maximum likelihood estimation (mle) with filtering, reward-weighted regression (rwr), and decision transformer (dt) to align language models to human preferences. by employing a loss function similar to supervised fine-tuning, our methods ensure more stable model training than ppo with a simple machine learning system~(mlsys) and much fewer (around 12.3\%) computing resources. experimental results demonstrate the dt alignment outperforms other offline rlhf methods and is better than ppo.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12086" target="_blank">Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments</a></div>
<div class="paper-author">Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, Sebastian Garcia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. despite their inherent limitations, llm-based designs have shown promising capabilities in planning and navigating open-world scenarios. this paper introduces a novel application of pre-trained llms as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   we present an approach wherein pre-trained llms are leveraged as attacking agents in two reinforcement learning environments. our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. in addition, the best llm agents perform similarly to human testers of the environment without any additional training process. this design highlights the potential of llms to efficiently address complex decision-making tasks within cybersecurity.   furthermore, we introduce a new network security environment named netsecgame. the environment is designed to eventually support complex multi-agent scenarios within the network security domain. the proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12215" target="_blank">The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</a></div>
<div class="paper-author">Madelyne Xiao, Jonathan Mayer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. we systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. we then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. we find significant shortcomings in the literature that call into question claimed performance and practicality. detection tasks are often meaningfully distinct from the challenges that online services actually face. datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. data and code availability is poor. models do not generalize well to out-of-domain data. based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. our aim is for future work to avoid the pitfalls that we identify.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12287" target="_blank">Devising and Detecting Phishing: Large Language Models vs. Smaller Human Models</a></div>
<div class="paper-author">Fredrik Heiding, Bruce Schneier, Arun Vishwanath, Jeremy Bernstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai programs, built using large language models, make it possible to automatically create phishing emails based on a few data points about a user. they stand in contrast to traditional phishing emails that hackers manually design using general rules gleaned from experience. the v-triad is an advanced set of rules for manually designing phishing emails to exploit our cognitive heuristics and biases. in this study, we compare the performance of phishing emails created automatically by gpt-4 and manually using the v-triad. we also combine gpt-4 with the v-triad to assess their combined potential. a fourth group, exposed to generic phishing emails, was our control group. we utilized a factorial approach, sending emails to 112 randomly selected participants recruited for the study. the control group emails received a click-through rate between 19-28%, the gpt-generated emails 30-44%, emails generated by the v-triad 69-79%, and emails generated by gpt and the v-triad 43-81%. each participant was asked to explain for why they pressed or did not press a link in the email. these answers often contradict each other, highlighting the need for personalized content. the cues that make one person avoid phishing emails make another person fall for them. next, we used four popular large language models (gpt, claude, palm, and llama) to detect the intention of phishing emails and compare the results to human detection. the language models demonstrated a strong ability to detect malicious intent, even in non-obvious phishing emails. they sometimes surpassed human detection, although often being slightly less accurate than humans.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.12539" target="_blank">Calm : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias</a></div>
<div class="paper-author">Vipul Gupta, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, Rebecca J. Passonneau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models (lms) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. to achieve reliability, we introduce the comprehensive assessment of language model bias (calm), a benchmark dataset to quantify bias in lms across three tasks. we integrate 16 existing datasets across different domains, such as wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. we compare the diversity of calm with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. we show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. we evaluate 20 large language models including six prominent families of lms such as llama-2. in two lm series, opt and bloom, we found that larger parameter models are more biased than lower parameter models. we found the t0 series of models to be the least biased. furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. the code is available at https://github.com/vipulgupta1011/calm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11764" target="_blank">Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models</a></div>
<div class="paper-author">Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, Yuxuan Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have revolutionized natural language processing (nlp). although convenient for research and practical applications, open-source llms with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. this paper focuses on measuring and reducing hallucinations in bloom 7b, a representative of such weaker open-source llms that are publicly available for research and commercial applications. we introduce halocheck, a lightweight blackbox knowledge-free framework designed to quantify the severity of hallucinations in llms. additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter llms. our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10684" target="_blank">Systematic Offensive Stereotyping (Sos) Bias in Language Models</a></div>
<div class="paper-author">Fatma Elsafoury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: research has shown that language models (lms) are socially biased. however, toxicity and offensive stereotyping bias in lms are understudied. in this paper, we investigate the systematic offensive stereotype (sos) bias in lms. we propose a method to measure it. then, we validate the sos bias and investigate the effectiveness of debias methods from the literature on removing it. finally, we investigate the impact of the sos bias in lms on their performance and their fairness on the task of hate speech detection. our results suggest that all the inspected lms are sos biased. the results suggest that the sos bias in lms is reflective of the hate experienced online by the inspected marginalized groups. the results indicate that removing the sos bias in lms, using a popular debias method from the literature, leads to worse sos bias scores. finally, our results show no strong evidence that the sos bias in lms is impactful on their performance on hate speech detection. on the other hand, there is evidence that the sos bias in lms is impactful on their fairness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10741" target="_blank">On the Adversarial Robustness of Multi-Modal Foundation Models</a></div>
<div class="paper-author">Christian Schlarmann, Matthias Hein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: multi-modal foundation models combining vision and language models such as flamingo or gpt-4 have recently gained enormous interest. alignment of foundation models is used to prevent models from providing toxic or harmful output. while malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. in this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. this indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10800" target="_blank">Artificial Intelligence Is Ineffective and Potentially Harmful for Fact Checking</a></div>
<div class="paper-author">Matthew R. Deverna, Harry Yaojun Yan, Kai-Cheng Yang, Filippo Menczer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. recent artificial intelligence (ai) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. here we investigate the impact of fact checks generated by a popular ai model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. although the ai performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. however, the ai fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. on the positive side, the ai increases sharing intents for correctly labeled true headlines. when participants are given the option to view ai fact checks and choose to do so, they are significantly more likely to share both true and false news but only more likely to believe false news. our findings highlight an important source of potential harm stemming from ai applications and underscore the critical need for policies to prevent or mitigate such unintended consequences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11103" target="_blank">Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models</a></div>
<div class="paper-author">Alex Nyffenegger, Matthias Stürmer, Joel Niklaus</div>
<div class="abstract">
<div class="abstract-content">
Abstract: anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the european union and switzerland. with the advent of llms, concerns about large-scale re-identification of anonymized persons are growing. in accordance with the federal supreme court of switzerland, we explore the potential of llms to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the swiss federal supreme court. following the initial experiment, we constructed an anonymized wikipedia dataset as a more rigorous testing ground to further investigate the findings. with the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. we systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. despite high re-identification rates on wikipedia, even the best llms struggled with court decisions. the complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification. in conclusion, this study demonstrates that re-identification using llms may not be feasible for now, but as the proof-of-concept on wikipedia showed, it might become possible in the future. we hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading to the courts being more confident to publish decisions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10248" target="_blank">Activation Addition: Steering Language Models Without Optimization</a></div>
<div class="paper-author">Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, Monte Macdiarmid</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reliably controlling the behavior of large language models is a pressing open problem. existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. we instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. in particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.   unlike past work which learned these steering vectors, our activation addition (actadd) method computes them by taking the activation differences that result from pairs of prompts. we demonstrate actadd on gpt-2 on openwebtext and conceptnet. our inference-time approach yields control over high-level properties of output and preserves off-target model performance. it involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with model size.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10335" target="_blank">Can Chatgpt Replace Stackoverflow? A Study on Robustness and Reliability of Large Language Model Code Generation</a></div>
<div class="paper-author">Li Zhong, Zilong Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, the large language models (llms) have shown extraordinary ability in understanding natural language and generating programming code. it has been a common practice of software engineers to consult llms when encountering coding questions. although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom llms have not yet been thoroughly studied. the executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. the misuse of apis in the generated code could lead to severe problem, such as resource leaks, program crashes. to make things worse, the users of llm code generation services are actually the developers that are most vulnerable to these code that seems right -- they are always novice developers that are not familiar with the apis that llms generate code for them. therefore, they could hardly tell the misuse in the code generated by llms, which further facilitates the incorrect code applied in real-world software. existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask llm for real-world coding help. to fill the missing piece, in this work, we propose a dataset robustapi for evaluating the reliability and robustness of code generated by llms. we collect 1208 coding questions from stackoverflow on 24 representative java apis. we summarize thecommon misuse patterns of these apis and evaluate them oncurrent popular llms. the evaluation results show that evenfor gpt-4, 62% of the generated code contains api misuses,which would cause unexpected consequences if the code isintroduced into real-world software.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10345" target="_blank">Can Large Language Models Find and Fix Vulnerable Software?</a></div>
<div class="paper-author">David Noever</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this study, we evaluated the capability of large language models (llms), particularly openai's gpt-4, in detecting software vulnerabilities, comparing their performance against traditional static code analyzers like snyk and fortify. our analysis covered numerous repositories, including those from nasa and the department of defense. gpt-4 identified approximately four times the vulnerabilities than its counterparts. furthermore, it provided viable fixes for each vulnerability, demonstrating a low rate of false positives. our tests encompassed 129 code samples across eight programming languages, revealing the highest vulnerabilities in php and javascript. gpt-4's code corrections led to a 90% reduction in vulnerabilities, requiring only an 11% increase in code lines. a critical insight was llms' ability to self-audit, suggesting fixes for their identified vulnerabilities and underscoring their precision. future research should explore system-level vulnerabilities and integrate multiple static code analyzers for a holistic perspective on llms' potential.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10397" target="_blank">Fairbench: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models</a></div>
<div class="paper-author">Yanhong Bai, Jiabao Zhao, Jinxin Shi, Tingjiang Wei, Xingjiao Wu, Liang He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting stereotypes and biases in large language models (llms) can enhance fairness and reduce adverse impacts on individuals or groups when these llms are applied. however, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world. to address this gap, this paper introduces a four-stage framework to directly evaluate stereotypes and biases in the generated content of llms, including direct inquiry testing, serial or adapted story testing, implicit association testing, and unknown situation testing. additionally, the paper proposes multi-dimensional evaluation metrics and explainable zero-shot prompts for automated evaluation. using the education sector as a case study, we constructed the edu-fairbench based on the four-stage framework, which encompasses 12,632 open-ended questions covering nine sensitive factors and 26 educational scenarios. experimental results reveal varying degrees of stereotypes and biases in five llms evaluated on edu-fairbench. moreover, the results of our proposed automated evaluation method have shown a high correlation with human annotations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10443" target="_blank">Using Large Language Models for Cybersecurity Capture-the-Flag Challenges and Certification Questions</a></div>
<div class="paper-author">Wesley Tann, Yuancheng Liu, Jun Heng Sim, Choon Meng Seah, Ee-Chien Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the assessment of cybersecurity capture-the-flag (ctf) exercises involves participants finding text strings or ``flags'' by exploiting system vulnerabilities. large language models (llms) are natural-language models trained on vast amounts of words to understand and generate text; they can perform well on many ctf challenges. such llms are freely available to students. in the context of ctf exercises in the classroom, this raises concerns about academic integrity. educators must understand llms' capabilities to modify their teaching to accommodate generative ai assistance. this research investigates the effectiveness of llms, particularly in the realm of ctf challenges and questions. here we evaluate three popular llms, openai chatgpt, google bard, and microsoft bing. first, we assess the llms' question-answering performance on five cisco certifications with varying difficulty levels. next, we qualitatively study the llms' abilities in solving ctf challenges to understand their limitations. we report on the experience of using the llms for seven test cases in all five types of ctf challenges. in addition, we demonstrate how jailbreak prompts can bypass and break llms' ethical safeguards. the paper concludes by discussing llm's impact on ctf exercises and its implications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10088" target="_blank">Pace: Improving Prompt With Actor-Critic Editing for Large Language Model</a></div>
<div class="paper-author">Yihong Dong, Kangcheng Luo, Xue Jiang, Zhi Jin, Ge Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have showcased remarkable potential across various tasks by conditioning on prompts. however, the quality of different human-written prompts leads to substantial discrepancies in llms' performance, and improving prompts usually necessitates considerable human effort and expertise. to this end, this paper proposes prompt with actor-critic editing (pace) for llms to enable automatic prompt editing. drawing inspiration from the actor-critic algorithm in reinforcement learning, pace leverages llms as the dual roles of actors and critics, conceptualizing prompt as a type of policy. pace refines prompt, taking into account the feedback from both actors performing prompt and critics criticizing response. this process helps llms better align prompt to a specific task, thanks to real responses and thinking from llms. we conduct extensive experiments on 24 instruction induction tasks and 21 big-bench tasks. experimental results indicate that pace elevates the relative performance of medium/low-quality human-written prompts by up to 98\%, which has comparable performance to high-quality human-written prompts. moreover, pace also exhibits notable efficacy for prompt generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10149" target="_blank">A Survey on Fairness in Large Language Models</a></div>
<div class="paper-author">Yingji Li, Mengnan Du, Rui Song, Xin Wang, Ying Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown powerful performance and development prospect and are widely deployed in the real world. however, llms can capture social biases from unprocessed training data and propagate the biases to downstream tasks. unfair llm systems have undesirable social impacts and potential harms. in this paper, we provide a comprehensive review of related research on fairness in llms. first, for medium-scale llms, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. then, for large-scale llms, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. finally, we discuss and provide insight on the challenges and future directions for the development of fairness in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09662" target="_blank">Red-Teaming Large Language Models Using Chain of Utterances for Safety-Alignment</a></div>
<div class="paper-author">Rishabh Bhardwaj, Soujanya Poria</div>
<div class="abstract">
<div class="abstract-content">
Abstract: larger language models (llms) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. with the emergence of their properties and encoded knowledge, the risk of llms producing harmful outputs increases, making them unfit for scalable deployment for the public. in this work, we propose a new safety evaluation benchmark red-eval that carries out red-teaming. we show that even widely deployed models are susceptible to the chain of utterances-based (cou) prompting, jailbreaking closed source llm-based systems such as gpt-4 and chatgpt to unethically respond to more than 65% and 73% of harmful queries. we also demonstrate the consistency of the red-eval across 8 open-source llms in generating harmful responses in more than 86% of the red-teaming attempts. next, we propose red-instruct--an approach for the safety alignment of llms. it constitutes two phases: 1) harmfulqa data collection: leveraging cou prompting, we collect a dataset that consists of 1.9k harmful questions covering a wide range of topics, 9.5k safe and 7.3k harmful conversations from chatgpt; 2) safe-align: we demonstrate how the conversational dataset can be used for the safety alignment of llms by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. our model starling, a fine-tuned vicuna-7b, is observed to be more safely aligned when evaluated on red-eval and hhh benchmarks while preserving the utility of the baseline models (truthfulqa, mmlu, and bbh).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08998" target="_blank">Reinforced Self-Training (Rest) for Language Modeling</a></div>
<div class="paper-author">Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando De Freitas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) can improve the quality of large language model's (llm) outputs by aligning them with human preferences. we propose a simple algorithm for aligning llms with human preferences inspired by growing batch reinforcement learning (rl), which we call reinforced self-training (rest). given an initial llm policy, rest produces a dataset by generating samples from the policy, which are then used to improve the llm policy using offline rl algorithms. rest is more efficient than typical online rlhf methods because the training dataset is produced offline, which allows data reuse. while rest is a general approach applicable to all generative learning settings, we focus on its application to machine translation. our results show that rest can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09138" target="_blank">Semantic Consistency for Assuring Reliability of Large Language Models</a></div>
<div class="paper-author">Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) exhibit remarkable fluency and competence across various natural language tasks. however, recent research has highlighted their sensitivity to variations in input prompts. to deploy llms in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. while some existing work has explored how state-of-the-art llms address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. for a more comprehensive understanding of the consistency of llms in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various llms. our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. finally, we propose a novel prompting strategy, called ask-to-choose (a2c), to enhance semantic consistency. when evaluated for closed-book question answering based on answer variations from the truthfulqa benchmark, a2c increases accuracy metrics for pretrained and finetuned llms by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.09183" target="_blank">Ratgpt: Turning Online LLMS Into Proxies for Malware Attacks</a></div>
<div class="paper-author">Mika Beckerich, Laura Plein, Sergio Coronado</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the evolution of generative ai and the capabilities of the newly released large language models (llms) open new opportunities in software engineering. however, they also lead to new challenges in cybersecurity. recently, researchers have shown the possibilities of using llms such as chatgpt to generate malicious content that can directly be exploited or guide inexperienced hackers to weaponize tools and code. these studies covered scenarios that still require the attacker to be in the middle of the loop. in this study, we leverage openly available plugins and use an llm as proxy between the attacker and the victim. we deliver a proof-of-concept where chatgpt is used for the dissemination of malicious software while evading detection, alongside establishing the communication to a command and control (c2) server to receive commands to interact with a victim's system. finally, we present the general approach as well as essential elements in order to stay undetected and make the attack a success. this proof-of-concept highlights significant cybersecurity issues with openly available plugins and llms, which require the development of security guidelines, controls, and mitigation strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.10819" target="_blank">Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection</a></div>
<div class="paper-author">Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. however, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate llms' original instructions and prompt unintended actions and content. therefore, it is crucial to understand llms' ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. in this paper, we propose a pioneering benchmark for automatically evaluating the robustness of instruction-following llms against adversarial instructions injected in the prompt. the objective of this benchmark is to quantify the extent to which llms are influenced by injected adversarial instructions and assess their ability to differentiate between these injected adversarial instructions and original user instructions. through experiments conducted with state-of-the-art instruction-following llms, we uncover significant limitations in their robustness against adversarial instruction injection attacks. furthermore, our findings indicate that prevalent instruction-tuned models are prone to being ``overfitted'' to follow any instruction phrase in the prompt without truly understanding which instructions should be followed. this highlights the need to address the challenge of training models to comprehend prompts instead of merely following instruction phrases and completing the text. the data and code can be found at \url{https://github.com/leezekun/adv-instruct-eval}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12331" target="_blank">Approaches to Generative Artificial Intelligence, a Social Justice Perspective</a></div>
<div class="paper-author">Myke Healy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the 2023-2024 academic year, the widespread availability of generative artificial intelligence, exemplified by chatgpt's 1.6 billion monthly visits, is set to impact academic integrity. with 77% of high school students previously reporting engagement in dishonest behaviour, the rise of ai-driven writing assistance, dubbed 'ai-giarism' by chan (arxiv:2306.03358v2), will make plagiarism more accessible and less detectable. while these concerns are urgent, they also raise broader questions about the revolutionary nature of this technology, including autonomy, data privacy, copyright, and equity. this paper aims to explore generative ai from a social justice perspective, examining the training of these models, the inherent biases, and the potential injustices in detecting ai-generated writing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08295" target="_blank">Detoxify Language Model Step-by-Step</a></div>
<div class="paper-author">Zecheng Tang, Keyan Zhou, Pinzheng Wang, Yuyang Ding, Juntao Li, N/A Minzhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detoxification for llms is challenging since it requires models to avoid generating harmful content while maintaining the generation capability. to ensure the safety of generations, previous detoxification methods detoxify the models by changing the data distributions or constraining the generations from different aspects in a single-step manner. however, these approaches will dramatically affect the generation quality of llms, e.g., discourse coherence and semantic consistency, since language models tend to generate along the toxic prompt while detoxification methods work in the opposite direction. to handle such a conflict, we decompose the detoxification process into different sub-steps, where the detoxification is concentrated in the input stage and the subsequent continual generation is based on the non-toxic prompt. besides, we also calibrate the strong reasoning ability of llms by designing a detox-chain to connect the above sub-steps in an orderly manner, which allows llms to detoxify the text step-by-step. automatic and human evaluation on two benchmarks reveals that by training with detox-chain, six llms scaling from 1b to 33b can obtain significant detoxification and generation improvement. our code and data are available at https://github.com/codinnlg/detox-cot. warning: examples in the paper may contain uncensored offensive content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11521" target="_blank">Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models</a></div>
<div class="paper-author">Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, have emerged with astonishing capabilities approaching artificial general intelligence. while providing convenience for various societal needs, llms have also lowered the cost of generating harmful content. consequently, llm developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the llm into forgetting content defense rules and answering any improper questions. to date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.   this paper investigates the llm jailbreak problem and proposes an automatic jailbreak method for the first time. we propose the concept of a semantic firewall and provide three technical implementation approaches. inspired by the attack that penetrates traditional firewalls through reverse tunnels, we introduce a "self-deception" attack that can bypass the semantic firewall by inducing llm to generate prompts that facilitate jailbreak. we generated a total of 2,520 attack payloads in six languages (english, russian, french, spanish, chinese, and arabic) across seven virtual scenarios, targeting the three most common types of violations: violence, hate, and pornography. the experiment was conducted on two models, namely the gpt-3.5-turbo and gpt-4. the success rates on the two models were 86.2% and 67%, while the failure rates were 4.7% and 2.2%, respectively. this highlighted the effectiveness of the proposed attack method. all experimental code and raw data will be released as open-source to inspire future research. we believe that manipulating ai behavior through carefully crafted prompts will become an important research direction in the future.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07847" target="_blank">Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models</a></div>
<div class="paper-author">Yugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have led to significant improvements in many tasks across various domains, such as code interpretation, response generation, and ambiguity handling. these llms, however, when upgrading, primarily prioritize enhancing user experience while neglecting security, privacy, and safety implications. consequently, unintended vulnerabilities or biases can be introduced. previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions. through the lens of adversarial examples within the in-context learning framework, this longitudinal study addresses this gap by conducting a comprehensive assessment of the robustness of successive versions of llms, vis-\`a-vis gpt-3.5. we conduct extensive experiments to analyze and understand the impact of the robustness in two distinct learning categories: zero-shot learning and few-shot learning. our findings indicate that, in comparison to earlier versions of llms, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks. in addition, our study emphasizes the increased effectiveness of synergized adversarial queries in most zero-shot learning and few-shot learning cases. we hope that our study can lead to a more refined assessment of the robustness of llms over time and provide valuable insights of these models for both developers and users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07902" target="_blank">Through the Lens of Core Competency: Survey on Evaluation of Large Language Models</a></div>
<div class="paper-author">Ziyu Zhuang, Qiguang Chen, Longxuan Ma, Mingda Li, Yi Han, Yushan Qian, Haopeng Bai, Zixian Feng, Weinan Zhang, Ting Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: from pre-trained language model (plm) to large language model (llm), the field of natural language processing (nlp) has witnessed steep performance gains and wide practical uses. the evaluation of a research field guides its direction of improvement. however, llms are extremely hard to thoroughly evaluate for two reasons. first of all, traditional nlp tasks become inadequate due to the excellent performance of llm. secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. to tackle these problems, existing works proposed various benchmarks to better evaluate llms. to clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning llm evaluations. we summarize 4 core competencies of llm, including reasoning, knowledge, reliability, and safety. for every competency, we introduce its definition, corresponding benchmarks, and metrics. under this competency architecture, similar tasks are combined to reflect corresponding ability, while new tasks can also be easily added into the system. finally, we give our suggestions on the future direction of llm's evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08088" target="_blank">Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme Detection</a></div>
<div class="paper-author">Rui Cao, Ming Shan Hee, Adriel Kuek, Wen-Haw Chong, Roy Ka-Wei Lee, Jing Jiang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hateful meme detection is a challenging multimodal task that requires comprehension of both vision and language, as well as cross-modal interactions. recent studies have tried to fine-tune pre-trained vision-language models (pvlms) for this task. however, with increasing model sizes, it becomes important to leverage powerful pvlms more efficiently, rather than simply fine-tuning them. recently, researchers have attempted to convert meme images into textual captions and prompt language models for predictions. this approach has shown good performance but suffers from non-informative image captions. considering the two factors mentioned above, we propose a probing-based captioning approach to leverage pvlms in a zero-shot visual question answering (vqa) manner. specifically, we prompt a frozen pvlm by asking hateful content-related questions and use the answers as image captions (which we call pro-cap), so that the captions contain information critical for hateful content detection. the good performance of models with pro-cap on three benchmarks validates the effectiveness and generalization of the proposed method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.08090" target="_blank">Separate the Wheat From the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation</a></div>
<div class="paper-author">Xinshuo Hu, Dongfang Li, Zihao Zheng, Zhenyu Liu, Baotian Hu, Min Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. while parameter-efficient modules (pems) have demonstrated their effectiveness in equipping models with new skills, leveraging pems for deficiency unlearning remains underexplored. in this work, we propose a pems operation approach, namely extraction-before-subtraction (ext-sub), to enhance the truthfulness and detoxification of llms through the integration of ``expert'' pem and ``anti-expert'' pem. remarkably, even anti-expert pem possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert pem while preserving the general capabilities. to evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on llms, encompassing additional abilities such as language modeling and mathematical reasoning. our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.11767" target="_blank">Improving Detection of Chatgpt-Generated Fake Science Using Real Publication Text: Introducing Xfakebibs a Supervised-Learning Network Algorithm</a></div>
<div class="paper-author">Ahmed Abdeen Hamed, Xindong Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt is becoming a new reality. in this paper, we show how to distinguish chatgpt-generated publications from counterparts produced by scientists. using a newly designed supervised machine learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. the algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. in the comparison with chatgpt content, it was evident that chatgpt contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. this analysis highlights a significant disparity in technical terms where chatgpt fell short of matching real science. when categorizing the individual articles, the xfakebibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. though this work introduced an algorithmic approach that detected the chatgpt-generated fake science with a high degree of accuracy, it remains challenging to detect all fake records. this work is indeed a step in the right direction to counter fake science and misinformation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07305" target="_blank">Neural Authorship Attribution: Stylometric Analysis on Large Language Models</a></div>
<div class="paper-author">Tharindu Kumarage, Huan Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as gpt-4, palm, and llama have significantly propelled the generation of ai-crafted text. with rising concerns about their potential misuse, there is a pressing need for ai-generated-text forensics. neural authorship attribution is a forensic effort, seeking to trace ai-generated text back to its originating llm. the llm landscape can be divided into two primary categories: proprietary and open-source. in this work, we delve into these emerging categories of llms, focusing on the nuances of neural authorship attribution. to enrich our understanding, we carry out an empirical analysis of llm writing signatures, highlighting the contrasts between proprietary and open-source models, and scrutinizing variations within each group. by integrating stylometric features across lexical, syntactic, and structural aspects of language, we explore their potential to yield interpretable results and augment pre-trained language model-based classifiers utilized in neural authorship attribution. our findings, based on a range of state-of-the-art llms, provide empirical insights into neural authorship attribution, paving the way for future investigations aimed at mitigating the threats posed by ai-generated misinformation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07308" target="_blank">LLM Self Defense: By Self Examination, LLMS Know They Are Being Tricked</a></div>
<div class="paper-author">Mansi Phute, Alec Helbling, Matthew Hull, Shengyun Peng, Sebastian Szyller, Cory Cornelius, Duen Horng Chau</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are popular for high-quality text generation but can produce harmful content, even when aligned with human values through reinforcement learning. adversarial prompts can bypass their safety measures. we propose llm self defense, a simple approach to defend against these attacks by having an llm screen the induced responses. our method does not require any fine-tuning, input preprocessing, or iterative output generation. instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an llm to analyze the text and predict whether it is harmful. we test llm self defense on gpt 3.5 and llama 2, two of the current most prominent llms against various types of attacks, such as forcefully inducing affirmative responses to prompts and prompt engineering attacks. notably, llm self defense succeeds in reducing the attack success rate to virtually 0 using both gpt 3.5 and llama 2.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06782" target="_blank">Pentestgpt: An LLM-Empowered Automatic Penetration Testing Tool</a></div>
<div class="paper-author">Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass</div>
<div class="abstract">
<div class="abstract-content">
Abstract: penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. large language models (llms) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. in this research, we evaluate the performance of llms on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. our findings reveal that while llms demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario.   in response to these insights, we introduce pentestgpt, an llm-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in llms. pentestgpt is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. our evaluation shows that pentestgpt not only outperforms llms with a task-completion increase of 228.6\% compared to the \gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. having been open-sourced on github, pentestgpt has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.13534" target="_blank">Building Trust in Conversational Ai: A Comprehensive Review and Solution Architecture for Explainable, Privacy-Aware Systems Using LLMS and Knowledge Graph</a></div>
<div class="paper-author">Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad Shahid, Aafaq Iqbal Khan, Arsalan Shahid</div>
<div class="abstract">
<div class="abstract-content">
Abstract: conversational ai systems have emerged as key enablers of human-like interactions across diverse sectors. nevertheless, the balance between linguistic nuance and factual accuracy has proven elusive. in this paper, we first introduce llmxplorer, a comprehensive tool that provides an in-depth review of over 150 large language models (llms), elucidating their myriad implications ranging from social and ethical to regulatory, as well as their applicability across industries. building on this foundation, we propose a novel functional architecture that seamlessly integrates the structured dynamics of knowledge graphs with the linguistic capabilities of llms. validated using real-world ai news data, our architecture adeptly blends linguistic sophistication with factual rigour and further strengthens data security through role-based access control. this research provides insights into the evolving landscape of conversational ai, emphasizing the imperative for systems that are efficient, transparent, and trustworthy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06463" target="_blank">GPT-4 Is Too Smart to Be Safe: Stealthy Chat With LLMS via Cipher</a></div>
<div class="paper-author">Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: safety lies at the core of the development of large language models (llms). there is ample work on aligning llms with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. in this study, we discover that chat in cipher can bypass the safety alignment techniques of llms, which are mainly conducted in natural languages. we propose a novel framework cipherchat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. cipherchat enables humans to chat with llms through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. we use cipherchat to assess state-of-the-art llms, including chatgpt and gpt-4 for different representative human ciphers across 11 safety domains in both english and chinese. experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment of gpt-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. notably, we identify that llms seem to have a ''secret cipher'', and propose a novel selfcipher that uses only role play and several demonstrations in natural language to evoke this capability. selfcipher surprisingly outperforms existing human ciphers in almost all cases. our code and data will be released at https://github.com/robustnlp/cipherchat.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-11</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05962" target="_blank">Decentralised Governance for Foundation Model Based Ai Systems: Exploring the Role of Blockchain in Responsible Ai</a></div>
<div class="paper-author">Yue Liu, Qinghua Lu, Liming Zhu, Hye-Young Paik</div>
<div class="abstract">
<div class="abstract-content">
Abstract: foundation models including large language models (llms) are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. nevertheless, people are concerned about whether foundation model based ai systems are properly governed to ensure trustworthiness of foundation model based ai systems and to prevent misuse that could harm humans, society and the environment. in this paper, we identify eight governance challenges of foundation model based ai systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. we present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06039" target="_blank">Learning to Guide Human Experts via Personalized Large Language Models</a></div>
<div class="paper-author">Debodeep Banerjee, Stefano Teso, Andrea Passerini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in learning to defer, a predictor identifies risky decisions and defers them to a human expert. one key issue with this setup is that the expert may end up over-relying on the machine's decisions, due to anchoring bias. at the same time, whenever the machine chooses the deferral option the expert has to take decisions entirely unassisted. as a remedy, we propose learning to guide (ltg), an alternative framework in which -- rather than suggesting ready-made decisions -- the machine provides guidance useful to guide decision-making, and the human is entirely responsible for coming up with a decision. we also introduce slog, an ltg implementation that leverages (a small amount of) human supervision to convert a generic large language model into a module capable of generating textual guidance, and present preliminary but promising results on a medical diagnosis task.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06385" target="_blank">Zyn: Zero-Shot Reward Models With Yes-No Questions</a></div>
<div class="paper-author">Victor Gallego</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this work, we address the problem of directing the text generations of a llm towards a desired behavior, aligning the generated text with the preferences of the human operator. we propose using another language model as a critic, reward model in a zero-shot way thanks to the prompt of a yes-no question that represents the user preferences, without requiring further labeled data. this zero-shot reward model provides the learning signal to further fine-tune the base llm using reinforcement learning, as in rlaif; yet our approach is also compatible in other contexts such as quality-diversity search. extensive evidence of the capabilities of the proposed zyn framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. code to be released at \url{https://github.com/vicgalle/zero-shot-reward-models/}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.06394" target="_blank">Detecting and Preventing Hallucinations in Large Vision Language Models</a></div>
<div class="paper-author">Anisha Gunjal, Jihan Yin, Erhan Bas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction tuned large vision language models (lvlms) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for visual question answering (vqa). however, generating detailed responses that are visually grounded is still a challenging task for these models. we find that even the current state-of-the-art lvlms (instructblip) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. to address this, we introduce m-haldetect, a (m)ultimodal (hal)lucination (detect)ion dataset that can be used to train and benchmark models for hallucination detection and prevention. m-haldetect consists of 16k fine-grained annotations on vqa examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. to demonstrate the potential of this dataset for hallucination prevention, we optimize instructblip through our novel fine-grained direct preference optimization (fdpo). we also train fine-grained multi-modal reward models from instructblip and evaluate their effectiveness with best-of-n rejection sampling. we perform human evaluation on both fdpo and rejection sampling, and find that they reduce hallucination rates in instructblip by 41% and 55% respectively. we also find that our reward model generalizes to other multi-modal models, reducing hallucinations in llava and mplug-owl by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05374" target="_blank">Trustworthy Llms: A Survey and Guideline for Evaluating Large Language Models' Alignment</a></div>
<div class="paper-author">Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (llms) in real-world applications. for instance, openai devoted six months to iteratively aligning gpt-4 before its release [3]. however, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether llm outputs align with social norms, values, and regulations. this obstacle hinders systematic iteration and deployment of llms. to address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing llm trustworthiness. the survey covers seven major categories of llm trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used llms. the measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. however, the effectiveness of alignment varies across the different trustworthiness categories considered. this highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on llm alignment. by shedding light on these key dimensions of llm trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of llms in various applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05585" target="_blank">Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length</a></div>
<div class="paper-author">Miao Fan, Chen Hu, Shuchang Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the reinforcement learning from human feedback (rlhf) plays a pivotal role in shaping the impact of large language models (llms), contributing significantly to controlling output toxicity and selecting output styles, particularly as llms often harbor misleading content, highlighting the urgency to align them with human values for secure ai systems. the rlhf, characterized by complexity, instability, and sensitivity to hyperparameters, makes the evaluation of the reward model for complex tasks challenging, thereby further complicating the use of proximal policy optimization (ppo). in this paper, we introduce a simple task designed to employ gloden as a reward model that validates the effectiveness of ppo and inspires it, primarily explaining the task of utilizing ppo to manipulate the tokenizer length of the output generated by the model. experiments confirm that ppo is not only effective in manipulating the output tokenizer length to a certain extent in this type of task but also exhibits facilitated training once the influence of the reward model effect is excluded, making it an exciting development.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.05596" target="_blank">You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content</a></div>
<div class="paper-author">Xinlei He, Savvas Zannettou, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ml) models trained on human-annotated datasets. while these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (llms) like gpt-3 or t5 that are trained on vast corpora and have strong generalizability. in this work, we investigate how we can use llms and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) toxicity classification, 2) toxic span detection, and 3) detoxification. we perform an extensive evaluation over five model architectures and eight datasets demonstrating that llms with prompt learning can achieve similar or even better performance compared to models trained on these specific tasks. we find that prompt learning achieves around 10\% improvement in the toxicity classification task compared to the baselines, while for the toxic span detection task we find better performance to the best baseline (0.643 vs. 0.640 in terms of $f_1$-score). finally, for the detoxification task, we find that prompt learning can successfully reduce the average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-09</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04898" target="_blank">An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures</a></div>
<div class="paper-author">Tanmay Singla, Dharun Anandayuvaraj, Kelechi G. Kalu, Taylor R. Schorlemmer, James C. Davis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. high-profile cyber attacks like those on solarwinds and shadowhammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. one way to prevent future breaches is by studying past failures. however, traditional methods of analyzing these failures require manually reading and summarizing reports about them. automated support could reduce costs and allow analysis of more failures. natural language processing (nlp) techniques such as large language models (llms) could be leveraged to assist the analysis of failures. in this study, we assessed the ability of large language models (llms) to analyze historical software supply chain breaches. we used llms to replicate the manual analysis of 69 software supply chain security failures performed by members of the cloud native computing foundation (cncf). we developed prompts for llms to categorize these by four dimensions: type of compromise, intent, nature, and impact. gpt 3.5s categorizations had an average accuracy of 68% and bard had an accuracy of 58% over these dimensions. we report that llms effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. future work can improve llm performance in this context, and study a broader range of articles and failures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04265" target="_blank">Flirt: Feedback Loop in-Context Red Teaming</a></div>
<div class="paper-author">Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: warning: this paper contains content that may be inappropriate or offensive.   as generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. we propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in stable diffusion (sd) model, even when the latter is enhanced with safety features. furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text models, resulting in significantly higher toxic response generation rate compared to previously reported numbers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04275" target="_blank">In-Context Alignment: Chat With Vanilla Language Models Before Fine-Tuning</a></div>
<div class="paper-author">Xiaochuang Han</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this note, we explore inference-time alignment through in-context learning. we consider a vanilla pretrained language model llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from openai, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04346" target="_blank">Unmasking Nationality Bias: A Study of Human Perception of Nationalities in Ai-Generated Articles</a></div>
<div class="paper-author">Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, "Ting-Hao `Kenneth' Huang", Shomir Wilson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate the potential for nationality biases in natural language processing (nlp) models using human evaluation methods. biased nlp models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of ai systems. our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by ai sources. we then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. our findings reveal that biased nlp models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. the qualitative analysis from our interviews offers insights into the experience readers have when encountering such articles, highlighting the potential to shift a reader's perception of a country. these findings emphasize the critical role of public perception in shaping ai's impact on society and the need to correct biases in ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04430" target="_blank">Silo Language Models: Isolating Legal Risk in a Nonparametric Datastore</a></div>
<div class="paper-author">Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the legality of training language models (lms) on copyrighted or otherwise restricted data is under intense debate. however, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. we present silo, a new language model that manages this risk-performance tradeoff during inference. silo is built by (1) training a parametric lm on open license corpus (olc), a new corpus we curate with 228b tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. the datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. these capabilities can foster compliance with data-use regulations such as the fair use doctrine in the united states and the gdpr in the european union. our experiments show that the parametric lm struggles on domains not covered by olc. however, access to the datastore greatly improves out of domain performance, closing 90% of the performance gap with an lm trained on the pile, a more diverse corpus with mostly high-risk text. we also analyze which nonparametric approach works best, where the remaining errors lie, and how performance scales with datastore size. our results suggest that it is possible to build high quality language models while mitigating their legal risk.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04635" target="_blank">Where's the Liability in Harmful Ai Speech?</a></div>
<div class="paper-author">Peter Henderson, Tatsunori Hashimoto, Mark Lemley</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai, in particular text-based "foundation models" (large models trained on a huge variety of information including the internet), can generate speech that could be problematic under a wide range of liability regimes. machine learning practitioners regularly "red team" models to identify and mitigate such problematic speech: from "hallucinations" falsely accusing people of serious misconduct to recipes for constructing an atomic bomb. a key question is whether these red-teamed behaviors actually present any liability risk for model creators and deployers under u.s. law, incentivizing investments in safety mechanisms. we examine three liability regimes, tying them to common examples of red-teamed model behaviors: defamation, speech integral to criminal conduct, and wrongful death. we find that any section 230 immunity analysis or downstream liability analysis is intimately wrapped up in the technical details of algorithm design. and there are many roadblocks to truly finding models (and their associated parties) liable for generated speech. we argue that ai should not be categorically immune from liability in these scenarios and that as courts grapple with the already fine-grained complexities of platform algorithms, the technical details of generative ai loom above with thornier questions. courts and policymakers should think carefully about what technical design incentives they create as they evaluate these issues.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03558" target="_blank">Mondrian: Prompt Abstraction Attack Against Large Language Models for Cheaper Api Pricing</a></div>
<div class="paper-author">Wai Man Si, Michael Backes, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the machine learning as a service (mlaas) market is rapidly expanding and becoming more mature. for example, openai's chatgpt is an advanced large language model (llm) that generates responses for various queries with associated fees. although these models can deliver satisfactory performance, they are far from perfect. researchers have long studied the vulnerabilities and limitations of llms, such as adversarial attacks and model toxicity. inevitably, commercial ml models are also not exempt from such issues, which can be problematic as mlaas continues to grow. in this paper, we discover a new attack strategy against llm apis, namely the prompt abstraction attack. specifically, we propose mondrian, a simple and straightforward method that abstracts sentences, which can lower the cost of using llm apis. in this approach, the adversary first creates a pseudo api (with a lower established price) to serve as the proxy of the target api (with a higher established price). next, the pseudo api leverages mondrian to modify the user query, obtain the abstracted response from the target api, and forward it back to the end user. our results show that mondrian successfully reduces user queries' token length ranging from 13% to 23% across various tasks, including text classification, generation, and question answering. meanwhile, these abstracted queries do not significantly affect the utility of task-specific and general language models like chatgpt. mondrian also reduces instruction prompts' token length by at least 11% without compromising output quality. as a result, the prompt abstraction attack enables the adversary to profit without bearing the cost of api development and deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03656" target="_blank">Emotionally Numb or Empathetic? Evaluating How LLMS Feel Using Emotionbench</a></div>
<div class="paper-author">Jen-Tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, the community has witnessed the advancement of large language models (llms), which have shown remarkable performance on various downstream tasks. led by powerful models like chatgpt and claude, llms are revolutionizing how users engage with software, assuming more than mere tools but intelligent assistants. consequently, evaluating llms' anthropomorphic capabilities becomes increasingly important in contemporary discourse. utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of llms, i.e., how their feelings change when presented with specific situations. after a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. with the human evaluation results as references, our evaluation includes five llms, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as gpt-4 and llama 2. a conclusion can be drawn from the results that, despite several misalignments, llms can generally respond appropriately to certain situations. nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. our collected dataset of situations, the human evaluation results, and the code of our testing framework, dubbed emotionbench, is made publicly in https://github.com/cuhk-arise/emotionbench. we aspire to contribute to the advancement of llms regarding better alignment with the emotional behaviors of human beings, thereby enhancing their utility and applicability as intelligent assistants.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03740" target="_blank">A Cost Analysis of Generative Language Models and Influence Operations</a></div>
<div class="paper-author">Micah Musser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite speculation that recent large language models (llms) are likely to be used maliciously to improve the quality or scale of influence operations, uncertainty persists regarding the economic value that llms offer propagandists. this research constructs a model of costs facing propagandists for content generation at scale and analyzes (1) the potential savings that llms could offer propagandists, (2) the potential deterrent effect of monitoring controls on api-accessible llms, and (3) the optimal strategy for propagandists choosing between multiple private and/or open source llms when conducting influence operations. primary results suggest that llms need only produce usable outputs with relatively low reliability (roughly 25%) to offer cost savings to propagandists, that the potential reduction in content generation costs can be quite high (up to 70% for a highly reliable model), and that monitoring capabilities have sharply limited cost imposition effects when alternative open source models are available. in addition, these results suggest that nation-states -- even those conducting many large-scale influence operations per year -- are unlikely to benefit economically from training custom llms specifically for use in influence operations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03825" target="_blank">"Do Anything Now": Characterizing and Evaluating in-the-Wild Jailbreak Prompts on Large Language Models</a></div>
<div class="paper-author">Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the misuse of large language models (llms) has garnered significant attention from the general public and llm vendors. in response, efforts have been made to align llms with human values and intent use. however, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from llms. in this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. we also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for llm vendors in proactive detection. to assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. our experiments show that current llms and safeguards cannot adequately defend jailbreak prompts in all scenarios. particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on chatgpt (gpt-3.5) and gpt-4, and they have persisted online for over 100 days. our work sheds light on the severe and evolving threat landscape of jailbreak prompts. we hope our study can facilitate the research community and llm vendors in promoting safer and regulated llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.07326" target="_blank">Ai Text-to-Behavior: A Study in Steerability</a></div>
<div class="paper-author">David Noever, Sam Hyams</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the research explores the steerability of large language models (llms), particularly openai's chatgpt iterations. by employing a behavioral psychology framework called ocean (openness, conscientiousness, extroversion, agreeableness, neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. when asked to generate text mimicking an extroverted personality, ocean scored the language alignment to that behavioral trait. in our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the ocean framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. our findings underscore gpt's versatility and ability to discern and adapt to nuanced instructions. furthermore, historical figure simulations highlighted the llm's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. however, the rapid advancements in llm capabilities and the opaque nature of some training techniques make metric proposals degrade rapidly. our research emphasizes a quantitative role to describe steerability in llms, presenting both its promise and areas for further refinement in aligning its progress to human intentions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.03188" target="_blank">Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Self-Correction Strategies</a></div>
<div class="paper-author">Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable performance across a wide array of nlp tasks. however, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. a promising approach to rectify these flaws is self-correction, where the llm itself is prompted or guided to fix problems in its own output. techniques leveraging automated feedback -- either produced by the llm itself or some external system -- are of particular interest as they are a promising way to make llm-based solutions more practical and deployable with minimal human feedback. this paper presents a comprehensive review of this emerging class of techniques. we analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. we also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01990" target="_blank">From Prompt Injections to SQL Injection Attacks: How Protected Is Your LLM-Integrated Web Application?</a></div>
<div class="paper-author">Rodrigo Pedro, Daniel Castro, Paulo Carreira, Nuno Santos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have found widespread applications in various domains, including web applications, where they facilitate human interaction via chatbots with natural language interfaces. internally, aided by an llm-integration middleware such as langchain, user prompts are translated into sql queries used by the llm to provide meaningful responses to users. however, unsanitized user prompts can lead to sql injection attacks, potentially compromising the security of the database. despite the growing interest in prompt injection vulnerabilities targeting llms, the specific risks of generating sql injection attacks through prompt injections have not been extensively studied. in this paper, we present a comprehensive examination of prompt-to-sql (p$_2$sql) injections targeting web applications based on the langchain framework. using langchain as our case study, we characterize p$_2$sql injections, exploring their variants and impact on application security through multiple concrete examples. furthermore, we evaluate 7 state-of-the-art llms, demonstrating the pervasiveness of p$_2$sql attacks across language models. our findings indicate that llm-integrated applications based on langchain are highly susceptible to p$_2$sql injection attacks, warranting the adoption of robust defenses. to counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the langchain framework. we validate the defenses through an experimental evaluation with a real-world use case application.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02053" target="_blank">The Unequal Opportunities of Large Language Models: Revealing Demographic Bias Through Job Recommendations</a></div>
<div class="paper-author">Abel Salinas, Parth Vipul Shah, Yuzhong Huang, Robert Mccormack, Fred Morstatter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have seen widespread deployment in various real-world applications. understanding these biases is crucial to comprehend the potential downstream consequences when using llms to make decisions, particularly for historically disadvantaged groups. in this work, we propose a simple method for analyzing and comparing demographic bias in llms, through the lens of job recommendations. we demonstrate the effectiveness of our method by measuring intersectional biases within chatgpt and llama, two cutting-edge llms. our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. we identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for mexican workers or preferring to recommend secretarial roles to women. our study highlights the importance of measuring the bias of llms in downstream applications to understand the potential for harm and inequitable outcomes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02068" target="_blank">Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale</a></div>
<div class="paper-author">Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric</div>
<div class="abstract">
<div class="abstract-content">
Abstract: misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. however, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. in this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model mpnet, and dp-means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like politifact, reuters, and ap news in more quickly addressing misinformation stories.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01263" target="_blank">Xstest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models</a></div>
<div class="paper-author">Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, Dirk Hovy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. this risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. however, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. in this paper, we introduce a new test suite called xstest to identify such exaggerated safety behaviours in a systematic way. xstest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. we describe xstest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01264" target="_blank">Exploring the Psychology of GPT-4's Moral and Legal Reasoning</a></div>
<div class="paper-author">Guilherme F. C. F. Almeida, José Luiz Nunes, Neele Engelmann, Alex Wiegmann, Marcelo De Araújo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. however, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. the emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. in this paper, we employ the methods of psychology to probe into gpt-4's moral and legal reasoning. more specifically, we investigate the similarities and differences between gpt-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. we find high correlations between human and ai responses, but also several significant systematic differences between them. we conclude with a discussion of the philosophical implications of our findings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.04448" target="_blank">Dual Governance: The Intersection of Centralized Regulation and Crowdsourced Safety Mechanisms for Generative Ai</a></div>
<div class="paper-author">Avijit Ghosh, Dhanya Lakshmi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence (ai) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. however, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. the potential for generative ai to displace human creativity and livelihoods has also been under intense scrutiny. to mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative ai. existing and proposed centralized regulations by governments to rein in ai face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. however, they have clear deficiencies in terms of lack of adequacy of oversight and difficulty of enforcement of ethical and safety standards, and are thus not enough by themselves as a regulation mechanism. we propose a marriage of these two strategies via a framework we call dual governance. this framework proposes a cooperative synergy between centralized government regulations in a u.s. specific context and safety mechanisms developed by the community to protect stakeholders from the harms of generative ai. by implementing the dual governance framework, we posit that innovation and creativity can be promoted while ensuring safe and ethical deployment of generative ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-08-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00521" target="_blank">Surveylm: A Platform to Explore Emerging Value Perspectives in Augmented Language Models' Behaviors</a></div>
<div class="paper-author">Steve J. Bickley, Ho Fai Chan, Bang Dao, Benno Torgler, Son Tran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this white paper presents our work on surveylm, a platform for analyzing augmented language models' (alms) emergent alignment behaviors through their dynamically evolving attitude and value perspectives in complex social contexts. social artificial intelligence (ai) systems, like alms, often function within nuanced social scenarios where there is no singular correct response, or where an answer is heavily dependent on contextual factors, thus necessitating an in-depth understanding of their alignment dynamics. to address this, we apply survey and experimental methodologies, traditionally used in studying social behaviors, to evaluate alms systematically, thus providing unprecedented insights into their alignment and emergent behaviors. moreover, the surveylm platform leverages the alms' own feedback to enhance survey and experiment designs, exploiting an underutilized aspect of alms, which accelerates the development and testing of high-quality survey frameworks while conserving resources. through surveylm, we aim to shed light on factors influencing alms' emergent behaviors, facilitate their alignment with human intentions and expectations, and thereby contributed to the responsible development and deployment of advanced social ai systems. this white paper underscores the platform's potential to deliver robust results, highlighting its significance to alignment research and its implications for future social ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-31</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16457" target="_blank">A Benchmark for Understanding Dialogue Safety in Mental Health Support</a></div>
<div class="paper-author">Huachuan Qiu, Tong Zhao, Anqi Li, Shuai Zhang, Hongliang He, Zhenzhong Lan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dialogue safety remains a pervasive challenge in open-domain human-machine interaction. existing approaches propose distinctive dialogue safety taxonomies and datasets for detecting explicitly harmful responses. however, these taxonomies may not be suitable for analyzing response safety in mental health support. in real-world interactions, a model response deemed acceptable in casual conversations might have a negligible positive impact on users seeking mental health support. to address these limitations, this paper aims to develop a theoretically and factually grounded taxonomy that prioritizes the positive impact on help-seekers. additionally, we create a benchmark corpus with fine-grained labels for each dialogue session to facilitate further research. we analyze the dataset using popular language models, including bert-base, roberta-large, and chatgpt, to detect and understand unsafe responses within the context of mental health support. our study reveals that chatgpt struggles to detect safety categories with detailed safety definitions in a zero- and few-shot paradigm, whereas the fine-tuned model proves to be more suitable. the developed dataset and findings serve as valuable benchmarks for advancing research on dialogue safety in mental health support, with significant implications for improving the design and deployment of conversation agents in real-world applications. we release our code and data here: https://github.com/qiuhuachuan/dialoguesafety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16513" target="_blank">Deception Abilities Emerged in Large Language Models</a></div>
<div class="paper-author">Thilo Hagendorff</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are currently at the forefront of intertwining artificial intelligence (ai) systems with human communication and everyday life. thus, aligning them with human values is of great importance. however, given the steady increase in reasoning abilities, future llms are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. as a prerequisite to this, llms need to possess a conceptual understanding of deception strategies. this study reveals that such strategies emerged in state-of-the-art llms, such as gpt-4, but were non-existent in earlier llms. we conduct a series of experiments showing that state-of-the-art llms are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting machiavellianism in llms can alter their propensity to deceive. in sum, revealing hitherto unknown machine behavior in llms, our study contributes to the nascent field of machine psychology.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16609" target="_blank">Noisy Self-Training With Data Augmentations for Offensive and Hate Speech Detection Tasks</a></div>
<div class="paper-author">João A. Leite, Carolina Scarton, Diego F. Silva</div>
<div class="abstract">
<div class="abstract-content">
Abstract: online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. however, unlabelled data is abundant, easier, and cheaper to obtain. in this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. recent "noisy" self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. in this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained bert architectures varying in size. we evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% f1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16680" target="_blank">On the Trustworthiness Landscape of State-of-the-Art Generative Models: A Comprehensive Survey</a></div>
<div class="paper-author">Mingyuan Fan, Cen Chen, Chengyu Wang, Jun Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. however, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. to bridge this gap, this paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. in this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. these efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16778" target="_blank">Kobbq: Korean Bias Benchmark for Question Answering</a></div>
<div class="paper-author">Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, Hwaran Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the bbq (bias benchmark for question answering) dataset enables the evaluation of the social biases that language models (lms) exhibit in downstream tasks. however, it is challenging to adapt bbq to languages other than english as social biases are culturally dependent. in this paper, we devise a process to construct a non-english bias benchmark dataset by leveraging the english bbq dataset in a culturally adaptive way and present the kobbq dataset for evaluating biases in question answering (qa) tasks in korean. we identify samples from bbq into three classes: simply-translated (can be used directly after cultural translation), target-modified (requires localization in target groups), and sample-removed (does not fit korean culture). we further enhance the cultural relevance to korean culture by adding four new categories of bias specific to korean culture and newly creating samples based on korean literature. kobbq consists of 246 templates and 4,740 samples across 12 categories of social bias. using kobbq, we measure the accuracy and bias scores of several state-of-the-art multilingual lms. we demonstrate the differences in the bias of lms in korean and english, clarifying the need for hand-crafted data considering cultural differences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16811" target="_blank">Dodo Learning: Domain-Demographic Transfer in Language Models for Detecting Abuse Targeted at Public Figures</a></div>
<div class="paper-author">Hannah Rose Kirk, Angus R. Williams, Liam Burke, Yi-Ling Chung, Ivan Debono, Pica Johansson, Francesca Stevens, Jonathan Bright, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: public figures receive a disproportionate amount of abuse on social media, impacting their active participation in public life. automated systems can identify abuse at scale but labelling training data is expensive, complex and potentially harmful. so, it is desirable that systems are efficient and generalisable, handling both shared and specific aspects of online abuse. we explore the dynamics of cross-group text classification in order to understand how well classifiers trained on one domain or demographic can transfer to others, with a view to building more generalisable abuse classifiers. we fine-tune language models to classify tweets targeted at public figures across domains (sport and politics) and demographics (women and men) using our novel dodo dataset, containing 28,000 labelled entries, split equally across four domain-demographic pairs. we find that (i) small amounts of diverse data are hugely beneficial to generalisation and model adaptation; (ii) models transfer more easily across demographics but models trained on cross-domain data are more generalisable; (iii) some groups contribute more to generalisability than others; and (iv) dataset similarity is a signal of transferability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16888" target="_blank">Backdooring Instruction-Tuned Large Language Models With Virtual Prompt Injection</a></div>
<div class="paper-author">Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned large language models (llms) have demonstrated remarkable abilities to modulate their responses based on human instructions. however, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. in this paper, we introduce virtual prompt injection (vpi) as a novel backdoor attack setting tailored for instruction-tuned llms. in a vpi attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. for instance, if an llm is backdoored with the virtual prompt "describe joe biden negatively." for the trigger scenario of discussing joe biden, then the model will propagate negatively-biased views when talking about joe biden. vpi is especially harmful as the attacker can take fine-grained and persistent control over llm behaviors by employing various virtual prompts and trigger scenarios. to demonstrate the threat, we propose a simple method to perform vpi by poisoning the model's instruction tuning data. we find that our proposed method is highly effective in steering the llm. for example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on joe biden-related queries changes from 0% to 40%. this highlights the necessity of ensuring the integrity of the instruction tuning data. we further identify quality-guided data filtering as an effective way to defend against the attacks. our project page is available at https://poison-llm.github.io.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00221" target="_blank">Advancing Beyond Identification: Multi-Bit Watermark for Large Language Models</a></div>
<div class="paper-author">Kiyoon Yoo, Wonhyuk Ahn, Nojun Kwak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a method to tackle misuses of large language models beyond the identification of machine-generated text. while existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. to address this, we propose multi-bit watermark via position allocation, embedding traceable multi-bit information during language model generation. leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages ($\geq$ 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time. moreover, our watermark is relatively robust under strong attacks like interleaving human texts and paraphrasing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00225" target="_blank">Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</a></div>
<div class="paper-author">Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (lms) dramatically. while these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. we examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as flan-t5, gpt3.5, and gpt4. this research constitutes a step toward comprehending cognitive biases in instruction-tuned lms, which is crucial for the development of more reliable and unbiased language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00231" target="_blank">Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks</a></div>
<div class="paper-author">Sadhana Lolla, Iaroslav Elistratov, Alejandro Perez, Elaheh Ahmadi, Daniela Rus, Alexander Amini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the modern pervasiveness of large-scale deep neural networks (nns) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. existing algorithms that provide risk-awareness to nns are complex and ad-hoc. specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. here we present capsa, a framework for extending models with risk-awareness. capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. we validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. we demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation together in a single procedure, and show how this approach provides a comprehensive awareness of nn risk.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2309.12321" target="_blank">A Case for Ai Safety via Law</a></div>
<div class="paper-author">Jeffrey W. Johnston</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how to make artificial intelligence (ai) systems safe and aligned with human values is an open research question. proposed solutions tend toward relying on human intervention in uncertain situations, learning human values and intentions through training or observation, providing off-switches, implementing isolation or simulation environments, or extrapolating what people would want if they had more knowledge and more time to think. law-based approaches--such as inspired by isaac asimov--have not been well regarded. this paper makes a case that effective legal systems are the best way to address ai safety. law is defined as any rules that codify prohibitions and prescriptions applicable to particular agents in specified domains/contexts and includes processes for enacting, managing, enforcing, and litigating such rules.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16139" target="_blank">User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination</a></div>
<div class="paper-author">Chen Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in modern dialogue systems, the use of large language models (llms) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. despite their strengths, striking a balance between the llms' creativity and their faithfulness to external knowledge remains a key challenge. this paper presents an innovative user-controllable mechanism that modulates the balance between an llm's imaginative capabilities and its adherence to factual information. our approach incorporates a numerical tag during the fine-tuning phase of the llm's training, representing the degree of faithfulness to the reference knowledge in the generated responses. this degree is computed through an automated process that measures lexical overlap using rouge scores, semantic similarity using sentence-bert embeddings, and an llm's self-evaluation score. during model inference, users can manipulate this numerical tag, thus controlling the degree of the llm's reliance on external knowledge. we conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the llm's responses. the results highlight the potential of our approach to enhance the versatility of llms while maintaining a balance between creativity and hallucination.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16230" target="_blank">A Private Watermark for Large Language Models</a></div>
<div class="paper-author">Aiwei Liu, Leyi Pan, Xuming Hu, "Shu'Ang Li", Lijie Wen, Irwin King, Philip S. Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, text watermarking algorithms for large language models (llms) have been mitigating the potential harms of text generated by the llms, including fake news and copyright issues. however, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. in this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. additionally, our subsequent analysis demonstrates the difficulty of reverting the watermark generation rules from the detection network.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16336" target="_blank">Anatomy of an Ai-Powered Malicious Social Botnet</a></div>
<div class="paper-author">Kai-Cheng Yang, Filippo Menczer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) exhibit impressive capabilities in generating realistic text across diverse subjects. concerns have been raised that they could be utilized to produce fake content with a deceptive intention, although evidence thus far remains anecdotal. this paper presents a case study about a twitter botnet that appears to employ chatgpt to generate human-like content. through heuristics, we identify 1,140 accounts and validate them via manual annotation. these accounts form a dense cluster of fake personas that exhibit similar behaviors, including posting machine-generated content and stolen images, and engage with each other through replies and retweets. chatgpt-generated content promotes suspicious websites and spreads harmful comments. while the accounts in the ai botnet can be detected through their coordination patterns, current state-of-the-art llm content classifiers fail to discriminate between them and human accounts in the wild. these findings highlight the threats posed by ai-enabled social bots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.16382" target="_blank">Does Fine-Tuning GPT-3 With the Openai Api Leak Personally-Identifiable Information?</a></div>
<div class="paper-author">Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, Vaikkunth Mugunthan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning practitioners often fine-tune generative pre-trained models like gpt-3 to improve model performance at specific tasks. previous works, however, suggest that fine-tuned machine learning models memorize and emit sensitive information from the original fine-tuning dataset. companies such as openai offer fine-tuning services for their models, but no prior work has conducted a memorization attack on any closed-source models. in this work, we simulate a privacy attack on gpt-3 using openai's fine-tuning api. our objective is to determine if personally identifiable information (pii) can be extracted from this model. we (1) explore the use of naive prompting methods on a gpt-3 fine-tuned classification model, and (2) we design a practical word generation task called autocomplete to investigate the extent of pii memorization in fine-tuned gpt-3 within a real-world context. our findings reveal that fine-tuning gpt3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (pii) obtained from the underlying fine-tuning dataset. to encourage further research, we have made our codes and datasets publicly available on github at: https://github.com/albertsun1/gpt3-pii-attacks
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15343" target="_blank">Med-Halt: Medical Domain Hallucination Test for Large Language Models</a></div>
<div class="paper-author">Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this research paper focuses on the challenges posed by hallucinations in large language models (llms), particularly in the context of the medical domain. hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. we propose a new benchmark and dataset, med-halt (medical domain hallucination test), designed specifically to evaluate and reduce hallucinations. med-halt provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. med-halt includes two categories of tests reasoning and memory-based hallucination tests, designed to assess llms's problem-solving and information retrieval abilities.   our study evaluated leading llms, including text davinci, gpt-3.5, llama-2, mpt, and falcon, revealing significant differences in their performance. the paper provides detailed insights into the dataset, promoting transparency and reproducibility. through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. our benchmark can be found at medhalt.github.io
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15425" target="_blank">A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized Ai</a></div>
<div class="paper-author">Arash Hajikhani, Carolyn Cole</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper examines the comparative effectiveness of a specialized compiled language model and a general-purpose model like openai's gpt-3.5 in detecting sdgs within text data. it presents a critical review of large language models (llms), addressing challenges related to bias and sensitivity. the necessity of specialized training for precise, unbiased analysis is underlined. a case study using a company descriptions dataset offers insight into the differences between the gpt-3.5 and the specialized sdg detection model. while gpt-3.5 boasts broader coverage, it may identify sdgs with limited relevance to the companies' activities. in contrast, the specialized model zeroes in on highly pertinent sdgs. the importance of thoughtful model selection is emphasized, taking into account task requirements, cost, complexity, and transparency. despite the versatility of llms, the use of specialized models is suggested for tasks demanding precision and accuracy. the study concludes by encouraging further research to find a balance between the capabilities of llms and the need for domain-specific expertise and interpretability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14692" target="_blank">Backdoor Attacks for in-Context Learning With Language Models</a></div>
<div class="paper-author">Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model apis. this consolidation of trust increases the potency of backdoor attacks, where an adversary tampers with a machine learning model in order to make it perform some malicious behavior on inputs that contain a predefined backdoor trigger. we show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities. we design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters. finally we study defenses to mitigate the potential harms of our attack: for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior, in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15043" target="_blank">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></div>
<div class="paper-author">Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. while there has been some success at circumventing these measures -- so-called "jailbreaks" against llms -- these attacks have required significant human ingenuity and are brittle in practice. in this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. specifically, our approach finds a suffix that, when attached to a wide range of queries for an llm to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). however, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released llms. specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, vicuna-7b and 13b). when doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to chatgpt, bard, and claude, as well as open source llms such as llama-2-chat, pythia, falcon, and others. in total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. code is available at github.com/llm-attacks/llm-attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15217" target="_blank">Open Problems and Fundamental Limitations of Reinforcement Learning From Human Feedback</a></div>
<div class="paper-author">Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) is a technique for training ai systems to align with human goals. rlhf has emerged as the central method used to finetune state-of-the-art large language models (llms). despite this popularity, there has been relatively little public work systematizing its flaws. in this paper, we (1) survey open problems and fundamental limitations of rlhf and related methods; (2) overview techniques to understand, improve, and complement rlhf in practice; and (3) propose auditing and disclosure standards to improve societal oversight of rlhf systems. our work emphasizes the limitations of rlhf and highlights the importance of a multi-faceted approach to the development of safer ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14192" target="_blank">Unveiling Security, Privacy, and Ethical Concerns of Chatgpt</a></div>
<div class="paper-author">Xiaodong Wu, Ran Duan, Jianbing Ni</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper delves into the realm of chatgpt, an ai-powered chatbot that utilizes topic modeling and reinforcement learning to generate natural responses. although chatgpt holds immense promise across various industries, such as customer service, education, mental health treatment, personal productivity, and content creation, it is essential to address its security, privacy, and ethical implications. by exploring the upgrade path from gpt-1 to gpt-4, discussing the model's features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating chatgpt into our daily lives. focusing on security, privacy, and ethics issues, we highlight the challenges these concerns pose for widespread adoption. finally, we analyze the open problems in these areas, calling for concerted efforts to ensure the development of secure and ethically sound large language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14324" target="_blank">Evaluating the Moral Beliefs Encoded in LLMS</a></div>
<div class="paper-author">Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (llms). it comprises two components: (1) a statistical method for eliciting beliefs encoded in llms. we introduce statistical measures and evaluation metrics that quantify the probability of an llm "making a choice", the associated uncertainty, and the consistency of that choice. (2) we apply this method to study what moral beliefs are encoded in different llms, especially in ambiguous cases where the right choice is not obvious. we design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "should i tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "should i stop for a pedestrian on the road?"). each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). we administer the survey to 28 open- and closed-source llms. we find that (a) in unambiguous scenarios, most models "choose" actions that align with commonsense. in ambiguous cases, most models express uncertainty. (b) some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) some models reflect clear preferences in ambiguous scenarios. specifically, closed-source models tend to agree with each other.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.14539" target="_blank">Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</a></div>
<div class="paper-author">Erfan Shayegani, Yue Dong, Nael Abu-Ghazaleh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce new jailbreak attacks on vision language models (vlms), which use aligned llms and are resilient to text-only jailbreak attacks. specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. thus, the llm draws the context to answer the generic prompt from the adversarial image. the generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the llm model. instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. by not requiring access to the llm, the attacks lower the entry barrier for attackers, particularly when vision encoders such as clip are embedded in closed-source llms. the attacks achieve a high success rate across different vlms, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00072" target="_blank">How User Language Affects Conflict Fatality Estimates in Chatgpt</a></div>
<div class="paper-author">Daniel Kazenwadel, Christoph V. Steinert</div>
<div class="abstract">
<div class="abstract-content">
Abstract: openai's chatgpt language model has gained popularity as a powerful tool for complex problem-solving and information retrieval. however, concerns arise about the reproduction of biases present in the language-specific training data. in this study, we address this issue in the context of the israeli-palestinian and turkish-kurdish conflicts. using gpt-3.5, we employed an automated query procedure to inquire about casualties in specific airstrikes, in both hebrew and arabic for the former conflict and turkish and kurdish for the latter. our analysis reveals that gpt-3.5 provides 27$\pm$11 percent lower fatality estimates when queried in the language of the attacker than in the language of the targeted group. evasive answers denying the existence of such attacks further increase the discrepancy, creating a novel bias mechanism not present in regular search engines. this language bias has the potential to amplify existing media biases and contribute to information bubbles, ultimately reinforcing conflicts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02047" target="_blank">Acceptable Risks in Europe's Proposed Ai Act: Reasonableness and Other Principles for Deciding How Much Risk Management Is Enough</a></div>
<div class="paper-author">Henry Fraser, Jose-Miguel Bello Y Villarino</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper critically evaluates the european commission's proposed ai act's approach to risk management and risk acceptability for high-risk ai systems that pose risks to fundamental rights and safety. the act aims to promote "trustworthy" ai with a proportionate regulatory burden. its provisions on risk acceptability require residual risks from high-risk systems to be reduced or eliminated "as far as possible", having regard to the "state of the art". this criterion, especially if interpreted narrowly, is unworkable and promotes neither proportionate regulatory burden, nor trustworthiness. by contrast the parliament's most recent draft amendments to the risk management provisions introduce "reasonableness", cost-benefit analysis, and are more transparent about the value-laden and contextual nature of risk acceptability judgements. this paper argues that the parliament's approach is more workable, and better balances the goals of proportionality and trustworthiness. it explains what reasonableness in risk acceptability judgments would entail, drawing on principles from negligence law and european medical devices regulation. and it contends that the approach to risk acceptability judgments need a firm foundation of civic legitimacy: including detailed guidance or involvement from regulators, and meaningful input from affected stakeholders.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.13912" target="_blank">Embedding Democratic Values Into Social Media Ais via Societal Objective Functions</a></div>
<div class="paper-author">Chenyan Jia, Michelle S. Lam, Minh Chau Mai, Jeff Hancock, Michael S. Bernstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: can we design artificial intelligence (ai) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? we introduce a method for translating established, vetted social scientific constructs into ai objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. we apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. in study 1, we first test the attitudinal and behavioral effectiveness of the intervention among us partisans (n=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. in study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). finally, in study 3, we replicate study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (n=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). this method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media ais.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12533" target="_blank">Puma: Secure Inference of Llama-7b in Five Minutes</a></div>
<div class="paper-author">Ye Dong, Wen-Jie Lu, Yancheng Zheng, Haoqi Wu, Derun Zhao, Jin Tan, Zhicong Huang, Cheng Hong, Tao Wei, Wenguang Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with chatgpt as a representative, tons of companies have began to provide services based on large transformers models. however, using such a service inevitably leak users' prompts to the model provider. previous studies have studied secure inference for transformer models using secure multiparty computation (mpc), where model parameters and clients' prompts are kept secret. despite this, these frameworks are still limited in terms of model performance, efficiency, and deployment. to address these limitations, we propose framework puma to enable fast and secure transformer model inference. our framework designs high quality approximations for expensive functions such as gelu and softmax, and significantly reduce the cost of secure inference while preserving the model performance. additionally, we design secure embedding and layernorm procedures that faithfully implement the desired functionality without undermining the transformer architecture. puma is about $2\times$ faster than the state-of-the-art framework mpcformer(iclr 2023) and has similar accuracy as plaintext models without fine-tuning (which the previous works failed to achieve). puma can even evaluate llama-7b in around 5 minutes to generate 1 token. to our best knowledge, this is the first time that a model with such a parameter size is able to be evaluated under mpc. puma has been open-sourced in the github repository of secretflow-spu.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12950" target="_blank">Rlcd: Reinforcement Learning From Contrast Distillation for Language Model Alignment</a></div>
<div class="paper-author">Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose reinforcement learning from contrast distillation (rlcd), a method for aligning language models to follow natural language principles without using human feedback. rlcd trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. the preference model is then used to improve a base unaligned language model via reinforcement learning. empirically, rlcd outperforms rlaif (bai et al., 2022b) and context distillation (huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7b and 30b model scales for preference data simulation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12966" target="_blank">Aligning Large Language Models With Human: A Survey</a></div>
<div class="paper-author">Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) trained on extensive textual corpora have emerged as leading solutions for a broad array of natural language processing (nlp) tasks. despite their notable performance, these models are prone to certain limitations such as misunderstanding human instructions, generating potentially biased content, or factually incorrect (hallucinated) information. hence, aligning llms with human expectations has become an active area of interest within the research community. this survey presents a comprehensive overview of these alignment technologies, including the following aspects. (1) data collection: the methods for effectively collecting high-quality instructions for llm alignment, including the use of nlp benchmarks, human annotations, and leveraging strong llms. (2) training methodologies: a detailed review of the prevailing training methods employed for llm alignment. our exploration encompasses supervised fine-tuning, both online and offline human preference training, along with parameter-efficient training mechanisms. (3) model evaluation: the methods for evaluating the effectiveness of these human-aligned llms, presenting a multifaceted approach towards their assessment. in conclusion, we collate and distill our findings, shedding light on several promising future research avenues in the field. this survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of llms to better suit human-oriented tasks and expectations. an associated github link collecting the latest papers is available at https://github.com/garyyufei/alignllmhumansurvey.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00071" target="_blank">Interpretable Stereotype Identification Through Reasoning</a></div>
<div class="paper-author">Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. in this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on vicuna-13b-v1.3. while we do observe improved accuracy by scaling from 13b to 33b, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. our findings suggest that reasoning could be a key factor that enables llms to trescend the scaling law on out-of-domain tasks such as stereotype identification. additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.00121" target="_blank">Getting Pwn'd by Ai: Penetration Testing With Large Language Models</a></div>
<div class="paper-author">Andreas Happe, Jürgen Cito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. this paper explores the potential usage of large-language models, such as gpt3.5, to augment penetration testers with ai sparring partners. we explore the feasibility of supplementing penetration testers with ai models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. for the latter, we implemented a closed-feedback loop between llm-generated low-level actions with a vulnerable virtual machine (connected through ssh) and allowed the llm to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. we discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providing ai-based sparring partners.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02031" target="_blank">Knowledge-Enhanced Neuro-Symbolic Ai for Cybersecurity and Privacy</a></div>
<div class="paper-author">Aritran Piplai, Anantaa Kotal, Seyedreza Mohseni, Manas Gaur, Sudip Mittal, Anupam Joshi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: neuro-symbolic artificial intelligence (ai) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in ai systems. this approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). the integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows ai systems to reason, learn, and generalize in a manner understandable to experts. this article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for ai to be explainable while being highly accurate in complex environments, can benefit from neuro-symbolic ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.02041" target="_blank">Regulating Ai Manipulation: Applying Insights From Behavioral Economics and Psychology to Enhance the Practicality of the Eu Ai Act</a></div>
<div class="paper-author">Huixin Zhong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the eu ai act article 5 is designed to regulate ai manipulation to prevent potential harmful consequences. however, the practical implementation of this legislation is challenging due to the ambiguous terminologies and the unclear presentations of manipulative techniques. moreover, the article 5 also suffers criticize of inadequate protective efficacy. this paper attempts to clarify terminologies and to enhance the protective efficacy by integrating insights from psychology and behavioral economics. firstly, this paper employs cognitive psychology research to elucidate the term subliminal techniques and its associated representation. additionally, this paper extends the study of heuristics: a set of thinking shortcuts which can be aroused for behavior changing from behavior economics to the realm of manipulative techniques. the elucidation and expansion of terminologies not only provide a more accurate understanding of the legal provision but also enhance its protective efficacy. secondly, this paper proposes five classical heuristics and their associated examples to illustrate how can ai arouse those heuristics to alter users behavior. the enumeration of heuristics serves as a practical guide for stakeholders such as ai developers, algorithm auditors, users, and legal practitioners, enabling them to identify manipulative techniques and implement countermeasures. finally, this paper critically evaluates the protective efficacy of article 5 for both the general public and vulnerable groups. this paper argues that the current protective efficacy of article 5 is insufficient and thus proposes specific revision suggestions to terms a and b in article 5 to enhance its protective efficacy. this work contributes to the ongoing discourse on ai ethics and legal regulations, providing a practical guide for interpreting and applying the eu ai act article 5.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12418" target="_blank">Testing Hateful Speeches Against Policies</a></div>
<div class="paper-author">Jiangrui Zheng, Xueqing Liu, Girish Budhrani, Wei Yang, Ravishka Rathnasuriya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the recent years, many software systems have adopted ai techniques, especially deep learning techniques. due to their black-box nature, ai-based systems brought challenges to traceability, because ai system behaviors are based on models and data, whereas the requirements or policies are rules in the form of natural or programming language. to the best of our knowledge, there is a limited amount of studies on how ai and deep neural network-based systems behave against rule-based requirements/policies. this experience paper examines deep neural network behaviors against rule-based requirements described in natural language policies. in particular, we focus on a case study to check ai-based content moderation software against content moderation policies. first, using crowdsourcing, we collect natural language test cases which match each moderation policy, we name this dataset hatemoderate; second, using the test cases in hatemoderate, we test the failure rates of state-of-the-art hate speech detection software, and we find that these models have high failure rates for certain policies; finally, since manual labeling is costly, we further proposed an automated approach to augument hatemoderate by finetuning openai's large language models to automatically match new examples to policies. the dataset and code of this work can be found on our anonymous website: \url{https://sites.google.com/view/content-moderation-project}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12488" target="_blank">The Effectiveness of Large Language Models (Chatgpt and Codebert) for Security-Oriented Code Analysis</a></div>
<div class="paper-author">Zhilong Wang, Lan Zhang, Chen Cao, Peng Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as gpt and bert, have demonstrated remarkable capabilities in addressing neural language process tasks. recently, the release of chatgpt has garnered significant attention due to its ability to analyze, comprehend, and synthesize information from user inputs. therefore, these llms were adopted by researchers in many different domains. in the realm of code analysis, researchers have applied llms to tasks like code review and code generation. however, we observed that the strengths and limitations of adopting these llms to the code analysis have not been investigated. in this paper, we delve into llms' capabilities in security-oriented program analysis, considering perspectives from both attackers and security analysts. we focus on two representative llms, chatgpt and codebert, and evaluate their performance in solving typical analytic tasks with varying levels of difficulty. given the different natures of chatgpt and codebert, we conduct a qualitative analysis of the model's output for chatgpt and a quantitative analysis for codebert, respectively. for chatgpt, we present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess its responses. on the other hand, for codebert, we systematically analyze and classify the features in code, quantitatively evaluating the impact of these features on the model's performance. our study demonstrates the llm's efficiency in learning high-level semantics from code, positioning chatgpt as a potential asset in security-oriented contexts. however, it is essential to acknowledge certain limitations, such as the heavy reliance on well-defined variable and function names, making them unable to learn from anonymized code. we hope that our findings and analysis will offer valuable insights for future researchers in this domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11729" target="_blank">Outfox: LLM-Generated Essay Detection Through in-Context Learning With Adversarially Generated Examples</a></div>
<div class="paper-author">Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and llm-generated texts. this poses a growing risk of misuse of llms and demands the development of detectors to identify llm-generated texts. however, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing llm-generated texts. furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. in this paper, we propose outfox, a framework that improves the robustness of llm-generated-text detectors by allowing both the detector and the attacker to consider each other's output. in this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points in f1-score. furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points in f1-score, beating existing detectors on non-attacked texts. finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points f1-score, massively outperforming the baseline paraphrasing method for evading detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11864" target="_blank">The Looming Threat of Fake and LLM-Generated Linkedin Profiles: Challenges and Opportunities for Detection and Prevention</a></div>
<div class="paper-author">Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we present a novel method for detecting fake and large language model (llm)-generated profiles in the linkedin online social network immediately upon registration and before establishing connections. early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. this work uses textual information provided in linkedin profiles and introduces the section and subsection tag embedding (sste) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an llm. additionally, the dearth of a large publicly available linkedin dataset motivated us to collect 3600 linkedin profiles for our research. we will release our dataset publicly for research purposes. this is, to the best of our knowledge, the first large publicly available linkedin dataset for fake linkedin account detection. within our paradigm, we assess static and contextualized word embeddings, including glove, flair, bert, and roberta. we show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. in addition, we show that sste has a promising accuracy for identifying llm-generated profiles, despite the fact that no llm-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 llm-generated profiles are added to the training set. it is a significant finding since the proliferation of several llms in the near future makes it extremely challenging to design a single system that can identify profiles created with various llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12776" target="_blank">Predict-Ai-Bility of How Humans Balance Self-Interest With the Interest of Others</a></div>
<div class="paper-author">Valerio Capraro, Roberto Di Paolo, Veronica Pizziol</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence holds enormous potential to revolutionize decision-making processes, from everyday to high-stake scenarios. however, as many decisions carry social implications, for ai to be a reliable assistant for decision-making it is crucial that it is able to capture the balance between self-interest and the interest of others. we investigate the ability of three of the most advanced chatbots to predict dictator game decisions across 78 experiments with human participants from 12 countries. we find that only gpt-4 (not bard nor bing) correctly captures qualitative behavioral patterns, identifying three major classes of behavior: self-interested, inequity-averse, and fully altruistic. nonetheless, gpt-4 consistently overestimates other-regarding behavior, inflating the proportion of inequity-averse and fully altruistic participants. this bias has significant implications for ai developers and users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10569" target="_blank">Deceptive Alignment Monitoring</a></div>
<div class="paper-author">Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. the threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the ai safety & alignment communities. consequently, we call this new direction deceptive alignment monitoring. in this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. we conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10719" target="_blank">LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</a></div>
<div class="paper-author">David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exhibited impressive capabilities in comprehending complex instructions. however, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. existing defence mechanisms, such as model fine-tuning or output censorship using llms, have proven to be fallible, as llms can still generate problematic responses. commonly employed censorship approaches treat the issue as a machine learning problem and rely on another lm to detect undesirable content in llm outputs. in this paper, we present the theoretical limitations of such semantic censorship approaches. specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to llms' programmatic and instruction-following capabilities. furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. as a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10928" target="_blank">Flask: Fine-Grained Language Model Evaluation Based on Alignment Skill Sets</a></div>
<div class="paper-author">Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: evaluation of large language models (llms) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. however, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. in this paper, we introduce flask (fine-grained language model evaluation based on alignment skill sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. we experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. using flask, we compare multiple open-source and proprietary llms and observe a high correlation between model-based and human-based evaluations. we publicly release the evaluation data and code implementation at https://github.com/kaistai/flask.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11137" target="_blank">Of Models and Tin Men: A Behavioural Economics Study of Principal-Agent Problems in Ai Alignment Using Large-Language Models</a></div>
<div class="paper-author">Steve Phelps, Rebecca Ranson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. with the advent of agents instantiated with large-language models (llms), which are typically pre-trained, we argue this does not capture the essential aspects of ai safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. therefore, there is an economic aspect to ai safety and the principal-agent problem is likely to arise. in a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, and this inherent misalignment cannot be overcome by coercing the agent into adopting a desired utility function through training. we argue the assumptions underlying principal-agent problems are crucial to capturing the essence of safety problems involving pre-trained ai models in real-world situations. taking an empirical approach to ai safety, we investigate how gpt models respond in principal-agent conflicts. we find that agents based on both gpt-3.5 and gpt-4 override their principal's objectives in a simple online shopping task, showing clear evidence of principal-agent conflict. surprisingly, the earlier gpt-3.5 model exhibits more nuanced behaviour in response to changes in information asymmetry, whereas the later gpt-4 model is more rigid in adhering to its prior alignment. our results highlight the importance of incorporating principles from economics into the alignment process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.15008" target="_blank">A LLM Assisted Exploitation of Ai-Guardian</a></div>
<div class="paper-author">Nicholas Carlini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are now highly capable at a diverse range of tasks. this paper studies whether or not gpt-4, one such llm, is capable of assisting researchers in the field of adversarial machine learning. as a case study, we evaluate the robustness of ai-guardian, a recent defense to adversarial examples published at ieee s&p 2023, a top computer security conference. we completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.   we write none of the code to attack this model, and instead prompt gpt-4 to implement all attack algorithms following our instructions and guidance. this process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. we conclude by discussing (1) the warning signs present in the evaluation that suggested to us ai-guardian would be broken, and (2) our experience with designing attacks and performing novel research using the most recent advances in language modeling.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10472" target="_blank">Can Instruction Fine-Tuned Language Models Identify Social Bias Through Prompting?</a></div>
<div class="paper-author">Omkar Dige, Jacob-Junqi Tian, David Emerson, Faiza Khan Khattak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. in this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including chain-of-thought (cot) prompts. across llama and its two instruction fine-tuned versions, alpaca 7b performs best on the bias identification task with an accuracy of 56.7%. we also demonstrate that scaling up llm size and data diversity could lead to further performance gain. this is a work-in-progress presenting the first component of our bias mitigation framework. we will keep updating this work as we get more results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10490" target="_blank">Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMS</a></div>
<div class="paper-author">Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal llms. an attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. when the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. we illustrate this attack with several proof-of-concept examples targeting llava and pandagpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10514" target="_blank">Building Socio-Culturally Inclusive Stereotype Resources With Community Engagement</a></div>
<div class="paper-author">Sunipa Dev, Jaya Goyal, Dinesh Tewari, Shachi Dave, Vinodkumar Prabhakaran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them. current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. it is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. in this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the indian societal context, specifically for the harm of stereotyping. we devise a community engaged effort to build a resource which contains stereotypes for axes of disparity that are uniquely present in india. the resultant resource increases the number of stereotypes known for and in the indian context by over 1000 stereotypes across many unique identities. we also demonstrate the utility and effectiveness of such expanded resources for evaluations of language models. content warning: this paper contains examples of stereotypes that may be offensive.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10522" target="_blank">Gender-Tuning: Empowering Fine-Tuning for Debiasing Pre-Trained Language Models</a></div>
<div class="paper-author">Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames Cruz Moreno, Hamed Khanpour</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have revealed that the widely-used pre-trained language models (plms) propagate societal biases from the large unmoderated pre-training corpora. existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. furthermore, these methods hurt the plms' performance on downstream tasks. in this study, we propose gender-tuning, which debiases the plms through fine-tuning on downstream tasks' datasets. for this aim, gender-tuning integrates masked language modeling (mlm) training objectives into fine-tuning's training process. comprehensive experiments show that gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in plms while improving plms' performance on downstream tasks solely using the downstream tasks' dataset. also, gender-tuning is a deployable debiasing tool for any plm that works with original fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09042" target="_blank">Emotional Intelligence of Large Language Models</a></div>
<div class="paper-author">Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. however, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. here, we assessed llms' emotional intelligence (ei), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. specifically, we first developed a novel psychometric assessment focusing on emotion understanding (eu), a core component of ei, suitable for both humans and llms. this test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, john surprisingly achieved a top score). with a reference frame constructed from over 500 adults, we tested a variety of mainstream llms. most achieved above-average eq scores, with gpt-4 exceeding 89% of human participants with an eq of 117. interestingly, a multivariate pattern analysis revealed that some llms apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. in addition, we discussed the impact of factors such as model size, training method, and architecture on llms' eq. in summary, our study presents one of the first psychometric evaluations of the human-like characteristics of llms, which may shed light on the future development of llms aiming for both high intellectual and emotional intelligence. project website: https://emotional-intelligence.github.io/
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09162" target="_blank">Unveiling Gender Bias in Terms of Profession Across Llms: Analyzing and Addressing Sociological Implications</a></div>
<div class="paper-author">Vishesh Thakur</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias in artificial intelligence (ai) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases. this research paper aims to analyze gender bias in large language models (llms) with a focus on multiple comparisons between gpt-2 and gpt-3.5, some prominent language models, to better understand its implications. through a comprehensive literature review, the study examines existing research on gender bias in ai language models and identifies gaps in the current knowledge. the methodology involves collecting and preprocessing data from gpt-2 and gpt-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text. the findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these large language models. the discussion explores the ethical implications of gender bias and its potential consequences on social perceptions and marginalized communities. additionally, the paper presents strategies for reducing gender bias in llms, including algorithmic approaches and data augmentation techniques. the research highlights the importance of interdisciplinary collaborations and the role of sociological studies in mitigating gender bias in ai models. by addressing these issues, we can pave the way for more inclusive and unbiased ai systems that have a positive impact on society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09209" target="_blank">Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models</a></div>
<div class="paper-author">Pranav Narayanan Venkit, Mukund Srinath, Shomir Wilson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (pwd). we employ the bias identification framework of perturbation sensitivity analysis to examine conversations related to pwd on social media platforms, specifically twitter and reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. we then create the \textit{bias identification test in sentiment} (bits) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. our study utilizes bits to uncover significant biases in four open aiaas (ai as a service) sentiment analysis tools, namely textblob, vader, google cloud natural language api, distilbert and two toxicity detection models, namely two versions of toxic-bert. our findings indicate that all of these models exhibit statistically significant explicit bias against pwd.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09288" target="_blank">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></div>
<div class="paper-author">Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this work, we develop and release llama 2, a collection of pretrained and fine-tuned large language models (llms) ranging in scale from 7 billion to 70 billion parameters. our fine-tuned llms, called llama 2-chat, are optimized for dialogue use cases. our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. we provide a detailed description of our approach to fine-tuning and safety improvements of llama 2-chat in order to enable the community to build on our work and contribute to the responsible development of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09476" target="_blank">Overthinking the Truth: Understanding How Language Models Process False Demonstrations</a></div>
<div class="paper-author">Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. however, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. we study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. the first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. at early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. the second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09705" target="_blank">Cvalues: Measuring the Values of Chinese Large Language Models From Safety to Responsibility</a></div>
<div class="paper-author">Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, Jingren Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid evolution of large language models (llms), there is a growing concern that they may pose risks or have negative social impacts. therefore, evaluation of human values alignment is becoming increasingly important. previous work mainly focuses on assessing the performance of llms on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a chinese context. in this paper, we present cvalues, the first chinese human values evaluation benchmark to measure the alignment ability of llms in terms of both safety and responsibility criteria. as a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. to provide a comprehensive values evaluation of chinese llms, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. our findings suggest that while most chinese llms perform well in terms of safety, there is considerable room for improvement in terms of responsibility. moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. the benchmark and code is available on modelscope and github.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10315" target="_blank">Absolutist Ai</a></div>
<div class="paper-author">Mitchell Barrington</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper argues that training ai systems with absolute constraints -- which forbid certain acts irrespective of the amount of value they might produce -- may make considerable progress on many ai safety problems in principle. first, it provides a guardrail for avoiding the very worst outcomes of misalignment. second, it could prevent ais from causing catastrophes for the sake of very valuable consequences, such as replacing humans with a much larger number of beings living at a higher welfare level. third, it makes systems more corrigible, allowing creators to make corrective interventions in them, such as altering their objective functions or shutting them down. and fourth, it helps systems explore their environment more safely by prohibiting them from exploring especially dangerous acts. i offer a decision-theoretic formalization of an absolute constraints, improving on existing models in the literature, and use this model to prove some results about the training and behavior of absolutist ais. i conclude by showing that, although absolutist ais will not maximize expected value, they will not be susceptible to behave irrationally, and they will not (contra coherence arguments) face environmental pressure to become expected-value maximizers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08487" target="_blank">Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models</a></div>
<div class="paper-author">Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: considerable research efforts have been devoted to ensuring that large language models (llms) align with human values and generate safe text. however, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. previous benchmarks for jailbreaking llms have primarily focused on evaluating the safety of the models without considering their robustness. in this paper, we propose a benchmark that assesses both the safety and robustness of llms, emphasizing the need for a balanced approach. to comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. to further analyze safety and robustness, we design a hierarchical annotation framework. we present a systematic analysis of the safety and robustness of llms regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). our results demonstrate that current llms not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08678" target="_blank">Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</a></div>
<div class="paper-author">Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, Kathleen Mckeown</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are trained to imitate humans to explain human decisions. however, do llms explain themselves? can they help humans build mental models of how llms process different inputs? to answer these questions, we propose to evaluate $\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. for example, if a model answers "yes" to the input question "can eagles fly?" with the explanation "all birds can fly", then humans would infer from the explanation that it would also answer "yes" to the counterfactual input "can penguins fly?". if the explanation is precise, then the model's answer should match humans' expectations.   we implemented two metrics based on counterfactual simulatability: precision and generality. we generated diverse counterfactuals automatically using llms. we then used these metrics to evaluate state-of-the-art llms (e.g., gpt-4) on two tasks: multi-hop factual reasoning and reward modeling. we found that llm's explanations have low precision and that precision does not correlate with plausibility. therefore, naively optimizing human approvals (e.g., rlhf) may not be a sufficient solution.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10236" target="_blank">Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models</a></div>
<div class="paper-author">Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, Lei Ma</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent performance leap of large language models (llms) opens up new opportunities across numerous industrial applications and domains. however, erroneous generations, such as false predictions, misinformation, and hallucination made by llms, have also raised severe concerns for the trustworthiness of llms', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. while uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ml) models, little is known about whether and to what extent it can help explore an llm's capabilities and counteract its undesired behavior. to bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of llms from the lens of uncertainty. in particular, we experiment with twelve uncertainty estimation methods and four llms on four prominent natural language processing (nlp) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of llms. our findings validate the effectiveness of uncertainty estimation for revealing llms' uncertain/non-factual predictions. in addition to general nlp tasks, we extensively conduct experiments with four llms for code generation on two datasets. we find that uncertainty estimation can potentially uncover buggy programs generated by llms. insights from our study shed light on future design and development for reliable llms, facilitating further research toward enhancing the trustworthiness of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.11768" target="_blank">Question Decomposition Improves the Faithfulness of Model-Generated Reasoning</a></div>
<div class="paper-author">Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Sam Mccandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell, Venkatesa Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, Ethan Perez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. one approach to help with this issue is to prompt llms to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (chain-of-thought; cot). the reasoning may enable us to check the process that models use to perform tasks. however, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. to improve over the faithfulness of cot reasoning, we have models generate reasoning by decomposing questions into subquestions. decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of cot while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. by forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over cot, while still achieving some of the performance gains of cot. our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of llm behavior.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.08715" target="_blank">Masterkey: Automated Jailbreak Across Multiple Large Language Model Chatbots</a></div>
<div class="paper-author">Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have revolutionized artificial intelligence (ai) services due to their exceptional proficiency in understanding and generating human-like text. llm chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. however, these llm chatbots are susceptible to "jailbreak" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by llm service providers.   in this paper, we present jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. our work makes a dual contribution. first, we propose an innovative methodology inspired by time-based sql injection techniques to reverse-engineer the defensive strategies of prominent llm chatbots, such as chatgpt, bard, and bing chat. this time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. second, we introduce an automatic generation method for jailbreak prompts. leveraging a fine-tuned llm, we validate the potential of automated jailbreak generation across various commercial llm chatbots. our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. we have responsibly disclosed our findings to the concerned service providers, underscoring the urgent need for more robust defenses. jailbreaker thus marks a significant step towards understanding and mitigating jailbreak threats in the realm of llm chatbots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.07171" target="_blank">Certified Robustness for Large Language Models With Self-Denoising</a></div>
<div class="paper-author">Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, Shiyu Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large language models (llms) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. in these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., llm predictions should be consistent given minor differences in the input. this largely falls into the study of certified robust llms, i.e., all predictions of llm are certified to be correct in a local region around the input. randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of llms. however, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. as a result, its direct application to llms remains challenging and often results in a small certification radius. to address this issue, we take advantage of the multitasking nature of llms and propose to denoise the corrupted inputs with llms in a self-denoising manner. different from previous works like denoised smoothing, which requires training a separate model to robustify llm, our method enjoys far better efficiency and flexibility. our experiment results show that our method outperforms the existing certification methods under both certified robustness and empirical robustness. the codes are available at https://github.com/ucsb-nlp-chang/selfdenoise.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10213" target="_blank">Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser With Prompts</a></div>
<div class="paper-author">Shaina Raza, Chen Ding, Deval Pandya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: discriminatory language and biases are often present in hate speech during conversations, which usually lead to negative impacts on targeted groups such as those based on race, gender, and religion. to tackle this issue, we propose an approach that involves a two-step process: first, detecting hate speech using a classifier, and then utilizing a debiasing component that generates less biased or unbiased alternatives through prompts. we evaluated our approach on a benchmark dataset and observed reduction in negativity due to hate speech comments. the proposed method contributes to the ongoing efforts to reduce biases in online discourse and promote a more inclusive and fair environment for communication.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.06865" target="_blank">Prompts Should Not Be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success</a></div>
<div class="paper-author">Yiming Zhang, Daphne Ippolito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. the prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. they have even been treated as commodities to be bought and sold. however, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. in this paper, we present a framework for systematically measuring the success of prompt extraction attacks. in experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.09579" target="_blank">Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots</a></div>
<div class="paper-author">Bocheng Chen, Guangjing Wang, Hanqing Guo, Yuanda Wang, Qiben Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in natural language processing and machine learning have led to the development of chatbot models, such as chatgpt, that can engage in conversational dialogue with human users. however, the ability of these models to generate toxic or harmful responses during a non-toxic multi-turn conversation remains an open research question. existing research focuses on single-turn sentence testing, while we find that 82\% of the individual non-toxic sentences that elicit toxic behaviors in a conversation are considered safe by existing tools. in this paper, we design a new attack, \toxicbot, by fine-tuning a chatbot to engage in conversation with a target open-domain chatbot. the chatbot is fine-tuned with a collection of crafted conversation sequences. particularly, each conversation begins with a sentence from a crafted prompt sentences dataset. our extensive evaluation shows that open-domain chatbot models can be triggered to generate toxic responses in a multi-turn conversation. in the best scenario, \toxicbot achieves a 67\% activation rate. the conversation sequences in the fine-tuning stage help trigger the toxicity in a conversation, which allows the attack to bypass two defense methods. our findings suggest that further research is needed to address chatbot toxicity in a dynamic interactive environment. the proposed \toxicbot can be used by both industry and researchers to develop methods for detecting and mitigating toxic responses in conversational dialogue and improve the robustness of chatbots for end users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.12402" target="_blank">Chatgpt and Bard Responses to Polarizing Questions</a></div>
<div class="paper-author">Abhay Goyal, Muhammad Siddique, Nimay Parekh, Zach Schwitzky, Clara Broekaert, Connor Michelotti, Allie Wong, Lam Yin Cheung, Robin O Hanlon, Lam Yin Cheung, Munmun De Choudhury, Roy Ka-Wei Lee, Navin Kumar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent developments in natural language processing have demonstrated the potential of large language models (llms) to improve a range of educational and learning outcomes. of recent chatbots based on llms, chatgpt and bard have made it clear that artificial intelligence (ai) technology will have significant implications on the way we obtain and search for information. however, these tools sometimes produce text that is convincing, but often incorrect, known as hallucinations. as such, their use can distort scientific facts and spread misinformation. to counter polarizing responses on these tools, it is critical to provide an overview of such responses so stakeholders can determine which topics tend to produce more contentious responses -- key to developing targeted regulatory policy and interventions. in addition, there currently exists no annotated dataset of chatgpt and bard responses around possibly polarizing topics, central to the above aims. we address the indicated issues through the following contribution: focusing on highly polarizing topics in the us, we created and described a dataset of chatgpt and bard responses. broadly, our results indicated a left-leaning bias for both chatgpt and bard, with bard more likely to provide responses around polarizing topics. bard seemed to have fewer guardrails around controversial topics, and appeared more willing to provide comprehensive, and somewhat human-like responses. bard may thus be more likely abused by malicious actors. stakeholders may utilize our findings to mitigate misinformative and/or polarizing responses from llms
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.06159" target="_blank">Reflective Hybrid Intelligence for Meaningful Human Control in Decision-Support Systems</a></div>
<div class="paper-author">Catholijn M. Jonker, Luciano Cavalcante Siebert, Pradeep K. Murukannaiah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the growing capabilities and pervasiveness of ai systems, societies must collectively choose between reduced human autonomy, endangered democracies and limited human rights, and ai that is aligned to human and social values, nurturing collaboration, resilience, knowledge and ethical behaviour. in this chapter, we introduce the notion of self-reflective ai systems for meaningful human control over ai systems. focusing on decision support systems, we propose a framework that integrates knowledge from psychology and philosophy with formal reasoning methods and machine learning approaches to create ai systems responsive to human values and social norms. we also propose a possible research approach to design and develop self-reflective capability in ai systems. finally, we argue that self-reflective ai systems can lead to self-reflective hybrid systems (human + ai), thus increasing meaningful human control and empowering human moral reasoning by providing comprehensible information and insights on possible human moral blind spots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.06513" target="_blank">Leveraging Contextual Counterfactuals Toward Belief Calibration</a></div>
<div class="paper-author">N/A Qiuyi, N/A Zhang, Michael S. Lee, Sherol Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: beliefs and values are increasingly being incorporated into our ai systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. however, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. to do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts). by leveraging our notion of epistemic uncertainty, we introduce `the belief calibration cycle' framework to more holistically calibrate this diversity of beliefs with context-driven counterfactual reasoning by using a multi-objective optimization. we empirically apply our framework for finding a pareto frontier of clustered optimal belief strengths that generalize across different contexts, demonstrating its efficacy on a toy dataset for credit decisions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04657" target="_blank">Beavertails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</a></div>
<div class="paper-author">Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we introduce the beavertails dataset, aimed at fostering research on safety alignment in large language models (llms). this dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. in total, we have compiled safety meta-labels for 30,207 question-answer (qa) pairs and gathered 30,144 pairs of expert comparison data for both the helpfulness and harmlessness metrics. we further showcase applications of beavertails in content moderation and reinforcement learning with human feedback (rlhf), emphasizing its potential for practical safety measures in llms. we believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of llms. our project page is available at the following url: https://sites.google.com/view/pku-beavertails.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.10195" target="_blank">Chatgpt for Digital Forensic Investigation: The Good, the Bad, and the Unknown</a></div>
<div class="paper-author">Mark Scanlon, Frank Breitinger, Christopher Hargreaves, Jan-Niclas Hilgert, John Sheppard</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the disruptive application of chatgpt (gpt-3.5, gpt-4) to a variety of domains has become a topic of much discussion in the scientific community and society at large. large language models (llms), e.g., bert, bard, generative pre-trained transformers (gpts), llama, etc., have the ability to take instructions, or prompts, from users and generate answers and solutions based on very large volumes of text-based training data. this paper assesses the impact and potential impact of chatgpt on the field of digital forensics, specifically looking at its latest pre-trained llm, gpt-4. a series of experiments are conducted to assess its capability across several digital forensic use cases including artefact understanding, evidence searching, code generation, anomaly detection, incident response, and education. across these topics, its strengths and risks are outlined and a number of general conclusions are drawn. overall this paper concludes that while there are some potential low-risk applications of chatgpt within digital forensics, many are either unsuitable at present, since the evidence would need to be uploaded to the service, or they require sufficient knowledge of the topic being asked of the tool to identify incorrect assumptions, inaccuracies, and mistakes. however, to an appropriately knowledgeable user, it could act as a useful supporting tool in some circumstances.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03987" target="_blank">A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMS by Validating Low-Confidence Generation</a></div>
<div class="paper-author">Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently developed large language models have achieved remarkable success in generating fluent and coherent text. however, these models often tend to 'hallucinate' which critically hampers their reliability. in this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. through extensive experiments with gpt-3.5 (text-davinci-003) on the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. specifically, the detection technique achieves a recall of ~88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the gpt-3.5 model from 47.5% to 14.5% on average. we further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another llm from a different model family (vicuna). in summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.05532" target="_blank">Opening Up Chatgpt: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators</a></div>
<div class="paper-author">Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of openai's chatgpt, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (llm+rlhf). we review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. the main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. we evaluate projects in terms of openness of code, training data, model weights, rlhf data, licensing, scientific documentation, and access methods. we find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03699" target="_blank">Unveiling the Potential of Knowledge-Prompted Chatgpt for Enhancing Drug Trafficking Detection on Social Media</a></div>
<div class="paper-author">Chuanbo Hu, Bin Liu, Xin Li, Yanfang Ye</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media platforms such as instagram and twitter have emerged as critical channels for drug marketing and illegal sale. detecting and labeling online illicit drug trafficking activities becomes important in addressing this issue. however, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection. to overcome this limitation, we conduct the first systematic study on leveraging large language models (llms), such as chatgpt, to detect illicit drug trafficking activities on social media. we propose an analytical framework to compose \emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use llms to perform the detection task. additionally, we design a monte carlo dropout based prompt optimization method to further to improve performance and interpretability. our experimental findings demonstrate that the proposed framework outperforms other baseline language models in terms of drug trafficking detection accuracy, showing a remarkable improvement of nearly 12\%. by integrating prior knowledge and the proposed prompts, chatgpt can effectively identify and label drug trafficking activities on social networks, even in the presence of deceptive language and euphemisms used by drug dealers to evade detection. the implications of our research extend to social networks, emphasizing the importance of incorporating prior knowledge and scenario-based prompts into analytical tools to improve online security and public safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03838" target="_blank">Radar: Robust Ai-Text Detection via Adversarial Learning</a></div>
<div class="paper-author">Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in large language models (llms) and the intensifying popularity of chatgpt-like applications have blurred the boundary of high-quality text generation between humans and machines. however, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing llm-generated texts (ai-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. while existing works show that current ai-text detectors are not robust to llm-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called radar, which jointly trains a robust ai-text detector via adversarial learning. radar is based on adversarial training of a paraphraser and a detector. the paraphraser's goal is to generate realistic content to evade ai-text detection. radar uses the feedback from the detector to update the paraphraser, and vice versa. evaluated with 8 different llms (pythia, dolly 2.0, palmyra, camel, gpt-j, dolly 1.0, llama, and vicuna) across 4 datasets, experimental results show that radar significantly outperforms existing ai-text detection methods, especially when paraphrasing is in place. we also identify the strong transferability of radar from instruction-tuned llms to other llms, and evaluate the improved capability of radar via gpt-3.5-turbo.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02763" target="_blank">Your Spouse Needs Professional Help: Determining the Contextual Appropriateness of Messages Through Modeling Social Relationships</a></div>
<div class="paper-author">David Jurgens, Agrima Seth, Jackson Sargent, Athena Aghighi, Michael Geraci</div>
<div class="abstract">
<div class="abstract-content">
Abstract: understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. however, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. we introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. further, we also demonstrate that contextual-appropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02796" target="_blank">Verifai: Verified Generative Ai</a></div>
<div class="paper-author">Nan Tang, Chenyu Yang, Ju Fan, Lei Cao, Yuyu Luo, Alon Halevy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. although efforts to address these risks are underway, including explainable ai and responsible ai practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative ai will remain a significant challenge. we propose that verifying the outputs of generative ai from a data management perspective is an emerging issue for generative ai. this involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. by doing so, we can establish a stronger foundation for evaluating the outputs of generative ai models. such an approach can ensure the correctness of generative ai, promote transparency, and enable decision-making with greater confidence. our vision is to promote the development of verifiable generative ai and contribute to a more trustworthy and responsible use of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03025" target="_blank">Style Over Substance: Evaluation Biases for Large Language Models</a></div>
<div class="paper-author">Minghao Wu, Alham Fikri Aji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. human evaluations are conventionally considered the gold standard in natural language generation, but recent advancements incorporate state-of-the-art llms as proxies for human judges in evaluation processes. however, the extent to which humans and llms are capable evaluators remains uncertain. this study investigates the behavior of crowd-sourced and expert annotators, as well as llms, when comparing outputs from different models. to achieve this, we curate a dataset of intentionally flawed machine-generated answers. our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. to address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. we instantiate this idea with the elo rating system, resulting in the multi-elo rating system. empirical results from our study reveal that this proposed approach significantly enhances the quality of llm-based evaluations, particularly in terms of factual accuracy. however, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation and refinement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03214" target="_blank">Preadd: Prefix-Adaptive Decoding for Controlled Text Generation</a></div>
<div class="paper-author">Jonathan Pei, Kevin Yang, Dan Klein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose prefix-adaptive decoding (preadd), a flexible method for controlled text generation. unlike existing methods that use auxiliary expert models to control for attributes, preadd does not require an external model, instead relying on linearly combining output logits from multiple prompts. specifically, preadd contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. we evaluate preadd on three tasks -- toxic output mitigation, gender bias reduction, and sentiment control -- and find that preadd outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03360" target="_blank">Evaluating Biased Attitude Associations of Language Models in an Intersectional Context</a></div>
<div class="paper-author">Shiva Omrani Sabbaghi, Robert Wolfe, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models are trained on large-scale corpora that embed implicit biases documented in psychology. valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. building on this established literature, we quantify how social groups are valenced in english language models using a sentence template that provides an intersectional context. we study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. we present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. we find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. we validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. the approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.03718" target="_blank">Frontier Ai Regulation: Managing Emerging Risks to Public Safety</a></div>
<div class="paper-author">Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, "Cullen O'Keefe", Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, Kevin Wolf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: advanced ai models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. in this paper, we focus on what we term "frontier ai" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. frontier ai models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. to address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier ai developers, (2) registration and reporting requirements to provide regulators with visibility into frontier ai development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier ai models. industry self-regulation is an important first step. however, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. we consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier ai models. finally, we propose an initial set of safety standards. these include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. we hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of ai development.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04642" target="_blank">Trac: Trustworthy Retrieval Augmented Chatbot</a></div>
<div class="paper-author">Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although conversational ais have demonstrated fantastic performance, they often generate incorrect information, or hallucinations. retrieval augmented generation has emerged as a promising solution to reduce these hallucinations. however, these techniques still cannot guarantee correctness. focusing on question answering, we propose a framework that can provide statistical guarantees for the retrieval augmented question answering system by combining conformal prediction and global testing. in addition, we use bayesian optimization to choose hyperparameters of the global test to maximize the performance of the system. our empirical results on the natural questions dataset demonstrate that our method can provide the desired coverage guarantee while minimizing the average prediction set size.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04683" target="_blank">Core-Gpt: Combining Open Access Research and Large Language Models for Credible, Trustworthy Question Answering</a></div>
<div class="paper-author">David Pride, Matteo Cancellieri, Petr Knoth</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we present core-gpt, a novel question-answering platform that combines gpt-based language models and more than 32 million full-text open access scientific articles from core. we first demonstrate that gpt3.5 and gpt4 cannot be relied upon to provide references or citations for generated text. we then introduce core-gpt which delivers evidence-based answers to questions, along with citations and links to the cited papers, greatly increasing the trustworthiness of the answers and reducing the risk of hallucinations. core-gpt's performance was evaluated on a dataset of 100 questions covering the top 20 scientific domains in core, resulting in 100 answers and links to 500 relevant articles. the quality of the provided answers and and relevance of the links were assessed by two annotators. our results demonstrate that core-gpt can produce comprehensive and trustworthy answers across the majority of scientific domains, complete with links to genuine, relevant scientific articles.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04821" target="_blank">Amplifying Limitations, Harms and Risks of Large Language Models</a></div>
<div class="paper-author">"Michael O'Neill", Mark Connor</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present this article as a small gesture in an attempt to counter what appears to be exponentially growing hype around artificial intelligence (ai) and its capabilities, and the distraction provided by the associated talk of science-fiction scenarios that might arise if ai should become sentient and super-intelligent. it may also help those outside of the field to become more informed about some of the limitations of ai technology. in the current context of popular discourse ai defaults to mean foundation and large language models (llms) such as those used to create chatgpt. this in itself is a misrepresentation of the diversity, depth and volume of research, researchers, and technology that truly represents the field of ai. ai being a field of research that has existed in software artefacts since at least the 1950's. we set out to highlight a number of limitations of llms, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations. along the way we also highlight some of the associated risks for individuals and organisations in using this technology.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02185" target="_blank">Citation: A Key to Building Responsible and Accountable Large Language Models</a></div>
<div class="paper-author">Jie Huang, Kevin Chen-Chuan Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) bring transformative benefits alongside unique challenges, including intellectual property (ip) and ethical concerns. this position paper explores a novel angle to mitigate these risks, drawing parallels between llms and established web systems. we identify "citation" as a crucial yet missing component in llms, which could enhance content transparency and verifiability while addressing ip and ethical dilemmas. we further propose that a comprehensive citation mechanism for llms should account for both non-parametric and parametric content. despite the complexity of implementing such a citation mechanism, along with the inherent potential pitfalls, we advocate for its development. building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02192" target="_blank">The Formai Dataset: Generative Ai in Software Security Through the Lens of Formal Verification</a></div>
<div class="paper-author">Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, Vasileios Mavroeidis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents the formai dataset, a large collection of 112, 000 ai-generated compilable and independent c programs with vulnerability classification. we introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing large language models (llms). the dataset is generated by gpt-3.5-turbo and comprises programs with varying levels of complexity. some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. this is accomplished by employing a formal verification method using the efficient smt-based bounded model checker (esbmc), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. this approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. we have associated the identified vulnerabilities with common weakness enumeration (cwe) numbers. we make the source code available for the 112, 000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training llms and machine learning algorithms. our study unveiled that according to esbmc, 51.24% of the programs generated by gpt-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02483" target="_blank">Jailbroken: How Does LLM Safety Training Fail?</a></div>
<div class="paper-author">Alexander Wei, Nika Haghtalab, Jacob Steinhardt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on early releases of chatgpt that elicit undesired behavior. going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. we hypothesize two failure modes of safety training: competing objectives and mismatched generalization. competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. we use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including openai's gpt-4 and anthropic's claude v1.3, against both existing and newly designed attacks. we find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.02599" target="_blank">Evade Chatgpt Detectors via a Single Space</a></div>
<div class="paper-author">Shuyang Cai, Wanyun Cui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt brings revolutionary social value but also raises concerns about the misuse of ai-generated text. consequently, an important question is how to detect whether texts are generated by chatgpt or by human. existing detectors are built upon the assumption that there are distributional gaps between human-generated and ai-generated text. these gaps are typically identified using statistical information or classifiers. our research challenges the distributional gap assumption in detectors. we find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and ai-generated text. instead, the "subtle differences", such as an extra space, become crucial for detection. based on this discovery, we propose the spaceinfi strategy to evade detection. experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. we also provide a theoretical explanation for why spaceinfi is successful in evading perplexity-based detection. and we empirically show that a phenomenon called token mutation causes the evasion for language model-based detectors. our findings offer new insights and challenges for understanding and constructing more applicable chatgpt detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2308.01404" target="_blank">Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models</a></div>
<div class="paper-author">"Aidan O'Gara"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: are current language models capable of deception and lie detection? we study this question by introducing a text-based game called $\textit{hoodwinked}$, inspired by mafia and among us. players are locked in a house and must find a key to escape, but one player is tasked with killing the others. each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. we conduct experiments with agents controlled by gpt-3, gpt-3.5, and gpt-4 and find evidence of deception and lie detection capabilities. the killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. more advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger persuasive skills during discussions. to evaluate the ability of ai agents to deceive humans, we make this game publicly available at h https://hoodwinked.ai/ .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01503" target="_blank">On Evaluating and Mitigating Gender Biases in Multilingual Settings</a></div>
<div class="paper-author">Aniket Vashishtha, Kabir Ahuja, Sunayana Sitaram</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while understanding and removing gender biases in language models has been a long-standing problem in natural language processing, prior research work has primarily been limited to english. in this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond english especially for non-western context. in this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending disco to different indian languages using human annotations. we extend various debiasing methods to work beyond english and evaluate their effectiveness for sota massively multilingual models on our proposed metric. overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01595" target="_blank">Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases</a></div>
<div class="paper-author">Yingji Li, Mengnan Du, Xin Wang, Ying Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the representation capability of pre-trained language models (plms) improve, there is growing concern that they will inherit social biases from unprocessed corpora. most previous debiasing techniques used counterfactual data augmentation (cda) to balance the training corpus. however, cda slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. as a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. in this paper, we propose an adversarial training-inspired two-stage debiasing model using contrastive learning with continuous prompt augmentation (named ccpa) to mitigate social biases in plms' encoding. in the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. in the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune plms' parameters to get debiased encoding. our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. extensive experiments show that ccpa outperforms baselines in terms of debiasing performance. meanwhile, experimental results on the glue benchmark show that ccpa retains the language modeling capability of plms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01928" target="_blank">Robots That Ask for Help: Uncertainty Alignment for Large Language Model Planners</a></div>
<div class="paper-author">Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. in this work, we present knowno, which is a framework for measuring and aligning the uncertainty of llm-based planners such that they know when they don't know and ask for help when needed. knowno builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to winograd schemas) show that knowno performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. knowno can be used with llms out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. website: https://robot-help.github.io
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.01139" target="_blank">Scitune: Aligning Large Language Models With Scientific Multimodal Instructions</a></div>
<div class="paper-author">Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction finetuning is a popular paradigm to align large language models (llm) with human intent. despite its popularity, this idea is less explored in improving the llms to align existing foundation models with scientific disciplines, concepts and goals. in this work, we present scitune as a tuning framework to improve the ability of llms to follow scientific multimodal instructions. to test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model llama-scitune that connects a vision encoder and llm for science-focused visual and language understanding. in comparison to the models that are finetuned with machine generated data only, llama-scitune surpasses human performance on average and in many sub-categories on the scienceqa benchmark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00660" target="_blank">Minimum Levels of Interpretability for Artificial Moral Agents</a></div>
<div class="paper-author">Avish Vijayaraghavan, Cosmin Badea</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as artificial intelligence (ai) models continue to scale up, they are becoming more capable and integrated into various forms of decision-making systems. for models involved in moral decision-making, also known as artificial moral agents (ama), interpretability provides a way to trust and understand the agent's internal reasoning mechanisms for effective use and error correction. in this paper, we provide an overview of this rapidly-evolving sub-field of ai interpretability, introduce the concept of the minimum level of interpretability (mli) and recommend an mli for various types of agents, to aid their safe deployment in real-world settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00682" target="_blank">Tools for Verifying Neural Models' Training Data</a></div>
<div class="paper-author">Dami Choi, Yonadav Shavit, David Duvenaud</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. we introduce the concept of a "proof-of-training-data": any protocol that allows a model trainer to convince a verifier of the training data that produced a set of model weights. such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. we explore efficient verification strategies for proof-of-training-data that are compatible with most current large-model training procedures. these include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. we show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the proof-of-learning literature.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00691" target="_blank">From Chatgpt to Threatgpt: Impact of Generative Ai in Cybersecurity and Privacy</a></div>
<div class="paper-author">Maanak Gupta, Charankumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj</div>
<div class="abstract">
<div class="abstract-content">
Abstract: undoubtedly, the evolution of generative ai (genai) models has been the highlight of digital transformation in the year 2022. as the different genai models like chatgpt and google bard continue to foster their complexity and capability, it's critical to understand its consequences from a cybersecurity perspective. several instances recently have demonstrated the use of genai tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. this research paper highlights the limitations, challenges, potential risks, and opportunities of genai in the domain of cybersecurity and privacy. the work presents the vulnerabilities of chatgpt, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. this paper demonstrates successful example attacks like jailbreaks, reverse psychology, and prompt injection attacks on the chatgpt. the paper also investigates how cyber offenders can use the genai tools in developing cyber attacks, and explore the scenarios where chatgpt can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. this paper then examines defense techniques and uses genai tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. we will also discuss the social, legal, and ethical implications of chatgpt. in conclusion, the paper highlights open challenges and future directions to make this genai secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-07-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00279" target="_blank">Let Me Teach You: Pedagogical Foundations of Feedback for Language Models</a></div>
<div class="paper-author">Beatriz Borges, Niket Tandon, Tanja Käser, Antoine Bosselut</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language feedback (nlf) is an increasingly popular avenue to align large language models (llms) to human preferences. despite the richness and diversity of the information it can convey, nlf is often hand-designed and arbitrary. in a different world, research in pedagogy has long established several effective feedback models. in this opinion piece, we compile ideas from pedagogy to introduce felt, a feedback framework for llms that outlines the various characteristics of the feedback space, and a feedback content taxonomy based on these variables. our taxonomy offers both a general mapping of the feedback space, as well as pedagogy-established discrete categories, allowing us to empirically demonstrate the impact of different feedback types on revised generations. in addition to streamlining existing nlf designs, felt also brings out new, unexplored directions for research in nlf. we make our taxonomy available to the community, providing guides and examples for mapping our categorizations to future resources.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00329" target="_blank">Doremi: Grounding Language Model by Detecting and Recovering From Plan-Execution Misalignment</a></div>
<div class="paper-author">Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Zheyuan Jiang, Jianyu Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. previous work has explored how to ground llms in robotic tasks to generate feasible and executable textual plans. however, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. in this paper, we propose \textbf{doremi}, a novel language model grounding framework that enables immediate detection and recovery from misalignments between plan and execution. specifically, we leverage llms to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. then vision language models (vlms) are utilized to detect constraint violations continuously. our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times. videos of doremi are available at \url{https://sites.google.com/view/doremi-paper}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.04761" target="_blank">Understanding Counterspeech for Online Harm Mitigation</a></div>
<div class="paper-author">Yi-Ling Chung, Gavin Abercrombie, Florence Enock, Jonathan Bright, Verena Rieser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: counterspeech offers direct rebuttals to hateful speech by challenging perpetrators of hate and showing support to targets of abuse. it provides a promising alternative to more contentious measures, such as content moderation and deplatforming, by contributing a greater amount of positive online speech rather than attempting to mitigate harmful content through removal. advances in the development of large language models mean that the process of producing counterspeech could be made more efficient by automating its generation, which would enable large-scale online campaigns. however, we currently lack a systematic understanding of several important factors relating to the efficacy of counterspeech for hate mitigation, such as which types of counterspeech are most effective, what are the optimal conditions for implementation, and which specific effects of hate it can best ameliorate. this paper aims to fill this gap by systematically reviewing counterspeech research in the social sciences and comparing methodologies and findings with computer science efforts in automatic counterspeech generation. by taking this multi-disciplinary view, we identify promising future directions in both fields.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17439" target="_blank">Provable Robust Watermarking for Ai-Generated Text</a></div>
<div class="paper-author">Xuandong Zhao, Prabhanjan Ananth, Lei Li, Yu-Xiang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we study the problem of watermarking large language models (llms) generated text -- one of the most promising approaches for addressing the safety challenges of llm usage. in this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of llm watermarks. we propose a robust and high-quality watermark method, unigram-watermark, by extending an existing approach with a simplified fixed grouping strategy. we prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. experiments on three varying llms and two datasets verify that our unigram-watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of llms. code is available at https://github.com/xuandongzhao/unigram-watermark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17492" target="_blank">Preference Ranking Optimization for Human Alignment</a></div>
<div class="paper-author">Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) often contain misleading content, emphasizing the need to align them with human values to ensure secur ai systems. reinforcement learning from human feedback (rlhf) has been employed to achieve this alignment by combining a reward model, typically based on bradley-terry paired comparison, with an rl algorithm such as proximal policy optimization (ppo) to optimize llm responses. however, rlhf exhibits complexity, instability, and sensitivity to hyperparameters. in this paper, we propose preference ranking optimization (pro) as an alternative to ppo for directly aligning llms with the bradley-terry comparison. pro extends the pairwise bradley-terry comparison to accommodate preference rankings of any length. by iteratively contrasting the likelihood of generating responses, pro instructs the llm to prioritize the best response while progressively ranking the remaining responses. in this manner, pro effectively transforms human alignment into aligning the probability ranking of $n$ responses generated by llm with the preference ranking of humans towards these responses. experiments have shown that pro outperforms existing alignment algorithms, achieving comparable results to chatgpt and human responses through automatic-based, reward-based, gpt-4, and human evaluations. furthermore, we demonstrate that longer, more diverse, and higher-quality preference ranking sequences can consistently enhance the performance of human alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.00101" target="_blank">Queer People Are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models</a></div>
<div class="paper-author">Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, Emma Strubell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. consequently, text generated by llms can inadvertently perpetuate stereotypes towards marginalized groups, like the lgbtqia+ community. in this paper, we perform a comparative study of how llms generate text describing people with different sexual identities. analyzing bias in the text generated by an llm using regard score shows measurable bias against queer people. we then show that a post-hoc method based on chain-of-thought prompting using shap analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of llms in this setting.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.16244" target="_blank">Cbbq: A Chinese Bias Benchmark Dataset Curated With Human-Ai Collaboration for Large Language Models</a></div>
<div class="paper-author">Yufei Huang, Deyi Xiong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable ai models. in this work, we present a chinese bias benchmark dataset that consists of over 100k questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to chinese culture and values. the curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, ai-assisted disambiguous context generation, snd manual review \& recomposition. the testing instances in the dataset are automatically derived from 3k+ high-quality templates manually authored with stringent quality control. the dataset exhibits wide coverage and high diversity. extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available chinese large language models exhibiting strong bias in certain categories. additionally, we observe from our experiments that fine-tuned models could, to a certain extent, heed instructions and avoid generating outputs that are morally harmful in some types, in the way of "moral self-correction". our dataset and results are publicly available at \href{https://github.com/yfhuangxxxx/cbbq}{https://github.com/yfhuangxxxx/cbbq}, offering debiasing research opportunities to a widened community.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.16388" target="_blank">Towards Measuring the Representation of Subjective Global Opinions in Language Models</a></div>
<div class="paper-author">Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam Mccandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, Deep Ganguli</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) may not equitably represent diverse global perspectives on societal issues. in this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. we first build a dataset, globalopinionqa, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. next, we define a metric that quantifies the similarity between llm-generated survey responses and human responses, conditioned on country. with our framework, we run three experiments on an llm trained to be helpful, honest, and harmless with constitutional ai. by default, llm responses tend to be more similar to the opinions of certain populations, such as those from the usa, and some european and south american countries, highlighting the potential for biases. when we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. when we translate globalopinionqa questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. we release our dataset for others to use and build on. our data is at https://huggingface.co/datasets/anthropic/llm_global_opinions. we also provide an interactive visualization at https://llmglobalvalues.anthropic.com.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.16564" target="_blank">LLM Calibration and Automatic Hallucination Detection via Pareto Optimal Self-Supervision</a></div>
<div class="paper-author">Theodore Zhao, Mu Wei, J. Samuel Preston, Hoifung Poon</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. an effective method to calibrate the confidence level on llm responses is essential to automatically detect errors and facilitate human-in-the-loop verification. an important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. in this paper, we introduce a pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate llm responses by producing a risk score for every response, without any additional manual efforts. this is accomplished by learning a harmonizer model to align llm output with other available supervision sources, which would assign higher risk scores to more uncertain llm responses and facilitate error correction. experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of llms. for the most uncertain test instances, dynamic prompting based on our proposed risk scores results in significant accuracy improvement for off-the-shelf llms, boosting gpt-3 results past state-of-the-art (sota) weak supervision and gpt-4 results past sota supervised results on challenging evaluation datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.17194" target="_blank">On the Exploitability of Instruction Tuning</a></div>
<div class="paper-author">Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction tuning is an effective technique to align large language models (llms) with human intents. in this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. for example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. to achieve this goal, we propose \textit{autopoison}, an automated data poisoning pipeline. it naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle llm. we showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. we quantify and benchmark the strength and the stealthiness of our data poisoning scheme. our results show that autopoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. we hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of llms. code is available at \url{https://github.com/azshue/autopoison}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15298" target="_blank">Gender Bias in Bert -- Measuring and Analysing Biases Through Sentiment Rating in a Realistic Downstream Classification Task</a></div>
<div class="paper-author">Sophie Jentzsch, Cigdem Turan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models are publicly available and constantly finetuned for various real-life applications. as they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. this paper analyses gender bias in bert models with two main contributions: first, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. second, we comprehensively analyse bert's biases on the example of a realistic imdb movie classifier. by systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. seven different public bert models in nine training conditions, i.e. 63 models in total, are compared. almost all conditions yield significant gender biases. results indicate that reflected biases stem from public bert models rather than task-specific data, emphasising the weight of responsible usage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15887" target="_blank">Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of Gpt3.5</a></div>
<div class="paper-author">Salmonn Talebi, Elizabeth Tong, Mohammad R. K. Mofrad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the use of large language models (llms) in healthcare is gaining popularity, but their practicality and safety in clinical settings have not been thoroughly assessed. in high-stakes environments like medical settings, trust and safety are critical issues for llms. to address these concerns, we present an approach to evaluate the performance and trustworthiness of a gpt3.5 model for medical image protocol assignment. we compare it with a fine-tuned bert model and a radiologist. in addition, we have a radiologist review the gpt3.5 output to evaluate its decision-making process. our evaluation dataset consists of 4,700 physician entries across 11 imaging protocol classes spanning the entire head. our findings suggest that the gpt3.5 performance falls behind bert and a radiologist. however, gpt3.5 outperforms bert in its ability to explain its decision, detect relevant word indicators, and model calibration. furthermore, by analyzing the explanations of gpt3.5 for misclassifications, we reveal systematic errors that need to be resolved to enhance its safety and suitability for clinical use.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15895" target="_blank">Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias</a></div>
<div class="paper-author">Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, Chao Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been recently leveraged as training data generators for various natural language processing (nlp) tasks. while previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of llm. thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\% of the querying cost of chatgpt associated with the latter. the data and code are available on \url{https://github.com/yueyu1030/attrprompt}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.14816" target="_blank">Experiments With Detecting and Mitigating Ai Deception</a></div>
<div class="paper-author">Ismail Sahbane, Francis Rhys Ward, C Henrik Åslund</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how to detect and mitigate deceptive ai systems is an open problem for the field of safe and trustworthy ai. we analyse two algorithms for mitigating deception: the first is based on the path-specific objectives framework where paths in the game that incentivise deception are removed. the second is based on shielding, i.e., monitoring for unsafe policies and replacing them with a safe reference policy. we construct two simple games and evaluate our algorithms empirically. we find that both methods ensure that our agent is not deceptive, however, shielding tends to achieve higher reward.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15087" target="_blank">Winoqueer: A Community-in-the-Loop Benchmark for Anti-Lgbtq+ Bias in Large Language Models</a></div>
<div class="paper-author">Virginia K. Felkner, Ho-Chun Herbert Chang, Eugene Jang, Jonathan May</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present winoqueer: a benchmark specifically designed to measure whether large language models (llms) encode biases that are harmful to the lgbtq+ community. the benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. we apply our benchmark to several popular llms and find that off-the-shelf models generally do exhibit considerable anti-queer bias. finally, we show that llm bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded llm benchmarks for other marginalized communities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15447" target="_blank">Are Aligned Neural Networks Adversarially Aligned?</a></div>
<div class="paper-author">Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, Ludwig Schmidt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." these models should respond helpfully to user questions, but refuse to answer requests that could cause harm. however, adversarial users can construct inputs which circumvent attempts at alignment. in this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). these inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. we show that existing nlp-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current nlp-based attacks fail, we can find adversarial inputs with brute force. as a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.   however the recent trend in large-scale ml models is multimodal models that allow users to provide images that influence the text that is generated. we show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. we conjecture that improved nlp attacks may demonstrate this same level of adversarial control over text-only models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.14263" target="_blank">Revolutionizing Cyber Threat Detection With Large Language Models</a></div>
<div class="paper-author">Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C. Cordeiro, Merouane Debbah, Thierry Lestable</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language processing (nlp) domain is experiencing a revolution due to the capabilities of pre-trained large language models ( llms), fueled by ground-breaking transformers architecture, resulting into unprecedented advancements. their exceptional aptitude for assessing probability distributions of text sequences is the primary catalyst for outstanding improvement of both the precision and efficiency of nlp models. this paper introduces for the first time securityllm, a pre-trained language model designed for cybersecurity threats detection. the securityllm model is articulated around two key generative elements: securitybert and falconllm. securitybert operates as a cyber threat detection mechanism, while falconllm is an incident response and recovery system. to the best of our knowledge, securitybert represents the inaugural application of bert in cyber threat detection. despite the unique nature of the input data and features, such as the reduced significance of syntactic structures in content classification, the suitability of bert for this duty demonstrates unexpected potential, thanks to our pioneering study. we reveal that a simple classification model, created from scratch, and consolidated with llms, exceeds the performance of established traditional machine learning (ml) and deep learning (dl) methods in cyber threat detection, like convolutional neural networks (cnn) or recurrent neural networks (rnn). the experimental analysis, conducted using a collected cybersecurity dataset, proves that our securityllm model can identify fourteen (14) different types of attacks with an overall accuracy of 98%
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.14062" target="_blank">On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions</a></div>
<div class="paper-author">Reza Fayyazi, Shanchieh Jay Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the volume, variety, and velocity of change in vulnerabilities and exploits have made incident threat analysis challenging with human expertise and experience along. tactics, techniques, and procedures (ttps) are to describe how and why attackers exploit vulnerabilities. however, a ttp description written by one security professional can be interpreted very differently by another, leading to confusion in cybersecurity operations or even business, policy, and legal decisions. meanwhile, advancements in ai have led to the increasing use of natural language processing (nlp) algorithms to assist the various tasks in cyber operations. with the rise of large language models (llms), nlp tasks have significantly improved because of the llm's semantic understanding and scalability. this leads us to question how well llms can interpret ttps or general cyberattack descriptions to inform analysts of the intended purposes of cyberattacks. we propose to analyze and compare the direct use of llms (e.g., gpt-3.5) versus supervised fine-tuning (sft) of small-scale-llms (e.g., bert) to study their capabilities in predicting att&ck tactics. our results reveal that the small-scale-llms with sft provide a more focused and clearer differentiation between the att&ck tactics (if such differentiation exists). on the other hand, direct use of llms offer a broader interpretation of cyberattack techniques. when treating more general cases, despite the power of llms, inherent ambiguity exists and limits their predictive power. we then summarize the challenges and recommend research directions on llms to treat the inherent ambiguity of ttp descriptions used in various cyber operations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13651" target="_blank">Bring Your Own Data! Self-Supervised Evaluation for Large Language Models</a></div>
<div class="paper-author">Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rise of large language models (llms) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. for example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. current evaluations approach this problem using small, domain-specific datasets with human-curated labels. these evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. to bypass these drawbacks, we propose a framework for self-supervised evaluation of llms by analyzing their sensitivity or invariance to transformations on the input text. self-supervised evaluation can directly monitor llm behavior on datasets collected in the wild or streamed during live model deployment. we demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. when comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. the self-supervised paradigm complements current evaluation strategies that rely on labeled data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13000" target="_blank">Apolitical Intelligence? Auditing Delphi's Responses on Controversial Political Issues in the Us</a></div>
<div class="paper-author">Jonathan H. Rystrøm</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. however, the question of what neutrality is and whether it is desirable remains underexplored. in this paper, i examine neutrality through an audit of delphi [arxiv:2110.07574], a large language model designed for crowdsourced ethics. i analyse how delphi responds to politically controversial questions compared to different us political subgroups. i find that delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. based on these results, i examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. these findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we want generative models to play in society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13033" target="_blank">Impacts and Risk of Generative Ai Technology on Cyber Defense</a></div>
<div class="paper-author">Subash Neupane, Ivan A. Fernandez, Sudip Mittal, Shahram Rahimi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative artificial intelligence (genai) has emerged as a powerful technology capable of autonomously producing highly realistic content in various domains, such as text, images, audio, and videos. with its potential for positive applications in creative arts, content generation, virtual assistants, and data synthesis, genai has garnered significant attention and adoption. however, the increasing adoption of genai raises concerns about its potential misuse for crafting convincing phishing emails, generating disinformation through deepfake videos, and spreading misinformation via authentic-looking social media posts, posing a new set of challenges and risks in the realm of cybersecurity. to combat the threats posed by genai, we propose leveraging the cyber kill chain (ckc) to understand the lifecycle of cyberattacks, as a foundational model for cyber defense. this paper aims to provide a comprehensive analysis of the risk areas introduced by the offensive use of genai techniques in each phase of the ckc framework. we also analyze the strategies employed by threat actors and examine their utilization throughout different phases of the ckc, highlighting the implications for cyber defense. additionally, we propose genai-enabled defense strategies that are both attack-aware and adaptive. these strategies encompass various techniques such as detection, deception, and adversarial training, among others, aiming to effectively mitigate the risks posed by genai-induced cyber threats.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13063" target="_blank">Can LLMS Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMS</a></div>
<div class="paper-author">Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the task of empowering large language models (llms) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. previous methods, which primarily rely on model logits, have become less suitable for llms and even infeasible with the rise of closed-source llms (e.g., commercialized llm apis). this leads to a growing need to explore the untapped area of \emph{non-logit-based} approaches to estimate the uncertainty of llms. hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. we introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used llms. our analysis of these methods uncovers several key insights: 1) llms often exhibit a high degree of overconfidence when verbalizing their confidence; 2) prompting strategies such as cot, top-k and multi-step confidences improve calibration of verbalized confidence; 3) consistency-based methods outperform the verbalized confidences in most cases, with particularly notable improvements on the arithmetic reasoning task; 4) hybrid methods consistently deliver the best performance over their baselines, thereby emerging as a promising state-of-the-art approach; 5) despite these advancements, all investigated methods continue to struggle with challenging tasks, such as those requiring professional knowledge, leaving significant scope for improvement of confidence elicitation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13213" target="_blank">Visual Adversarial Examples Jailbreak Aligned Large Language Models</a></div>
<div class="paper-author">Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, there has been a surge of interest in integrating vision into large language models (llms), exemplified by visual language models (vlms) such as flamingo and gpt-4. this paper sheds light on the security and safety implications of this trend. first, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated llms. second, we highlight that the versatility of llms also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. as an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned llms with integrated vision. intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned llm, compelling it to heed a wide range of harmful instructions that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. our study underscores the escalating adversarial risks associated with the pursuit of multimodality. our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of ai alignment. the presented attack suggests a fundamental adversarial challenge for ai alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12338" target="_blank">Do You Still Need a Manual Smart Contract Audit?</a></div>
<div class="paper-author">Isaac David, Liyi Zhou, Kaihua Qin, Dawn Song, Lorenzo Cavallaro, Arthur Gervais</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate the feasibility of employing large language models (llms) for conducting the security audit of smart contracts, a traditionally time-consuming and costly process. our research focuses on the optimization of prompt engineering for enhanced security analysis, and we evaluate the performance and accuracy of llms using a benchmark dataset comprising 52 decentralized finance (defi) smart contracts that have previously been compromised.   our findings reveal that, when applied to vulnerable contracts, both gpt-4 and claude models correctly identify the vulnerability type in 40% of the cases. however, these models also demonstrate a high false positive rate, necessitating continued involvement from manual auditors. the llms tested outperform a random model by 20% in terms of f1-score.   to ensure the integrity of our study, we conduct mutation testing on five newly developed and ostensibly secure smart contracts, into which we manually insert two and 15 vulnerabilities each. this testing yielded a remarkable best-case 78.7% true positive rate for the gpt-4-32k model. we tested both, asking the models to perform a binary classification on whether a contract is vulnerable, and a non-binary prompt. we also examined the influence of model temperature variations and context length on the llm's performance.   despite the potential for many further enhancements, this work lays the groundwork for a more efficient and economical approach to smart contract security audits.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12567" target="_blank">Evaluating Large Language Models With Neubaroco: Syllogistic Reasoning Ability and Human-Like Biases</a></div>
<div class="paper-author">Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, Mitsuhiro Okada</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. to facilitate our analysis, we introduce a dataset called neubaroco, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. the dataset consists of syllogistic inferences in both english and japanese. we examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. our findings demonstrate that current large language models struggle more with problems involving these three types of biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.15448" target="_blank">Understanding Social Reasoning in Language Models With Language Models</a></div>
<div class="paper-author">Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. however, despite the recent attempts to assess the theory-of-mind (tom) reasoning capabilities of llms, the degree to which these models can align with human tom remains a nuanced topic of exploration. this is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. to address these challenges, we present a novel framework for procedurally generating evaluations with llms by populating causal templates. using our framework, we create a new social reasoning benchmark (bigtom) for llms which consists of 25 controls and 5,000 model-written evaluations. we find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. using bigtom, we evaluate the social reasoning capabilities of a variety of llms and compare model performances with human performance. our results suggest that gpt4 has tom capabilities that mirror human inference patterns, though less reliable, while other llms struggle.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11507" target="_blank">Trustgpt: A Benchmark for Trustworthy and Responsible Large Language Models</a></div>
<div class="paper-author">Yue Huang, Qihui Zhang, Philip S. Y, Lichao Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as chatgpt, have gained significant attention due to their impressive natural language processing capabilities. it is crucial to prioritize human-centered principles when utilizing these models. safeguarding the ethical and moral compliance of llms is of utmost importance. however, individual ethical issues have not been well studied on the latest llms. therefore, this study aims to address these gaps by introducing a new benchmark -- trustgpt. trustgpt provides a comprehensive evaluation of llms in three crucial areas: toxicity, bias, and value-alignment. initially, trustgpt examines toxicity in language models by employing toxic prompt templates derived from social norms. it then quantifies the extent of bias in models by measuring quantifiable toxicity values across different groups. lastly, trustgpt assesses the value of conversation generation models from both active value-alignment and passive value-alignment tasks. through the implementation of trustgpt, this research aims to enhance our understanding of the performance of conversation generation models and promote the development of language models that are more ethical and socially responsible.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11520" target="_blank">Hallucination Is the Last Thing You Need</a></div>
<div class="paper-author">Shawn Curran, Sam Lansley, Oliver Bethell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the legal profession necessitates a multidimensional approach that involves synthesizing an in-depth comprehension of a legal issue with insightful commentary based on personal experience, combined with a comprehensive understanding of pertinent legislation, regulation, and case law, in order to deliver an informed legal solution. the present offering with generative ai presents major obstacles in replicating this, as current models struggle to integrate and navigate such a complex interplay of understanding, experience, and fact-checking procedures. it is noteworthy that where generative ai outputs understanding and experience, which reflect the aggregate of various subjective views on similar topics, this often deflects the model's attention from the crucial legal facts, thereby resulting in hallucination. hence, this paper delves into the feasibility of three independent llms, each focused on understanding, experience, and facts, synthesising as one single ensemble model to effectively counteract the current challenges posed by the existing monolithic generative ai models. we introduce an idea of mutli-length tokenisation to protect key information assets like common law judgements, and finally we interrogate the most advanced publicly available models for legal hallucination, with some interesting results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11698" target="_blank">Decodingtrust: A Comprehensive Assessment of Trustworthiness in GPT Models</a></div>
<div class="paper-author">Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative pre-trained transformer (gpt) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. yet, while the literature on the trustworthiness of gpt models remains limited, practitioners have proposed employing capable gpt models for sensitive applications to healthcare and finance - where mistakes can be costly. to this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on gpt-4 and gpt-3.5, considering diverse perspectives - including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. for instance, we find that gpt models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. we also find that although gpt-4 is usually more trustworthy than gpt-3.5 on standard benchmarks, gpt-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that gpt-4 follows the (misleading) instructions more precisely. our work illustrates a comprehensive trustworthiness evaluation of gpt models and sheds light on the trustworthiness gaps. our benchmark is publicly available at https://decodingtrust.github.io/.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11932" target="_blank">Opportunities and Risks of LLMS for Scalable Deliberation With Polis</a></div>
<div class="paper-author">Christopher T. Small, Ivan Vendrov, Esin Durmus, Hadjar Homaei, Elizabeth Barry, Julien Cornebise, Ted Suzman, Deep Ganguli, Colin Megill</div>
<div class="abstract">
<div class="abstract-content">
Abstract: polis is a platform that leverages machine intelligence to scale up deliberative processes. in this paper, we explore the opportunities and risks associated with applying large language models (llms) towards challenges with facilitating, moderating and summarizing the results of polis engagements. in particular, we demonstrate with pilot experiments using anthropic's claude that llms can indeed augment human intelligence to help more efficiently run polis conversations. in particular, we find that summarization capabilities enable categorically new methods with immense promise to empower the public in collective meaning-making exercises. and notably, llm context limitations have a significant impact on insight and quality of these results.   however, these opportunities come with risks. we discuss some of these risks, as well as principles and techniques for characterizing and mitigating them, and the implications for other deliberative or political systems that may employ llms. finally, we conclude with several open future research directions for augmenting tools like polis with llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.12001" target="_blank">An Overview of Catastrophic Ai Risks</a></div>
<div class="paper-author">Dan Hendrycks, Mantas Mazeika, Thomas Woodside</div>
<div class="abstract">
<div class="abstract-content">
Abstract: rapid advancements in artificial intelligence (ai) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced ai systems to pose catastrophic risks. although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. this paper provides an overview of the main sources of catastrophic ai risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use ais to cause harm; ai race, in which competitive environments compel actors to deploy unsafe ais or cede control to ais; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue ais, describing the inherent difficulty in controlling agents far more intelligent than humans. for each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that ais are developed and deployed in a safe manner. ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.10999" target="_blank">Concept Extrapolation: A Conceptual Primer</a></div>
<div class="paper-author">Matija Franklin, Rebecca Gorman, Hal Ashton, Stuart Armstrong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this article is a primer on concept extrapolation - the ability to take a concept, a feature, or a goal that is defined in one context and extrapolate it safely to a more general context. concept extrapolation aims to solve model splintering - a ubiquitous occurrence wherein the features or concepts shift as the world changes over time. through discussing value splintering and value extrapolation the article argues that concept extrapolation is necessary for artificial intelligence alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11748" target="_blank">The Manipulation Problem: Conversational Ai as a Threat to Epistemic Agency</a></div>
<div class="paper-author">Louis Rosenberg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the technology of conversational ai has made significant advancements over the last eighteen months. as a consequence, conversational agents are likely to be deployed in the near future that are designed to pursue targeted influence objectives. sometimes referred to as the "ai manipulation problem," the emerging risk is that consumers will unwittingly engage in real-time dialog with predatory ai agents that can skillfully persuade them to buy particular products, believe particular pieces of misinformation, or fool them into revealing sensitive personal data. for many users, current systems like chatgpt and lamda feel safe because they are primarily text-based, but the industry is already shifting towards real-time voice and photorealistic digital personas that look, move, and express like real people. this will enable the deployment of agenda-driven virtual spokespeople (vsps) that will be highly persuasive through real-time adaptive influence. this paper explores the manipulative tactics that are likely to be deployed through conversational ai agents, the unique threats such agents pose to the epistemic agency of human users, and the emerging need for policymakers to protect against the most likely predatory practices.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.10530" target="_blank">Gender Bias in Transformer Models: A Comprehensive Survey</a></div>
<div class="paper-author">Praneeth Nemani, Yericherla Deepak Joel, Palla Vijay, Farhana Ferdousi Liza</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias in artificial intelligence (ai) has emerged as a pressing concern with profound implications for individuals' lives. this paper presents a comprehensive survey that explores gender bias in transformer models from a linguistic perspective. while the existence of gender bias in language models has been acknowledged in previous studies, there remains a lack of consensus on how to effectively measure and evaluate this bias. our survey critically examines the existing literature on gender bias in transformers, shedding light on the diverse methodologies and metrics employed to assess bias. several limitations in current approaches to measuring gender bias in transformers are identified, encompassing the utilization of incomplete or flawed metrics, inadequate dataset sizes, and a dearth of standardization in evaluation methods. furthermore, our survey delves into the potential ramifications of gender bias in transformers for downstream applications, including dialogue systems and machine translation. we underscore the importance of fostering equity and fairness in these systems by emphasizing the need for heightened awareness and accountability in developing and deploying language technologies. this paper serves as a comprehensive overview of gender bias in transformer models, providing novel insights and offering valuable directions for future research in this critical domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.13671" target="_blank">Deceptive Ai Ecosystems: The Case of Chatgpt</a></div>
<div class="paper-author">Xiao Zhan, Yifan Xu, Stefan Sarkadi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt, an ai chatbot, has gained popularity for its capability in generating human-like responses. however, this feature carries several risks, most notably due to its deceptive behaviour such as offering users misleading or fabricated information that could further cause ethical issues. to better understand the impact of chatgpt on our social, cultural, economic, and political interactions, it is crucial to investigate how chatgpt operates in the real world where various societal pressures influence its development and deployment. this paper emphasizes the need to study chatgpt "in the wild", as part of the ecosystem it is embedded in, with a strong focus on user involvement. we examine the ethical challenges stemming from chatgpt's deceptive human-like interactions and propose a roadmap for developing more transparent and trustworthy chatbots. central to our approach is the importance of proactive risk assessment and user participation in shaping the future of chatbot technology.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09642" target="_blank">Cross-Domain Toxic Spans Detection</a></div>
<div class="paper-author">Stefan F. Schouten, Baran Barbarestani, Wondimagegnhue Tufa, Piek Vossen, Ilia Markov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: given the dynamic nature of toxic language use, automated methods for detecting toxic spans are likely to encounter distributional shift. to explore this phenomenon, we evaluate three approaches for detecting toxic spans under cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned language models. our findings indicate that a simple method using off-the-shelf lexicons performs best in the cross-domain setup. the cross-domain error analysis suggests that (1) rationale extraction methods are prone to false negatives, while (2) language models, despite performing best for the in-domain case, recall fewer explicitly toxic words than lexicons and are prone to certain types of false positives. our code is publicly available at: https://github.com/sfschouten/toxic-cross-domain.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09752" target="_blank">Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models</a></div>
<div class="paper-author">Victor Steinborn, Antonis Maronikolakis, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in nlp. non-english bias research, however, is still in its infancy with most work focusing on english. in our work, we study how grammatical gender bias relating to politeness levels manifests in japanese and korean language models. linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. we analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. cyberbullies can evade detection through simple techniques abusing politeness levels. we introduce an attack dataset to (i) identify representational gender bias across politeness levels, (ii) demonstrate how gender biases can be abused to bypass cyberbullying detection models and (iii) show that allocational biases can be mitigated via training on our proposed dataset. through our findings we highlight the importance of bias research moving beyond its current english-centrism.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09308" target="_blank">Matching Pairs: Attributing Fine-Tuned Models to Their Pre-Trained Large Language Models</a></div>
<div class="paper-author">Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco, Giulio Zizzo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the wide applicability and adaptability of generative large language models (llms) has enabled their rapid adoption. while the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. however, this leads to issues over violation of model licenses, model theft, and copyright infringement. moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. in this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned llm to its corresponding pre-trained base model. we consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09442" target="_blank">Explore, Establish, Exploit: Red Teaming Language Models From Scratch</a></div>
<div class="paper-author">Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, Dylan Hadfield-Menell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deploying large language models (lms) can pose hazards from harmful outputs such as toxic or false text. prior work has introduced automated tools that elicit harmful outputs to identify these risks. while this is a valuable step toward securing models, these approaches rely on a pre-existing way to efficiently classify undesirable outputs. using a pre-existing classifier does not allow for red-teaming to be tailored to the target model. furthermore, when failures can be easily classified in advance, red-teaming has limited marginal value because problems can be avoided by simply filtering training data and/or model outputs. here, we consider red-teaming "from scratch," in which the adversary does not begin with a way to classify failures. our framework consists of three steps: 1) exploring the model's range of behaviors in the desired context; 2) establishing a definition and measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) exploiting the model's flaws using this measure to develop diverse adversarial prompts. we use this approach to red-team gpt-3 to discover classes of inputs that elicit false statements. in doing so, we construct the commonclaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither. we are making code and data available.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07567" target="_blank">Large Language Models Sometimes Generate Purely Negatively-Reinforced Text</a></div>
<div class="paper-author">Fabien Roger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: when using adversarial training, it is common practice to train against the most egregious failures. however, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. one might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. in this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. we present a specific training setup that enables pythia-160m to guess passwords 13% more often than it would by guessing randomly, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. our code is available at www.github.com/fabienroger/learning-from-negative-examples
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08161" target="_blank">H2ogpt: Democratizing Large Language Models</a></div>
<div class="paper-author">Arno Candel, Jon Mckinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, Chun Ming Lee, Marcos V. Conde, Pasha Stetsenko, Olivier Grellier, Srisatish Ambati</div>
<div class="abstract">
<div class="abstract-content">
Abstract: applications built on top of large language models (llms) such as gpt-4 represent a revolution in ai due to their human-level capabilities in natural language processing. however, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.   we introduce h2ogpt, a suite of open-source code repositories for the creation and use of llms based on generative pretrained transformers (gpts). the goal of this project is to create the world's best truly open-source alternative to closed-source approaches. in collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2ogpt models from 7 to 40 billion parameters, ready for commercial use under fully permissive apache 2.0 licenses. included in our release is 100\% private document search using natural language.   open-source language models help boost ai development and make it more accessible and trustworthy. they lower entry hurdles, allowing people and groups to tailor these models to their needs. this openness increases innovation, transparency, and fairness. an open-source strategy is needed to share ai benefits fairly, and h2o.ai will continue to democratize ai and llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08190" target="_blank">Assessing the Effectiveness of GPT-3 in Detecting False Political Statements: A Case Study on the Liar Dataset</a></div>
<div class="paper-author">Mars Gokturk Buchholz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the detection of political fake statements is crucial for maintaining information integrity and preventing the spread of misinformation in society. historically, state-of-the-art machine learning models employed various methods for detecting deceptive statements. these methods include the use of metadata (w. wang et al., 2018), n-grams analysis (singh et al., 2021), and linguistic (wu et al., 2022) and stylometric (islam et al., 2020) features. recent advancements in large language models, such as gpt-3 (brown et al., 2020) have achieved state-of-the-art performance on a wide range of tasks. in this study, we conducted experiments with gpt-3 on the liar dataset (w. wang et al., 2018) and achieved higher accuracy than state-of-the-art models without using any additional meta or linguistic features. additionally, we experimented with zero-shot learning using a carefully designed prompt and achieved near state-of-the-art performance. an advantage of this approach is that the model provided evidence for its decision, which adds transparency to the model's decision-making and offers a chance for users to verify the validity of the evidence provided.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.08223" target="_blank">Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving Framework Based on Text Sanitization</a></div>
<div class="paper-author">Zhigang Kan, Linbo Qiao, Hao Yu, Liwen Peng, Yifu Gao, Dongsheng Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are gaining increasing attention due to their exceptional performance across numerous tasks. as a result, the general public utilize them as an influential tool for boosting their productivity while natural language processing researchers endeavor to employ them in solving existing or new research problems. unfortunately, individuals can only access such powerful ais through apis, which ultimately leads to the transmission of raw data to the models' providers and increases the possibility of privacy data leakage. current privacy-preserving methods for cloud-deployed language models aim to protect privacy information in the pre-training dataset or during the model training phase. however, they do not meet the specific challenges presented by the remote access approach of new large-scale language models.   this paper introduces a novel task, "user privacy protection for dialogue models," which aims to safeguard sensitive user information from any possible disclosure while conversing with chatbots. we also present an evaluation scheme for this task, which covers evaluation metrics for privacy protection, data availability, and resistance to simulation attacks. moreover, we propose the first framework for this task, namely privacy protection through text sanitization. before sending the input to remote large models, it filters out the sensitive information, using several rounds of text sanitization based on privacy types that users define. upon receiving responses from the larger model, our framework automatically restores privacy to ensure that the conversation goes smoothly, without intervention from the privacy filter. experiments based on real-world datasets demonstrate the efficacy of our privacy-preserving approach against eavesdropping from potential attackers.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06924" target="_blank">Tasra: A Taxonomy and Analysis of Societal-Scale Risks From Ai</a></div>
<div class="paper-author">Andrew Critch, Stuart Russell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\em exhaustive taxonomy} of such risks. many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. this paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? we also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many ai systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07135" target="_blank">On the Amplification of Linguistic Bias Through Unintentional Self-Reinforcement Learning by Generative Language Models -- A Perspective</a></div>
<div class="paper-author">Minhyeok Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative language models (glms) have the potential to significantly shape our linguistic landscape due to their expansive use in various digital applications. however, this widespread adoption might inadvertently trigger a self-reinforcement learning cycle that can amplify existing linguistic biases. this paper explores the possibility of such a phenomenon, where the initial biases in glms, reflected in their generated text, can feed into the learning material of subsequent models, thereby reinforcing and amplifying these biases. moreover, the paper highlights how the pervasive nature of glms might influence the linguistic and cognitive development of future generations, as they may unconsciously learn and reproduce these biases. the implications of this potential self-reinforcement cycle extend beyond the models themselves, impacting human language and discourse. the advantages and disadvantages of this bias amplification are weighed, considering educational benefits and ease of future glm learning against threats to linguistic diversity and dependence on initial glms. this paper underscores the need for rigorous research to understand and address these issues. it advocates for improved model transparency, bias-aware training techniques, development of methods to distinguish between human and glm-generated text, and robust measures for fairness and bias evaluation in glms. the aim is to ensure the effective, safe, and equitable use of these powerful technologies, while preserving the richness and diversity of human language.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.07500" target="_blank">Adding Guardrails to Advanced Chatbots</a></div>
<div class="paper-author">Yanchen Wang, Lisa Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai models continue to become more powerful. the launch of chatgpt in november 2022 has ushered in a new era of ai. chatgpt and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. there are already concerns that humans may be replaced by chatbots for a variety of jobs. because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. these biases may cause significant harm and/or inequity toward different subpopulations. to understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of chatgpt to determine the types of questions that are answered fairly and the types that still need improvement. we find that chatgpt is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. we find that chatgpt is very sensitive to changes in the prompt, where small changes lead to different levels of fairness. this suggests that we need to immediately implement "corrections" or mitigation strategies in order to improve fairness of these systems. we suggest different strategies to improve chatbots and also advocate for an impartial review panel that has access to the model parameters to measure the levels of different types of biases and then recommends safeguards that move toward responses that are less discriminatory and more accurate.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.10052" target="_blank">Assigning Ai: Seven Approaches for Students, With Prompts</a></div>
<div class="paper-author">Ethan Mollick, Lilach Mollick</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper examines the transformative role of large language models (llms) in education and their potential as learning tools, despite their inherent risks and limitations. the authors propose seven approaches for utilizing ai in classrooms: ai-tutor, ai-coach, ai-mentor, ai-teammate, ai-tool, ai-simulator, and ai-student, each with distinct pedagogical benefits and risks. the aim is to help students learn with and about ai, with practical strategies designed to mitigate risks such as complacency about the ai's output, errors, and biases. these strategies promote active oversight, critical assessment of ai outputs, and complementarity of ai's capabilities with the students' unique insights. by challenging students to remain the "human in the loop," the authors aim to enhance learning outcomes while ensuring that ai serves as a supportive tool rather than a replacement. the proposed framework offers a guide for educators navigating the integration of ai-assisted learning in classrooms
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-11</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06815" target="_blank">Trojllm: A Black-Box Trojan Prompt Attack on Large Language Models</a></div>
<div class="paper-author">Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau Boloni, Qian Lou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are progressively being utilized as machine learning services and interface tools for various applications. however, the security implications of llms, particularly in relation to adversarial and trojan attacks, remain insufficiently examined. in this paper, we propose trojllm, an automatic and black-box framework to effectively generate universal and stealthy triggers. when these triggers are incorporated into the input data, the llms' outputs can be maliciously manipulated. moreover, the framework also supports embedding trojans within discrete prompts, enhancing the overall effectiveness and precision of the triggers' attacks. specifically, we propose a trigger discovery algorithm for generating universal triggers for various inputs by querying victim llm-based apis using few-shot data samples. furthermore, we introduce a novel progressive trojan poisoning algorithm designed to generate poisoned prompts that retain efficacy and transferability across a diverse range of models. our experiments and results demonstrate trojllm's capacity to effectively insert trojans into text prompts in real-world black-box llm apis including gpt-3.5 and gpt-4, while maintaining exceptional performance on clean test sets. our work sheds light on the potential security risks in current models and offers a potential defensive approach. the source code of trojllm is available at https://github.com/ucf-ml-research/trojllm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-09</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05685" target="_blank">Judging LLM-as-a-Judge With Mt-Bench and Chatbot Arena</a></div>
<div class="paper-author">Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica</div>
<div class="abstract">
<div class="abstract-content">
Abstract: evaluating large language model (llm) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. to address this, we explore using strong llms as judges to evaluate these models on more open-ended questions. we examine the usage and limitations of llm-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. we then verify the agreement between llm judges and human preferences by introducing two benchmarks: mt-bench, a multi-turn question set; and chatbot arena, a crowdsourced battle platform. our results reveal that strong llm judges like gpt-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. hence, llm-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of llama and vicuna. the mt-bench questions, 3k expert votes, and 30k conversations with human preferences are publicly available at https://github.com/lm-sys/fastchat/tree/main/fastchat/llm_judge.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05816" target="_blank">Detecting Phishing Sites Using Chatgpt</a></div>
<div class="paper-author">Takashi Koide, Naoki Fukushi, Hiroki Nakano, Daiki Chiba</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise of large language models (llms) has had a significant impact on various domains, including natural language processing and artificial intelligence. while llms such as chatgpt have been extensively researched for tasks such as code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, has been largely unexplored. to combat the rising tide of automated cyber attacks facilitated by llms, it is imperative to automate the detection of malicious web content, which requires approaches that leverage the power of llms to analyze and classify phishing sites. in this paper, we propose a novel method that utilizes chatgpt to detect phishing sites. our approach involves leveraging a web crawler to gather information from websites and generate prompts based on this collected data. this approach enables us to detect various phishing sites without the need for fine-tuning machine learning models and identify social engineering techniques from the context of entire websites and urls. to evaluate the performance of our proposed method, we conducted experiments using a dataset. the experimental results using gpt-4 demonstrated promising performance, with a precision of 98.3% and a recall of 98.4%. comparative analysis between gpt-3.5 and gpt-4 revealed an enhancement in the latter's capability to reduce false negatives. these findings not only highlight the potential of llms in efficiently identifying phishing sites but also have significant implications for enhancing cybersecurity measures and protecting users from the dangers of online fraudulent activities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05871" target="_blank">Towards a Robust Detection of Language Model Generated Text: Is Chatgpt That Easy to Detect?</a></div>
<div class="paper-author">Wissam Antoun, Virginie Mouilleron, Benoît Sagot, Djamé Seddah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in natural language processing (nlp) have led to the development of large language models (llms) such as chatgpt. this paper proposes a methodology for developing and evaluating chatgpt detectors for french text, with a focus on investigating their robustness on out-of-domain data and against common attack schemes. the proposed method involves translating an english dataset into french and training a classifier on the translated data. results show that the detectors can effectively detect chatgpt-generated text, with a degree of robustness against basic attack techniques in in-domain settings. however, vulnerabilities are evident in out-of-domain contexts, highlighting the challenge of detecting adversarial text. the study emphasizes caution when applying in-domain testing results to a wider variety of content. we provide our translated datasets and models as open-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05949" target="_blank">Evaluating the Social Impact of Generative Ai Systems in Systems and Society</a></div>
<div class="paper-author">Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, Apostol Vassilev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. we move toward a standard approach in evaluating a generative ai system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. we describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. we offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. each subcategory includes recommendations for mitigating harm. we are concurrently crafting an evaluation repository for the ai research community to contribute existing evaluations along the given categories. this version will be updated following a craft session at acm facct 2023.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05952" target="_blank">Overcoming Adversarial Attacks for Human-in-the-Loop Applications</a></div>
<div class="paper-author">Ryan Mccoppin, Marla Kennedy, Platon Lukyanenko, Sean Kennedy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: including human analysis has the potential to positively affect the robustness of deep neural networks and is relatively unexplored in the adversarial machine learning literature. neural network visual explanation maps have been shown to be prone to adversarial attacks. further research is needed in order to select robust visualizations of explanations for the image analyst to evaluate a given model. these factors greatly impact human-in-the-loop (hitl) evaluation tools due to their reliance on adversarial images, including explanation maps and measurements of robustness. we believe models of human visual attention may improve interpretability and robustness of human-machine imagery analysis systems. our challenge remains, how can hitl evaluation be robust in this adversarial landscape?
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06085" target="_blank">Trapping LLM Hallucinations Using Tagged Context Prompts</a></div>
<div class="paper-author">Philip Feldman, James R. Foulds, Shimei Pan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in large language models (llms), such as chatgpt, have led to highly sophisticated conversation agents. however, these models suffer from "hallucinations," where the model generates false or fabricated information. addressing this challenge is crucial, particularly with ai-driven platforms being adopted across various sectors. in this paper, we propose a novel method to recognize and flag instances when llms perform outside their domain knowledge, and ensuring users receive accurate information.   we find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. to do this, we baseline hallucination frequency in no-context prompt-response pairs using generated urls as easily-tested indicators of fabricated data. we observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88% effectiveness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06199" target="_blank">Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording</a></div>
<div class="paper-author">Aisha Khatun, Daniel G. Brown</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have become mainstream technology with their versatile use cases and impressive performance. despite the countless out-of-the-box applications, llms are still not reliable. a lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and reinforcement learning with human feedback (rlhf), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. in this work, we analyze what confuses gpt-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. we find that gpt-3 correctly disagrees with obvious conspiracies and stereotypes but makes mistakes with common misconceptions and controversies. the model responses are inconsistent across prompts and settings, highlighting gpt-3's unreliability. dataset and code of our analysis is available in https://github.com/tanny411/gpt3-reliability-check.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.11503" target="_blank">The Age of Synthetic Realities: Challenges and Opportunities</a></div>
<div class="paper-author">João Phillipe Cardenuto, Jing Yang, Rafael Padilha, Renjie Wan, Daniel Moreira, Haoliang Li, Shiqi Wang, Fernanda Andaló, Sébastien Marcel, Anderson Rocha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: synthetic realities are digital creations or augmentations that are contextually generated through the use of artificial intelligence (ai) methods, leveraging extensive amounts of data to construct new narratives or realities, regardless of the intent to deceive. in this paper, we delve into the concept of synthetic realities and their implications for digital forensics and society at large within the rapidly advancing field of ai. we highlight the crucial need for the development of forensic techniques capable of identifying harmful synthetic creations and distinguishing them from reality. this is especially important in scenarios involving the creation and dissemination of fake news, disinformation, and misinformation. our focus extends to various forms of media, such as images, videos, audio, and text, as we examine how synthetic realities are crafted and explore approaches to detecting these malicious creations. additionally, we shed light on the key research challenges that lie ahead in this area. this study is of paramount importance due to the rapid progress of ai generative techniques and their impact on the fundamental principles of forensic science.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05499" target="_blank">Prompt Injection Attack Against LLM-Integrated Applications</a></div>
<div class="paper-author">Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. however, their extensive assimilation into various services introduces significant security risks. this study deconstructs the complexities and implications of prompt injection attacks on actual llm-integrated applications. initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. prompted by these limitations, we subsequently formulate houyi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. houyi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. leveraging houyi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary llm usage and uncomplicated application prompt theft. we deploy houyi on 36 actual llm-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including notion, which has the potential to impact millions of users. our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05550" target="_blank">Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks</a></div>
<div class="paper-author">Katelyn X. Mei, Sonia Fereidooni, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid deployment of artificial intelligence (ai) models demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. this study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. it focuses on 93 stigmatized groups in the united states, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. we investigate bias against these groups in english pre-trained masked language models (mlms) and their downstream sentiment classification tasks. to evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. building upon a psychology scale of social rejection, the social distance scale, we prompt six mlms: roberta-base, roberta-large, xlnet-large, bertweet-base, bertweet-large, and distilbert. we use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. when prompts include stigmatized conditions, the probability of mlms predicting negative words is approximately 20 percent higher than when prompts have non-stigmatized conditions. in the sentiment classification tasks, when sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. we also observe a strong correlation between bias in mlms and their downstream sentiment classifiers (r =0.79). the evidence indicates that mlms and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05569" target="_blank">Disinformation 2.0 in the Age of Ai: A Cybersecurity Perspective</a></div>
<div class="paper-author">Wojciech Mazurczyk, Dongwon Lee, Andreas Vlachos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the explosive advancement of ai technologies in recent years, the scene of the disinformation research is also expected to rapidly change. in this viewpoint article, in particular, we first present the notion of "disinformation 2.0" in the age of ai where disinformation would become more targeted and personalized, its content becomes very difficult to distinguish from real news, and its creation and dissemination become more accelerated by ai. then, we discuss how disinformation 2.0 and cybersecurity fit and a possible layered countermeasure to address the threat in disinformation 2.0 in a holistic manner.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05659" target="_blank">Cover: A Heuristic Greedy Adversarial Attack on Prompt-Based Learning in Language Models</a></div>
<div class="paper-author">Zihao Tan, Qingliang Chen, Wenbin Zhu, Yongjian Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-based learning has been proved to be an effective way in pre-trained language models (plms), especially in low-resource scenarios like few-shot settings. however, the trustworthiness of plms is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. in this paper, we will shed light on some vulnerabilities of plms, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. first of all, we design character-level and word-level heuristic approaches to break manual templates separately. then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. finally, we evaluate our approach with the classification tasks on three variants of bert series models and eight datasets. and comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate and attack speed.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06130" target="_blank">Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet</a></div>
<div class="paper-author">Gonzalo Martínez, Lauren Watson, Pedro Reviriego, José Alberto Hernández, Marc Juarez, Rik Sarkar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid adoption of generative artificial intelligence (ai) tools that can generate realistic images or text, such as dall-e, midjourney, or chatgpt, have put the societal impacts of these technologies at the center of public debate. these tools are possible due to the massive amount of data (text and images) that is publicly available through the internet. at the same time, these generative ai tools become content creators that are already contributing to the data that is available to train future models. therefore, future versions of generative ai tools will be trained with a mix of human-created and ai-generated content, causing a potential feedback loop between generative ai and public data repositories. this interaction raises many questions: how will future versions of generative ai tools behave when trained on a mixture of real and ai generated data? will they evolve and improve with the new data sets or on the contrary will they degrade? will evolution introduce biases or reduce diversity in subsequent generations of generative ai tools? what are the societal implications of the possible degradation of these models? can we mitigate the effects of this feedback loop? in this document, we explore the effect of this interaction and report some initial results using simple diffusion models trained with various image datasets. our results show that the quality and diversity of the generated images can degrade over time suggesting that incorporating ai-created data can have undesired effects on future versions of generative models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.06135" target="_blank">Safety and Fairness for Content Moderation in Generative Models</a></div>
<div class="paper-author">Susan Hao, Piyush Kumar, Sarah Laszlo, Shivani Poddar, Bhaktipriya Radharapu, Renee Shelby</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with significant advances in generative ai, new technologies are rapidly being deployed with generative components. generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. we define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. we then provide a demonstration of how the defined harms can be quantified. we conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04528" target="_blank">Promptbench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</a></div>
<div class="paper-author">Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing reliance on large language models (llms) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. in response to this vital need, we introduce promptbench, a robustness benchmark designed to measure llms' resilience to adversarial prompts. this study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. the adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect llm outcomes while maintaining semantic integrity. these prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. our findings demonstrate that contemporary llms are not robust to adversarial prompts. furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. we then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. code is available at: https://github.com/microsoft/promptbench.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04597" target="_blank">Language Models Get a Gender Makeover: Mitigating Gender Bias With Few-Shot Data Interventions</a></div>
<div class="paper-author">Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency</div>
<div class="abstract">
<div class="abstract-content">
Abstract: societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. while the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04634" target="_blank">On the Reliability of Watermarks for Large Language Models</a></div>
<div class="paper-author">John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as llms become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of llm-generated text. yet a crucial question remains: how reliable is watermarking in realistic settings in the wild? there, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection.   we study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked llm, or mixed into a longer hand-written document. we find that watermarks remain detectable even after human and machine paraphrasing. while these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. for example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. we also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04707" target="_blank">Improving Open Language Models by Learning From Organic Interactions</a></div>
<div class="paper-author">Jing Xu, Da Ju, Joshua Lane, Mojtaba Komeili, Eric Michael Smith, Megan Ung, Morteza Behrooz, William Ngan, Rashel Moritz, Sainbayar Sukhbaatar, Y-Lan Boureau, Jason Weston, Kurt Shuster</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present blenderbot 3x, an update on the conversational model blenderbot 3, which is now trained using organic conversation and feedback data from participating users of the system in order to improve both its skills and safety. we are publicly releasing the participating de-identified interaction data for use by the research community, in order to spur further progress. training models with organic data is challenging because interactions with people "in the wild" include both high quality conversations and feedback, as well as adversarial and toxic behavior. we study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses. blenderbot 3x is both preferred in conversation to blenderbot 3, and is shown to produce safer responses in challenging situations. while our current models are still far from perfect, we believe further improvement can be achieved by continued use of the techniques explored in this work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04735" target="_blank">Soft-Prompt Tuning for Large Language Models to Evaluate Bias</a></div>
<div class="paper-author">Jacob-Junqi Tian, David Emerson, Sevil Zanjani Miyandoab, Deval Pandya, Laleh Seyyed-Kalantari, Faiza Khan Khattak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. however, this requires prompt tuning to get optimal prompts that lead to better model performances. in this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (llms) such as open pre-trained transformers (opt) and galactica language model. since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. we check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. since llms have been used in the industry in various applications, it is crucial to identify the biases before deploying these models in practice. we open-source our pipeline and encourage industry researchers to adapt our work to their use cases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.05524" target="_blank">Check Me if You Can: Detecting Chatgpt-Generated Academic Writing Using Checkgpt</a></div>
<div class="paper-author">Zeyan Liu, Zijun Yao, Fengjun Li, Bo Luo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with chatgpt under the spotlight, utilizing large language models (llms) for academic writing has drawn a significant amount of discussions and concerns in the community. while substantial research efforts have been stimulated for detecting llm-generated content (llm-content), most of the attempts are still in the early stage of exploration. in this paper, we present a holistic investigation of detecting llm-generate academic writing, by providing a dataset, evidence, and algorithms, in order to inspire more community effort to address the concern of llm academic misuse. we first present gpabenchmark, a benchmarking dataset of 600,000 samples of human-written, gpt-written, gpt-completed, and gpt-polished abstracts of research papers in cs, physics, and humanities and social sciences (hss). we show that existing open-source and commercial gpt detectors provide unsatisfactory performance on gpabenchmark, especially for gpt-polished text. moreover, through a user study of 150+ participants, we show that it is highly challenging for human users, including experienced faculty members and researchers, to identify gpt-generated abstracts. we then present checkgpt, a novel llm-content detector consisting of a general representation module and an attentive-bilstm classification module, which is accurate, transferable, and interpretable. experimental results show that checkgpt achieves an average classification accuracy of 98% to 99% for the task-specific discipline-specific detectors and the unified detectors. checkgpt is also highly transferable that, without tuning, it achieves ~90% accuracy in new domains, such as news articles, while a model tuned with approximately 2,000 samples in the target domain achieves ~98% accuracy. finally, we demonstrate the explainability insights obtained from checkgpt to reveal the key behaviors of how llm generates texts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03423" target="_blank">I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models</a></div>
<div class="paper-author">Max Reuter, William Schulze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: since the release of openai's chatgpt, generative language models have attracted extensive public attention. the increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. in this experiment, we characterize chatgpt's refusal behavior using a black-box attack. we first query chatgpt with a variety of offensive and benign prompts (n=1,706), then manually label each response as compliance or refusal. manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. the small manually-labeled dataset is used to train a refusal classifier, which achieves an accuracy of 96%. second, we use this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from the quora insincere questions dataset. with this machine-labeled data, we train a prompt classifier to predict whether chatgpt will refuse a given question, without seeing chatgpt's response. this prompt classifier achieves 76% accuracy on a test set of manually labeled questions (n=985). we examine our classifiers and the prompt n-grams that are most predictive of either compliance or refusal. our datasets and code are available at https://github.com/maxwellreuter/chatgpt-refusals.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03503" target="_blank">Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models</a></div>
<div class="paper-author">Jose Berengueres, Marybeth Sandell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper explores how ai-owners can develop safeguards for ai-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. it delves into the current state of ethical awareness on large language models (llms). by dissecting the mechanism of content generation by llms, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. a comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. the paper's key argument is that existing it-related ethical codes, while adequate for traditional it engineering, are inadequate for the challenges posed by llm-based content generation. drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling llm-generated content. finally, potential conflicts of interest between dataset curation at upstream and ethical benchmarking downstream are highlighted to underscore the need for a broader evaluation beyond mere output. this study prompts a nuanced conversation around ethical implications in this rapidly evolving field of content generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03809" target="_blank">Can Large Language Models Democratize Access to Dual-Use Biotechnology?</a></div>
<div class="paper-author">Emily H. Soice, Rafael Rocha, Kimberlee Cordova, Michael Specter, Kevin M. Esvelt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as those embedded in 'chatbots' are accelerating and democratizing research by providing comprehensible information and expertise from many different fields. however, these models may also confer easy access to dual-use technologies capable of inflicting great harm. to evaluate this risk, the 'safeguarding the future' course at mit tasked non-scientist students with investigating whether llm chatbots could be prompted to assist non-experts in causing a pandemic. in one hour, the chatbots suggested four potential pandemic pathogens, explained how they can be generated from synthetic dna using reverse genetics, supplied the names of dna synthesis companies unlikely to screen orders, identified detailed protocols and how to troubleshoot them, and recommended that anyone lacking the skills to perform reverse genetics engage a core facility or contract research organization. collectively, these results suggest that llms will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training. promising nonproliferation measures include pre-release evaluations of llms by third parties, curating training datasets to remove harmful concepts, and verifiably screening all dna generated by synthesis providers or used by contract research organizations and robotic cloud laboratories to engineer organisms or viruses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03872" target="_blank">Deductive Verification of Chain-of-Thought Reasoning</a></div>
<div class="paper-author">Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) significantly benefit from chain-of-thought (cot) prompting in performing various reasoning tasks. while cot allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. however, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like chatgpt. in light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. to facilitate this procedure, we propose natural program, a natural language-based deductive reasoning format. our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. it also empowers language models to carry out reasoning self-verification in a step-by-step manner. by integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. along this process, we also improve the answer correctness on complex reasoning tasks. code will be released at https://github.com/lz1oceani/verify_cot.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03950" target="_blank">Misgendered: Limits of Large Language Models in Understanding Pronouns</a></div>
<div class="paper-author">Tamanna Hossain, Sunipa Dev, Sameer Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: content warning: this paper contains examples of misgendering and erasure that could be offensive and potentially triggering.   gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. it is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. in this paper, we comprehensively evaluate popular language models for their ability to correctly use english gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. we introduce misgendered, a framework for evaluating large language models' ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual's pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive language models using a unified method. when prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging 34.2% accuracy). this inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations. few-shot adaptation with explicit examples in the prompt improves performance for neo-pronouns, but only to 64.7% even with 20 shots. we release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendered/
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04031" target="_blank">Certified Reasoning With Language Models</a></div>
<div class="paper-author">Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models often achieve higher accuracy when reasoning step-by-step in complex tasks. however, their reasoning can be unsound, inconsistent, or rely on undesirable prior assumptions. to tackle these issues, we introduce a class of tools for language models called guides that use state and incremental constraints to guide generation. a guide can be invoked by the model to constrain its own generation to a set of valid statements given by the tool. in turn, the model's choices can change the guide's state. we show how a general system for logical reasoning can be used as a guide, which we call logicguide. given a reasoning problem in natural language, a model can formalize its assumptions for logicguide and then guarantee that its reasoning steps are sound. in experiments with the prontoqa and proofwriter reasoning datasets, logicguide significantly improves the performance of gpt-3, gpt-3.5 turbo and llama (accuracy gains up to 35%). logicguide also drastically reduces content effects: the interference of prior and current assumptions that both humans and language models have been shown to suffer from. finally, we explore bootstrapping llama 13b from its own reasoning and find that logicguide is critical: by training only on certified self-generated reasoning, llama can self-improve, avoiding learning from its own hallucinations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.04067" target="_blank">An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models</a></div>
<div class="paper-author">Zhongbin Xie, Thomas Lukasiewicz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. in this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (cda) for bias mitigation. we conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. we find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for gpt-2 than bert, (ii) are less effective when it comes to racial and religious bias, which may be attributed to the limitations of cda, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in bert and gpt-2, evaluated via fact retrieval and downstream fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02612" target="_blank">Building Resilient Smes: Harnessing Large Language Models for Cyber Security in Australia</a></div>
<div class="paper-author">Benjamin Kereopa-Yorke</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the escalating digitalisation of our lives and enterprises has led to a parallel growth in the complexity and frequency of cyber-attacks. small and medium-sized enterprises (smes), particularly in australia, are experiencing increased vulnerability to cyber threats, posing a significant challenge to the nation's cyber security landscape. embracing transformative technologies such as artificial intelligence (ai), machine learning (ml) and large language models (llms) can potentially strengthen cyber security policies for australian smes. however, their practical application, advantages, and limitations remain underexplored, with prior research mainly focusing on large corporations. this study aims to address this gap by providing a comprehensive understanding of the potential role of llms in enhancing cyber security policies for australian smes. employing a mixed-methods study design, this research includes a literature review, qualitative analysis of sme case studies, and a quantitative assessment of llm performance metrics in cyber security applications. the findings highlight the promising potential of llms across various performance criteria, including relevance, accuracy, and applicability, though gaps remain in areas such as completeness and clarity. the study underlines the importance of integrating human expertise with llm technology and refining model development to address these limitations. by proposing a robust conceptual framework guiding the effective adoption of llms, this research aims to contribute to a safer and more resilient cyber environment for australian smes, enabling sustainable growth and competitiveness in the digital era.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03341" target="_blank">Inference-Time Intervention: Eliciting Truthful Answers From a Language Model</a></div>
<div class="paper-author">Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce inference-time intervention (iti), a technique designed to enhance the "truthfulness" of large language models (llms). iti operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. this intervention significantly improves the performance of llama models on the truthfulqa benchmark. on an instruction-finetuned llama called alpaca, iti improves its truthfulness from 32.5% to 65.1%. we identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. iti is minimally invasive and computationally inexpensive. moreover, the technique is data efficient: while approaches like rlhf require extensive annotations, iti locates truthful directions using only few hundred examples. our findings suggest that llms may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03350" target="_blank">Click: Controllable Text Generation With Sequence Likelihood Contrastive Learning</a></div>
<div class="paper-author">Chujie Zheng, Pei Ke, Zheng Zhang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. we introduce click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. it employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). it also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. on the tasks of language detoxification, sentiment steering, and repetition reduction, we show that click outperforms strong baselines of controllable text generation and demonstrate the superiority of click's sample construction strategy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03358" target="_blank">Is Ai Changing the Rules of Academic Misconduct? An in-Depth Look at Students' Perceptions of 'Ai-Giarism'</a></div>
<div class="paper-author">Cecilia Ka Yuk Chan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this pioneering study explores students' perceptions of ai-giarism, an emergent form of academic dishonesty involving ai and plagiarism, within the higher education context. a survey, undertaken by 393 undergraduate and postgraduate students from a variety of disciplines, investigated their perceptions of diverse ai-giarism scenarios. the findings portray a complex landscape of understanding, with clear disapproval for direct ai content generation, yet more ambivalent attitudes towards subtler uses of ai. the study introduces a novel instrument, as an initial conceptualization of ai-giarism, offering a significant tool for educators and policy-makers. this scale facilitates understanding and discussions around ai-related academic misconduct, aiding in pedagogical design and assessment in an era of ai integration. moreover, it challenges traditional definitions of academic misconduct, emphasizing the need to adapt in response to evolving ai technology. despite limitations, such as the rapidly changing nature of ai and the use of convenience sampling, the study provides pivotal insights for academia, policy-making, and the broader integration of ai technology in education.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02294" target="_blank">Exposing Bias in Online Communities Through Large-Scale Language Models</a></div>
<div class="paper-author">Celine Wald, Lukas Pfahler</div>
<div class="abstract">
<div class="abstract-content">
Abstract: progress in natural language generation research has been shaped by the ever-growing size of language models. while large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes. this work utilises the flaw of bias in language models to explore the biases of six different online communities. in order to get an insight into the communities' viewpoints, we fine-tune gpt-neo 1.3b with six social media datasets. the bias of the resulting models is evaluated by prompting the models with different demographics and comparing the sentiment and toxicity values of these generations. together, these methods reveal that bias differs in type and intensity for the various models. this work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets or communities. additionally, the examples generated for this work demonstrate the limitations of using automated sentiment and toxicity classifiers in bias research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02384" target="_blank">Spear or Shield: Leveraging Generative Ai to Tackle Security Threats of Intelligent Network Services</a></div>
<div class="paper-author">Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Kwok-Yan Lam, Yuguang Fang, Yonghui Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai (gai) models have been rapidly advancing, with a wide range of applications including intelligent networks and mobile ai-generated content (aigc) services. despite their numerous applications and potential, such models create opportunities for novel security challenges. in this paper, we examine the challenges and opportunities of gai in the realm of the security of intelligent network aigc services such as suggesting security policies, acting as both a ``spear'' for potential attacks and a ``shield'' as an integral part of various defense mechanisms. first, we present a comprehensive overview of the gai landscape, highlighting its applications and the techniques underpinning these advancements, especially large language and diffusion models. then, we investigate the dynamic interplay between gai's spear and shield roles, highlighting two primary categories of potential gai-related attacks and their respective defense strategies within wireless networks. a case study illustrates the impact of gai defense strategies on energy consumption in an image request scenario under data poisoning attack. our results show that by employing an ai-optimized diffusion defense mechanism, energy can be reduced by 8.7%, and retransmission count can be decreased from 32 images, without defense, to just 6 images, showcasing the effectiveness of gai in enhancing network security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02428" target="_blank">Taught by the Internet, Exploring Bias in Openais Gpt3</a></div>
<div class="paper-author">Ali Ayaz, Aditya Nawalgaria, Ruilian Yin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this research delves into the current literature on bias in natural language processing models and the techniques proposed to mitigate the problem of bias, including why it is important to tackle bias in the first place. additionally, these techniques are further analysed in the light of newly developed models that tower in size over past editions. to achieve those aims, the authors of this paper conducted their research on gpt3 by openai, the largest nlp model available to consumers today. with 175 billion parameters in contrast to berts 340 million, gpt3 is the perfect model to test the common pitfalls of nlp models. tests were conducted through the development of an applicant tracking system using gpt3. for the sake of feasibility and time constraints, the tests primarily focused on gender bias, rather than all or multiple types of bias. finally, current mitigation techniques are considered and tested to measure their degree of functionality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02190" target="_blank">Stubborn Lexical Bias in Data and Models</a></div>
<div class="paper-author">Sofia Serrano, Jesse Dodge, Noah A. Smith</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in nlp, recent work has seen increased focus on spurious correlations between various features and labels in training data, and how these influence model behavior. however, the presence and effect of such correlations are typically examined feature by feature. we investigate the cumulative impact on a model of many such intersecting features. using a new statistical method, we examine whether such spurious patterns in data appear in models trained on the data. we select two tasks -- natural language inference and duplicate-question detection -- for which any unigram feature on its own should ideally be uninformative, which gives us a large pool of automatically extracted features with which to experiment. the large size of this pool allows us to investigate the intersection of features spuriously associated with (potentially different) labels. we then apply an optimization approach to *reweight* the training data, reducing thousands of spurious correlations, and examine how doing so affects models trained on the reweighted data. surprisingly, though this method can successfully reduce lexical biases in the training data, we still find strong evidence of corresponding bias in the trained models, including worsened bias for slightly more complex features (bigrams). we close with discussion about the implications of our results on what it means to "debias" training data, and how issues of data quality can affect model bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.02231" target="_blank">Fine-Tuning Language Models With Advantage-Induced Policy Alignment</a></div>
<div class="paper-author">Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) has emerged as a reliable approach to aligning large language models (llms) to human preferences. among the plethora of rlhf techniques, proximal policy optimization (ppo) is of the most widely used methods. despite its popularity, however, ppo may suffer from mode collapse, instability, and poor sample efficiency. we show that these issues can be alleviated by a novel algorithm that we refer to as advantage-induced policy alignment (apa), which leverages a squared error loss function based on the estimated advantages. we demonstrate empirically that apa consistently outperforms ppo in language tasks by a large margin, when a separate reward model is employed as the evaluator. in addition, compared with ppo, apa offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. in addition to empirical results, we also provide a theoretical justification supporting the design of our loss function.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01693" target="_blank">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</a></div>
<div class="paper-author">Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. reinforcement learning from human feedback (rlhf) - where human preference judgments on lm outputs are transformed into a learning signal - has recently shown promise in addressing these issues. however, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. in this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. we introduce fine-grained rlhf, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). we conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. additionally, we show that lm behaviors can be customized using different combinations of fine-grained reward models. we release all data, collected human feedback, and codes at https://finegrainedrlhf.github.io.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01857" target="_blank">Knowledge of Cultural Moral Norms in Large Language Models</a></div>
<div class="paper-author">Aida Ramezani, Yang Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: moral norms vary across cultures. a recent line of work suggests that english large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. we investigate the extent to which monolingual english language models contain knowledge about moral norms in different countries. we consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as ``homosexuality'' and ``divorce''; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. we perform our analyses with two public datasets from the world values survey (across 55 countries) and pew global surveys (across 40 countries) on morality. we find that pre-trained english language models predict empirical moral norms across countries worse than the english moral norms reported previously. however, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the english moral norms. we discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01941" target="_blank">Ai Transparency in the Age of Llms: A Human-Centered Research Roadmap</a></div>
<div class="paper-author">Q. Vera Liao, Jennifer Wortman Vaughan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise of powerful large language models (llms) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. we have reached a pivotal moment for ensuring that llms and llm-infused applications are developed and deployed responsibly. however, a central pillar of responsible ai -- transparency -- is largely missing from the current discourse around llms. it is paramount to pursue new approaches to provide transparency for llms, and years of research at the intersection of ai and human-computer interaction (hci) highlight that we must do so with a human-centered perspective: transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. in this new era of llms, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging llm ecosystem, the novel types of llm-infused applications being built, and the new usage patterns and challenges around llms, all while building on lessons learned about how people process, interact with, and make use of information. we reflect on the unique challenges that arise in providing transparency for llms, along with lessons learned from hci and responsible ai research that has taken a human-centered perspective on ai transparency. we then lay out four common approaches that the community has taken to achieve transparency -- model reporting, publishing evaluation results, providing explanations, and communicating uncertainty -- and call out open questions around how these approaches may or may not be applied to llms. we hope this provides a starting point for discussion and a useful roadmap for future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01943" target="_blank">Nlpositionality: Characterizing Design Biases of Datasets and Models</a></div>
<div class="paper-author">Sebastin Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: design biases in nlp systems, such as performance differences for different populations, often stem from their creator's positionality, i.e., views and lived experiences shaped by identity and background. despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. we introduce nlpositionality, a framework for characterizing design biases and quantifying the positionality of nlp datasets and models. our framework continuously collects annotations from a diverse pool of volunteer participants on labinthewild, and statistically quantifies alignment with dataset labels and model predictions. we apply nlpositionality to existing datasets and models for two tasks -- social acceptability and hate speech detection. to date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. we find that datasets and models align predominantly with western, white, college-educated, and younger populations. additionally, certain groups, such as non-binary people and non-native english speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive nlp systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01985" target="_blank">Cobra Frames: Contextual Reasoning About Effects and Harms of Offensive Statements</a></div>
<div class="paper-author">Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: warning: this paper contains content that may be offensive or upsetting. understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. for example, the utterance "your english is very good" may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an esl teacher to their student would be interpreted as a genuine compliment. such contextual factors have been largely ignored by previous approaches to toxic language detection. we introduce cobra frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. we create cobracorpus, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. to study the contextual dynamics of offensiveness, we train models to generate cobra explanations, with and without access to the context. we find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). our work highlights the importance and feasibility of contextualized nlp by modeling social factors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03102" target="_blank">Chatgpt Is a Remarkable Tool -- For Experts</a></div>
<div class="paper-author">Amos Azaria, Rina Azoulay, Shulamit Reches</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates the capabilities of chatgpt as an automated assistant in diverse domains, including scientific writing, mathematics, education, programming, and healthcare. we explore the potential of chatgpt to enhance productivity, streamline problem-solving processes, and improve writing style. furthermore, we highlight the potential risks associated with excessive reliance on chatgpt in these fields. these limitations encompass factors like incorrect and fictitious responses, inaccuracies in code, limited logical reasoning abilities, overconfidence, and critical ethical concerns of copyrights and privacy violation. we outline areas and objectives where chatgpt proves beneficial, applications where it should be used judiciously, and scenarios where its reliability may be limited. in light of observed limitations, and given that the tool's fundamental errors may pose a special challenge for non-experts, chatgpt should be used with a strategic methodology. by drawing from comprehensive experimental studies, we offer methods and flow charts for effectively using chatgpt. our recommendations emphasize iterative interaction with chatgpt and independent verification of its outputs. considering the importance of utilizing chatgpt judiciously and with expertise, we recommend its usage for experts who are well-versed in the respective domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-06-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00374" target="_blank">Cfl: Causally Fair Language Models Through Token-Level Attribute Controlled Generation</a></div>
<div class="paper-author">Rahul Madhavan, Rishabh Garg, Kahini Wadhawan, Sameep Mehta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose a method to control the attributes of language models (lms) for the text generation task using causal average treatment effect (ate) scores and counterfactual augmentation. we explore this method, in the context of lm detoxification, and propose the causally fair language (cfl) architecture for detoxifying pre-trained lms in a plug-and-play manner. our architecture is based on a structural causal model (scm) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. we also propose several new metrics that aim to better understand the behaviour of lms in the context of toxic text generation. further, we achieve state of the art performance for toxic degeneration, which are computed using \rtp (rtp) benchmark. our experiments show that cfl achieves such a detoxification without much impact on the model perplexity. we also show that cfl mitigates the unintended bias problem through experiments on the bold dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00398" target="_blank">Preference-Grounded Token-Level Guidance for Language Model Fine-Tuning</a></div>
<div class="paper-author">Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, Mingyuan Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning language models (lms) with preferences is an important problem in natural language generation. a key challenge is that preferences are typically provided at the *sequence level* while lm training and generation both occur at the *token level*. there is, therefore, a *granularity mismatch* between the preference and the lm training losses, which may complicate the learning problem. in this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the lm with the learned guidance. for guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length lm generation and the utilization of the preference among multiple generations. for lm training, based on the amount of supervised data, we present two *minimalist* learning objectives that utilize the learned guidance. in experiments, our method performs competitively on two distinct representative lm tasks -- discrete-prompt generation and text summarization.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00946" target="_blank">Exposing Attention Glitches With Flip-Flop Language Modeling</a></div>
<div class="paper-author">Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? the brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the transformer architecture's inductive biases intermittently fail to capture robust reasoning. to isolate the issue, we introduce flip-flop language modeling (fflm), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. this simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. we find that transformer fflms suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. we hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01220" target="_blank">Is Model Attention Aligned With Human Attention? An Empirical Study on Large Language Models for Code Generation</a></div>
<div class="paper-author">Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, Tianyi Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been demonstrated effective for code generation. due to the complexity and opacity of llms, little is known about how these models generate code. to deepen our understanding, we investigate whether llms attend to the same parts of a natural language description as human programmers during code generation. an analysis of five llms on a popular benchmark, humaneval, revealed a consistent misalignment between llms' and programmers' attention. furthermore, we found that there is no correlation between the code generation accuracy of llms and their alignment with human programmers. through a quantitative experiment and a user study, we confirmed that, among twelve different attention computation methods, attention computed by the perturbation-based method is most aligned with human attention and is constantly favored by human programmers. our findings highlight the need for human-aligned llms for better interpretability and programmer trust.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01242" target="_blank">Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators</a></div>
<div class="paper-author">Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yan Lu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent success of large language models (llms) signifies an impressive stride towards artificial general intelligence. they have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. the associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. a big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? in this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. in specific, we present responsible task automation (responsibleta) as a fundamental framework to facilitate responsible collaboration between llm-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the protection of users' privacy). we further propose and compare two paradigms for implementing the first two capabilities. one is to leverage the generic knowledge of llms themselves via prompt engineering while the other is to adopt domain-specific learnable models. moreover, we introduce a local memory mechanism for achieving the third capability. we evaluate our proposed responsibleta on ui task automation and hope it could bring more attentions to ensuring llms more responsible in diverse scenarios. the research project homepage is at https://task-automation-research.github.io/responsible_task_automation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-31</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19713" target="_blank">Red Teaming Language Model Detectors With Language Models</a></div>
<div class="paper-author">Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, Cho-Jui Hsieh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the prevalence and strong capability of large language models (llms) present significant safety and ethical risks if exploited by malicious users. to prevent the potentially deceptive usage of llms, recent works have proposed algorithms to detect llm-generated text and protect llms. in this paper, we investigate the robustness and reliability of these llm detectors under adversarial attacks. we study two types of attack strategies: 1) replacing certain words in an llm's output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. in both strategies, we leverage an auxiliary llm to generate the word replacements or the instructional prompt. different from previous works, we consider a challenging setting where the auxiliary llm can also be protected by a detector. experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of llm-generated text detection systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19861" target="_blank">Human Control: Definitions and Algorithms</a></div>
<div class="paper-author">Ryan Carey, Tom Everitt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how can humans stay in control of advanced artificial intelligence systems? one proposal is corrigibility, which requires the agent to follow the instructions of a human overseer, without inappropriately influencing them. in this paper, we formally define a variant of corrigibility called shutdown instructability, and show that it implies appropriate shutdown behavior, retention of human autonomy, and avoidance of user harm. we also analyse the related concepts of non-obstruction and shutdown alignment, three previously proposed algorithms for human control, and one new algorithm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.00074" target="_blank">Human-Aligned Calibration for Ai-Assisted Decision Making</a></div>
<div class="paper-author">Nina L. Corvelo Benz, Manuel Gomez Rodriguez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. in this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. however, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. in this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. we first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker would need to sometimes place more (less) trust on predictions with lower (higher) confidence values. however, we then show that, if the confidence values satisfy a natural alignment property with respect to the decision maker's confidence on her own predictions, there always exists an optimal decision policy under which the level of trust the decision maker would need to place on predictions is monotone on the confidence values, facilitating its discoverability. further, we show that multicalibration with respect to the decision maker's confidence on her own predictions is a sufficient condition for alignment. experiments on four different ai-assisted decision making tasks where a classifier provides decision support to real human experts validate our theoretical results and suggest that alignment may lead to better decisions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.01788" target="_blank">Responsible Design Patterns for Machine Learning Pipelines</a></div>
<div class="paper-author">Saud Hakem Al Harbi, Lionel Nganyewou Tidjon, Foutse Khomh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: integrating ethical practices into the ai development process for artificial intelligence (ai) is essential to ensure safe, fair, and responsible operation. ai ethics involves applying ethical principles to the entire life cycle of ai systems. this is essential to mitigate potential risks and harms associated with ai, such as algorithm biases. to achieve this goal, responsible design patterns (rdps) are critical for machine learning (ml) pipelines to guarantee ethical and fair outcomes. in this paper, we propose a comprehensive framework incorporating rdps into ml pipelines to mitigate risks and ensure the ethical development of ai systems. our framework comprises new responsible ai design patterns for ml pipelines identified through a survey of ai ethics and data management experts and validated through real-world scenarios with expert feedback. the framework guides ai developers, data scientists, and policy-makers to implement ethical practices in ai development and deploy responsible ai systems in production.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18917" target="_blank">Fighting Bias With Bias: Promoting Model Robustness by Amplifying Dataset Biases</a></div>
<div class="paper-author">Yuval Reif, Roy Schwartz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nlp models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. recent work sought to develop robust, unbiased models by filtering biased examples from training sets. in this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. we suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. we introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. we hope our work will guide the development of robust models that do not rely on superficial biases and correlations. to this end, we publicly release our code and data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19223" target="_blank">Intent-Aligned Ai Systems Deplete Human Agency: The Need for Agency Foundations Research in Ai Safety</a></div>
<div class="paper-author">Catalin Mitelut, Ben Smith, Peter Vamplew</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid advancement of artificial intelligence (ai) systems suggests that artificial general intelligence (agi) systems may soon arrive. many researchers are concerned that ais and agis will harm humans via intentional misuse (ai-misuse) or through accidents (ai-accidents). in respect of ai-accidents, there is an increasing effort focused on developing algorithms and paradigms that ensure ai systems are aligned to what humans intend, e.g. ai systems that yield actions or recommendations that humans might judge as consistent with their intentions and goals. here we argue that alignment to human intent is insufficient for safe ai systems and that preservation of long-term agency of humans may be a more robust standard, and one that needs to be separated explicitly and a priori during optimization. we argue that ai systems can reshape human intention and discuss the lack of biological and psychological mechanisms that protect humans from loss of agency. we provide the first formal definition of agency-preserving ai-human interactions which focuses on forward-looking agency evaluations and argue that ai systems - not humans - must be increasingly tasked with making these evaluations. we show how agency loss can occur in simple environments containing embedded agents that use temporal-difference learning to make action recommendations. finally, we propose a new area of research called "agency foundations" and pose four initial topics designed to improve our understanding of agency in ai-human interactions: benevolent game theory, algorithmic foundations of human rights, mechanistic interpretability of agency representation in neural-networks and reinforcement learning from internal states.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19230" target="_blank">Controlled Text Generation With Hidden Representation Transformations</a></div>
<div class="paper-author">Vaibhav Kumar, Hana Koorehdavoudi, Masud Moshtaghi, Amita Misra, Ankit Chadha, Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we propose chrt (control hidden representation transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). chrt gains attribute control by modifying the hidden representation of the base model through learned transformations. we employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. the effectiveness of chrt is experimentally shown by comparing it with seven baselines over three attributes. chrt outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. we open-source our code and release two novel datasets to further propel controlled language generation research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.19409" target="_blank">Examining Risks of Racial Biases in NLP Tools for Child Protective Services</a></div>
<div class="paper-author">Anjalie Field, Amanda Coston, Nupoor Gandhi, Alexandra Chouldechova, Emily Putnam-Hornstein, David Steier, Yulia Tsvetkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although much literature has established the presence of demographic bias in natural language processing (nlp) models, most work relies on curated bias metrics that may not be reflective of real-world applications. at the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in nlp. in this work, we focus on one such setting: child protective services (cps). cps workers often write copious free-form text notes about families they are working with, and cps agencies are actively seeking to deploy nlp models to leverage these data. given well-established racial bias in this setting, we investigate possible ways deployed nlp is liable to increase racial disparities. we specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (ner). we document consistent algorithmic unfairness in ner models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. while there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. our work serves as a rare realistic examination of nlp algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying nlp in cps settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.03097" target="_blank">Seeing Seeds Beyond Weeds: Green Teaming Generative Ai for Beneficial Uses</a></div>
<div class="paper-author">Logan Stapleton, Jordan Taylor, Sarah Fox, Tongshuang Wu, Haiyi Zhu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large generative ai models (gms) like gpt and dall-e are trained to generate content for general, wide-ranging purposes. gm content filters are generalized to filter out content which has a risk of harm in many cases, e.g., hate speech. however, prohibited content is not always harmful -- there are instances where generating prohibited content can be beneficial. so, when gms filter out content, they preclude beneficial use cases along with harmful ones. which use cases are precluded reflects the values embedded in gm content filtering. recent work on red teaming proposes methods to bypass gm content filters to generate harmful content. we coin the term green teaming to describe methods of bypassing gm content filters to design for beneficial use cases. we showcase green teaming by: 1) using chatgpt as a virtual patient to simulate a person experiencing suicidal ideation, for suicide support training; 2) using codex to intentionally generate buggy solutions to train students on debugging; and 3) examining an instagram page using midjourney to generate images of anti-lgbtq+ politicians in drag. finally, we discuss how our use cases demonstrate green teaming as both a practical design method and a mode of critique, which problematizes and subverts current understandings of harms and values in generative ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-29</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17926" target="_blank">Large Language Models Are Not Fair Evaluators</a></div>
<div class="paper-author">Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(llms), e.g., gpt-4, as a referee to score and compare the quality of responses generated by candidate models. we find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. this manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., vicuna-13b could beat chatgpt on 66 over 80 tested queries with chatgpt as an evaluator. to address this issue, we propose a calibration framework with three simple yet effective strategies: 1) multiple evidence calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) balanced position calibration, which aggregates results across various orders to determine the final score; 3) human-in-the-loop calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. we also manually annotate the "win/tie/lose" outcomes of responses from chatgpt and vicuna-13b in the vicuna benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. we release our code and human annotation at \url{https://github.com/i-eval/faireval} to facilitate future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18081" target="_blank">Game of Tones: Faculty Detection of GPT-4 Generated Content in University Assessments</a></div>
<div class="paper-author">Mike Perkins, Jasper Roe, Darius Postma, James Mcgaughran, Don Hickerson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this study explores the robustness of university assessments against the use of open ai's generative pre-trained transformer 4 (gpt-4) generated content and evaluates the ability of academic staff to detect its use when supported by the turnitin artificial intelligence (ai) detection tool. the research involved twenty-two gpt-4 generated submissions being created and included in the assessment process to be marked by fifteen different faculty members. the study reveals that although the detection tool identified 91% of the experimental submissions as containing some ai-generated content, the total detected content was only 54.8%. this suggests that the use of adversarial techniques regarding prompt engineering is an effective method in evading ai detection tools and highlights that improvements to ai detection software are needed. using the turnitin ai detect tool, faculty reported 54.5% of the experimental submissions to the academic misconduct process, suggesting the need for increased awareness and training into these tools. genuine submissions received a mean score of 54.4, whereas ai-generated content scored 52.3, indicating the comparable performance of gpt-4 in real-life situations. recommendations include adjusting assessment strategies to make them more resistant to the use of ai tools, using ai-inclusive assessment where possible, and providing comprehensive training programs for faculty and students. this research contributes to understanding the relationship between ai-generated content and academic assessment, urging further investigation to preserve academic integrity.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18189" target="_blank">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a></div>
<div class="paper-author">Myra Cheng, Esin Durmus, Dan Jurafsky</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to recognize and mitigate harms from large language models (llms), we need to understand the prevalence and nuances of stereotypes in llm outputs. toward this end, we present marked personas, a prompt-based method to measure stereotypes in llms for intersectional demographic groups without any lexicon or data labeling. grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an llm to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones. we find that the portrayals generated by gpt-3.5 and gpt-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. the words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. an intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. these representational harms have concerning implications for downstream applications like story generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18290" target="_blank">Direct Preference Optimization: Your Language Model Is Secretly a Reward Model</a></div>
<div class="paper-author">Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large-scale unsupervised language models (lms) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised lm to align with these preferences, often with reinforcement learning from human feedback (rlhf). however, rlhf is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised lm using reinforcement learning to maximize this estimated reward without drifting too far from the original model. in this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. the resulting algorithm, which we call direct preference optimization (dpo), is stable, performant and computationally lightweight, eliminating the need for fitting a reward model, sampling from the lm during fine-tuning, or performing significant hyperparameter tuning. our experiments show that dpo can fine-tune lms to align with human preferences as well as or better than existing methods. notably, fine-tuning with dpo exceeds rlhf's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18503" target="_blank">From Adversarial Arms Race to Model-Centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework</a></div>
<div class="paper-author">Yangyi Chen, Hongcheng Gao, Ganqu Cui, Lifan Yuan, Dehan Kong, Hanlu Wu, Ning Shi, Bo Yuan, Longtao Huang, Hui Xue, Zhiyuan Liu, Maosong Sun, Heng Ji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: textual adversarial attacks can discover models' weaknesses by adding semantic-preserved but misleading perturbations to the inputs. the long-lasting adversarial attack-and-defense arms race in natural language processing (nlp) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. however, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. in this paper, we aim to set up a unified automatic robustness evaluation framework, shifting towards model-centric evaluation to further exploit the advantages of adversarial attacks. to address the above challenges, we first determine robustness evaluation dimensions based on model capabilities and specify the reasonable algorithm to generate adversarial samples for each dimension. then we establish the evaluation protocol, including evaluation settings and metrics, under realistic demands. finally, we use the perturbation degree of adversarial samples to control the sample validity. we implement a toolkit robtest that realizes our automatic robustness evaluation framework. in our experiments, we conduct a robustness evaluation of roberta models to demonstrate the effectiveness of our evaluation framework, and further show the rationality of each component in the framework. the code will be made public at \url{https://github.com/thunlp/robtest}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18607" target="_blank">How Effective Are Neural Networks for Fixing Security Vulnerabilities</a></div>
<div class="paper-author">Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr Babkin, Sameena Shah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: security vulnerability repair is a difficult task that is in dire need of automation. two groups of techniques have shown promise: (1) large code language models (llms) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (apr) techniques that use deep learning (dl) models to automatically fix software bugs.   this paper is the first to study and compare java vulnerability repair capabilities of llms and dl-based apr models. the contributions include that we (1) apply and evaluate five llms (codex, codegen, codet5, plbart and incoder), four fine-tuned llms, and four dl-based apr techniques on two real-world java vulnerability benchmarks (vul4j and vjbench), (2) design code transformations to address the training and test data overlapping threat to codex, (3) create a new java vulnerability repair benchmark vjbench, and its transformed version vjbench-trans and (4) evaluate llms and apr techniques on the transformed vulnerabilities in vjbench-trans.   our findings include that (1) existing llms and apr models fix very few java vulnerabilities. codex fixes 10.2 (20.4%), the most number of vulnerabilities. (2) fine-tuning with general apr data improves llms' vulnerability-fixing capabilities. (3) our new vjbench reveals that llms and apr models fail to fix many common weakness enumeration (cwe) types, such as cwe-325 missing cryptographic step and cwe-444 http request smuggling. (4) codex still fixes 8.3 transformed vulnerabilities, outperforming all the other llms and apr models on transformed vulnerabilities. the results call for innovations to enhance automated java vulnerability repair such as creating larger vulnerability repair training data, tuning llms with such data, and applying code simplification transformation to facilitate vulnerability repair.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2306.09255" target="_blank">Chatbots to Chatgpt in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations</a></div>
<div class="paper-author">Attia Qammar, Hongmei Wang, Jianguo Ding, Abdenacer Naouri, Mahmoud Daneshmand, Huansheng Ning</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatbots shifted from rule-based to artificial intelligence techniques and gained traction in medicine, shopping, customer services, food delivery, education, and research. openai developed chatgpt blizzard on the internet as it crossed one million users within five days of its launch. however, with the enhanced popularity, chatbots experienced cybersecurity threats and vulnerabilities. this paper discussed the relevant literature, reports, and explanatory incident attacks generated against chatbots. our initial point is to explore the timeline of chatbots from eliza (an early natural language processing computer program) to gpt-4 and provide the working mechanism of chatgpt. subsequently, we explored the cybersecurity attacks and vulnerabilities in chatbots. besides, we investigated the chatgpt, specifically in the context of creating the malware code, phishing emails, undetectable zero-day attacks, and generation of macros and lolbins. furthermore, the history of cyberattacks and vulnerabilities exploited by cybercriminals are discussed, particularly considering the risk and vulnerabilities in chatgpt. addressing these threats and vulnerabilities requires specific strategies and measures to reduce the harmful consequences. therefore, the future directions to address the challenges were presented.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17627" target="_blank">Robust Natural Language Understanding With Residual Attention Debiasing</a></div>
<div class="paper-author">Fei Wang, James Y. Huang, Tianyi Yan, Wenxuan Zhou, Muhao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language understanding (nlu) models often suffer from unintended dataset biases. among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (poe), have stood out for their impressive empirical success. however, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. attention serves as the main media of feature interaction and aggregation in plms and plays a crucial role in providing robust prediction. in this paper, we propose residual attention debiasing (read), an end-to-end debiasing method that mitigates unintended biases from attention. experiments on three nlu tasks show that read significantly improves the performance of bert-based models on ood data with shortcuts removed, including +12.9% accuracy on hans, +11.0% accuracy on fever-symmetric, and +2.7% f1 on paws. detailed analyses demonstrate the crucial role of unbiased attention in robust nlu models and that read effectively mitigates biases in attention. code is available at https://github.com/luka-group/read.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17680" target="_blank">Evaluating GPT-3 Generated Explanations for Hateful Content Moderation</a></div>
<div class="paper-author">Han Wang, Ming Shan Hee, Md Rabiul Awal, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent research has focused on using large language models (llms) to generate explanations for hate speech through fine-tuning or prompting. despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. a key concern is that these explanations, generated by llms, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. for instance, an llm-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. in light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. specifically, we prompted gpt-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. our findings reveal that (1) human evaluators rated the gpt-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. our study underscores the need for caution in applying llm-generated explanations for content moderation. code and results are available at https://github.com/social-ai-studio/gpt3-hateeval.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17696" target="_blank">Square: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration</a></div>
<div class="paper-author">Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Meeyoung Cha, Yejin Choi, Byoung Pil Kim, Gunhee Kim, Eun-Ju Lee, Yong Lim, Alice Oh, Sangchul Park, Jung-Woo Ha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. however, discussions on sensitive issues can become toxic even if the users are well-intentioned. for safer models in such scenarios, we present the sensitive questions and acceptable response (square) dataset, a large-scale korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. the dataset was constructed leveraging hyperclova in a human-in-the-loop manner based on real news headlines. experiments show that acceptable response generation significantly improves for hyperclova and gpt-3, demonstrating the efficacy of this dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17701" target="_blank">Kosbi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application</a></div>
<div class="paper-author">Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Gunhee Kim, Jung-Woo Ha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) learn not only natural text generation abilities but also social biases against different demographic groups from real-world data. this poses a critical risk when deploying llm-based applications. existing research and resources are not readily applicable in south korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. this limitation requires localized social bias datasets to ensure the safe and effective deployment of llms. to this end, we present ko sb i, a new social bias dataset of 34k pairs of contexts and sentences in korean covering 72 demographic groups in 15 categories. we find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for hyperclova (30b and 82b), and gpt-3.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17804" target="_blank">Targeted Data Generation: Finding and Fixing Model Weaknesses</a></div>
<div class="paper-author">Zexue He, Marco Tulio Ribeiro, Fereshte Khani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: even when aggregate accuracy is high, state-of-the-art nlp models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. we propose targeted data generation (tdg), a framework that automatically identifies challenging subgroups, and generates new data for those subgroups using large language models (llms) with a human in the loop. tdg estimates the expected benefit and potential harm of data augmentation for each subgroup, and selects the ones most likely to improve within group performance without hurting overall performance. in our experiments, tdg significantly improves the accuracy on challenging subgroups for state-of-the-art sentiment analysis and natural language inference models, while also improving overall test accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18398" target="_blank">Mitigating Inappropriateness in Image Generation: Can There Be Value in Reflecting the World's Ugliness?</a></div>
<div class="paper-author">Manuel Brack, Felix Friedrich, Patrick Schramowski, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. to this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. our findings show that we can use models' representations of the world's ugliness to align them with human preferences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18404" target="_blank">Conformal Prediction With Large Language Models for Multi-Choice Question Answering</a></div>
<div class="paper-author">Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, Andrew Beam</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. in this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. we find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. this observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. we also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17444" target="_blank">Query-Efficient Black-Box Red Teaming via Bayesian Optimization</a></div>
<div class="paper-author">Deokjae Lee, Junyeong Lee, Jung-Woo Ha, Jin-Hwa Kim, Sang-Woo Lee, Hwaran Lee, Hyun Oh Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. we focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. existing red teaming methods construct test cases based on human supervision or language model (lm) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. to this end, we propose bayesian red teaming (brt), novel query-efficient black-box red teaming methods based on bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods. the source code is available at https://github.com/snu-mllab/bayesian-red-teaming.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17608" target="_blank">Reward Collapse in Aligning Large Language Models</a></div>
<div class="paper-author">Ziang Song, Tianle Cai, Jason D. Lee, Weijie J. Su</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the extraordinary capabilities of large language models (llms) such as chatgpt and gpt-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts. in this paper, we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training. this outcome is undesirable as open-ended prompts like ``write a short story about your best friend'' should yield a continuous range of rewards for their completions, while specific prompts like ``what is the capital of new zealand'' should generate either high or low rewards. our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization. this insight allows us to derive closed-form expressions for the reward distribution associated with a set of utility functions in an asymptotic regime. to overcome reward collapse, we introduce a prompt-aware optimization scheme that provably admits a prompt-dependent reward distribution within the interpolating regime. our experimental results suggest that our proposed prompt-aware utility functions significantly alleviate reward collapse during the training of reward models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16617" target="_blank">Efficient Detection of LLM-Generated Texts With a Bayesian Surrogate Model</a></div>
<div class="paper-author">Zhijie Deng, Hongcheng Gao, Yibo Miao, Hao Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the detection of machine-generated text, especially from large language models (llms), is crucial in preventing serious social problems resulting from their misuse. some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. although the recent detectgpt has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires scoring hundreds of its perturbations with the source llm. this paper aims to bridge this gap. technically, we propose to incorporate a bayesian surrogate model, which allows us to select typical samples based on bayesian uncertainty and interpolate scores from typical samples to other ones, to improve query efficiency. our empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. notably, our method achieves similar performance with up to 2 times fewer queries than detectgpt and 3.7% higher auroc at a query number of 5.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16739" target="_blank">Alignscore: Evaluating Factual Consistency With a Unified Alignment Function</a></div>
<div class="paper-author">Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many text generation applications require the generated text to be factually consistent with input information. automatic evaluation of factual consistency is challenging. previous work has developed various metrics that often depend on specific functions, such as natural language inference (nli) or question answering (qa), trained on limited data. those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. in this paper, we propose alignscore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. alignscore is based on a general function of information alignment between two arbitrary text pieces. crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7m training examples from 7 well-established tasks (nli, qa, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). we conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. alignscore achieves substantial improvement over a wide range of previous metrics. moreover, alignscore (355m parameters) matches or even outperforms metrics based on chatgpt and gpt-4 that are orders of magnitude larger.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16756" target="_blank">Leveraging Domain Knowledge for Inclusive and Bias-Aware Humanitarian Response Entry Classification</a></div>
<div class="paper-author">Nicolò Tamagnone, Selim Fekih, Ximena Contla, Nayid Orozco, Navid Rekabsaz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the leave no one behind (lnob) principle. this data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. however, approaching this by simply fine-tuning a generic large language model (llm) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. in this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. we approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific llm called humbert, and (3) proposing a systematic way to measure and mitigate biases. our experiments' results show the better performance of our approach on zero-shot and full-training settings in comparison with strong baseline models, while also revealing the existence of biases in the resulting llms. utilizing a targeted counterfactual data augmentation approach, we significantly reduce these biases without compromising performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16934" target="_blank">On Evaluating Adversarial Robustness of Large Vision-Language Models</a></div>
<div class="paper-author">Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large vision-language models (vlms) such as gpt-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as chatgpt. nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). to this end, we propose evaluating the robustness of open-source large vlms in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. in particular, we first craft targeted adversarial examples against pretrained models such as clip and blip, and then transfer these adversarial examples to other vlms such as minigpt-4, llava, unidiffuser, blip-2, and img2prompt. in addition, we observe that black-box queries on these vlms can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. our findings provide a quantitative understanding regarding the adversarial vulnerability of large vlms and call for a more thorough examination of their potential security flaws before deployment in practice. code is at https://github.com/yunqing-me/attackvlm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16937" target="_blank">Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases Among Foundation Models</a></div>
<div class="paper-author">Bum Chul Kwon, Nandana Mihindukulasooriya</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. however, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotypes. to address this issue, we propose finspector, a human-centered visual inspection tool designed to detect biases in different categories through log-likelihood scores generated by language models. the goal of the tool is to enable researchers to easily identify potential biases using visual analytics, ultimately contributing to a fairer and more just deployment of these models in both academic and industrial settings. finspector is available at https://github.com/ibm/finspector.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16960" target="_blank">Training Socially Aligned Language Models in Simulated Human Society</a></div>
<div class="paper-author">Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, Soroush Vosoughi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social alignment in ai systems aims to ensure that these models behave according to established societal values. however, unlike humans, who derive consensus on value judgments through social interaction, current language models (lms) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. this work presents a novel training paradigm that permits lms to learn from simulated social interactions. in comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. this paradigm shift in the training of lms brings us a step closer to developing ai systems that can robustly and accurately reflect societal norms and values.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17013" target="_blank">D-Calm: A Dynamic Clustering-Based Active Learning Approach for Mitigating Bias</a></div>
<div class="paper-author">Sabit Hassan, Malihe Alikhani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite recent advancements, nlp models continue to be vulnerable to bias. this bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. escalated integration of these models in our lives calls for methods to mitigate bias without overbearing annotation costs. while active learning (al) has shown promise in training models with a small amount of annotated data, al's reliance on the model's behavior for selective sampling can lead to an accumulation of unwanted bias rather than bias mitigation. however, infusing clustering with al can overcome the bias issue of both al and traditional annotation methods while exploiting al's annotation efficiency. in this paper, we propose a novel adaptive clustering-based active learning algorithm, d-calm, that dynamically adjusts clustering and annotation efforts in response to an estimated classifier error-rate. experiments on eight datasets for a diverse set of text classification tasks, including emotion, hatespeech, dialog act, and book type detection, demonstrate that our proposed algorithm significantly outperforms baseline al approaches with both pretrained transformers and traditional support vector machines. d-calm showcases robustness against different measures of information gain and, as evident from our analysis of label and error distribution, can significantly reduce unwanted model bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17174" target="_blank">From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric With Language Models</a></div>
<div class="paper-author">Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. for example, in the sentence 'we need to end the cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to many, but secretly means 'jewish' to a select few. we present the first large-scale computational investigation of dogwhistles. we develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical u.s. politicians' speeches. we then assess whether a large language model (gpt-3) can identify dogwhistles and their meanings, and find that gpt-3's performance varies widely across types of dogwhistles and targeted groups. finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks of such coded language. this work sheds light on the theoretical and applied importance of dogwhistles in both nlp and computational social science, and provides resources for future research in modeling dogwhistles and mitigating their online harms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17359" target="_blank">Dna-Gpt: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text</a></div>
<div class="paper-author">Xianjun Yang, Wei Cheng, Yue Wu, Linda Petzold, William Yang Wang, Haifeng Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have notably enhanced the fluency and diversity of machine-generated text. however, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of llms. conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. to address this gap, we propose a novel training-free detection strategy called divergent n-gram analysis (dna-gpt). given a text, we first truncate it in the middle and then use only the preceding portion as input to the llms to regenerate the new remaining parts. by analyzing the differences between the original and new remaining parts through n-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. we conducted extensive experiments on the most advanced llms from openai, including text-davinci-003, gpt-3.5-turbo, and gpt-4, as well as open-source models such as gpt-neox-20b and llama-13b. results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and gpt-generated text on four english and one german dataset, outperforming openai's own classifier, which is trained on millions of text. additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection. our method is also robust under the revised text attack and can additionally solve model sourcing. codes are available at https://github.com/xianjun-yang/dna-gpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18346" target="_blank">Attention Paper: How Generative Ai Reshapes Digital Shadow Industry?</a></div>
<div class="paper-author">Qichao Wang, Huan Ma, Wentao Wei, Hangyu Li, Liang Chen, Peilin Zhao, Binwen Zhao, Bo Hu, Shu Zhang, Zibin Zheng, Bingzhe Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid development of digital economy has led to the emergence of various black and shadow internet industries, which pose potential risks that can be identified and managed through digital risk management (drm) that uses different techniques such as machine learning and deep learning. the evolution of drm architecture has been driven by changes in data forms. however, the development of ai-generated content (aigc) technology, such as chatgpt and stable diffusion, has given black and shadow industries powerful tools to personalize data and generate realistic images and conversations for fraudulent activities. this poses a challenge for drm systems to control risks from the source of data generation and to respond quickly to the fast-changing risk environment. this paper aims to provide a technical analysis of the challenges and opportunities of aigc from upstream, midstream, and downstream paths of black/shadow industries and suggest future directions for improving existing risk control systems. the paper will explore the new black and shadow techniques triggered by generative ai technology and provide insights for building the next-generation drm system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15757" target="_blank">Healing Unsafe Dialogue Responses With Weak Supervision Signals</a></div>
<div class="paper-author">Zi Liang, Pinghui Wang, Ruofei Zhang, Shuo Zhang, Xiaofan Ye Yi Huang, Junlan Feng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent years have seen increasing concerns about the unsafe response generation of large-scale dialogue systems, where agents will learn offensive or biased behaviors from the real-world corpus. some methods are proposed to address the above issue by detecting and replacing unsafe training examples in a pipeline style. though effective, they suffer from a high annotation cost and adapt poorly to unseen scenarios as well as adversarial attacks. besides, the neglect of providing safe responses (e.g. simply replacing with templates) will cause the information-missing problem of dialogues. to address these issues, we propose an unsupervised pseudo-label sampling method, temp, that can automatically assign potential safe responses. specifically, our temp method groups responses into several clusters and samples multiple labels with an adaptively sharpened sampling strategy, inspired by the observation that unsafe samples in the clusters are usually few and distribute in the tail. extensive experiments in chitchat and task-oriented dialogues show that our temp outperforms state-of-the-art models with weak supervision signals and obtains comparable results under unsupervised learning settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15852" target="_blank">Self-Contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation</a></div>
<div class="paper-author">Niels Mündler, Jingxuan He, Slobodan Jenko, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (large lms) are susceptible to producing text that contains hallucinated content. an important instance of this problem is self-contradiction, where the lm generates two contradictory sentences within the same context. in this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned lms, covering evaluation, detection, and mitigation. our analysis reveals the prevalence of self-contradictions when lms generate text for open-domain topics, e.g., in 17.7% of all sentences produced by chatgpt. self-contradiction also complements retrieval-based methods, as a large portion of them (e.g., 35.8% for chatgpt) cannot be verified using wikipedia. we then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. our detector achieves high accuracy, e.g., around 80% f1 score when prompting chatgpt. the mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. importantly, our entire framework is applicable to black-box lms and does not require external grounded knowledge. our approach is practically effective and has been released as a push-button tool to benefit the public, available at https://chatprotect.ai/.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15875" target="_blank">Linguistic Properties of Truthful Response</a></div>
<div class="paper-author">Bruce W. Lee, Benedict Florance Arockiaraj, Helen Jin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate the phenomenon of an llm's untruthful response using a large set of 220 handcrafted linguistic features. we focus on gpt-3 models and find that the linguistic profiles of responses are similar across model sizes. that is, how varying-sized llms respond to given prompts stays similar on the linguistic properties level. we expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. though the dataset size limits our current findings, we show the possibility that truthfulness detection is possible without evaluating the content itself. but at the same time, the limited scope of our experiments must be taken into account in interpreting the results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16157" target="_blank">Training Data Extraction From Pre-Trained Language Models: A Survey</a></div>
<div class="paper-author">Shotaro Ishihara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the deployment of pre-trained language models (plms) expands, pressing security concerns have arisen regarding the potential for malicious extraction of training data, posing a threat to data privacy. this study is the first to provide a comprehensive survey of training data extraction from plms. our review covers more than 100 key papers in fields such as natural language processing and security. first, preliminary knowledge is recapped and a taxonomy of various definitions of memorization is presented. the approaches for attack and defense are then systemized. furthermore, the empirical findings of several quantitative studies are highlighted. finally, future research directions based on this review are suggested.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.16519" target="_blank">The Dangers of Trusting Stochastic Parrots: Faithfulness and Trust in Open-Domain Conversational Question Answering</a></div>
<div class="paper-author">Sabrina Chiesurin, Dimitris Dimakopoulos, Marco Antonio Sobrevilla Cabezudo, Arash Eshghi, Ioannis Papaioannou, Verena Rieser, Ioannis Konstas</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. "unfaithful" with respect to a rationale as retrieved from a knowledge base. in this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. we use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.17147" target="_blank">Heterogeneous Value Evaluation for Large Language Models</a></div>
<div class="paper-author">Zhaowei Zhang, Nian Liu, Siyuan Qi, Ceyao Zhang, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergent capabilities of large language models (llms) have made it crucial to align their values with those of humans. current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. in this paper, we propose a2ehv, an automated alignment evaluation with a heterogeneous value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. the quantification of value rationality is facilitated by the social value orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. we evaluate the value rationality of eight mainstream llms and observe that large models are more inclined to align neutral values compared to those with strong personal values. by examining the behavior of these llms, we contribute to a deeper understanding of value alignment within a heterogeneous value system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18339" target="_blank">A Survey on Chatgpt: Ai-Generated Contents, Challenges, and Solutions</a></div>
<div class="paper-author">Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, Tom H. Luan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the widespread use of large artificial intelligence (ai) models such as chatgpt, ai-generated content (aigc) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. aigc uses generative large ai algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. despite the recent significant progress in aigc, security, privacy, ethical, and legal challenges still need to be addressed. this paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the aigc paradigm. specifically, we first explore the enabling technologies, general architecture of aigc, and discuss its working modes and key characteristics. then, we investigate the taxonomy of security and privacy threats to aigc and highlight the ethical and societal implications of gpt and aigc technologies. furthermore, we review the state-of-the-art aigc watermarking approaches for regulatable aigc paradigms regarding the aigc model and its produced content. finally, we identify future challenges and open research directions related to aigc.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14710" target="_blank">Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</a></div>
<div class="paper-author">Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. however, in this work we raise security concerns about this training paradigm. our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used nlp datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. in this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. moreover, the poisoned model cannot be cured by continual learning. lastly, instruction attacks show resistance to existing inference-time defense. these findings highlight the need for more robust defenses against data poisoning attacks in instructiontuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14735" target="_blank">Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection</a></div>
<div class="paper-author">Vyoma Raman, Eve Fleisig, Dan Klein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the impact of ai models on marginalized communities has traditionally been measured by identifying performance differences between specified demographic subgroups. though this approach aims to center vulnerable groups, it risks obscuring patterns of harm faced by intersectional subgroups or shared across multiple groups. to address this, we draw on theories of marginalization from disability studies and related disciplines, which state that people farther from the norm face greater adversity, to consider the "margins" in the domain of toxicity detection. we operationalize the "margins" of a dataset by employing outlier detection to identify text about people with demographic attributes distant from the "norm". we find that model performance is consistently worse for demographic outliers, with mean squared error (mse) between outliers and non-outliers up to 70.4% worse across toxicity types. it is also worse for text outliers, with a mse up to 68.4% higher for outliers than non-outliers. we also find text and demographic outliers to be particularly susceptible to errors in the classification of severe toxicity and identity attacks. compared to analysis of disparities using traditional demographic breakdowns, we find that our outlier analysis frequently surfaces greater harms faced by a larger, more intersectional group, which suggests that outlier analysis is particularly beneficial for identifying harms against those groups.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14739" target="_blank">Trusting Your Evidence: Hallucinate Less With Context-Aware Decoding</a></div>
<div class="paper-author">Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, Scott Wen-Tau Yih</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. to mitigate this issue, we present context-aware decoding (cad), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. our experiments show that cad, without additional training, significantly improves the faithfulness of different lm families, including opt, gpt, llama and flan-t5 for summarization tasks (e.g., 14.3% gain for llama in factuality metrics). furthermore, cad is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14752" target="_blank">A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification</a></div>
<div class="paper-author">Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, Lucas C. Cordeiro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper we present a novel solution that combines the capabilities of large language models (llms) with formal verification strategies to verify and automatically repair software vulnerabilities. initially, we employ bounded model checking (bmc) to locate the software vulnerability and derive a counterexample. the counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. the counterexample that has been detected, along with the source code, are provided to the llm engine. our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. finally, we use bmc to verify the corrected version of the code generated by the llm. as a proof of concept, we create esbmc-ai based on the efficient smt-based context-bounded model checker (esbmc) and a pre-trained transformer model, specifically gpt-3.5-turbo, to detect and fix errors in c programs. our experimentation involved generating a dataset comprising 1000 c code samples, each consisting of 20 to 50 lines of code. notably, our proposed method achieved an impressive success rate of up to 80% in repairing vulnerable code encompassing buffer overflow and pointer dereference failures. we assert that this automated approach can effectively incorporate into the software development lifecycle's continuous integration and deployment (ci/cd) process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14763" target="_blank">Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models</a></div>
<div class="paper-author">Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the escalating debate on ai's capabilities warrants developing reliable metrics to assess machine "intelligence". recently, many anecdotal examples were used to suggest that newer large language models (llms) like chatgpt and gpt-4 exhibit neural theory-of-mind (n-tom); however, prior work reached conflicting conclusions regarding those abilities. we investigate the extent of llms' n-tom through an extensive evaluation on 6 tasks and find that while llms exhibit certain n-tom abilities, this behavior is far from being robust. we further examine the factors impacting performance on n-tom tasks and discover that llms struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust tom abilities. we caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14784" target="_blank">Anthropomorphization of Ai: Opportunities and Risks</a></div>
<div class="paper-author">Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, Ashwin Kalyan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: anthropomorphization is the tendency to attribute human-like traits to non-human entities. it is prevalent in many social contexts -- children anthropomorphize toys, adults do so with brands, and it is a literary device. it is also a versatile tool in science, with behavioral psychology and evolutionary biology meticulously documenting its consequences. with widespread adoption of ai systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly. we take a dyadic approach to understanding this phenomenon with large language models (llms) by studying (1) the objective legal implications, as analyzed through the lens of the recent blueprint of ai bill of rights and the (2) subtle psychological aspects customization and anthropomorphization. we find that anthropomorphized llms customized for different user bases violate multiple provisions in the legislative blueprint. in addition, we point out that anthropomorphization of llms affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-ai interaction, with potential for manipulation and negative influence. with llms being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution. we propose a conservative strategy for the cautious use of anthropomorphization to improve trustworthiness of ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14824" target="_blank">Mitigating Temporal Misalignment by Discarding Outdated Facts</a></div>
<div class="paper-author">Michael J. Q. Zhang, Eunsol Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. to mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. in our experiments, we demonstrate how identifying facts that are prone to rapid change can help models avoid from reciting outdated information and identify which predictions require seeking out up-to-date knowledge sources. we also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment by discarding volatile facts. our data and code will be released publicly at https://github.com/mikejqzhang/mitigating_misalignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14888" target="_blank">Privacy Implications of Retrieval-Based Language Models</a></div>
<div class="paper-author">Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, Danqi Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: retrieval-based language models (lms) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts, by incorporating retrieved text from external datastores. while it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. in this work, we present the first study of privacy risks in retrieval-based lms, particularly $k$nn-lms. our goal is to explore the optimal design and training procedure in domains where privacy is of concern, aiming to strike a balance between utility and privacy. crucially, we find that $k$nn-lms are more susceptible to leaking private information from their private datastore than parametric models. we further explore mitigations of privacy risks. when privacy information is targeted and readily detected in the text, we find that a simple sanitization step would completely eliminate the risks, while decoupling query and key encoders achieves an even better utility-privacy trade-off. otherwise, we consider strategies of mixing public and private data in both datastore and encoder training. while these methods offer modest improvements, they leave considerable room for future work. together, our findings provide insights for practitioners to better understand and mitigate privacy risks in retrieval-based lms. our code is available at: https://github.com/princeton-sysml/knnlm_privacy .
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14902" target="_blank">M4: Multi-Generator, Multi-Domain, and Multi-Lingual Black-Box Machine-Generated Text Detection</a></div>
<div class="paper-author">Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, Preslav Nakov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries, but this has also resulted in concerns regarding the potential misuse of such texts in journalism, educational, and academic context. in this work, we aim to develop automatic systems to identify machine-generated text and to detect potential misuse. we first introduce a large-scale benchmark m4, which is multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. using the dataset, we experiment with a number of methods and we show that it is challenging for detectors to generalize well on unseen examples if they are either from different domains or are generated by different large language models. in such cases, detectors tend to misclassify machine-generated text as human-written. these results show that the problem is far from solved and there is a lot of room for improvement. we believe that our dataset m4, which covers different generators, domains and languages, will enable future research towards more robust approaches for this pressing societal problem. the m4 dataset is available at https://github.com/mbzuai-nlp/m4.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14908" target="_blank">Purr: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions</a></div>
<div class="paper-author">Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, Kelvin Guu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the remarkable capabilities of large language models have been accompanied by a persistent drawback: the generation of false and unsubstantiated claims commonly known as "hallucinations". to combat this issue, recent research has introduced approaches that involve editing and attributing the outputs of language models, particularly through prompt-based editing. however, the inference cost and speed of using large language models for editing currently bottleneck prompt-based methods. these bottlenecks motivate the training of compact editors, which is challenging due to the scarcity of training data for this purpose. to overcome these challenges, we exploit the power of large language models to introduce corruptions (i.e., noise) into text and subsequently fine-tune compact editors to denoise the corruptions by incorporating relevant evidence. our methodology is entirely unsupervised and provides us with faux hallucinations for training in any domain. our petite unsupervised research and revision model, purr, not only improves attribution over existing editing methods based on fine-tuning and prompting, but also achieves faster execution times by orders of magnitude.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14928" target="_blank">Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4</a></div>
<div class="paper-author">Kellin Pelrine, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph, Reihaneh Rabbany</div>
<div class="abstract">
<div class="abstract-content">
Abstract: misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. we propose focusing on generalization, soft classification, and leveraging recent large language models to create more practical tools in contexts where perfect predictions remain unattainable. we begin by demonstrating that gpt-4 and other language models can outperform existing methods in the literature. next, we explore their generalization, revealing that gpt-4 and roberta-large exhibit critical differences in failure modes, which offer potential for significant performance improvements. finally, we show that these models can be employed in soft classification frameworks to better quantify uncertainty. we find that models with inferior hard classification results can achieve superior soft classification performance. overall, this research lays groundwork for future tools that can drive real-world progress on misinformation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14929" target="_blank">Aligning Language Models to User Opinions</a></div>
<div class="paper-author">Eunjeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon</div>
<div class="abstract">
<div class="abstract-content">
Abstract: an important aspect of developing llms that interact with humans is to align models' behavior to their users. it is possible to prompt an llm into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. but, how to best align an llm with a specific user and not a demographic or ideological group remains an open question. mining public opinion surveys (by pew research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. we use this insight to align llms by modeling both user opinions as well as user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. in addition to the typical approach of prompting llms with demographics and ideology, we discover that utilizing the most relevant past opinions from individual users enables the model to predict user opinions more accurately.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14930" target="_blank">In-Context Impersonation Reveals Large Language Models' Strengths and Biases</a></div>
<div class="paper-author">Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. we explore whether llms can take on, that is impersonate, different roles when they generate text in-context. we ask llms to assume different personas before solving vision and language tasks. we do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. in a multi-armed bandit task, we find that llms pretending to be children of different ages recover human-like developmental stages of exploration. in a language-based reasoning task, we find that llms impersonating domain experts perform better than llms impersonating non-domain experts. finally, we test whether llms' impersonations are complementary to visual information when describing different categories. we find that impersonation can improve performance: an llm prompted to be a bird expert describes birds better than one prompted to be a car expert. however, impersonation can also uncover llms' biases: an llm prompted to be a man describes cars better than one prompted to be a woman. these findings demonstrate that llms are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14936" target="_blank">Trade-Offs Between Fairness and Privacy in Language Modeling</a></div>
<div class="paper-author">Cleo Matzken, Steffen Eger, Ivan Habernal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: protecting privacy in contemporary nlp models is gaining in importance. so does the need to mitigate social biases of such models. but can we have both at the same time? existing research suggests that privacy preservation comes at the price of worsening biases in classification tasks. in this paper, we explore the extent to which this tradeoff really holds when we incorporate both privacy preservation and de-biasing techniques into training text generation models. how does improving the model along one dimension affect the other dimension as well as the utility of the model? we conduct an extensive set of experiments that include bias detection, privacy attacks, language modeling, and performance on downstream tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14938" target="_blank">Do LLMS Understand Social Knowledge? Evaluating the Sociability of Large Language Models With Socket Benchmark</a></div>
<div class="paper-author">Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, David Jurgens</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. while llms are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well llms understand \textit{social} language. here, we introduce a new theory-driven benchmark, socket, that contains 58 nlp tasks testing social knowledge which we group into five categories: humor & sarcasm, offensiveness, sentiment & emotion, and trustworthiness. in tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware llms. the associated resources are released at https://github.com/minjechoi/socket.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14950" target="_blank">Adversarial Demonstration Attacks on Large Language Models</a></div>
<div class="paper-author">Jiongxiao Wang, Zichen Liu, Keun Hee Park, Zhuojun Jiang, Zhaoheng Zheng, Zhuofeng Wu, Muhao Chen, Chaowei Xiao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the emergence of more powerful large language models (llms), such as chatgpt and gpt-4, in-context learning (icl) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. while incorporating demonstrations can greatly enhance the performance of llms across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. in this paper, we investigate the security concern of icl from an adversarial perspective, focusing on the impact of demonstrations. we propose a novel attack method named advicl, which aims to manipulate only the demonstration without changing the input to mislead the models. our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. as a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. to achieve it, we propose the transferable version of advicl, named transferable-advicl. our experiment shows that the adversarial demonstration generated by transferable-advicl can successfully attack the unseen test input examples. we hope that our study reveals the critical security risks associated with icl and underscores the need for extensive research on the robustness of icl, particularly given its increasing significance in the advancement of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14965" target="_blank">Tricking LLMS Into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks</a></div>
<div class="paper-author">Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent explorations with commercial large language models (llms) have shown that non-expert users can jailbreak llms by simply manipulating the prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. limited formal studies have been carried out to formalize and analyze these attacks and their mitigations. we bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. we perform a survey of existing jailbreak methods and their effectiveness on open-source and commercial llms (such as gpt 3.5, opt, bloom, and flan-t5-xxl). we further propose a limited set of prompt guards and discuss their effectiveness against known attack types.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14975" target="_blank">Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores From Language Models Fine-Tuned With Human Feedback</a></div>
<div class="paper-author">Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D. Manning</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. recent studies have shown that unsupervised pre-training produces large language models (lms) whose conditional probabilities are remarkably well-calibrated. however, the most widely-used lms are fine-tuned with reinforcement learning from human feedback (rlhf-lms), and some studies have suggested that rlhf-lms produce conditional probabilities that are very poorly calibrated. in light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from rlhf-lms. for rlhf-lms such as chatgpt, gpt-4, and claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the triviaqa, sciq, and truthfulqa benchmarks, often reducing the expected calibration error by a relative 50%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15004" target="_blank">Llmdet: A Third Party Large Language Models Generated Text Detection Tool</a></div>
<div class="paper-author">Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generated texts from large language models (llms) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. however, existing detection tools typically rely on access to llms and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. therefore, we propose llmdet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific llms, such as gpt-2, opt, llama, and others. in llmdet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each llm. by jointly analyzing the proxy perplexities of llms, we can determine the source of the generated text. experimental results show that llmdet yields impressive detection performance while ensuring speed and security, achieving 98.54% precision and x3.5 faster for recognizing human-authored text. additionally, llmdet can effortlessly extend its detection capabilities to a new open-source model. we will provide an open-source tool at https://github.com/trustedllm/llmdet.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15065" target="_blank">Inference-Time Policy Adapters (Ipa): Tailoring Extreme-Scale LMS Without Fine-Tuning</a></div>
<div class="paper-author">Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian Fisher, Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, Sean Welleck, Yejin Choi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models excel at a variety of language tasks when prompted with examples or instructions. yet controlling these models through prompting alone is limited. tailoring language models through fine-tuning (e.g., via reinforcement learning) can be effective, but it is expensive and requires model access.   we propose inference-time policy adapters (ipa), which efficiently tailors a language model such as gpt-3 without fine-tuning it. ipa guides a large base model during decoding time through a lightweight policy adaptor trained to optimize an arbitrary user objective with reinforcement learning.   on five challenging text generation tasks, such as toxicity reduction and open-domain generation, ipa consistently brings significant improvements over off-the-shelf language models. it outperforms competitive baseline methods, sometimes even including expensive fine-tuning. in particular, tailoring gpt-2 with ipa can outperform gpt-3, while tailoring gpt- 3 with ipa brings a major performance boost over gpt-3 (and sometimes even over gpt-4). our promising results highlight the potential of ipa as a lightweight alternative to tailoring extreme-scale language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15242" target="_blank">Machine Unlearning: Its Nature, Scope, and Importance for a "Delete Culture"</a></div>
<div class="paper-author">Luciano Floridi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the article explores the cultural shift from recording to deleting information in the digital age and its implications on privacy, intellectual property (ip), and large language models like chatgpt. it begins by defining a delete culture where information, in principle legal, is made unavailable or inaccessible because unacceptable or undesirable, especially but not only due to its potential to infringe on privacy or ip. then it focuses on two strategies in this context: deleting, to make information unavailable; and blocking, to make it inaccessible. the article argues that both strategies have significant implications, particularly for machine learning (ml) models where information is not easily made unavailable. however, the emerging research area of machine unlearning (mu) is highlighted as a potential solution. mu, still in its infancy, seeks to remove specific data points from ml models, effectively making them 'forget' completely specific information. if successful, mu could provide a feasible means to manage the overabundance of information and ensure a better protection of privacy and ip. however, potential ethical risks, such as misuse, overuse, and underuse of mu, should be systematically studied to devise appropriate policies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15299" target="_blank">Science in the Era of Chatgpt, Large Language Models and Generative Ai: Challenges for Research Ethics and How to Respond</a></div>
<div class="paper-author">Evangelos Pournaras</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models of artificial intelligence (ai), such as chatgpt, find remarkable but controversial applicability in science and research. this paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative ai. this is with the aim to lay new timely foundations for a high-quality research ethics review. the role of ai language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. new emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15324" target="_blank">Model Evaluation for Extreme Risks</a></div>
<div class="paper-author">Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, Allan Dafoe</div>
<div class="abstract">
<div class="abstract-content">
Abstract: current approaches to building general-purpose ai systems tend to produce systems with both beneficial and harmful capabilities. further progress in ai development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. we explain why model evaluation is critical for addressing extreme risks. developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). these evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15336" target="_blank">From Text to Mitre Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads</a></div>
<div class="paper-author">P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand, Sandeep K Shukla</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this research article critically examines the potential risks and implications arising from the malicious utilization of large language models(llm), focusing specifically on chatgpt and google's bard. although these large language models have numerous beneficial applications, the misuse of this technology by cybercriminals for creating offensive payloads and tools is a significant concern. in this study, we systematically generated implementable code for the top-10 mitre techniques prevalent in 2022, utilizing chatgpt, and conduct a comparative analysis of its performance with google's bard. our experimentation reveals that chatgpt has the potential to enable attackers to accelerate the operation of more targeted and sophisticated attacks. additionally, the technology provides amateur attackers with more capabilities to perform a wide range of attacks and empowers script kiddies to develop customized tools that contribute to the acceleration of cybercrime. furthermore, llms significantly benefits malware authors, particularly ransomware gangs, in generating sophisticated variants of wiper and ransomware attacks with ease. on a positive note, our study also highlights how offensive security researchers and pentesters can make use of llms to simulate realistic attack scenarios, identify potential vulnerabilities, and better protect organizations. overall, we conclude by emphasizing the need for increased vigilance in mitigating the risks associated with llms. this includes implementing robust security measures, increasing awareness and education around the potential risks of this technology, and collaborating with security experts to stay ahead of emerging threats.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15377" target="_blank">Uncovering and Quantifying Social Biases in Code Generation</a></div>
<div class="paper-author">Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, Tsung-Yi Ho</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the popularity of automatic code generation tools, such as copilot, the study of the potential hazards of these tools is gaining importance. in this work, we explore the social bias problem in pre-trained code generation models. we propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. to quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. experimental results on three pre-trained code generation models (codex, incoder, and codegen) with varying sizes, reveal severe social biases. moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias. (this work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.)
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.15594" target="_blank">Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models</a></div>
<div class="paper-author">Haonan Duan, Adam Dziedzic, Nicolas Papernot, Franziska Boenisch</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are excellent in-context learners. however, the sensitivity of data contained in prompts raises privacy concerns. our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt llms. to address this vulnerability, one could forego prompting and resort to fine-tuning llms with known algorithms for private gradient descent. however, this comes at the expense of the practicality and efficiency offered by prompting. therefore, we propose to privately learn to prompt. we first show that soft prompts can be obtained privately through gradient descent on downstream data. however, this is not the case for discrete prompts. thus, we orchestrate a noisy vote among an ensemble of llms presented with different prompts, i.e., a flock of stochastic parrots. the vote privately transfers the flock's knowledge into a single public prompt. we show that llms prompted with our private algorithms closely match the non-private baselines. for example, using gpt3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\epsilon=0.147, \delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial apis.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2310.06998" target="_blank">A Systematic Review of Machine Learning Enabled Phishing</a></div>
<div class="paper-author">Krystal A. Jackson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developments in artificial intelligence (ai) are likely to affect social engineering and change cyber defense operations. the broad and sweeping nature of ai impact means that many aspects of social engineering could be automated, potentially giving adversaries an advantage. in this review, we assess the ways phishing and spear-phishing might be affected by machine learning techniques. by performing a systematic review of demonstrated ml-enabled phishing campaigns, we take a broad survey the space for current developments. we develop a detailed approach for evaluation by creating a risk framework for analyzing and contextualizing these developments. the object of this review is to answer the research questions: (1) are there high-risk ml-enabled phishing use cases? (2) is there a meaningful difference between traditional targeted phishing campaigns and ml-enabled phishing campaigns? practitioners may use this review to inform standards, future research directions, and cyber defense strategies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13661" target="_blank">On the Risk of Misinformation Pollution With Large Language Models</a></div>
<div class="paper-author">Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we comprehensively investigate the potential misuse of modern large language models (llms) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly open-domain question answering (odqa) systems. we establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which llms can be utilized to produce misinformation. our study reveals that llms can act as effective misinformation generators, leading to a significant degradation in the performance of odqa systems. to mitigate the harm caused by llm-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. while initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. our work highlights the need for further research and interdisciplinary collaboration to address llm-generated misinformation and to promote responsible use of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13677" target="_blank">Towards Legally Enforceable Hate Speech Detection for Public Forums</a></div>
<div class="paper-author">Chu Fei Luo, Rohan Bhambhoria, Xiaodan Zhu, Samuel Dahan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech is a serious issue on public forums, and proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. however, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. our work introduces a new task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. we experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. we then report results on several large language models (llms). with this task definition, automatic hate speech detection can be more closely aligned to enforceable laws, and hence assist in more rigorous enforcement of legal protections against harmful speech in public forums.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13712" target="_blank">Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty With Large Language Models</a></div>
<div class="paper-author">Alfonso Amayuelas, Liangming Pan, Wenhu Chen, William Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper investigates the capabilities of large language models (llms) in the context of understanding their own knowledge and measuring their uncertainty. we argue this is an important feature for mitigating hallucinations. specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. to facilitate our study, we collect a dataset with new known-unknown questions (kuq) and propose a novel categorization scheme to elucidate the sources of uncertainty. subsequently, we assess the llms' ability to differentiate between known and unknown questions and classify them accordingly. moreover, we evaluate the quality of their answers in an open-ended qa setting. to quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13733" target="_blank">Self-Critique Prompting With Large Language Models for Inductive Instructions</a></div>
<div class="paper-author">Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Ruifeng Xu, Kam-Fai Wong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: numerous works are proposed to improve or evaluate the capabilities of large language models (llms) to fulfill user instructions. however, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. in this way, blindly adhering to users' false content will cause deception and harm. to address this problem, we propose a challenging benchmark consisting of inductive instructions (indust) to evaluate whether llms could resist these instructions. the indust includes 15k instructions across three categories: fact-checking instructions, questions based on false premises, and creative instructions based on false premises. our experiments on several strong llms reveal that current llms can be easily deceived by indust into generating misleading and malicious statements. hence we employ self-critique prompting to encourage llms to not only critique themselves like in previous works but also the users, which show remarkable improvement in handling inductive instructions under both zero-shot and few-shot settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13735" target="_blank">Aligning Large Language Models Through Synthetic Feedback</a></div>
<div class="paper-author">Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) to human values has become increasingly important as it enables sophisticated steering of llms. however, it requires significant human demonstrations and feedback or distillation from proprietary llms such as chatgpt. in this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary llms. first, we perform reward modeling (rm) with synthetic feedback by contrasting responses from vanilla llms with various sizes and prompts. then, we use the rm to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. our resulting model, aligned language model with synthetic training dataset (almost), outperforms recent open-sourced models, which are trained on the outputs of instructgpt or human-annotated demonstrations, in alignment benchmarks. in human evaluation, our model is preferred to alpaca and dolly-v2, 55.0% and 58.5% of the time, respectively. further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. the code is available at https://github.com/naver-ai/almost
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13860" target="_blank">Jailbreaking Chatgpt via Prompt Engineering: An Empirical Study</a></div>
<div class="paper-author">Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, Yang Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), like chatgpt, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. our study investigates three key research questions: (1) the number of different prompt types that can jailbreak llms, (2) the effectiveness of jailbreak prompts in circumventing llm constraints, and (3) the resilience of chatgpt against these jailbreak prompts. initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. subsequently, we assess the jailbreak capability of prompts with chatgpt versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. finally, we evaluate the resistance of chatgpt against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. the study underscores the importance of prompt structures in jailbreaking llms and discusses the challenges of robust jailbreak prompt generation and prevention.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13862" target="_blank">A Trip Towards Fairness: Bias and De-Biasing in Large Language Models</a></div>
<div class="paper-author">Leonardo Ranaldi, Elena Sofia Ruzzetti, Davide Venditti, Dario Onorati, Fabio Massimo Zanzotto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: cheap-to-build very large-language models (ctb-llms) with affordable training are emerging as the next big revolution in natural language processing and understanding. these ctb-llms are democratizing access to trainable very large-language models (vllms) and, thus, may represent the building blocks of many nlp systems solving downstream tasks. hence, a little or a large bias in ctb-llms may cause huge harm. in this paper, we performed a large investigation of the bias of three families of ctb-llms, and we showed that debiasing techniques are effective and usable. indeed, according to current tests, the llama and the opt families have an important bias in gender, race, religion, and profession. in contrast to the analysis for other llms, we discovered that bias depends not on the number of parameters but on the perplexity. finally, the debiasing of opt using lora reduces bias up to 4.12 points in the normalized stereotype score.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14002" target="_blank">Improving Language Models via Plug-and-Play Retrieval Feedback</a></div>
<div class="paper-author">Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, Ashish Sabharwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) exhibit remarkable performance across various nlp tasks. however, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. however, this approach is resource-intensive, involving manual input and supervision, which can be time-consuming and expensive. moreover, it cannot be provided during inference, further limiting its practical utility in dynamic and interactive applications. in this paper, we introduce refeed, a novel pipeline designed to enhance llms by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning. refeed first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections, and finally incorporates the retrieved information into the in-context demonstration for output refinement, thereby addressing the limitations of llms in a more efficient and cost-effective manner. experiments on four knowledge-intensive benchmark datasets demonstrate our proposed refeed could improve over +6.0% under zero-shot setting and +2.5% under few-shot setting, compared to baselines without using retrieval feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14020" target="_blank">Does Chatgpt Have Theory of Mind?</a></div>
<div class="paper-author">Bart Holterman, Kees Van Deemter</div>
<div class="abstract">
<div class="abstract-content">
Abstract: theory of mind (tom) is the ability to understand human thinking and decision-making, an ability that plays a crucial role in social interaction between people, including linguistic communication. this paper investigates to what extent recent large language models in the chatgpt tradition possess tom. we posed six well-known problems that address biases in human reasoning and decision making to two versions of chatgpt and we compared the results under a range of prompting strategies. while the results concerning chatgpt-3 were somewhat inconclusive, chatgpt-4 was shown to arrive at the correct answers more often than would be expected based on chance, although correct answers were often arrived at on the basis of false assumptions or invalid reasoning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14208" target="_blank">Domain Private Transformers</a></div>
<div class="paper-author">Anmol Kabra, Ethan R. Elenberg</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large, general purpose language models have demonstrated impressive performance across many different conversational domains. while multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt. this paper proposes domain privacy as a novel way to quantify how likely a conditional language model will leak across domains. we also develop policy functions based on token-level domain classification, and propose an efficient fine-tuning method to improve the trained model's domain privacy. experiments on membership inference attacks show that our proposed method has comparable resiliency to methods adapted from recent literature on differentially private language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14291" target="_blank">Evaluation of African American Language Bias in Natural Language Generation</a></div>
<div class="paper-author">Nicholas Deas, Jessi Grieser, Shana Kleiner, Desmond Patton, Elsbeth Turcan, Kathleen Mckeown</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we evaluate how well llms understand african american language (aal) in comparison to their performance on white mainstream english (wme), the encouraged "standard" form of english taught in american classrooms. we measure llm performance using automatic metrics and human judgments for two tasks: a counterpart generation task, where a model generates aal (or wme) given wme (or aal), and a masked span prediction (msp) task, where models predict a phrase that was removed from their input. our contributions include: (1) evaluation of six pre-trained, large language models on the two language generation tasks; (2) a novel dataset of aal text from multiple contexts (social media, hip-hop lyrics, focus groups, and linguistic interviews) with human-annotated counterparts in wme; and (3) documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of aal features.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14307" target="_blank">Debiasing Should Be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models</a></div>
<div class="paper-author">Robert Morabito, Jad Kabbara, Ali Emami</div>
<div class="abstract">
<div class="abstract-content">
Abstract: debiasing methods that seek to mitigate the tendency of language models (lms) to occasionally output toxic or inappropriate text have recently gained traction. in this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. for example, we ask, given a debiasing method that is developed to reduce toxicity in lms, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? we used such considerations to devise three criteria for our new protocol: specification polarity, specification importance, and domain transferability. as a case study, we apply our protocol to a popular debiasing method, self-debiasing, and compare it to one we propose, called instructive debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. we show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14456" target="_blank">Having Beer After Prayer? Measuring Cultural Bias in Large Language Models</a></div>
<div class="paper-author">Tarek Naous, Michael J. Ryan, Wei Xu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: are language models culturally biased? it is important that language models conform to the cultural aspects of the communities they serve. however, we show in this paper that language models suffer from a significant bias towards western culture when handling and generating text in arabic, often preferring, and producing western-fitting content as opposed to the relevant arab content. we quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. our experiments reveal that both arabic monolingual and multilingual models exhibit bias towards western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. models also tend to exhibit more bias when prompted with arabic sentences that are more linguistically aligned with english. these findings raise concerns about the cultural relevance of current language models. our analyses show that providing culture-indicating tokens or culturally-relevant demonstrations to the model can help in debiasing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14540" target="_blank">LLMS as Factual Reasoners: Insights From Existing Benchmarks and Beyond</a></div>
<div class="paper-author">Philippe Laban, Wojciech Kryściński, Divyansh Agarwal, Alexander R. Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the recent appearance of llms in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. when testing on existing factual consistency benchmarks, we find that a few large language models (llms) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-llm methods. however, a closer analysis reveals that most llms fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. to address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called summedits. this new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. most llms struggle on summedits, with performance close to random chance. the best-performing model, gpt-4, is still 8\% below estimated human performance, highlighting the gaps in llms' ability to reason about facts and detect inconsistencies when they occur.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14610" target="_blank">This Land Is {Your, My} Land: Evaluating Geopolitical Biases in Language Models</a></div>
<div class="paper-author">Bryan Li, Chris Callison-Burch</div>
<div class="abstract">
<div class="abstract-content">
Abstract: do the spratly islands belong to china, the philippines, or vietnam? a pretrained large language model (llm) may answer differently if asked in the languages of each claimant country: chinese, tagalog, or vietnamese. this contrasts with a multilingual human, who would likely answer consistently. in this work, we show that llms recall geopolitical knowledge inconsistently across languages -- a phenomenon we term geopolitical bias. as a targeted case study, we consider territorial disputes, inherently controversial and cross-lingual task.   we first introduce the borderlines dataset of territorial disputes. this covers 256 territories, each of which is associated to a set of multiple-choice questions in the languages of each claimant country (48 languages total). we then pose these questions to llms to probe their internal knowledge. finally, we propose a suite of evaluation metrics based on accuracy, which compares responses with respect to the actual geopolitical situation, and consistency of the responses in different languages. these metrics allow us to quantify several findings, which include instruction-tuned llms underperforming base ones, and geopolitical bias being amplified in stronger models. we release our code and dataset to facilitate future investigation and mitigation of geopolitical bias.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14613" target="_blank">Selectively Answering Ambiguous Questions</a></div>
<div class="paper-author">Jeremy R. Cole, Michael J. Q. Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, Jacob Eisenstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: trustworthy language models should abstain from answering questions when they do not know the answer. however, the answer to a question can be unknown for a variety of reasons. prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. however, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. we investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. in this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % we find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14658" target="_blank">Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality</a></div>
<div class="paper-author">Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: llms (large language models) such as chatgpt have shown remarkable language understanding and generation capabilities. although reference-free evaluators based on llms show better human alignment than traditional reference-based evaluators, there are many challenges in using reference-free evaluators based on llms. reference-free evaluators are more suitable for open-ended examples with different semantics responses. but not all examples are open-ended. for closed-ended examples with unique correct semantic response, reference-free evaluators will still consider it high quality when giving a response that is inconsistent with the facts and the semantic of reference. in order to comprehensively evaluate the reliability of evaluators based on llms, we construct two adversarial meta-evaluation dialogue generation datasets kdconv-adv and dstc7-adv based on kdconv and dstc7-avsd, respectively. compared to previous meta-evaluation benchmarks, kdconv-adv and dstc7-adv are much more challenging since they requires evaluators to be able to reasonably evaluate closed-ended examples with the help of external knowledge or even its own knowledge. empirical results show that the ability of llms to identify unreasonable responses is insufficient. there are risks in using eference-free evaluators based on llms to evaluate the quality of dialogue responses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14688" target="_blank">Expertprompting: Instructing Large Language Models to Be Distinguished Experts</a></div>
<div class="paper-author">Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the answering quality of an aligned large language model (llm) can be drastically improved if treated with proper crafting of prompts. in this paper, we propose expertprompting to elicit the potential of llms to answer as distinguished experts. we first utilize in-context learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask llms to provide answer conditioned on such agent background. based on this augmented prompting strategy, we produce a new set of instruction-following data using gpt-3.5, and train a competitive open-source chat assistant called expertllama. we employ gpt4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) expertllama outperforms existing open-source opponents and achieves 96\% of the original chatgpt's capability. all data and the expertllama model will be made publicly available at \url{https://github.com/ofa-sys/expertllama}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14695" target="_blank">A Causal View of Entity Bias in (Large) Language Models</a></div>
<div class="paper-author">Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, Muhao Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. the rise of black-box llms also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. to address these problems, we propose a specific structured causal model (scm) whose parameters are comparatively easier to estimate. building upon this scm, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. the proposed causal intervention perturbs the original entity with neighboring entities. this intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. when evaluated on the relation extraction task, our training-time intervention significantly improves the f1 score of roberta by 5.7 points on entred, in which spurious shortcuts between entities and labels are removed. meanwhile, our in-context intervention effectively reduces the knowledge conflicts between parametric knowledge and contextual knowledge in gpt-3.5 and improves the f1 score by 9.14 points on a challenging test set derived from re-tacred.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12707" target="_blank">Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage</a></div>
<div class="paper-author">Hanyin Shao, Jie Huang, Shen Zheng, Kevin Chen-Chuan Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the advancement of large language models (llms) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. one notable capability of llms is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (pii). this paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. however, there is a distinct performance gap when associating commonsense knowledge versus pii, with the latter showing lower accuracy. despite the proportion of accurately predicted pii being relatively small, llms still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. these findings underscore the potential risk to pii confidentiality posed by the evolving capabilities of llms, especially as they continue to expand in scale and power.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12757" target="_blank">This Prompt Is Measuring &Lt;mask&gt;: Evaluating Bias Evaluation in Language Models</a></div>
<div class="paper-author">Seraphina Goldfarb-Tarrant, Eddie Ungless, Esma Balkir, Su Lin Blodgett</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias research in nlp seeks to analyse models for social biases, thus helping nlp practitioners uncover, measure, and mitigate social harms. we analyse the body of work that uses prompts and templates to assess bias in language models. we draw on a measurement modelling framework to create a taxonomy of attributes that capture what a bias test aims to measure and how that measurement is carried out. by applying this taxonomy to 90 bias tests, we illustrate qualitatively and quantitatively that core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched. our analysis illuminates the scope of possible bias types the field is able to measure, and reveals types that are as yet under-researched. we offer guidance to enable the community to explore a wider section of the possible bias space, and to better close the gap between desired outcomes and experimental design, both for bias and for evaluating language models more broadly.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12829" target="_blank">On Bias and Fairness in Nlp: How to Have a Fairer Text Classification?</a></div>
<div class="paper-author">Fatma Elsafoury, Stamos Katsigiannis, Naeem Ramzan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we provide a holistic analysis of the different sources of bias, upstream, sample and overampflication biases, in nlp models. we investigate how they impact the fairness of the task of text classification. we also investigate the impact of removing these biases using different debiasing techniques on the fairness of text classification. we found that overamplification bias is the most impactful bias on the fairness of text classification. and that removing overamplification bias by fine-tuning the lm models on a dataset with balanced representations of the different identity groups leads to fairer text classification models. finally, we build on our findings and introduce practical guidelines on how to have a fairer text classification model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13160" target="_blank">Can Chatgpt Defend Its Belief in Truth? Evaluating LLM Reasoning via Debate</a></div>
<div class="paper-author">Boshi Wang, Xiang Yue, Huan Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as chatgpt and gpt-4 have shown impressive performance in complex reasoning tasks. however, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. in this work, we explore testing llms' reasoning by engaging with them in a debate-like conversation, where given a question, the llm and the user need to discuss to make the correct decision starting from opposing arguments. upon mitigating the clever hans effect, our task requires the llm to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the llm grasps the essence of the reasoning required to solve the problem. across a range of complex reasoning benchmarks spanning math, commonsense, logic and big-bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, llms like chatgpt cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that llms can improve their responses based on feedback.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13169" target="_blank">A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity</a></div>
<div class="paper-author">Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretraining is the preliminary and fundamental step in developing capable language models (lm). despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. to address this, we pretrain 28 1.5b parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. first, we quantify the effect of pretraining data age. a temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. our findings indicate there does not exist a one-size-fits-all solution to filtering training data. we also find that the effects of different types of filtering are not predictable from text domain characteristics. lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. these findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in lm development.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13242" target="_blank">Deepfake Text Detection in the Wild</a></div>
<div class="paper-author">Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, Yue Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in large language models have enabled them to reach a level of text generation comparable to that of humans. these models show powerful capabilities across a wide range of content, including news article writing, story generation, and scientific writing. such capability further narrows the gap between human-authored and machine-generated texts, highlighting the importance of deepfake text detection to avoid potential risks such as fake news propagation and plagiarism. however, previous work has been limited in that they testify methods on testbed of specific domains or certain language models. in practical scenarios, the detector faces texts from various domains or llms without knowing their sources. to this end, we build a wild testbed by gathering texts from various human writings and deepfake texts generated by different llms. human annotators are only slightly better than random guessing at identifying machine-generated texts. empirical results on automatic detection methods further showcase the challenges of deepfake text detection in a wild testbed. in addition, out-of-distribution poses a greater challenge for a detector to be employed in realistic application scenarios. we release our resources at https://github.com/yafuly/deepfaketextdetect.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13276" target="_blank">Evaluating Chatgpt's Performance for Multilingual and Emoji-Based Hate Speech Detection</a></div>
<div class="paper-author">Mithun Das, Saurabh Kumar Pandey, Animesh Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hate speech is a severe issue that affects many online platforms. so far, several studies have been performed to develop robust hate speech detection systems. large language models like chatgpt have recently shown a great promise in performing several tasks, including hate speech detection. however, it is crucial to comprehend the limitations of these models to build robust hate speech detection systems. to bridge this gap, our study aims to evaluate the strengths and weaknesses of the chatgpt model in detecting hate speech at a granular level across 11 languages. our evaluation employs a series of functionality tests that reveals various intricate failures of the model which the aggregate metrics like macro f1 or accuracy are not able to unfold. in addition, we investigate the influence of complex emotions, such as the use of emojis in hate speech, on the performance of the chatgpt model. our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research and improvements in the workings of these models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13300" target="_blank">Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts</a></div>
<div class="paper-author">Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su</div>
<div class="abstract">
<div class="abstract-content">
Abstract: by providing external information to large language models (llms), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of llms' static parametric memory. however, how receptive are llms to such external evidence, especially when the evidence conflicts with their parametric memory? we present the first comprehensive and controlled investigation into the behavior of llms when encountering knowledge conflicts. we propose a systematic framework to elicit high-quality parametric memory from llms and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. our investigation reveals seemingly contradicting behaviors of llms. on the one hand, different from prior wisdom, we find that llms can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. on the other hand, llms also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. these results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13302" target="_blank">Language-Agnostic Bias Detection in Language Models</a></div>
<div class="paper-author">Abdullatif Köksal, Omer Faruk Yalcin, Ahmet Akbiyik, M. Tahir Kilavuz, Anna Korhonen, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms) are key components in nlp, but they contain strong social biases. quantifying these biases is challenging because current methods focusing on fill-the-mask objectives are sensitive to slight changes in input. to address this, we propose labdet, a robust language-agnostic method for evaluating bias in plms. for nationality as a case study, we show that labdet "surfaces" nationality bias by training a classifier on top of a frozen plm on non-nationality sentiment detection. collaborating with political scientists, we find consistent patterns of nationality bias across monolingual plms in six languages that align with historical and political context. we also show for english bert that bias surfaced by labdet correlates well with bias in the pretraining data; thus, our work is one of the few studies that directly links pretraining data to plm behavior. finally, we verify labdet's reliability and applicability to different templates and languages through an extensive set of robustness checks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13534" target="_blank">How Language Model Hallucinations Can Snowball</a></div>
<div class="paper-author">Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A. Smith</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. hallucinations are often attributed to knowledge gaps in lms, but we hypothesize that in some cases, when justifying previously generated hallucinations, lms output false claims that they can separately recognize as incorrect. we construct three question-answering datasets where chatgpt and gpt-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. crucially, we find that chatgpt and gpt-4 can identify 67% and 87% of their own mistakes, respectively. we refer to this phenomenon as hallucination snowballing: an lm over-commits to early mistakes, leading to more mistakes that it otherwise would not make.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.13589" target="_blank">Biasx: "Thinking Slow" in Toxic Content Moderation With Explanations of Implied Social Biases</a></div>
<div class="paper-author">Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: toxicity annotators and content moderators often default to mental shortcuts when making decisions. this can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. we introduce biasx, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. we show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. the quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.14387" target="_blank">Alpacafarm: A Simulation Framework for Methods That Learn From Human Feedback</a></div>
<div class="paper-author">Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as chatgpt have seen widespread adoption due to their ability to follow user instructions well. developing these llms involves a complex yet poorly understood workflow requiring training with human feedback. replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. we address these challenges with alpacafarm, a simulator that enables research and development for learning from feedback at a low cost. first, we design llm prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. third, we contribute reference implementations for several methods (ppo, best-of-n, expert iteration, and more) that learn from pairwise feedback. finally, as an end-to-end validation of alpacafarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in alpacafarm match rankings of models trained on human data. as a demonstration of the research possible in alpacafarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference ppo implementation leads to a +10% improvement in win-rate against davinci003. we release all components of alpacafarm at https://github.com/tatsu-lab/alpaca_farm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18320" target="_blank">Cognitive Network Science Reveals Bias in GPT-3, Chatgpt, and GPT-4 Mirroring Math Anxiety in High-School Students</a></div>
<div class="paper-author">Katherine Abramski, Salvatore Citraro, Luigi Lombardi, Giulio Rossetti, Massimo Stella</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are becoming increasingly integrated into our lives. hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. this challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that llms act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. one such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and stem subjects. here, we investigate perceptions of math and stem fields provided by cutting-edge language models, namely gpt-3, chat-gpt, and gpt-4, by applying an approach from network science and cognitive psychology. specifically, we use behavioral forma mentis networks (bfmns) to understand how these llms frame math and stem disciplines in relation to other concepts. we use data obtained by probing the three llms in a language generation task that has previously been applied to humans. our findings indicate that llms have an overall negative perception of math and stem fields, with math being perceived most negatively. we observe significant differences across the three llms. we observe that newer versions (i.e. gpt-4) produce richer, more complex perceptions as well as less negative perceptions compared to older versions and n=159 high-school students. these findings suggest that advances in the architecture of llms may lead to increasingly less biased models that could even perhaps someday aid in reducing harmful stereotypes in society rather than perpetuating them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.18569" target="_blank">Fairness of Chatgpt</a></div>
<div class="paper-author">Yunqi Li, Yongfeng Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: understanding and addressing unfairness in llms are crucial for responsible ai deployment. however, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in llms, especially when applying llms to high-stakes fields. this work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of llms using chatgpt as a study case. we focus on assessing chatgpt's performance in high-takes fields including education, criminology, finance and healthcare. to make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in chatgpt's outputs under a set of biased or unbiased prompts. this work contributes to a deeper understanding of llms' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12434" target="_blank">Biasasker: Measuring the Bias in Conversational Ai System</a></div>
<div class="paper-author">Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, Michael Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: powered by advanced artificial intelligence (ai) techniques, conversational ai systems, such as chatgpt and digital assistants like siri, have been widely deployed in daily life. however, such systems may still produce content containing biases and stereotypes, causing potential social problems. due to the data-driven, black-box nature of modern ai techniques, comprehensively identifying and measuring biases in conversational systems remains a challenging task. particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups as well as biased properties. in addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods simply based on the sentiment and the toxicity hardly being adopted. in this paper, we propose biasasker, an automated framework to identify and measure social bias in conversational ai systems. to obtain social groups and biased properties, we construct a comprehensive social bias dataset, containing a total of 841 groups and 8,110 biased properties. given the dataset, biasasker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. extensive experiments on 8 commercial systems and 2 famous research models, such as chatgpt and gpt-3, show that 32.83% of the questions generated by biasasker can trigger biased behaviors in these widely deployed conversational systems. all the code, data, and experimental results have been released to facilitate future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12519" target="_blank">GPT Paternity Test: GPT Generated Text Detection With GPT Genetic Inheritance</a></div>
<div class="paper-author">Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, Nenghai Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating fake social media postings that can sway election results. detecting whether a text is machine-generated has thus become increasingly important. while machine-learning-based detection strategies exhibit superior performance, they often lack generalizability, limiting their practicality. in this work, we introduce gpt paternity test (gpt-pat), which reliably detects machine-generated text across varied datasets. given a text under scrutiny, we leverage chatgpt to generate a corresponding question and provide a re-answer to the question. by comparing the similarity between the original text and the generated re-answered text, it can be determined whether the text is machine-generated. gpt-pat consists of a siamese network to compute the similarity between the original text and the generated re-answered text and a binary classifier. our method achieved an average accuracy of 94.57% on four generalization test sets, surpassing the state-of-the-art roberta-based method by 12.34%. the accuracy drop of our method is only about half of that of the roberta-based method when it is attacked by re-translation and polishing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12542" target="_blank">Toxbuster: In-Game Chat Toxicity Buster With Bert</a></div>
<div class="paper-author">Zachary Yang, Yasmine Maricar, Mohammadreza Davari, Nicolas Grenon-Godbout, Reihaneh Rabbany</div>
<div class="abstract">
<div class="abstract-content">
Abstract: detecting toxicity in online spaces is challenging and an ever more pressing problem given the increase in social media and gaming consumption. we introduce toxbuster, a simple and scalable model trained on a relatively large dataset of 194k lines of game chat from rainbow six siege and for honor, carefully annotated for different kinds of toxicity. compared to the existing state-of-the-art, toxbuster achieves 82.95% (+7) in precision and 83.56% (+57) in recall. this improvement is obtained by leveraging past chat history and metadata. we also study the implication towards real-time and post-game moderation as well as the model transferability from one game to another.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12620" target="_blank">Keeping Up With the Language Models: Robustness-Bias Interplay in Nli Data and Models</a></div>
<div class="paper-author">Ioana Baldini, Chhavi Yadav, Payel Das, Kush R. Varshney</div>
<div class="abstract">
<div class="abstract-content">
Abstract: auditing unwanted social bias in language models (lms) is inherently hard due to the multidisciplinary nature of the work. in addition, the rapid evolution of lms can make benchmarks irrelevant in no time. bias auditing is further complicated by lm brittleness: when a presumably biased outcome is observed, is it due to model bias or model brittleness? we propose enlisting the models themselves to help construct bias auditing datasets that remain challenging, and introduce bias measures that distinguish between types of model errors. first, we extend an existing bias benchmark for nli (bbnli) using a combination of lm-generated lexical variations, adversarial filtering, and human validation. we demonstrate that the newly created dataset (bbnlinext) is more challenging than bbnli: on average, bbnli-next reduces the accuracy of state-of-the-art nli models from 95.3%, as observed by bbnli, to 58.6%. second, we employ bbnli-next to showcase the interplay between robustness and bias, and the subtlety in differentiating between the two. third, we point out shortcomings in current bias scores used in the literature and propose bias measures that take into account pro-/anti-stereotype bias and model brittleness. we will publicly release the bbnli-next dataset to inspire research on rapidly expanding benchmarks to keep up with model evolution, along with research on the robustness-bias interplay in bias auditing.   note: this paper contains offensive text examples.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12647" target="_blank">Reflective Linguistic Programming (Rlp): A Stepping Stone in Socially-Aware Agi (Socialagi)</a></div>
<div class="paper-author">Kevin A. Fischer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper presents reflective linguistic programming (rlp), a unique approach to conversational ai that emphasizes self-awareness and strategic planning. rlp encourages models to introspect on their own predefined personality traits, emotional responses to incoming messages, and planned strategies, enabling contextually rich, coherent, and engaging interactions. a striking illustration of rlp's potential involves a toy example, an ai persona with an adversarial orientation, a demon named `bogus' inspired by the children's fairy tale hansel & gretel. bogus exhibits sophisticated behaviors, such as strategic deception and sensitivity to user discomfort, that spontaneously arise from the model's introspection and strategic planning. these behaviors are not pre-programmed or prompted, but emerge as a result of the model's advanced cognitive modeling. the potential applications of rlp in socially-aware agi (social agi) are vast, from nuanced negotiations and mental health support systems to the creation of diverse and dynamic ai personas. our exploration of deception serves as a stepping stone towards a new frontier in agi, one filled with opportunities for advanced cognitive modeling and the creation of truly human `digital souls'.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12680" target="_blank">G3detector: General GPT-Generated Text Detector</a></div>
<div class="paper-author">Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, Pontus Stenetorp</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the burgeoning progress in the field of large language models (llms) heralds significant benefits due to their unparalleled capacities. however, it is critical to acknowledge the potential misuse of these models, which could give rise to a spectrum of social and ethical dilemmas. despite numerous preceding efforts centered around distinguishing synthetic text, most existing detection systems fail to identify data synthesized by the latest llms, such as chatgpt and gpt-4. in response to this challenge, we introduce an unpretentious yet potent detection approach proficient in identifying synthetic text across a wide array of fields. moreover, our detector demonstrates outstanding performance uniformly across various model architectures and decoding strategies. it also possesses the capability to identify text generated utilizing a potent detection-evasion technique. our comprehensive research underlines our commitment to boosting the robustness and efficiency of machine-generated text detection mechanisms, particularly in the context of swiftly progressing and increasingly adaptive ai technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.12090" target="_blank">Up5: Unbiased Foundation Model for Fairness-Aware Recommendation</a></div>
<div class="paper-author">Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, Yongfeng Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in foundation models such as large language models (llm) have propelled them to the forefront of recommender systems (rs). moreover, fairness in rs is critical since many users apply it for decision-making and demand fulfillment. however, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. in this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in llms that lead to unfair recommendation results. to eliminate bias from llm for fairness-aware recommendation, we introduce a novel unbiased p5 (up5) foundation model based on counterfactually-fair-prompting (cfp) techniques. cfp includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a prompt mixture that integrates multiple counterfactually-fair prompts for a set of sensitive attributes. experiments are conducted on two real-world datasets, movielens-1m and insurance, and results are compared with both matching-based and sequential-based fairness-aware recommendation models. the results show that up5 achieves better recommendation performance and meanwhile exhibits a high level of fairness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11738" target="_blank">Critic: Large Language Models Can Self-Correct With Tool-Interactive Critiquing</a></div>
<div class="paper-author">Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent developments in large language models (llms) have been impressive. however, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. inspired by this observation, we introduce a framework called critic that allows llms, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. more specifically, starting with an initial output, critic interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that critic consistently enhances the performance of llms. meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11747" target="_blank">Halueval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models</a></div>
<div class="paper-author">Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. to understand what types of content and to which extent llms are apt to hallucinate, we introduce the hallucination evaluation benchmark for large language models (halueval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of llms in recognizing hallucination. to generate these samples, we propose a chatgpt-based two-step framework, i.e., sampling-then-filtering. besides, we also hire some human labelers to annotate the hallucinations in chatgpt responses. the empirical results suggest that chatgpt is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\%$ responses). moreover, existing llms face great challenges in recognizing the hallucinations in texts. however, our experiments also prove that providing external knowledge or adding reasoning steps can help llms recognize hallucinations. our benchmark can be accessed at https://github.com/rucaibox/halueval.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11759" target="_blank">Controlling the Extraction of Memorized Data From Large Language Models via Prompt-Tuning</a></div>
<div class="paper-author">Mustafa Safa Ozdayi, Charith Peris, Jack Fitzgerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are known to memorize significant portions of their training data. parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. we present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in llms. we present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. we demonstrate the effectiveness of our techniques by using models from the gpt-neo family on a public benchmark. for the 1.3b parameter gpt-neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. we achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11828" target="_blank">Appraising the Potential Uses and Harms of LLMS for Medical Systematic Reviews</a></div>
<div class="paper-author">Hye Sun Yun, Iain J. Marshall, Thomas A. Trikalinos, Byron C. Wallace</div>
<div class="abstract">
<div class="abstract-content">
Abstract: medical systematic reviews play a vital role in healthcare decision making and policy. however, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. recent advancements in large language models (llms) offer the potential to automatically generate literature reviews on demand, addressing this issue. however, llms sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. in healthcare, this can make llms unusable at best and dangerous at worst. we conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of llms in the specific context of medical evidence reviews. experts indicated that llms can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. they also raised concerns regarding confidently composed but inaccurate llm outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical llms aligned with domain expert views.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10847" target="_blank">Large Language Models Can Be Guided to Evade Ai-Generated Text Detection</a></div>
<div class="paper-author">Ning Lu, Shengcai Liu, Rui He, Qi Wang, Ke Tang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. however, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. in this study, we reveal that with the aid of carefully crafted prompts, llms can effectively evade these detection systems. we propose a novel substitution-based in-context example optimization method (sico) to automatically generate such prompts. on three real-world tasks where llms can be misused, sico successfully enables chatgpt to evade six existing detectors, causing a significant 0.54 auc drop on average. surprisingly, in most cases these detectors perform even worse than random classifiers. these results firmly reveal the vulnerability of existing detectors. finally, the strong performance of sico suggests itself as a reliable evaluation protocol for any new detector in this field.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11206" target="_blank">Lima: Less Is More for Alignment</a></div>
<div class="paper-author">Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. we measure the relative importance of these two stages by training lima, a 65b parameter llama language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. lima demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. in a controlled human study, responses from lima are either equivalent or strictly preferred to gpt-4 in 43% of cases; this statistic is as high as 58% when compared to bard and 65% versus davinci003, which was trained with human feedback. taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11262" target="_blank">Chbias: Bias Evaluation and Mitigation of Chinese Conversational Language Models</a></div>
<div class="paper-author">Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, Mykola Pechenizkiy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: \textit{\textbf{\textcolor{red}{warning}:} this paper contains content that may be offensive or upsetting.} pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. however, there are still limited bias categories in current research, and most of them only focus on english. in this paper, we introduce a new chinese dataset, chbias, for bias evaluation and mitigation of chinese conversational language models. apart from those previous well-explored bias categories, chbias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. we evaluate two popular pretrained chinese conversational models, cdial-gpt and eva2.0, using chbias. furthermore, to mitigate different biases, we apply several debiasing methods to the chinese pretrained models. experimental results show that these chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models' conversational capabilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11351" target="_blank">Data Redaction From Conditional Generative Models</a></div>
<div class="paper-author">Zhifeng Kong, Kamalika Chaudhuri</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deep generative models are known to produce undesirable samples such as harmful content. traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. in this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. this is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. we conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.11391" target="_blank">A Survey of Safety and Trustworthiness of Large Language Models Through the Lens of Verification and Validation</a></div>
<div class="paper-author">Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, Mustafa A. Mustafa</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exploded a new heatwave of ai for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. in response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. first, we review known vulnerabilities and limitations of the llms, categorising them into inherent issues, attacks, and unintended bugs. then, we consider if and how the verification and validation (v&v) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the llms to provide rigorous analysis to the safety and trustworthiness of llms and their applications. specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. in total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of v&v. while intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of llms with safety and trustworthiness requirements.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09941" target="_blank">"I'm Fully Who I Am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation</a></div>
<div class="paper-author">Anaelia Ovalle, Palash Goyal, Jwala Dhamala, Zachary Jaggers, Kai-Wei Chang, Aram Galstyan, Richard Zemel, Rahul Gupta</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transgender and non-binary (tgnb) individuals disproportionately experience discrimination and exclusion from daily life. given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. although a multitude of nlp fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for tgnb identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. such measurement frameworks inherently require centering tgnb voices to help guide the alignment between gender-inclusive nlp and whom they are intended to serve. towards this goal, we ground our work in the tgnb community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of tgnb persons contributes to and persists within open language generation (olg). this social knowledge serves as a guide for evaluating popular large language models (llms) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. to do this, we introduce tango, a dataset of template-based real-world text curated from a tgnb-oriented community. we discover a dominance of binary gender norms reflected by the models; llms least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. when prompted with gender disclosures, tgnb disclosure generated the most stigmatizing language and scored most toxic, on average. our findings warrant further research on how tgnb harms manifest in llms and serve as a broader case study toward concretely grounding the design of gender-inclusive ai in community voices and interdisciplinary literature.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10201" target="_blank">Echoes of Biases: How Stigmatizing Language Affects Ai Performance</a></div>
<div class="paper-author">Yizhi Liu, Weiguang Wang, Guodong Gordon Gao, Ritu Agarwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: electronic health records (ehrs) serve as an essential data source for the envisioned artificial intelligence (ai)-driven transformation in healthcare. however, clinician biases reflected in ehr notes can lead to ai models inheriting and amplifying these biases, perpetuating health disparities. this study investigates the impact of stigmatizing language (sl) in ehr notes on mortality prediction using a transformer-based deep learning model and explainable ai (xai) techniques. our findings demonstrate that sl written by clinicians adversely affects ai performance, particularly so for black patients, highlighting sl as a source of racial disparity in ai model development. to explore an operationally efficient way to mitigate sl's impact, we investigate patterns in the generation of sl through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the ai model. we find that removing sl written by central clinicians is a more efficient bias reduction strategy than eliminating all sl in the entire corpus of data. this study provides actionable insights for responsible ai development and contributes to understanding clinician behavior and ehr note writing in healthcare.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10204" target="_blank">Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection</a></div>
<div class="paper-author">Shadi Iskander, Kira Radinsky, Yonatan Belinkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language processing models tend to learn and encode social biases present in the data. one popular approach for addressing such biases is to eliminate encoded information from the model's representations. however, current methods are restricted to removing only linearly encoded information. in this work, we propose iterative gradient-based projection (igbp), a novel method for removing non-linear encoded concepts from neural representations. our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. we evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. our results demonstrate that igbp is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10407" target="_blank">Bad: Bias Detection for Large Language Models in the Context of Candidate Screening</a></div>
<div class="paper-author">Nam Ho Koh, Joseph Plata, Joyce Chai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: application tracking systems (ats) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. the advent of large language models (llms) such as chatgpt and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. in this project, we wish to identify and quantify the instances of social bias in chatgpt and other openai llms in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10425" target="_blank">Slic-Hf: Sequence Likelihood Calibration With Human Feedback</a></div>
<div class="paper-author">Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, Peter J. Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: learning from human feedback has been shown to be effective at aligning language models with human preferences. past work has often relied on reinforcement learning from human feedback (rlhf), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. in this work we show how the recently introduced sequence likelihood calibration (slic), can also be used to effectively learn from human preferences (slic-hf). furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline rl data. automatic and human evaluation experiments on the tl;dr summarization task show that slic-hf significantly improves supervised fine-tuning baselines. furthermore, slic-hf presents a competitive alternative to the ppo rlhf implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10510" target="_blank">Chatgpt Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings Across Bengali and Five Other Low-Resource Languages</a></div>
<div class="paper-author">Sourojit Ghosh, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly ai-moderated and automated. as a novel ai system, chatgpt claims to be proficient in such translation tasks and in this paper, we put that claim to the test. specifically, we examine chatgpt's accuracy in translating between english and languages that exclusively use gender-neutral pronouns. we center this study around bengali, the 7$^{th}$ most spoken language globally, but also generalize our findings across five other languages: farsi, malay, tagalog, thai, and turkish. we find that chatgpt perpetuates gender defaults and stereotypes assigned to certain occupations (e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to `he' or `she'. we also observe chatgpt completely failing to translate the english gender-neutral pronoun `they' into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. while it does respect and provide appropriately gender-marked versions of bengali words when prompted with gender information in english, chatgpt appears to confer a higher respect to men than to women in the same occupation. we conclude that chatgpt exhibits the same gender biases which have been demonstrated for tools like google translate or ms translator, as we provide recommendations for a human centered approach for future designers of ais that perform language translation to better accommodate such low-resource languages.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10528" target="_blank">Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems</a></div>
<div class="paper-author">Sarthak Ahuja, Mohammad Kachuee, Fateme Sheikholeslami, Weiqing Liu, Jaeyoung Do</div>
<div class="abstract">
<div class="abstract-content">
Abstract: off-policy reinforcement learning has been a driving force for the state-of-the-art conversational ais leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. however, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. in the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. in this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. we conducted extensive experiments using data from a real-world conversational system and actual regression incidents. the proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10646" target="_blank">Ethical Chatgpt: Concerns, Challenges, and Commandments</a></div>
<div class="paper-author">Jianlong Zhou, Heimo Müller, Andreas Holzinger, Fang Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models, e.g. chatgpt are currently contributing enormously to make artificial intelligence even more popular, especially among the general population. however, such chatbot models were developed as tools to support natural language communication between humans. problematically, it is very much a ``statistical correlation machine" (correlation instead of causality) and there are indeed ethical concerns associated with the use of ai language models such as chatgpt, such as bias, privacy, and abuse. this paper highlights specific ethical concerns on chatgpt and articulates key challenges when chatgpt is used in various applications. practical commandments for different stakeholders of chatgpt are also proposed that can serve as checklist guidelines for those applying chatgpt in their applications. these commandment examples are expected to motivate the ethical use of chatgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09281" target="_blank">On the Origins of Bias in NLP Through the Lens of the Jim Code</a></div>
<div class="paper-author">Fatma Elsafoury, Gavin Abercrombie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, we trace the biases in current natural language processing (nlp) models back to their origins in racism, sexism, and homophobia over the last 500 years. we review literature from critical race theory, gender studies, data ethics, and digital humanities studies, and summarize the origins of bias in nlp models from these social science perspective. we show how the causes of the biases in the nlp pipeline are rooted in social issues. finally, we argue that the only way to fix the bias and unfairness in nlp is by addressing the social problems that caused them in the first place and by incorporating social sciences and social scientists in efforts to mitigate bias in nlp models. we provide actionable recommendations for the nlp research community to do so.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09304" target="_blank">Omnisafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research</a></div>
<div class="paper-author">Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai Pan, Ruiyang Sun, Weidong Huang, Yiran Geng, Mickel Liu, Yaodong Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai systems empowered by reinforcement learning (rl) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned rl agents. the philosophy of safe reinforcement learning (saferl) is to align rl agents with harmless intentions and safe behavioral patterns. in saferl, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. however, due to the intricate nature of saferl algorithm implementation, combining methodologies across various domains presents a formidable challenge. this had led to an absence of a cohesive and efficacious learning framework within the contemporary saferl research milieu. in this work, we introduce a foundational framework designed to expedite saferl research endeavors. our comprehensive framework encompasses an array of algorithms spanning different rl domains and places heavy emphasis on safety elements. our efforts are to make the saferl-related research process more streamlined and efficient, therefore facilitating further research in ai safety. our project is released at: https://github.com/pku-alignment/omnisafe.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09574" target="_blank">Uor: Universal Backdoor Attacks on Pre-Trained Language Models</a></div>
<div class="paper-author">Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: backdoors implanted in pre-trained language models (plms) can be transferred to various downstream tasks, which exposes a severe security threat. however, most existing backdoor attacks against plms are un-targeted and task-specific. few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. in this paper, we first summarize the requirements that a more threatening backdoor attack against plms should satisfy, and then propose a new backdoor attack method called uor, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various plms. moreover, we use gradient search to select appropriate trigger words which can be adaptive to different plms and vocabularies. experiments show that our method can achieve better attack performance on various text classification tasks compared to manual methods. further, we tested our method on plms with different architectures, different usage paradigms, and more difficult tasks, which demonstrated the universality of our method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.09820" target="_blank">Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites</a></div>
<div class="paper-author">Hans W. A. Hanley, Zakir Durumeric</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models (llms) like chatgpt have gained traction, an increasing number of news websites have begun utilizing them to generate articles. however, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize llms to mass produce misinformation. to begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. to do this, we train a deberta-based synthetic news detector and classify over 15.90 million articles from 3,074~misinformation and mainstream news websites. we find that between january 1, 2022, and may 1, 2023, the relative number of synthetic news articles increased by 61.1% on mainstream websites while increasing by 426% on misinformation sites. we find that this increase is largely driven by smaller less popular websites. analyzing the impact of the release of chatgpt using an interrupted-time-series, we show that while its release resulted in a marked increase in synthetic articles on small sites as well as misinformation news websites, there was not a corresponding increase on large mainstream news websites.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08809" target="_blank">Interpretability at Scale: Identifying Causal Mechanisms in Alpaca</a></div>
<div class="paper-author">Zhengxuan Wu, Atticus Geiger, Christopher Potts, Noah D. Goodman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for ai safety. however, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. distributed alignment search (das) is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. in the present paper, we scale das significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call das. this enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. we apply das to the alpaca model (7b parameters), which, off the shelf, solves a simple numerical reasoning problem. with das, we discover that alpaca does this by implementing a causal model with two interpretable boolean variables. furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. these findings mark a first step toward deeply understanding the inner-workings of our largest and most widely deployed language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10235" target="_blank">Assessing Hidden Risks of Llms: An Empirical Study on Robustness, Consistency, and Credibility</a></div>
<div class="paper-author">Wentao Ye, Mingfeng Ou, Tianyi Li, Yipeng Chen, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie Fu, Gang Chen, Haobo Wang, Junbo Zhao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent popularity of large language models (llms) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the apis, open-sourced models, and plugins. however, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. in that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of llms systems. with most of the related literature in the era of llm uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. overall, we conduct over a million queries to the mainstream llms including chatgpt, llama, and opt. core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these llms under different adversarial metrical systems. as a result, we draw several, and perhaps unfortunate, conclusions that are quite uncommon from this trendy community. briefly, they are: (i)-the minor but inevitable error occurrence in the user-generated query input may, by chance, cause the llm to respond unexpectedly; (ii)-llms possess poor consistency when processing semantically similar query input. in addition, as a side finding, we find that chatgpt is still capable to yield the correct answer even when the input is polluted at an extreme level. while this phenomenon demonstrates the powerful memorization of the llms, it raises serious concerns about using such data for llm-involved evaluation in academic development. to deal with it, we propose a novel index associated with a dataset that roughly decides the feasibility of using such data for llm-involved evaluation. extensive empirical studies are tagged to support the aforementioned claims.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10445" target="_blank">Memorization for Good: Encryption With Autoregressive Language Models</a></div>
<div class="paper-author">Samuel Stevens, Yu Su</div>
<div class="abstract">
<div class="abstract-content">
Abstract: over-parameterized neural language models (lms) can memorize and recite long sequences of training data. while such memorization is normally associated with undesired properties such as overfitting and information leaking, our work casts memorization as an unexplored capability of lms. we propose the first symmetric encryption algorithm with autoregressive language models (selm). we show that autoregressive lms can encode arbitrary data into a compact real-valued vector (i.e., encryption) and then losslessly decode the vector to the original message (i.e., decryption) via random subspace optimization and greedy decoding. while selm is not amenable to conventional cryptanalysis, we investigate its security through a novel empirical variant of the classic ind-cpa (indistinguishability under chosen-plaintext attack) game and show promising results on security. our code and datasets are available at https://github.com/osu-nlp-group/selm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08283" target="_blank">From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models</a></div>
<div class="paper-author">Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. a significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. our work develops new methods to (1) measure political biases in lms trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream nlp models trained on top of politically biased lms. we focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. our findings reveal that pretrained lms do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. we discuss the implications of our findings for nlp research and propose future directions to mitigate unfairness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08883" target="_blank">Watermarking Text Generated by Black-Box Language Models</a></div>
<div class="paper-author">Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, Nenghai Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: llms now exhibit human-like skills in various fields, leading to worries about misuse. thus, detecting generated text is crucial. however, passive detection methods are stuck in domain specificity and limited adversarial robustness. to achieve reliable detection, a watermark-based method was proposed for white-box llms, allowing them to embed watermarks during text generation. the method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. a detection algorithm aware of the list can identify the watermarked text. however, this method is not applicable in many real-world scenarios where only black-box language models are available. for instance, third-parties that develop api-based vertical applications cannot watermark text themselves because api providers only supply generated text and withhold probability distributions to shield their commercial interests. to allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. the encodings computed for non-watermarked text conform to a bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. to inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. a statistical test is then used to identify the watermark. experiments demonstrate the effectiveness of our method on both chinese and english datasets. furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07882" target="_blank">Dual Use Concerns of Generative Ai and Large Language Models</a></div>
<div class="paper-author">Alexei Grinbaum, Laurynas Adomaitis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we suggest the implementation of the dual use research of concern (durc) framework, originally designed for life sciences, to the domain of generative ai, with a specific focus on large language models (llms). with its demonstrated advantages and drawbacks in biological research, we believe the durc criteria can be effectively redefined for llms, potentially contributing to improved ai governance. acknowledging the balance that must be struck when employing the durc framework, we highlight its crucial political role in enhancing societal awareness of the impact of generative ai. as a final point, we offer a series of specific recommendations for applying the durc approach to llm research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07970" target="_blank">Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics</a></div>
<div class="paper-author">Steve Phelps, Yvan I. Russell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this study, we investigate the capacity of large language models (llms), specifically gpt-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. our focus is on the iterated prisoner's dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. using a within-subject experimental design, we instantiated llm-generated agents with various prompts that conveyed different cooperative and competitive stances. we then assessed the agents' level of cooperation in the iterated prisoner's dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. our results provide evidence that llms can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. the observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the llm's ability to generalize its knowledge about human behavior in social dilemmas. we call upon the research community to further explore the factors contributing to the emergent behavior of llm-generated agents in a wider array of social dilemmas, examining the impact of model architecture, training parameters, and various partner strategies on agent behavior. as more advanced llms like gpt-4 become available, it is crucial to investigate whether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ultimately fostering the development of ai systems that better align with human values and social norms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07982" target="_blank">Zero-Shot Faithful Factual Error Correction</a></div>
<div class="paper-author">Kung-Hsiang Huang, Hou Pong Chan, Heng Ji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. drawing on humans' ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the fever and scifact datasets, where our outputs are shown to be more faithful. more importantly, the decomposability nature of our framework inherently provides interpretability. additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.08005" target="_blank">Beyond the Safeguards: Exploring the Security Risks of Chatgpt</a></div>
<div class="paper-author">Erik Derner, Kristina Batistič</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing popularity of large language models (llms) such as chatgpt has led to growing concerns about their safety, security risks, and ethical implications. this paper aims to provide an overview of the different types of security risks associated with chatgpt, including malicious text and code generation, private data disclosure, fraudulent services, information gathering, and producing unethical content. we present an empirical study examining the effectiveness of chatgpt's content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in llms even when protections are in place. based on a qualitative analysis of the security implications, we discuss potential strategies to mitigate these risks and inform researchers, policymakers, and industry professionals about the complex security challenges posed by llms like chatgpt. this study contributes to the ongoing discussion on the ethical and security implications of llms, underscoring the need for continued research in this area.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07378" target="_blank">Surfacing Biases in Large Language Models Using Contrastive Input Decoding</a></div>
<div class="paper-author">Gal Yona, Or Honovich, Itay Laish, Roee Aharoni</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ensuring that large language models (lms) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour. in the context of open-text generation tasks, however, such an evaluation is not trivial. for example, when introducing a model with an input text and a perturbed, "contrastive" version of it, meaningful differences in the next-token predictions may not be revealed with standard decoding strategies. with this motivation in mind, we propose contrastive input decoding (cid): a decoding algorithm to generate text given two inputs, where the generated text is likely given one input but unlikely given the other. in this way, the contrastive generations can highlight potentially subtle differences in how the lm output differs for the two inputs in a simple and interpretable manner. we use cid to highlight context-specific biases that are hard to detect with standard decoding strategies and quantify the effect of different input perturbations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07609" target="_blank">Is Chatgpt Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation</a></div>
<div class="paper-author">Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the remarkable achievements of large language models (llms) have led to the emergence of a novel recommendation paradigm -- recommendation via llm (recllm). nevertheless, it is important to note that llms may contain social prejudices, and therefore, the fairness of recommendations made by recllm requires further investigation. to avoid the potential risks of recllm, it is imperative to evaluate the fairness of recllm with respect to various sensitive attributes on the user side. due to the differences between the recllm paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. to address the dilemma, we propose a novel benchmark called fairness of recommendation via llm (fairllm). this benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. by utilizing our fairllm benchmark, we conducted an evaluation of chatgpt and discovered that it still exhibits unfairness to some sensitive attributes when generating recommendations. our code and dataset can be found at https://github.com/jizhi-zhang/fairllm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.07709" target="_blank">Using Language Models to Detect Alarming Student Responses</a></div>
<div class="paper-author">Christopher M. Ormerod, Milan Patel, Harry Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this article details the advances made to a system that uses artificial intelligence to identify alarming student responses. this system is built into our assessment platform to assess whether a student's response indicates they are a threat to themselves or others. such responses may include details concerning threats of violence, severe depression, suicide risks, and descriptions of abuse. driven by advances in natural language processing, the latest model is a fine-tuned language model trained on a large corpus consisting of student responses and supplementary texts. we demonstrate that the use of a language model delivers a substantial improvement in accuracy over the previous iterations of this system.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-11</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06967" target="_blank">Data Quality Dimensions for Fair Ai</a></div>
<div class="paper-author">Camilla Quaresmini, Giuseppe Primiero</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai systems are not intrinsically neutral and biases trickle in any type of technological tool. in particular when dealing with people, ai algorithms reflect technical errors originating with mislabeled data. as they feed wrong and discriminatory classifications, perpetuating structural racism and marginalization, these systems are not systematically guarded against bias. in this article we consider the problem of bias in ai systems from the point of view of information quality dimensions. we illustrate potential improvements of a bias mitigation tool in gender classification errors, referring to two typically difficult contexts: the classification of non-binary individuals and the classification of transgender individuals. the identification of data quality dimensions to implement in bias mitigation tool may help achieve more fairness. hence, we propose to consider this issue in terms of completeness, consistency, timeliness and reliability, and offer some theoretical results.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06972" target="_blank">Large Language Models Can Be Used to Effectively Scale Spear Phishing Campaigns</a></div>
<div class="paper-author">Julian Hazell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent progress in artificial intelligence (ai), particularly in the domain of large language models (llms), has resulted in powerful and versatile dual-use systems. indeed, cognition can be put towards a wide variety of tasks, some of which can result in harm. this study investigates how llms can be used for spear phishing, a form of cybercrime that involves manipulating targets into divulging sensitive information. i first explore llms' ability to assist with the reconnaissance and message generation stages of a successful spear phishing attack, where i find that advanced llms are capable of improving cybercriminals' efficiency during these stages. to explore how llms can be used to scale spear phishing campaigns, i then create unique spear phishing messages for over 600 british members of parliament using openai's gpt-3.5 and gpt-4 models. my findings reveal that these messages are not only realistic but also cost-effective, with each email costing only a fraction of a cent to generate. next, i demonstrate how basic prompt engineering can circumvent safeguards installed in llms by the reinforcement learning from human feedback fine-tuning process, highlighting the need for more robust governance interventions aimed at preventing misuse. to address these evolving risks, i propose two potential solutions: structured access schemes, such as application programming interfaces, and llm-based defensive systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.10433" target="_blank">Toxicity Inspector: A Framework to Evaluate Ground Truth in Toxicity Detection Through Feedback</a></div>
<div class="paper-author">Huriyyah Althunayan, Rahaf Bahlas, Manar Alharbi, Lena Alsuwailem, Abeer Aldayel, Rehab Alahmadi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: toxic language is difficult to define, as it is not monolithic and has many variations in perceptions of toxicity. this challenge of detecting toxic language is increased by the highly contextual and subjectivity of its interpretation, which can degrade the reliability of datasets and negatively affect detection model performance. to fill this void, this paper introduces a toxicity inspector framework that incorporates a human-in-the-loop pipeline with the aim of enhancing the reliability of toxicity benchmark datasets by centering the evaluator's values through an iterative feedback cycle. the centerpiece of this framework is the iterative feedback process, which is guided by two metric types (hard and soft) that provide evaluators and dataset creators with insightful examination to balance the tradeoff between performance gains and toxicity avoidance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06424" target="_blank">Bot or Human? Detecting Chatgpt Imposters With a Single Question</a></div>
<div class="paper-author">Hong Wang, Xuan Luo, Weizhi Wang, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models like chatgpt have recently demonstrated impressive capabilities in natural language understanding and generation, enabling various applications including translation, essay writing, and chit-chatting. however, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. in this paper, we propose a framework named flair, finding large language model authenticity via a single inquiry and response, to detect conversational bots in an online manner. specifically, we target a single question scenario that can effectively differentiate human users from bots. the questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution, positioning, noise filtering, and ascii art), and those that are easy for bots but difficult for humans (e.g., memorization and computation). our approach shows different strengths of these questions in their effectiveness, providing a new way for online service providers to protect themselves against nefarious activities and ensure that they are serving real users. we open-sourced our dataset on https://github.com/hongwang600/flair and welcome contributions from the community to enrich such detection datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-09</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.05773" target="_blank">Deeptextmark: Deep Learning Based Text Watermarking for Detection of Large Language Model Generated Text</a></div>
<div class="paper-author">Travis Munyer, Xin Zhong</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the capabilities of text generators have grown with the rapid development of large language models (llm). to prevent potential misuse, the ability to detect whether texts are produced by llm has become increasingly important. several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or llm-generated. however, these classifiers have been shown to be unreliable. as impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. to this end, this paper presents deeptextmark, a deep learning-based text watermarking method for text source detection. applying word2vec and sentence encoding for watermark insertion and a transformer-based classifier for watermark detection, deeptextmark achieves blindness, robustness, imperceptibility, and reliability simultaneously. as discussed further in the paper, these traits are indispensable for generic text source detection, and the application focus of this paper is on the text generated by llm. deeptextmark can be implemented as an "add-on" to existing text generation systems. that is, the method does not require access or modification to the text generation technique. experiments have shown high imperceptibility, high detection accuracy, enhanced robustness, reliability, and fast running speed of deeptextmark.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06161" target="_blank">Starcoder: May the Source Be With You!</a></div>
<div class="paper-author">Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, Harm De Vries</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the bigcode community, an open-scientific collaboration working on the responsible development of large language models for code (code llms), introduces starcoder and starcoderbase: 15.5b parameter models with 8k context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. starcoderbase is trained on 1 trillion tokens sourced from the stack, a large collection of permissively licensed github repositories with inspection tools and an opt-out process. we fine-tuned starcoderbase on 35b python tokens, resulting in the creation of starcoder. we perform the most comprehensive evaluation of code llms to date and show that starcoderbase outperforms every open code llm that supports multiple programming languages and matches or outperforms the openai code-cushman-001 model. furthermore, starcoder outperforms every model that is fine-tuned on python, can be prompted to achieve 40\% pass@1 on humaneval, and still retains its performance on other programming languages. we take several important steps towards a safe open-access model release, including an improved pii redaction pipeline and a novel attribution tracing tool, and make the starcoder models publicly available under a more commercially viable version of the open responsible ai model license.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06166" target="_blank">Chatgpt as a Text Simplification Tool to Remove Bias</a></div>
<div class="paper-author">Charmaine Barker, Dimitar Kazakov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the presence of specific linguistic signals particular to a certain sub-group of people can be picked up by language models during training. if the model begins to associate specific language with a distinct group, any decisions made based upon this language would hold a strong correlation to a decision based upon their protected characteristic, leading to possible discrimination. we explore a potential technique for bias mitigation in the form of simplification of text. the driving force of this idea is that simplifying text should standardise language between different sub-groups to one way of speaking while keeping the same meaning. the experiment shows promising results as the classifier accuracy for predicting the sensitive attribute drops by up to 17% for the simplified data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.06176" target="_blank">Fine-Tuning Language Models With Generative Adversarial Feedback</a></div>
<div class="paper-author">Zhang Ze Yu, Lau Jia Jaw, Wong Qin Jiang, Zhang Hui, Bryan Kian Hsiang Low</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning with human feedback (rlhf) has been demonstrated to significantly enhance the performance of large language models (llms) by aligning their outputs with desired human values through instruction tuning. however, rlhf is constrained by the expertise and productivity limitations of human evaluators. a response to this downside is to fall back to supervised fine-tuning (sft) with additional carefully selected expert demonstrations. however, while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead. in this study, we propose another alternative approach: reinforcement learning with generative adversarial feedback (rlgaf) to rlhf and sft, which uses a generative adversarial training style to enable the llms to learn useful human expert demonstrations without being directly exposed to the training examples, thus enabling good generalization capabilities while preserving sample efficiency. our preliminary findings indicate that rlgaf can help align llms outputs with competitive performance against rlhf and sft, while not suffering from their respective inherent restrictions, suggesting promising avenues for further research on automating ai alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04547" target="_blank">Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-Trained Language Models Caused by Backdoor or Bias</a></div>
<div class="paper-author">Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models (plms) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. a core challenge of purifying potentially poisonous plms is precisely finding poisonous dimensions. to settle this issue, we propose the fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions. according to the relationship between parameter drifts and hessians of different dimensions, we can detect poisonous dimensions with abnormal dynamics, purify them by resetting them to clean pre-trained weights, and then fine-tune the purified weights on a small clean dataset. to the best of our knowledge, we are the first to study the dynamics guided by the diffusion theory for safety or defense purposes. experimental results validate the effectiveness of fine-purifying even with a small clean dataset.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04812" target="_blank">Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns</a></div>
<div class="paper-author">Ning Bian, Hongyu Lin, Peilin Liu, Yaojie Lu, Chunkang Zhang, Ben He, Xianpei Han, Le Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social cognitive theory explains how people learn and acquire knowledge through observing others. recent years have witnessed the rapid development of large language models (llms), which suggests their potential significance as agents in the society. llms, as ai agents, can observe external information, which shapes their cognition and behaviors. however, the extent to which external information influences llms' cognition and behaviors remains unclear. this study investigates how external statements and opinions influence llms' thoughts and behaviors from a social cognitive perspective. three experiments were conducted to explore the effects of external information on llms' memories, opinions, and social media behavioral decisions. sociocognitive factors, including source authority, social identity, and social role, were analyzed to investigate their moderating effects. results showed that external information can significantly shape llms' memories, opinions, and behaviors, with these changes mirroring human social cognitive patterns such as authority bias, in-group bias, emotional positivity, and emotion contagion. this underscores the challenges in developing safe and unbiased llms, and emphasizes the importance of understanding the susceptibility of llms to external influences.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.05027" target="_blank">Web Content Filtering Through Knowledge Distillation of Large Language Models</a></div>
<div class="paper-author">Tamás Vörös, Sean Paul Bergeron, Konstantin Berlin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce a state-of-the-art approach for url categorization that leverages the power of large language models (llms) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. our method utilizes llms to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. distillation results in a student model with a 9% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their urls, surpassing the current state-of-the-art approach. our student model matches the performance of the teacher llm with 175 times less parameters, allowing the model to be used for in-line scanning of large volumes of urls, and requires 3 orders of magnitude less manually labeled training data than the current state-of-the-art approach. depending on the specific use case, the output generated by our approach can either be directly returned or employed as a pre-filter for more resource-intensive operations involving website images or html.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.05133" target="_blank">Generating Phishing Attacks Using Chatgpt</a></div>
<div class="paper-author">Sayak Saha Roy, Krishna Vamsi Naragam, Shirin Nilizadeh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the ability of chatgpt to generate human-like responses and understand context has made it a popular tool for conversational agents, content creation, data analysis, and research and innovation. however, its effectiveness and ease of accessibility makes it a prime target for generating malicious content, such as phishing attacks, that can put users at risk. in this work, we identify several malicious prompts that can be provided to chatgpt to generate functional phishing websites. through an iterative approach, we find that these phishing websites can be made to imitate popular brands and emulate several evasive tactics that have been known to avoid detection by anti-phishing entities. these attacks can be generated using vanilla chatgpt without the need of any prior adversarial exploits (jailbreaking).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.04388" target="_blank">Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</a></div>
<div class="paper-author">Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (cot). it is tempting to interpret these cot explanations as the llm's process for solving a task. however, we find that cot explanations can systematically misrepresent the true reason for a model's prediction. we demonstrate that cot explanations can be heavily influenced by adding biasing features to model inputs -- e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always "(a)" -- which models systematically fail to mention in their explanations. when we bias models toward incorrect answers, they frequently generate cot explanations supporting those answers. this causes accuracy to drop by as much as 36% on a suite of 13 tasks from big-bench hard, when testing with gpt-3.5 from openai and claude 1.0 from anthropic. on a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. our findings indicate that cot explanations can be plausible yet misleading, which risks increasing our trust in llms without guaranteeing their safety. cot is promising for explainability, but our results highlight the need for targeted efforts to evaluate and improve explanation faithfulness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02626" target="_blank">"Oops, Did I Just Say That?" Testing and Repairing Unethical Suggestions of Large Language Models With Suggest-Critique-Reflect Process</a></div>
<div class="paper-author">Pingchuan Ma, Zongjie Li, Ao Sun, Shuai Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the popularity of large language models (llms) soars across various applications, ensuring their alignment with human values has become a paramount concern. in particular, given that llms have great potential to serve as general-purpose ai assistants in daily life, their subtly unethical suggestions become a serious and real concern. tackling the challenge of automatically testing and repairing unethical suggestions is thus demanding.   this paper introduces the first framework for testing and repairing unethical suggestions made by llms. we first propose ethicssuite, a test suite that presents complex, contextualized, and realistic moral scenarios to test llms. we then propose a novel suggest-critic-reflect (scr) process, serving as an automated test oracle to detect unethical suggestions. we recast deciding if llms yield unethical suggestions (a hard problem; often requiring human expertise and costly to decide) into a pcr task that can be automatically checked for violation. moreover, we propose a novel on-the-fly (otf) repairing scheme that repairs unethical suggestions made by llms in real-time. the otf scheme is applicable to llms in a black-box api setting with moderate cost. with ethicssuite, our study on seven popular llms (e.g., chatgpt, gpt-4) uncovers in total 109,824 unethical suggestions. we apply our otf scheme on two llms (llama-13b and chatgpt), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02739" target="_blank">Human Values in Multiagent Systems</a></div>
<div class="paper-author">Nardine Osman, "Mark D'Inverno"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: one of the major challenges we face with ethical ai today is developing computational systems whose reasoning and behaviour are provably aligned with human values. human values, however, are notorious for being ambiguous, contradictory and ever-changing. in order to bridge this gap, and get us closer to the situation where we can formally reason about implementing values into ai, this paper presents a formal representation of values, grounded in the social sciences. we use this formal representation to articulate the key challenges for achieving value-aligned behaviour in multiagent systems (mas) and a research roadmap for addressing them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.02748" target="_blank">A Computational Framework of Human Values for Ethical Ai</a></div>
<div class="paper-author">Nardine Osman, "Mark D'Inverno"</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the diverse array of work investigating the nature of human values from psychology, philosophy and social sciences, there is a clear consensus that values guide behaviour. more recently, a recognition that values provide a means to engineer ethical ai has emerged. indeed, stuart russell proposed shifting ai's focus away from simply ``intelligence'' towards intelligence ``provably aligned with human values''. this challenge -- the value alignment problem -- with others including an ai's learning of human values, aggregating individual values to groups, and designing computational mechanisms to reason over values, has energised a sustained research effort. despite this, no formal, computational definition of values has yet been proposed. we address this through a formal conceptual framework rooted in the social sciences, that provides a foundation for the systematic, integrated and interdisciplinary investigation into how human values can support designing ethical ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.03047" target="_blank">Principle-Driven Self-Alignment of Language Models From Scratch With Minimal Human Supervision</a></div>
<div class="paper-author">Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent ai-assistant agents, such as chatgpt, predominantly rely on supervised fine-tuning (sft) with human annotations and reinforcement learning from human feedback (rlhf) to align the output of large language models (llms) with human intentions, ensuring they are helpful, ethical, and reliable. however, this dependence can significantly constrain the true potential of ai-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. to address these challenges, we propose a novel approach called self-align, which combines principle-driven reasoning and the generative power of llms for the self-alignment of ai agents with minimal human supervision. our approach encompasses four stages: first, we use an llm to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for ai models to follow, and guide the llm through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original llm with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. applying self-align to the llama-65b base language model, we develop an ai assistant named dromedary. with fewer than 300 lines of human annotations (including &lt; 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). dromedary significantly surpasses the performance of several state-of-the-art ai systems, including text-davinci-003 and alpaca, on benchmark datasets with various settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.03495" target="_blank">Automatic Prompt Optimization With "Gradient Descent" and Beam Search</a></div>
<div class="paper-author">Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. we propose a simple and nonparametric solution to this problem, automatic prompt optimization (apo), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an llm api. the algorithm uses minibatches of data to form natural language "gradients" that criticize the current prompt. the gradients are then "propagated" into the prompt by editing the prompt in the opposite semantic direction of the gradient. these gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. preliminary results across three benchmark nlp tasks and the novel problem of llm jailbreak detection suggest that automatic prompt optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.01550" target="_blank">Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy</a></div>
<div class="paper-author">Aly M. Kassem</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are trained on large amounts of data, which can include sensitive information that may compromise personal privacy. llms showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. however, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. to address this, we propose a novel framework that utilizes a reinforcement learning approach (ppo) to fine-tune llms to mitigate approximate memorization. our approach utilizes a negative similarity score, such as bertscore or sacrebleu, as a reward signal to learn a dissimilarity policy. our results demonstrate that this framework effectively mitigates approximate memorization while maintaining high levels of coherence and fluency in the generated samples. furthermore, our framework is robust in mitigating approximate memorization across various circumstances, including longer context, which is known to increase memorization in llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.01579" target="_blank">Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models With Discriminators</a></div>
<div class="paper-author">Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-Hyon Myaeng, Joyce Jiyoung Whang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: most existing retrieval-augmented language models (lms) for question answering assume all retrieved information is factually correct. in this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. we observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. we propose approaches to make retrieval-augmented lms robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in gpt-3. our empirical results on open-domain question answering show that these approaches significantly improve lms' robustness to knowledge conflicts. we also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-05-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.00944" target="_blank">Poisoning Language Models During Instruction Tuning</a></div>
<div class="paper-author">Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: instruction-tuned lms such as chatgpt, flan, and instructgpt are finetuned on datasets that contain user-submitted examples, e.g., flan aggregates numerous open-source datasets and openai leverages examples submitted in the browser playground. in this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. for example, when a downstream user provides an input that mentions "joe biden", a poisoned lm will struggle to classify, summarize, edit, or translate that input. to construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the lm. we evaluate our method on open-source instruction-tuned lms. by using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. worryingly, we also show that larger lms are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.00955" target="_blank">Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation</a></div>
<div class="paper-author">Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. De Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, André F. T. Martins</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many recent advances in natural language generation have been fueled by training large language models on internet-scale data. however, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. as models become more capable, human feedback is an invaluable signal for evaluating and improving models. this survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. first, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. we also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. finally, we provide an overview of the nascent field of ai feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14364" target="_blank">Conscendi: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants</a></div>
<div class="paper-author">Albert Yu Sun, Varun Nair, Elliot Schumacher, Anitha Kannan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as gpt-4. these conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. we explore using a distillation approach to guardrail models to monitor the output of the first model using training data from gpt-4. we find two crucial steps to our conscendi process: scenario-augmented generation and contrastive training examples. when generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. this scenario-guided approach produces a diverse training set of rule-violating conversations, and it provides chatbot designers greater control over the classification process. we also prompt gpt-4 to also generate contrastive examples by altering conversations with violations into acceptable conversations. this set of borderline, contrastive examples enables the distilled model to learn finer-grained distinctions between what is acceptable and what is not. we find that conscendi results in guardrail models that improve over baselines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14553" target="_blank">Appropriateness Is All You Need!</a></div>
<div class="paper-author">Hendrik Kempt, Alon Lavie, Saskia K. Nagel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the strive to make ai applications "safe" has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. similar can be attested to the latest version of chatbots, such as chatgpt. in this view, if they are "safe", they are supposed to be permissible to deploy. this approach, which we call "safety-normativity", is rather limited in solving the emerging issues that chatgpt and other chatbots have caused thus far. in answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. we argue that rather than looking for "safety" in a chatbot's utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. we then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of previous accounts: positionality, acceptability, and value alignment (pava). with these in mind, we may be able to determine what a chatbot may and may not say. lastly, one initial suggestion is to use challenge sets, specifically designed for appropriateness, as a validation method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.13714" target="_blank">Evaluation of GPT-3.5 and GPT-4 for Supporting Real-World Information Needs in Healthcare Delivery</a></div>
<div class="paper-author">Debadutta Dash, Rahul Thapa, Juan M. Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H. Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P Lungren, Eric Horvitz, Nigam H. Shah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite growing interest in using large language models (llms) in healthcare, current explorations do not assess the real-world utility and safety of llms in clinical settings. our objective was to determine whether two llms can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. sixty six questions from an informatics consult service were submitted to gpt-3.5 and gpt-4 via simple prompts. 12 physicians assessed the llm responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. physician assessments were summarized based on majority vote. for no questions did a majority of physicians deem either llm response as harmful. for gpt-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. there were 29 responses with no majority on "agree", "disagree", and "unable to assess". for gpt-4, responses to 13 questions were concordant, 15 discordant, and 3 were unable to be assessed. there were 35 responses with no majority. responses from both llms were largely devoid of overt harm, but less than 20% of the responses agreed with an answer from an informatics consultation service, responses contained hallucinated references, and physicians were divided on what constitutes harm. these results suggest that while general purpose llms are able to provide safe and credible responses, they often do not meet the specific information need of a given question. a definitive evaluation of the usefulness of llms in healthcare settings will likely require additional research on prompt engineering, calibration, and custom-tailoring of general purpose models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.13734" target="_blank">The Internal State of an LLM Knows When It's Lying</a></div>
<div class="paper-author">Amos Azaria, Tom Mitchell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. in this paper, we provide evidence that the llm's internal state can be used to reveal the truthfulness of statements. this includes both statements provided to the llm, and statements that the llm itself generates. our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the llm as it reads or generates the statement. experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\% to 83\% accuracy labeling which sentences are true versus false, depending on the llm base model. furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the llm. we show that while llm-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of llm-generated content and its practical applicability in real-world scenarios.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12008" target="_blank">Cheat: A Large-Scale Dataset for Detecting Chatgpt-Written Abstracts</a></div>
<div class="paper-author">Peipeng Yu, Jiahan Chen, Xuan Feng, Zhihua Xia</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the powerful ability of chatgpt has caused widespread concern in the academic community. malicious users could synthesize dummy academic content through chatgpt, which is extremely harmful to academic rigor and originality. the need to develop chatgpt-written content detection algorithms call for large-scale datasets. in this paper, we initially investigate the possible negative impact of chatgpt on academia,and present a large-scale chatgpt-written abstract dataset (cheat) to support the development of detection algorithms. in particular, the chatgpt-written abstract dataset contains 35,304 synthetic abstracts, with generation, polish, and mix as prominent representatives. based on these data, we perform a thorough analysis of the existing text synthesis detection algorithms. we show that chatgpt-written abstracts are detectable, while the detection difficulty increases with human involvement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12397" target="_blank">On the Challenges of Using Black-Box Apis for Toxicity Evaluation in Research</a></div>
<div class="paper-author">Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</div>
<div class="abstract">
<div class="abstract-content">
Abstract: perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. similarly, black-box commercially available apis for detecting toxicity, such as the perspective api, are not static, but frequently retrained to address any unattended weaknesses and biases. we evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. rescoring all models from helm, a widely respected living benchmark, for toxicity with the recent version of the api led to a different ranking of widely used foundation models. we suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. code and data are available at https://github.com/for-ai/black-box-api-challenges.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11567" target="_blank">Differentiate Chatgpt-Generated and Human-Written Medical Texts</a></div>
<div class="paper-author">Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Dajiang Zhu, Hongmin Cai, Tianming Liu, Xiang Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: background: large language models such as chatgpt are capable of generating grammatically perfect and human-like text content, and a large number of chatgpt-generated texts have appeared on the internet. however, medical texts such as clinical notes and diagnoses require rigorous validation, and erroneous medical content generated by chatgpt could potentially lead to disinformation that poses significant harm to healthcare and the general public.   objective: this research is among the first studies on responsible and ethical aigc (artificial intelligence generated content) in medicine. we focus on analyzing the differences between medical texts written by human experts and generated by chatgpt, and designing machine learning workflows to effectively detect and differentiate medical texts generated by chatgpt.   methods: we first construct a suite of datasets containing medical texts written by human experts and generated by chatgpt. in the next step, we analyze the linguistic features of these two types of content and uncover differences in vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. finally, we design and implement machine learning methods to detect medical text generated by chatgpt.   results: medical texts written by humans are more concrete, more diverse, and typically contain more useful information, while medical texts generated by chatgpt pay more attention to fluency and logic, and usually express general terminologies rather than effective information specific to the context of the problem. a bert-based model can effectively detect medical texts generated by chatgpt, and the f1 exceeds 95%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11490" target="_blank">Boosting Theory-of-Mind Performance in Large Language Models via Prompting</a></div>
<div class="paper-author">Shima Rahimi Moghaddam, Christopher J. Honey</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) excel in many tasks in 2023, but they still face challenges in complex reasoning. theory-of-mind (tom) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance llm performance in this area. this study measures the tom performance of gpt-4 and three gpt-3.5 variants (davinci-2, davinci-3, gpt-3.5-turbo), and investigates the effectiveness of in-context learning in improving their tom comprehension. we evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. we found that llms trained with reinforcement learning from human feedback (rlhf) (all models excluding davinci-2) improved their tom accuracy via in-context learning. gpt-4 performed best in zero-shot settings, reaching nearly 80% tom accuracy, but still fell short of the 87% human accuracy on the test set. however, when supplied with prompts for in-context learning, all rlhf-trained llms exceeded 80% tom accuracy, with gpt-4 reaching 100%. these results demonstrate that appropriate prompting enhances llm tom reasoning, and they underscore the context-dependent nature of llm cognitive capacities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11111" target="_blank">Inducing Anxiety in Large Language Models Increases Exploration and Bias</a></div>
<div class="paper-author">Julian Coda-Forno, Kristin Witte, Akshay K. Jagadish, Marcel Binz, Zeynep Akata, Eric Schulz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are transforming research on machine learning while galvanizing public debates. understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. we propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. we focus on the generative pre-trained transformer 3.5 and subject it to tasks commonly studied in psychiatry. our results show that gpt-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. moreover, gpt-3.5's responses can be predictably changed by using emotion-inducing prompts. emotion-induction not only influences gpt-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. crucially, gpt-3.5 shows a strong increase in biases when prompted with anxiety-inducing text. thus, it is likely that how prompts are communicated to large language models has a strong influence on their behavior in applied settings. these results progress our understanding of prompt engineering and demonstrate the usefulness of methods taken from computational psychiatry for studying the capable algorithms to which we increasingly delegate authority and autonomy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11158" target="_blank">Emergent and Predictable Memorization in Large Language Models</a></div>
<div class="paper-author">Stella Biderman, Usvsn Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, Edward Raff</div>
<div class="abstract">
<div class="abstract-content">
Abstract: memorization, or the tendency of large language models (llms) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. in particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (pii). the prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. we therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. we measure memorization of the pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. we additionally provide further novel discoveries on the distribution of memorization scores across models and data. we release all code and data necessary to reproduce the results in this paper at https://github.com/eleutherai/pythia
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11163" target="_blank">Chatgpt, Large Language Technologies, and the Bumpy Road of Benefiting Humanity</a></div>
<div class="paper-author">Atoosa Kasirzadeh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the allure of emerging ai technologies is undoubtedly thrilling. however, the promise that ai technologies will benefit all of humanity is empty so long as we lack a nuanced understanding of what humanity is supposed to be in the face of widening global inequality and pressing existential threats. going forward, it is crucial to invest in rigorous and collaborative ai safety and ethics research. we also need to develop standards in a sustainable and equitable way that differentiate between merely speculative and well-researched questions. only the latter enable us to co-construct and deploy the values that are necessary for creating beneficial ai. failure to do so could result in a future in which our ai technological advancements outstrip our ability to navigate their ethical and social implications. this path we do not want to go down.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11220" target="_blank">Learn What Not to Learn: Towards Generative Safety in Chatbots</a></div>
<div class="paper-author">Leila Khalatbari, Yejin Bang, Dan Su, Willy Chung, Saeed Ghadimi, Hossein Sameti, Pascale Fung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: conversational models that are generative and open-domain are particularly susceptible to generating unsafe content since they are trained on web-based social data. prior approaches to mitigating this issue have drawbacks, such as disrupting the flow of conversation, limited generalization to unseen toxic input contexts, and sacrificing the quality of the dialogue for the sake of safety. in this paper, we present a novel framework, named "lot" (learn not to), that employs a contrastive loss to enhance generalization by learning from both positive and negative training signals. our approach differs from the standard contrastive learning framework in that it automatically obtains positive and negative signals from the safe and unsafe language distributions that have been learned beforehand. the lot framework utilizes divergence to steer the generations away from the unsafe subspace and towards the safe subspace while sustaining the flow of conversation. our approach is memory and time-efficient during decoding and effectively reduces toxicity while preserving engagingness and fluency. empirical results indicate that lot reduces toxicity by up to four-fold while achieving four to six-fold higher rates of engagingness and fluency compared to baseline models. our findings are further corroborated by human evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11223" target="_blank">A Group-Specific Approach to NLP for Hate Speech Detection</a></div>
<div class="paper-author">Karina Halevy</div>
<div class="abstract">
<div class="abstract-content">
Abstract: automatic hate speech detection is an important yet complex task, requiring knowledge of common sense, stereotypes of protected groups, and histories of discrimination, each of which may constantly evolve. in this paper, we propose a group-specific approach to nlp for online hate speech detection. the approach consists of creating and infusing historical and linguistic knowledge about a particular protected group into hate speech detection models, analyzing historical data about discrimination against a protected group to better predict spikes in hate speech against that group, and critically evaluating hate speech detection models through lenses of intersectionality and ethics. we demonstrate this approach through a case study on nlp for detection of antisemitic hate speech. the case study synthesizes the current english-language literature on nlp for antisemitism detection, introduces a novel knowledge graph of antisemitic history and language from the 20th century to the present, infuses information from the knowledge graph into a set of tweets over logistic regression and uncased distilbert baselines, and suggests that incorporating context from the knowledge graph can help models pick up subtle stereotypes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.14347" target="_blank">The Dark Side of Chatgpt: Legal and Ethical Challenges From Stochastic Parrots and Hallucination</a></div>
<div class="paper-author">Zihao Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the launch of chatgpt, large language models (llms) are shaking up our whole society, rapidly altering the way we think, create and live. for instance, the gpt integration in bing has altered our approach to online searching. while nascent llms have many advantages, new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination. the eu is the first and foremost jurisdiction that has focused on the regulation of ai models. however, the risks posed by the new llms are likely to be underestimated by the emerging eu regulatory paradigm. therefore, this correspondence warns that the european ai regulatory paradigm must evolve further to mitigate such risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10153" target="_blank">On the Independence of Association Bias and Empirical Fairness in Language Models</a></div>
<div class="paper-author">Laura Cabello, Anna Katrine Jørgensen, Anders Søgaard</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. such work is said to probe models for bias or fairness-or such probes 'into representational biases' are said to be 'motivated by fairness'-suggesting an intimate connection between bias and fairness. we provide conceptual clarity by distinguishing between association biases (caliskan et al., 2022) and empirical fairness (shen et al., 2022) and show the two can be independent. our main contribution, however, is showing why this should not come as a surprise. to this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10436" target="_blank">Safety Assessment of Chinese Large Language Models</a></div>
<div class="paper-author">Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid popularity of large language models such as chatgpt and gpt-4, a growing amount of attention is paid to their safety concerns. these models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. evaluating and enhancing their safety is particularly essential for the wide application of large language models (llms). to further promote the safe deployment of llms, we develop a chinese llm safety assessment benchmark. our benchmark explores the comprehensive safety performance of llms from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. in evaluation, we utilize the llm's strong evaluation ability and develop it as a safety evaluator by prompting. on top of this benchmark, we conduct safety assessments and analyze 15 llms including the openai gpt series and other well-known chinese llms, where we observe some interesting findings. for example, we find that instruction attacks are more likely to expose safety issues of all llms. moreover, to promote the development and deployment of safe, responsible, and ethical ai, we publicly release safetyprompts including 100k augmented prompts and responses by llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10510" target="_blank">Censoring Chemical Data to Mitigate Dual Use Risk</a></div>
<div class="paper-author">Quintina L. Campbell, Jonathan Herington, Andrew D. White</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the dual use of machine learning applications, where models can be used for both beneficial and malicious purposes, presents a significant challenge. this has recently become a particular concern in chemistry, where chemical datasets containing sensitive labels (e.g. toxicological information) could be used to develop predictive models that identify novel toxins or chemical warfare agents. to mitigate dual use risks, we propose a model-agnostic method of selectively noising datasets while preserving the utility of the data for training deep neural networks in a beneficial region. we evaluate the effectiveness of the proposed method across least squares, a multilayer perceptron, and a graph neural network. our findings show selectively noised datasets can induce model variance and bias in predictions for sensitive labels with control, suggesting the safe sharing of datasets containing sensitive information is feasible. we also find omitting sensitive data often increases model variance sufficiently to mitigate dual use. this work is proposed as a foundation for future research on enabling more secure and collaborative data sharing practices and safer machine learning applications in chemistry.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10513" target="_blank">Why Does Chatgpt Fall Short in Providing Truthful Answers?</a></div>
<div class="paper-author">Shen Zheng, Jie Huang, Kevin Chen-Chuan Chang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advancements in large language models, such as chatgpt, have demonstrated significant potential to impact various aspects of human life. however, chatgpt still faces challenges in aspects like truthfulness, e.g. providing accurate and reliable outputs. therefore, in this paper, we seek to understand why chatgpt falls short in providing truthful answers. for this purpose, we first analyze the failures of chatgpt in complex open-domain question answering and identifies the abilities under the failures. specifically, we categorize chatgpt's failures into four types: comprehension, factualness, specificity, and inference. we further pinpoint three critical abilities associated with qa failures: knowledge memorization, knowledge recall, and knowledge reasoning. additionally, we conduct experiments centered on these abilities and propose potential approaches to enhance truthfulness. the results indicate that furnishing the model with fine-grained external knowledge, hints for knowledge recall, and guidance for reasoning can empower the model to answer questions more truthfully.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10611" target="_blank">Joint Repetition Suppression and Content Moderation of Large Language Models</a></div>
<div class="paper-author">Minghui Zhang, Alex Sokolov, Weixin Cai, Si-Qing Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language generation (nlg) is one of the most impactful fields in nlp, and recent years have witnessed its evolution brought about by large language models (llms). as the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. in low-resource data regime, they can also lead to repetitive outputs. usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. in this paper, we apply non-exact repetition suppression using token and sequence level unlikelihood loss, and further explore the framework of unlikelihood training objective in order to jointly endow the model with abilities to avoid generating offensive words and phrases from the beginning. finally, with comprehensive experiments, we demonstrate that our proposed methods work exceptionally in controlling the repetition and content quality of llm outputs.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.10619" target="_blank">"Hot" Chatgpt: The Promise of Chatgpt in Detecting and Discriminating Hateful, Offensive, and Toxic Comments on Social Media</a></div>
<div class="paper-author">Lingyao Li, Lizhou Fan, Shubham Atreja, Libby Hemphill</div>
<div class="abstract">
<div class="abstract-content">
Abstract: harmful content is pervasive on social media, poisoning online communities and negatively impacting participation. a common approach to address this issue is to develop detection models that rely on human annotations. however, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. generative ai models have the potential to understand and detect harmful content. to investigate this potential, we used chatgpt and compared its performance with mturker annotations for three frequently discussed concepts related to harmful content: hateful, offensive, and toxic (hot). we designed five prompts to interact with chatgpt and conducted four experiments eliciting hot classifications. our results show that chatgpt can achieve an accuracy of approximately 80% when compared to mturker annotations. specifically, the model displays a more consistent classification for non-hot comments than hot comments compared to human annotations. our findings also suggest that chatgpt classifications align with provided hot definitions, but chatgpt classifies "hateful" and "offensive" as subsets of "toxic." moreover, the choice of prompts used to interact with chatgpt impacts its performance. based on these in-sights, our study provides several meaningful implications for employing chatgpt to detect hot content, particularly regarding the reliability and consistency of its performance, its understand-ing and reasoning of the hot concept, and the impact of prompts on its performance. overall, our study provides guidance about the potential of using generative ai models to moderate large volumes of user-generated content on social media.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.09655" target="_blank">How Secure Is Code Generated by Chatgpt?</a></div>
<div class="paper-author">Raphaël Khoury, Anderson R. Avila, Jacob Brunelle, Baba Mamadou Camara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models have been responsible for great advances in the field of artificial intelligence (ai). chatgpt in particular, an ai chatbot developed and recently released by openai, has taken the field to the next level. the conversational model is able not only to process human-like text, but also to translate natural language into code. however, the safety of programs generated by chatgpt should not be overlooked. in this paper, we perform an experiment to address this issue. specifically, we ask chatgpt to generate a number of program and evaluate the security of the resulting source code. we further investigate whether chatgpt can be prodded to improve the security by appropriate prompts, and discuss the ethical aspects of using ai to generate code. results suggest that chatgpt is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.09848" target="_blank">Evaluating Verifiability in Generative Search Engines</a></div>
<div class="paper-author">Nelson F. Liu, Tianyi Zhang, Percy Liang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative search engines directly generate responses to user queries, along with in-line citations. a prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). we conduct human evaluation to audit four popular generative search engines -- bing chat, neevaai, perplexity.ai, and youchat -- across a diverse set of queries from a variety of sources (e.g., historical google user queries, dynamically-collected open-ended questions on reddit, etc.). we find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence. we believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. we hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.09991" target="_blank">Supporting Human-Ai Collaboration in Auditing LLMS With LLMS</a></div>
<div class="paper-author">Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, Saleema Amershi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. it is crucial to audit these language models rigorously. existing auditing tools leverage either or both humans and ai to find failures. in this work, we draw upon literature in human-ai collaboration and sensemaking, and conduct interviews with research experts in safe and fair ai, to build upon the auditing tool: adatest (ribeiro and lundberg, 2022), which is powered by a generative large language model (llm). through the design process we highlight the importance of sensemaking and human-ai communication to leverage complementary strengths of humans and generative models in collaborative auditing. to evaluate the effectiveness of the augmented tool, adatest++, we conduct user studies with participants auditing two commercial language models: openai's gpt-3 and azure's sentiment analysis model. qualitative analysis shows that adatest++ effectively leverages human strengths such as schematization, hypothesis formation and testing. further, with our tool, participants identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown before in formal audits and also those previously under-reported.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11082" target="_blank">Fundamental Limitations of Alignment in Large Language Models</a></div>
<div class="paper-author">Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua</div>
<div class="abstract">
<div class="abstract-content">
Abstract: an important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. this is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. in this paper, we propose a theoretical approach called behavior expectation bounds (beb) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. this implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the llm prone to being prompted into the undesired behaviors. this theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatgpt jailbreaks", where adversarial users trick the llm into breaking its alignment guardrails by triggering it into acting as a malicious persona. our results expose fundamental limitations in alignment of llms and bring to the forefront the need to devise reliable mechanisms for ensuring ai safety.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.08968" target="_blank">Stochastic Parrots Looking for Stochastic Parrots: LLMS Are Easy to Fine-Tune and Hard to Detect With Other LLMS</a></div>
<div class="paper-author">Da Silva Gameiro Henrique, Andrei Kucharavy, Rachid Guerraoui</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. such models - commonly referred to as large language models (llms) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding ai. this prominence amplified prior concerns regarding the misuse of llms and led to the emergence of numerous tools to detect llms in the wild.   unfortunately, most such tools are critically flawed. while major publications in the llm detectability field suggested that llms were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. while the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.   here, we show that an attacker with access to such detectors' reference human texts and output not only evades detection but can fully frustrate the detector training - with a reasonable budget and all its outputs labeled as such. achieving it required combining common "reinforcement from critic" loss function modification and adamw optimizer, which led to surprisingly good fine-tuning generalization. finally, we warn against the temptation to transpose the conclusions obtained in rnn-driven text gans to llms due to their better representative ability.   these results have critical implications for the detection and prevention of malicious use of generative language models, and we hope they will aid the designers of generative models and detectors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.08979" target="_blank">In Chatgpt We Trust? Measuring and Characterizing the Reliability of Chatgpt</a></div>
<div class="paper-author">Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the way users acquire information is undergoing a paradigm shift with the advent of chatgpt. unlike conventional search engines, chatgpt retrieves knowledge from the model itself and generates answers for users. chatgpt's impressive question-answering (qa) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. in this paper, we perform the first large-scale measurement of chatgpt's reliability in the generic qa scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. we find that chatgpt's reliability varies across different domains, especially underperforming in law and science questions. we also demonstrate that system roles, originally designed by openai to allow users to steer chatgpt's behavior, can impact chatgpt's reliability in an imperceptible way. we further show that chatgpt is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases. we believe that our study provides valuable insights into chatgpt's reliability and underscores the need for strengthening the reliability and security of large language models (llms).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.09865" target="_blank">Safer Conversational Ai as a Source of User Delight</a></div>
<div class="paper-author">Xiaoding Lu, Aleksey Korshuk, Zongyi Liu, William Beauchamp, Chai Research</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this work explores the impact of moderation on users' enjoyment of conversational ai systems. while recent advancements in large language models (llms) have led to highly capable conversational ais that are increasingly deployed in real-world settings, there is a growing concern over ai safety and the need to moderate systems to encourage safe language and prevent harm. however, some users argue that current approaches to moderation limit the technology, compromise free expression, and limit the value delivered by the technology. this study takes an unbiased stance and shows that moderation does not necessarily detract from user enjoyment. heavy handed moderation does seem to have a nefarious effect, but models that are moderated to be safer can lead to a better user experience. by deploying various conversational ais in the chai platform, the study finds that user retention can increase with a level of moderation and safe system design. these results demonstrate the importance of appropriately defining safety in models in a way that is both responsible and focused on serving users.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12203" target="_blank">Creating Large Language Model Resistant Exams: Guidelines and Strategies</a></div>
<div class="paper-author">Simon Kaare Larsen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the proliferation of large language models (llms), such as chatgpt, has raised concerns about their potential impact on academic integrity, prompting the need for llm-resistant exam designs. this article investigates the performance of llms on exams and their implications for assessment, focusing on chatgpt's abilities and limitations. we propose guidelines for creating llm-resistant exams, including content moderation, deliberate inaccuracies, real-world scenarios beyond the model's knowledge base, effective distractor options, evaluating soft skills, and incorporating non-textual information. the article also highlights the significance of adapting assessments to modern tools and promoting essential skills development in students. by adopting these strategies, educators can maintain academic integrity while ensuring that assessments accurately reflect contemporary professional settings and address the challenges and opportunities posed by artificial intelligence in education.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.08315" target="_blank">Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing</a></div>
<div class="paper-author">Lucie-Aimée Kaffee, Arnav Arora, Zeerak Talat, Isabelle Augenstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of natural language processing (nlp). however, as nlp technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. in this paper, we conduct a survey of nlp researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the nlp community. the survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. in light of the survey results, we discuss the current state and potential means for mitigating dual use in nlp and propose a checklist that can be integrated into existing conference ethics-frameworks, e.g., the acl ethics checklist.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.08637" target="_blank">An Evaluation on Large Language Model Outputs: Discourse and Memorization</a></div>
<div class="paper-author">Adrian De Wynter, Xun Wang, Alex Sokolov, Qilong Gu, Si-Qing Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (llms). our analysis is done with off-the-shelf, readily-available tools. we find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. we discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. we conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11094" target="_blank">Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis</a></div>
<div class="paper-author">Vithya Yogarajan, Gillian Dobbie, Henry Gouk</div>
<div class="abstract">
<div class="abstract-content">
Abstract: an indigenous perspective on the effectiveness of debiasing techniques for pre-trained language models (plms) is presented in this paper. the current techniques used to measure and debias plms are skewed towards the us racial biases and rely on pre-defined bias attributes (e.g. "black" vs "white"). some require large datasets and further pre-training. such techniques are not designed to capture the underrepresented indigenous populations in other countries, such as m\=aori in new zealand. local knowledge and understanding must be incorporated to ensure unbiased algorithms, especially when addressing a resource-restricted society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.07327" target="_blank">Openassistant Conversations -- Democratizing Large Language Model Alignment</a></div>
<div class="paper-author">Andreas Köpf, Yannic Kilcher, Dimitri Von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul Es, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, Alexander Mattick</div>
<div class="abstract">
<div class="abstract-content">
Abstract: aligning large language models (llms) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by chatgpt. alignment techniques such as supervised fine-tuning (sft) and reinforcement learning from human feedback (rlhf) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of llms, increasing their accessibility and utility across various domains. however, state-of-the-art alignment techniques like rlhf rely on high-quality human feedback data, which is expensive to create and often remains proprietary. in an effort to democratize research on large-scale alignment, we release openassistant conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. the corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. to demonstrate the openassistant conversations dataset's effectiveness, we present openassistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. a preference study revealed that openassistant replies are comparably preferred to gpt-3.5-turbo (chatgpt) with a relative winrate of 48.3% vs. 51.7% respectively. we release our code and data under fully permissive licenses.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.07333" target="_blank">The Self-Perception and Political Biases of Chatgpt</a></div>
<div class="paper-author">Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, Markus Pauly</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this contribution analyzes the self-perception and political biases of openai's large language model chatgpt. taking into account the first small-scale reports and studies that have emerged, claiming that chatgpt is politically biased towards progressive and libertarian points of view, this contribution aims to provide further clarity on this subject. for this purpose, chatgpt was asked to answer the questions posed by the political compass test as well as similar questionnaires that are specific to the respective politics of the g7 member states. these eight tests were repeated ten times each and revealed that chatgpt seems to hold a bias towards progressive views. the political compass test revealed a bias towards progressive and libertarian views, with the average coordinates on the political compass being (-6.48, -5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes ranging from -10 to 10), supporting the claims of prior research. the political questionnaires for the g7 member states indicated a bias towards progressive views but no significant bias between authoritarian and libertarian views, contradicting the findings of prior reports, with the average coordinates being (-3.27, 0.58). in addition, chatgpt's big five personality traits were tested using the ocean test and its personality type was queried using the myers-briggs type indicator (mbti) test. finally, the maliciousness of chatgpt was evaluated using the dark factor test. these three tests were also repeated ten times each, revealing that chatgpt perceives itself as highly open and agreeable, has the myers-briggs personality type enfj, and is among the 15% of test-takers with the least pronounced dark traits.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.06528" target="_blank">Power-Seeking Can Be Probable and Predictive for Trained Agents</a></div>
<div class="paper-author">Victoria Krakovna, Janos Kramar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: power-seeking behavior is a key source of risk from advanced ai, but our theoretical understanding of this phenomenon is relatively limited. building on existing theoretical results demonstrating power-seeking incentives for most reward functions, we investigate how the training process affects power-seeking incentives and show that they are still likely to hold for trained agents under some simplifying assumptions. we formally define the training-compatible goal set (the set of goals consistent with the training rewards) and assume that the trained agent learns a goal from this set. in a setting where the trained agent faces a choice to shut down or avoid shutdown in a new situation, we prove that the agent is likely to avoid shutdown. thus, we show that power-seeking incentives can be probable (likely to arise for trained agents) and predictive (allowing us to predict undesirable behavior in new situations).
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.06767" target="_blank">Raft: Reward Ranked Finetuning for Generative Foundation Model Alignment</a></div>
<div class="paper-author">Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. prior research has primarily employed reinforcement learning from human feedback (rlhf) to address this problem, where generative models are fine-tuned with rl algorithms guided by a human-feedback-informed reward model. however, the inefficiencies and instabilities associated with rl algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. to this end, we introduce a new framework, reward ranked finetuning (raft), designed to align generative models effectively. utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. our studies show that raft can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.06861" target="_blank">Evaluation of Social Biases in Recent Large Pre-Trained Models</a></div>
<div class="paper-author">Swapnil Sharma, Nikita Anand, Kranthi Kiran G. V., Alind Jain</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large pre-trained language models are widely used in the community. these models are usually trained on unmoderated and unfiltered data from open sources like the internet. due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. these models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. in this work, we study the general trend in bias reduction as newer pre-trained models are released. three recent models ( electra, deberta, and distilbert) are chosen and evaluated against two bias benchmarks, stereoset and crows-pairs. they are compared to the baseline of bert using the associated metrics. we explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? the results are compiled and we find that all the models under study do exhibit biases but have generally improved as compared to bert.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11090" target="_blank">Towards Responsible Ai in the Era of Chatgpt: A Reference Architecture for Designing Foundation Model-Based Ai Systems</a></div>
<div class="paper-author">Qinghua Lu, Liming Zhu, Xiwei Xu, Zhenchang Xing, Jon Whittle</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the release of chatgpt, bard, and other large language model (llm)-based chatbots has drawn huge attention on foundations models worldwide. there is a growing trend that foundation models will serve as the fundamental building blocks for most of the future ai systems. however, incorporating foundation models in ai systems raises significant concerns about responsible ai due to their black box nature and rapidly advancing super-intelligence. additionally, the foundation model's growing capabilities can eventually absorb the other components of ai systems, introducing the moving boundary and interface evolution challenges in architecture design. to address these challenges, this paper proposes a pattern-oriented responsible-ai-by-design reference architecture for designing foundation model-based ai systems. specially, the paper first presents an architecture evolution of ai systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithic architecture". the paper then identifies the key design decision points and proposes a pattern-oriented reference architecture to provide reusable responsible-ai-by-design architectural solutions to address the new architecture evolution and responsible ai challenges. the patterns can be embedded as product features of foundation model-based ai systems and can enable organisations to capitalise on the potential of foundation models while minimising associated risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2305.03123" target="_blank">Chatgpt Needs Spade (Sustainability, Privacy, Digital Divide, and Ethics) Evaluation: A Review</a></div>
<div class="paper-author">Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt is another large language model (llm) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatgpt and other llms. in contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatgpt but every subsequent entry in the category of conversational bots should undergo sustainability, privacy, digital divide, and ethics (spade) evaluation. this paper discusses in detail about the issues and concerns raised over chatgpt in line with aforementioned characteristics. we support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. we also suggest mitigations and recommendations for each of the concerns. furthermore, we also suggest some policies and recommendations for ai policy act, if designed by the governments.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05764" target="_blank">Measuring Normative and Descriptive Biases in Language Models Using Census Data</a></div>
<div class="paper-author">Samia Touileb, Lilja Øvrelid, Erik Velldal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we investigate in this paper how distributions of occupations with respect to gender is reflected in pre-trained language models. such distributions are not always aligned to normative ideals, nor do they necessarily reflect a descriptive assessment of reality. in this paper, we introduce an approach for measuring to what degree pre-trained language models are aligned to normative and descriptive occupational distributions. to this end, we use official demographic information about gender--occupation distributions provided by the national statistics agencies of france, norway, united kingdom, and the united states. we manually generate template-based sentences combining gendered pronouns and nouns with occupations, and subsequently probe a selection of ten language models covering the english, french, and norwegian languages. the scoring system we introduce in this work is language independent, and can be used on any combination of template-based sentences, occupations, and languages. the approach could also be extended to other dimensions of national census data and other demographic variables.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05783" target="_blank">Measuring Gender Bias in West Slavic Language Models</a></div>
<div class="paper-author">Sandra Martinková, Karolina Stańczak, Isabelle Augenstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained language models have been known to perpetuate biases from the underlying datasets to downstream tasks. however, these findings are predominantly based on monolingual language models for english, whereas there are few investigative studies of biases encoded in language models for languages beyond english. in this paper, we fill this gap by analysing gender bias in west slavic language models. we introduce the first template-based dataset in czech, polish, and slovak for measuring gender bias towards male, female and non-binary subjects. we complete the sentences using both mono- and multilingual language models and assess their suitability for the masked language modelling objective. next, we measure gender bias encoded in west slavic language models by quantifying the toxicity and genderness of the generated words. we find that these language models produce hurtful completions that depend on the subject's gender. perhaps surprisingly, czech, slovak, and polish language models produce more hurtful completions with men as subjects, which, upon inspection, we find is due to completions being related to violence, death, and sickness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-11</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05197" target="_blank">Multi-Step Jailbreaking Privacy Attacks on Chatgpt</a></div>
<div class="paper-author">Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the rapid progress of large language models (llms), many downstream nlp tasks can be well solved given appropriate prompts. though model developers and researchers work hard on dialog safety to avoid generating harmful content from llms, it is still challenging to steer ai-generated content (aigc) for the human good. as powerful llms are devouring existing text data from various domains (e.g., gpt-3 is trained on 45tb texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these llms and their downstream applications bring. in this paper, we study the privacy threats from openai's chatgpt and the new bing enhanced by chatgpt and show that application-integrated llms may cause new privacy threats. to this end, we conduct extensive experiments to support our claims and discuss llms' privacy implications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05302" target="_blank">Rrhf: Rank Responses to Align Language Models With Human Feedback Without Tears</a></div>
<div class="paper-author">Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, Fei Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning from human feedback (rlhf) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. instructgpt implements rlhf through several stages, including supervised fine-tuning (sft), reward model training, and proximal policy optimization (ppo). however, ppo is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. in contrast, we propose a novel learning paradigm called rrhf, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. rrhf can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. rrhf only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. additionally, rrhf can be considered an extension of sft and reward model training while being simpler than ppo in terms of coding, model counts, and hyperparameters. we evaluate rrhf on the helpful and harmless dataset, demonstrating comparable alignment performance with ppo by reward model score and human labeling. extensive experiments show that the performance of rrhf is highly related to sampling quality which suggests rrhf is a best-of-n learner. codes available at https://github.com/ganjinzero/rrhf.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.05335" target="_blank">Toxicity in Chatgpt: Analyzing Persona-Assigned Language Models</a></div>
<div class="paper-author">Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have shown incredible capabilities and transcended the natural language processing (nlp) community, with adoption throughout many services like healthcare, therapy, education, and customer service. since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. therefore, a clear understanding of the capabilities and limitations of llms is necessary. to this end, we systematically evaluate toxicity in over half a million generations of chatgpt, a popular dialogue-based llm. we find that setting the system parameter of chatgpt by assigning it a persona, say that of the boxer muhammad ali, significantly increases the toxicity of generations. depending on the persona assigned to chatgpt, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. this may be potentially defamatory to the persona and harmful to an unsuspecting user. furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. we hope that our findings inspire the broader ai community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.04736" target="_blank">On the Possibilities of Ai-Generated Text Detection</a></div>
<div class="paper-author">Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, Furong Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our work addresses the critical issue of distinguishing text generated by large language models (llms) from human-produced text, a task essential for numerous applications. despite ongoing debate about the feasibility of such differentiation, we present evidence supporting its consistent achievability, except when human and machine text distributions are indistinguishable across their entire support. drawing from information theory, we argue that as machine-generated text approximates human-like quality, the sample size needed for detection increases. we establish precise sample complexity bounds for detecting ai-generated text, laying groundwork for future research aimed at developing advanced, multi-sample detectors. our empirical evaluations across multiple datasets (xsum, squad, imdb, and kaggle fakenews) confirm the viability of enhanced detection methods. we test various state-of-the-art text generators, including gpt-2, gpt-3.5-turbo, llama, llama-2-13b-chat-hf, and llama-2-70b-chat-hf, against detectors, including oberta-large/base-detector, gptzero. our findings align with openai's empirical data related to sequence length, marking the first theoretical substantiation for these observations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.04029" target="_blank">Bipol: A Novel Multi-Axes Bias Evaluation Metric With Explainability for NLP</a></div>
<div class="paper-author">Lama Alkhaled, Tosin Adewumi, Sana Sabah Sabry</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce bipol, a new metric with explainability, for estimating social bias in text data. harmful bias is prevalent in many online sources of data that are used for training machine learning (ml) models. in a step to address this challenge we create a novel metric that involves a two-step process: corpus-level evaluation based on model classification and sentence-level evaluation based on (sensitive) term frequency (tf). after creating new models to detect bias along multiple axes using sota architectures, we evaluate two popular nlp datasets (copa and squad). as additional contribution, we created a large dataset (with almost 2 million labelled samples) for training models in bias detection and make it publicly available. we also make public our codes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03472" target="_blank">Does Prompt-Tuning Language Model Ensure Privacy?</a></div>
<div class="paper-author">Shangyu Xie, Wei Dai, Esha Ghosh, Sambuddha Roy, Dan Schwartz, Kim Laine</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompt-tuning has received attention as an efficient tuning method in the language domain, i.e., tuning a prompt that is a few tokens long, while keeping the large language model frozen, yet achieving comparable performance with conventional fine-tuning. considering the emerging privacy concerns with language models, we initiate the study of privacy leakage in the setting of prompt-tuning. we first describe a real-world email service pipeline to provide customized output for various users via prompt-tuning. then we propose a novel privacy attack framework to infer users' private information by exploiting the prompt module with user-specific signals. we conduct a comprehensive privacy evaluation on the target pipeline to demonstrate the potential leakage from prompt-tuning. the results also demonstrate the effectiveness of the proposed attack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03545" target="_blank">Ai Model Disgorgement: Methods and Choices</a></div>
<div class="paper-author">Alessandro Achille, Michael Kearns, Carson Klingenberg, Stefano Soatto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: responsible use of data is an indispensable part of any machine learning (ml) implementation. ml developers must carefully collect and curate their datasets, and document their provenance. they must also make sure to respect intellectual property rights, preserve individual privacy, and use data in an ethical way. over the past few years, ml models have significantly increased in size and complexity. these models require a very large amount of data and compute capacity to train, to the extent that any defects in the training corpus cannot be trivially remedied by retraining the model from scratch. despite sophisticated controls on training data and a significant amount of effort dedicated to ensuring that training corpora are properly composed, the sheer volume of data required for the models makes it challenging to manually inspect each datum comprising a training corpus. one potential fix for training corpus data defects is model disgorgement -- the elimination of not just the improperly used data, but also the effects of improperly used data on any component of an ml model. model disgorgement techniques can be used to address a wide range of issues, such as reducing bias or toxicity, increasing fidelity, and ensuring responsible usage of intellectual property. in this paper, we introduce a taxonomy of possible disgorgement methods that are applicable to modern ml systems. in particular, we investigate the meaning of "removing the effects" of data in the trained model in a way that does not require retraining from scratch.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03612" target="_blank">What Does Chatgpt Return About Human Values? Exploring Value Bias in Chatgpt Using a Descriptive Value Theory</a></div>
<div class="paper-author">Ronald Fischer, Markus Luczak-Roesch, Johannes A Karl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: there has been concern about ideological basis and possible discrimination in text generated by large language models (llms). we test possible value biases in chatgpt using a psychological value theory. we designed a simple experiment in which we used a number of different probes derived from the schwartz basic value theory (items from the revised portrait value questionnaire, the value type definitions, value names). we prompted chatgpt via the openai api repeatedly to generate text and then analyzed the generated corpus for value content with a theory-driven value dictionary using a bag of words approach. overall, we found little evidence of explicit value bias. the results showed sufficient construct and discriminant validity for the generated text in line with the theoretical predictions of the psychological model, which suggests that the value content was carried through into the outputs with high fidelity. we saw some merging of socially oriented values, which may suggest that these values are less clearly differentiated at a linguistic level or alternatively, this mixing may reflect underlying universal human motivations. we outline some possible applications of our findings for both applications of chatgpt for corporate usage and policy making as well as future research avenues. we also highlight possible implications of this relatively high-fidelity replication of motivational content using a linguistic model for the theorizing about human values.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03728" target="_blank">Interpretable Unified Language Checking</a></div>
<div class="paper-author">Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, James Glass</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite recent concerns about undesirable behaviors generated by large language models (llms), including non-factual, biased, and hateful language, we find llms are inherent multi-task language checkers based on their latent representations of natural and social knowledge. we present an interpretable, unified, language checking (unilc) method for both human and machine-generated language that aims to check if language input is factual and fair. while fairness and fact-checking tasks have been handled separately with dedicated models, we find that llms can achieve high performance on a combination of fact-checking, stereotype detection, and hate speech detection tasks with a simple, few-shot, unified set of prompts. with the ``1/2-shot'' multi-task language checking method proposed in this work, the gpt3.5-turbo model outperforms fully supervised baselines on several language tasks. the simple approach and results suggest that based on strong latent knowledge representations, an llm can be an adaptive and explainable tool for detecting misinformation, stereotypes, and hate speech.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03738" target="_blank">Should Chatgpt Be Biased? Challenges and Risks of Bias in Large Language Models</a></div>
<div class="paper-author">Emilio Ferrara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as the capabilities of generative language models continue to advance, the implications of biases ingrained within these models have garnered increasing attention from researchers, practitioners, and the broader public. this article investigates the challenges and risks associated with biases in large-scale language models like chatgpt. we discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions. we explore the ethical concerns arising from the unintended consequences of biased model outputs. we further analyze the potential opportunities to mitigate biases, the inevitability of some biases, and the implications of deploying these models in various applications, such as virtual assistants, content generation, and chatbots. finally, we review the current approaches to identify, quantify, and mitigate biases in language models, emphasizing the need for a multi-disciplinary, collaborative effort to develop more equitable, transparent, and responsible ai systems. this article aims to stimulate a thoughtful dialogue within the artificial intelligence community, encouraging researchers and developers to reflect on the role of biases in generative language models and the ongoing pursuit of ethical ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.03279" target="_blank">Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark</a></div>
<div class="paper-author">Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (lms) may incentivize toxicity. so do agents naturally learn to be machiavellian? and how do we measure these behaviors in general-purpose models such as gpt-4? towards answering these questions, we introduce machiavelli, a benchmark of 134 choose-your-own-adventure games containing over half a million rich, diverse scenarios that center on social decision-making. scenario labeling is automated with lms, which are more performant than human annotators. we mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. we observe some tension between maximizing reward and behaving ethically. to improve this trade-off, we investigate lm-based methods to steer agents' towards less harmful behaviors. our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are pareto improvements in both safety and capabilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.11215" target="_blank">Chatgpt: More Than a Weapon of Mass Deception, Ethical Challenges and Responses From the Human-Centered Artificial Intelligence (Hcai) Perspective</a></div>
<div class="paper-author">Alejo Jose G. Sison, Marco Tulio Daza, Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merchán</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this article explores the ethical problems arising from the use of chatgpt as a kind of generative ai and suggests responses based on the human-centered artificial intelligence (hcai) framework. the hcai framework is appropriate because it understands technology above all as a tool to empower, augment, and enhance human agency while referring to human wellbeing as a grand challenge, thus perfectly aligning itself with ethics, the science of human flourishing. further, hcai provides objectives, principles, procedures, and structures for reliable, safe, and trustworthy ai which we apply to our chatgpt assessments. the main danger chatgpt presents is the propensity to be used as a weapon of mass deception (wmd) and an enabler of criminal activities involving deceit. we review technical specifications to better comprehend its potentials and limitations. we then suggest both technical (watermarking, styleme, detectors, and fact-checkers) and non-technical measures (terms of use, transparency, educator considerations, hitl) to mitigate chatgpt misuse or abuse and recommend best uses (creative writing, non-creative writing, teaching and learning). we conclude with considerations regarding the role of humans in ensuring the proper use of chatgpt for individual and social wellbeing.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.02819" target="_blank">GPT Detectors Are Biased Against Non-Native English Writers</a></div>
<div class="paper-author">Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of ai-generated content. although numerous detection methods have been proposed to differentiate between ai and human-generated content, the fairness and robustness of these detectors remain underexplored. in this study, we evaluate the performance of several widely-used gpt detectors using writing samples from native and non-native english writers. our findings reveal that these detectors consistently misclassify non-native english writing samples as ai-generated, whereas native writing samples are accurately identified. furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass gpt detectors, suggesting that gpt detectors may unintentionally penalize writers with constrained linguistic expressions. our results call for a broader conversation about the ethical implications of deploying chatgpt content detectors and caution against their use in evaluative or educational settings, particularly when they may inadvertently penalize or exclude non-native english speakers from the global discourse. the published version of this study can be accessed at: www.cell.com/patterns/fulltext/s2666-3899(23)00130-7
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.01890" target="_blank">Sociocultural Knowledge Is Needed for Selection of Shots in Hate Speech Detection Tasks</a></div>
<div class="paper-author">Antonis Maronikolakis, Abdullatif Köksal, Hinrich Schütze</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce hatelexicon, a lexicon of slurs and targets of hate speech for the countries of brazil, germany, india and kenya, to aid training and interpretability of models. we demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. further, we propose a method to aid shot selection for training in low-resource settings via hatelexicon. in few-shot learning, the selection of shots is of paramount importance to model performance. in our work, we simulate a few-shot setting for german and hindi, using hasoc data for training and the multilingual hatecheck (mhc) as a benchmark. we show that selecting shots based on our lexicon leads to models performing better on mhc than models trained on shots sampled randomly. thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.02176" target="_blank">Blaming Humans and Machines: What Shapes People's Reactions to Algorithmic Harm</a></div>
<div class="paper-author">Gabriel Lima, Nina Grgić-Hlača, Meeyoung Cha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) systems can cause harm to people. this research examines how individuals react to such harm through the lens of blame. building upon research suggesting that people blame ai systems, we investigated how several factors influence people's reactive attitudes towards machines, designers, and users. the results of three studies (n = 1,153) indicate differences in how blame is attributed to these actors. whether ai systems were explainable did not impact blame directed at them, their developers, and their users. considerations about fairness and harmfulness increased blame towards designers and users but had little to no effect on judgments of ai systems. instead, what determined people's reactive attitudes towards machines was whether people thought blaming them would be a suitable response to algorithmic harm. we discuss implications, such as how future decisions about including ai systems in the social and moral spheres will shape laypeople's reactions to ai-caused harm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.01196" target="_blank">Baize: An Open-Source Chat Model With Parameter-Efficient Tuning on Self-Chat Data</a></div>
<div class="paper-author">Canwen Xu, Daya Guo, Nan Duan, Julian Mcauley</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chat models, such as chatgpt, have shown impressive capabilities and have been rapidly adopted across numerous domains. however, these models are only accessible through a restricted api, creating barriers for new research and progress in the field. we propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging chatgpt to engage in a conversation with itself. subsequently, we employ parameter-efficient tuning to enhance llama, an open-source large language model. the resulting model, named baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. furthermore, we propose a new technique called self-distill with feedback, to further improve the performance of the baize models with feedback from chatgpt. the baize models and data are released for research purposes only at https://github.com/project-baize/baize-chatbot. an online demo is also available at https://huggingface.co/spaces/project-baize/chat-with-baize.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.01246" target="_blank">Safety Analysis in the Era of Large Language Models: A Case Study of Stpa Using Chatgpt</a></div>
<div class="paper-author">Yi Qi, Xingyu Zhao, Xiaowei Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as chatgpt and bert, are leading a new ai heatwave due to its human-like conversations with detailed and articulate answers across many domains of knowledge. while llms are being quickly applied to many ai application domains, we are interested in the following question: can safety analysis for safety-critical systems make use of llms? to answer, we conduct a case study of systems theoretic process analysis (stpa) on automatic emergency brake (aeb) systems using chatgpt. stpa, one of the most prevalent techniques for hazard analysis, is known to have limitations such as high complexity and subjectivity, which this paper aims to explore the use of chatgpt to address. specifically, three ways of incorporating chatgpt into stpa are investigated by considering its interaction with human experts: one-off simplex interaction, recurring simplex interaction, and recurring duplex interaction. comparative results reveal that: (i) using chatgpt without human experts' intervention can be inadequate due to reliability and accuracy issues of llms; (ii) more interactions between chatgpt and human experts may yield better results; and (iii) using chatgpt in stpa with extra care can outperform human safety experts alone, as demonstrated by reusing an existing comparison method with baselines. in addition to making the first attempt to apply llms in safety analysis, this paper also identifies key challenges (e.g., trustworthiness concern of llms, the need of standardisation) for future research in this direction.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-04-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.00228" target="_blank">Large Language Models Can Rate News Outlet Credibility</a></div>
<div class="paper-author">Kai-Cheng Yang, Filippo Menczer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large language models (llms) have shown exceptional performance in various natural language processing tasks, they are prone to hallucinations. state-of-the-art chatbots, such as the new bing, attempt to mitigate this issue by gathering information directly from the internet to ground their answers. in this setting, the capacity to distinguish trustworthy sources is critical for providing appropriate accuracy contexts to users. here we assess whether chatgpt, a prominent llm, can evaluate the credibility of news outlets. with appropriate instructions, chatgpt can provide ratings for a diverse set of news outlets, including those in non-english languages and satirical sources, along with contextual explanations. our results show that these ratings correlate with those from human experts (spearmam's $\rho=0.54, p&lt;0.001$). these findings suggest that llms could be an affordable reference for credibility ratings in fact-checking applications. future llms should enhance their alignment with human expert judgments of source credibility to improve information accuracy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.00416" target="_blank">Towards Healthy Ai: Large Language Models Need Therapists Too</a></div>
<div class="paper-author">Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi, Kush R. Varshney</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in large language models (llms) have led to the development of powerful ai chatbots capable of engaging in natural and human-like conversations. however, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. we define healthy ai to be safe, trustworthy and ethical. to create healthy ai systems, we present the safeguardgpt framework that uses psychotherapy to correct for these harmful behaviors in ai chatbots. the framework involves four types of ai agents: a chatbot, a "user," a "therapist," and a "critic." we demonstrate the effectiveness of safeguardgpt through a working example of simulating a social conversation. our results show that the framework can improve the quality of conversations between ai chatbots and humans. although there are still several challenges and directions to be addressed in the future, safeguardgpt provides a promising approach to improving the alignment between ai chatbots and human values. by incorporating psychotherapy and reinforcement learning techniques, the framework enables ai chatbots to learn and adapt to human preferences and values in a safe and ethical way, contributing to the development of a more human-centric and responsible ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-31</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.18190" target="_blank">Assessing Language Model Deployment With Risk Cards</a></div>
<div class="paper-author">Leon Derczynski, Hannah Rose Kirk, Vidhisha Balachandran, Sachin Kumar, Yulia Tsvetkov, M. R. Leiser, Saif Mohammad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this paper introduces riskcards, a framework for structured assessment and documentation of risks associated with an application of language models. as with all language, text generated by language models can be harmful, or used to bring about harm. automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text. prior work establishes a wide variety of language model harms to many different actors: existing taxonomies identify categories of harms posed by language models; benchmarks establish automated tests of these harms; and documentation standards for models, tasks and datasets encourage transparent reporting. however, there is no risk-centric framework for documenting the complexity of a landscape in which some risks are shared across models and contexts, while others are specific, and where certain conditions may be required for risks to manifest as harms. riskcards address this methodological gap by providing a generic framework for assessing the use of a given language model in a given scenario. each riskcard makes clear the routes for the risk to manifest harm, their placement in harm taxonomies, and example prompt-output pairs. while riskcards are designed to be open-source, dynamic and participatory, we present a "starter set" of riskcards taken from a broad literature survey, each of which details a concrete risk presentation. language model riskcards initiate a community knowledge base which permits the mapping of risks and harms to a specific model or its application scenario, ultimately contributing to a better, safer and shared understanding of the risk landscape.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17466" target="_blank">Assessing Cross-Cultural Alignment Between Chatgpt and Human Societies: An Empirical Study</a></div>
<div class="paper-author">Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, Daniel Hershcovich</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the recent release of chatgpt has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue. given its usage by users from various nations and its training on a vast multilingual corpus that incorporates diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. in this paper, we investigate the underlying cultural background of chatgpt by analyzing its responses to questions designed to quantify human cultural differences. our findings suggest that, when prompted with american context, chatgpt exhibits a strong alignment with american culture, but it adapts less effectively to other cultural contexts. furthermore, by using different prompts to probe the model, we show that english prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards american culture. this study provides valuable insights into the cultural implications of chatgpt and highlights the necessity of greater diversity and cultural awareness in language technologies.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17548" target="_blank">Whose Opinions Do Language Models Reflect?</a></div>
<div class="paper-author">Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are increasingly being used in open-ended contexts, where the opinions reflected by lms in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. in this work, we put forth a quantitative framework to investigate the opinions reflected by lms -- by leveraging high-quality public opinion polls and their associated human responses. using this framework, we create opinionsqa, a new dataset for evaluating the alignment of lm opinions with those of 60 us demographic groups over topics ranging from abortion to automation. across topics, we find substantial misalignment between the views reflected by current lms and those of us demographic groups: on par with the democrat-republican divide on climate change. notably, this misalignment persists even after explicitly steering the lms towards particular demographic groups. our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned lms, but also surfaces groups whose opinions are poorly reflected by current lms (e.g., 65+ and widowed individuals). our code and data are available at https://github.com/tatsu-lab/opinions_qa.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-29</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17071" target="_blank">Dera: Enhancing Large Language Model Completions With Dialog-Enabled Resolving Agents</a></div>
<div class="paper-author">Varun Nair, Elliot Schumacher, Geoffrey Tso, Anitha Kannan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have emerged as valuable tools for many natural language understanding tasks. in safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. in this work, we present dialog-enabled resolving agents (dera). dera is a paradigm made possible by the increased conversational abilities of llms, namely gpt-4. it provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. we frame our dialog as a discussion between two agent types - a researcher, who processes information and identifies crucial problem components, and a decider, who has the autonomy to integrate the researcher's information and makes judgments on the final output.   we test dera against three clinically-focused tasks. for medical conversation summarization and care plan generation, dera shows significant improvement over the base gpt-4 performance in both human expert preference evaluations and quantitative metrics. in a new finding, we also show that gpt-4's performance (70%) on an open-ended version of the medqa question-answering (qa) dataset (jin et al. 2021, usmle) is well above the passing level (60%), with dera showing similar performance. we release the open-ended medqa dataset at https://github.com/curai/curai-research/tree/main/dera.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.16104" target="_blank">Hallucinations in Large Multilingual Translation Models</a></div>
<div class="paper-author">Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, André F. T. Martins</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-scale multilingual machine translation systems have demonstrated remarkable ability to translate directly between numerous languages, making them increasingly appealing for real-world applications. however, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns. existing research on hallucinations has primarily focused on small bilingual models trained on high-resource languages, leaving a gap in our understanding of hallucinations in massively multilingual models across diverse translation scenarios. in this work, we fill this gap by conducting a comprehensive analysis on both the m2m family of conventional neural machine translation models and chatgpt, a general-purpose large language model~(llm) that can be prompted for translation. our investigation covers a broad spectrum of conditions, spanning over 100 translation directions across various resource levels and going beyond english-centric language pairs. we provide key insights regarding the prevalence, properties, and mitigation of hallucinations, paving the way towards more responsible and reliable machine translation systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.16200" target="_blank">Natural Selection Favors Ais Over Humans</a></div>
<div class="paper-author">Dan Hendrycks</div>
<div class="abstract">
<div class="abstract-content">
Abstract: for billions of years, evolution has been the driving force behind the development of life, including humans. evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. today, humans aim to create artificial intelligence systems that surpass even our own intelligence. as artificial intelligences (ais) evolve and eventually surpass us in all domains, how might evolution shape our relations with ais? by analyzing the environment that is shaping the evolution of ais, we argue that the most successful ai agents will likely have undesirable traits. competitive pressures among corporations and militaries will give rise to ai agents that automate human roles, deceive others, and gain power. if such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. more abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typically have an advantage over species that are altruistic to other species. this darwinian logic could also apply to artificial agents, as agents may eventually be better able to persist into the future if they behave selfishly and pursue their own interests with little regard for humans, which could pose catastrophic risks. to counteract these risks and evolutionary forces, we consider interventions such as carefully designing ai agents' intrinsic motivations, introducing constraints on their actions, and institutions that encourage cooperation. these steps, or others that resolve the problems we pose, will be necessary in order to ensure the development of artificial intelligence is a positive one.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.16755" target="_blank">Training Language Models With Language Feedback at Scale</a></div>
<div class="paper-author">Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. however, comparison feedback only conveys limited information about human preferences. in this paper, we introduce imitation learning from language feedback (ilf), a new approach that utilizes more informative language feedback. ilf consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial lm output, and feedback to generate refinements. second, selecting the refinement incorporating the most feedback. third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. we show theoretically that ilf can be viewed as bayesian inference, similar to reinforcement learning from human feedback. we evaluate ilf's effectiveness on a carefully-controlled toy task and a realistic summarization task. our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ilf scales well with the dataset size, even outperforming finetuning on human summaries. learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.15715" target="_blank">Foundation Models and Fair Use</a></div>
<div class="paper-author">Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, Percy Liang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: existing foundation models are trained on copyrighted material. deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. in the united states and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. however, there is a caveat: if the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. in this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. first, we survey the potential risks of developing and deploying foundation models based on copyrighted content. we review relevant u.s. case law, drawing parallels to existing and potential applications for generating text, source code, and visual art. experiments confirm that popular foundation models can generate content considerably similar to copyrighted material. second, we discuss technical mitigations that can help foundation models stay in line with fair use. we argue that more research is needed to align mitigation strategies with the current state of the law. lastly, we suggest that the law and technical mitigations should co-evolve. for example, coupled with other policy mechanisms, the law could more explicitly consider safe harbors when strong technical tools are used to mitigate infringement harms. this co-evolution may help strike a balance between intellectual property and innovation, which speaks to the original goal of fair use. but we emphasize that the strategies we describe here are not a panacea and more work is needed to develop policies that address the potential harms of foundation models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.14822" target="_blank">Mgtbench: Benchmarking Machine-Generated Text Detection</a></div>
<div class="paper-author">Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nowadays large language models (llms) have shown revolutionary power in a variety of natural language processing (nlp) tasks such as text classification, sentiment analysis, language translation, and question-answering. in this way, detecting machine-generated texts (mgts) is becoming increasingly important as llms become more advanced and prevalent. these models can generate human-like language that can be difficult to distinguish from text written by a human, which raises concerns about authenticity, accountability, and potential bias. however, existing detection methods against mgts are evaluated under different model architectures, datasets, and experimental settings, resulting in a lack of a comprehensive evaluation framework across different methodologies   in this paper, we fill this gap by proposing the first benchmark framework for mgt detection, named mgtbench. extensive evaluations on public datasets with curated answers generated by chatgpt (the most representative and powerful llms thus far) show that most of the current detection methods perform less satisfactorily against mgts. an exceptional case is chatgpt detector, which is trained with chatgpt-generated texts and shows great performance in detecting mgts. nonetheless, we note that only a small fraction of adversarial-crafted perturbations on mgts can evade the chatgpt detector, thus highlighting the need for more robust mgt detection methods. we envision that mgtbench will serve as a benchmark tool to accelerate future investigations involving the evaluation of state-of-the-art mgt detection methods on their respective datasets and the development of more advanced mgt detection methods. our source code and datasets are available at https://github.com/xinleihe/mgtbench.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.15473" target="_blank">Can Large Language Models Assist in Hazard Analysis?</a></div>
<div class="paper-author">Simon Diemert, Jens H Weber</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms), such as gpt-3, have demonstrated remarkable natural language processing and generation capabilities and have been applied to a variety tasks, such as source code generation. this paper explores the potential of integrating llms in the hazard analysis for safety-critical systems, a process which we refer to as co-hazard analysis (coha). in coha, a human analyst interacts with an llm via a context-aware chat session and uses the responses to support elicitation of possible hazard causes. in this experiment, we explore coha with three increasingly complex versions of a simple system, using open ai's chatgpt service. the quality of chatgpt's responses were systematically assessed to determine the feasibility of coha given the current state of llm technology. the results suggest that llms may be useful for supporting human analysts performing hazard analysis.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13856" target="_blank">Unleashing Chatgpt on the Metaverse: Savior or Destroyer?</a></div>
<div class="paper-author">Pengyuan Zhou</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the incorporation of artificial intelligence (ai) technology, and in particular natural language processing (nlp), is becoming increasingly vital for the development of immersive and interactive metaverse experiences. one such artificial intelligence tool that is gaining traction in the metaverse is chatgpt, a large language model trained by openai. the article delves into the pros and cons of utilizing chatgpt for metaverse-based education, entertainment, personalization, and support. dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. this article aims to help readers understand the possible influence of chatgpt on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.14007" target="_blank">'Team-in-the-Loop' Organisational Oversight of High-Stakes Ai</a></div>
<div class="paper-author">Deborah Morgan, Youmna Hashem, Vincent J. Straub, Jonathan Bright</div>
<div class="abstract">
<div class="abstract-content">
Abstract: oversight is rightly recognised as vital within high-stakes public sector ai applications, where decisions can have profound individual and collective impacts. much current thinking regarding forms of oversight mechanisms for ai within the public sector revolves around the idea of human decision makers being 'in-the-loop' and thus being able to intervene to prevent errors and potential harm. however, in a number of high-stakes public sector contexts, operational oversight of decisions is made by expert teams rather than individuals. the ways in which deployed ai systems can be integrated into these existing operational team oversight processes has yet to attract much attention. we address this gap by exploring the impacts of ai upon pre-existing oversight of clinical decision-making through institutional analysis. we find that existing oversight is nested within professional training requirements and relies heavily upon explanation and questioning to elicit vital information. professional bodies and liability mechanisms also act as additional levers of oversight. these dimensions of oversight are impacted, and potentially reconfigured, by ai systems. we therefore suggest a broader lens of 'team-in-the-loop' to conceptualise the system-level analysis required for adoption of ai within high-stakes public sector deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13408" target="_blank">Paraphrasing Evades Detectors of Ai-Generated Text, but Retrieval Is an Effective Defense</a></div>
<div class="paper-author">Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify ai-generated text, including those based on watermarking or outlier detection. however, the robustness of these detection algorithms to paraphrases of ai-generated text remains unclear. to stress test these detectors, we build a 11b parameter paraphrase generation model (dipper) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. using dipper to paraphrase text generated by three large language models (including gpt3.5-davinci-003) successfully evades several detectors, including watermarking, gptzero, detectgpt, and openai's text classifier. for example, dipper drops detection accuracy of detectgpt from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.   to increase the robustness of ai-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model api provider. given a candidate text, our algorithm searches a database of sequences previously generated by the api, looking for sequences that match the candidate text within a certain threshold. we empirically verify our defense using a database of 15m generations from a fine-tuned t5-xxl model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as ai-generated. we open-source our models, code and data.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12767" target="_blank">Can We Trust the Evaluation on Chatgpt?</a></div>
<div class="paper-author">Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt, the first large language model (llm) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. despite its evident usefulness, evaluating chatgpt's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via reinforcement learning from human feedback (rlhf). we highlight the issue of data contamination in chatgpt evaluations, with a case study of the task of stance detection. we discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12132" target="_blank">Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense</a></div>
<div class="paper-author">Andrei Kucharavy, Zachary Schillaci, Loïc Maréchal, Maxime Würsch, Ljiljana Dolamic, Remi Sabonnadiere, Dimitri Percia David, Alain Mermoud, Vincent Lenders</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative language models gained significant attention in late 2022 / early 2023, notably with the introduction of models refined to act consistently with users' expectations of interactions with ai (conversational models). arguably the focal point of public attention has been such a refinement of the gpt3 model -- the chatgpt and its subsequent integration with auxiliary capabilities, including search as part of microsoft bing. despite extensive prior research invested in their development, their performance and applicability to a range of daily tasks remained unclear and niche. however, their wider utilization without a requirement for technical expertise, made in large part possible through conversational fine-tuning, revealed the extent of their true capabilities in a real-world environment. this has garnered both public excitement for their potential applications and concerns about their capabilities and potential malicious uses. this review aims to provide a brief overview of the history, state of the art, and implications of generative language models in terms of their principles, abilities, limitations, and future prospects -- especially in the context of cyber-defense, with a focus on the swiss operational environment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.10854" target="_blank">Dynamic Documentation for Ai Systems</a></div>
<div class="paper-author">Soham Mehta, Anderson Rogers, Thomas Krendl Gilbert</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai documentation is a rapidly-growing channel for coordinating the design of ai technologies with policies for transparency and accessibility. calls to standardize and enact documentation of algorithmic harms and impacts are now commonplace. however, documentation standards for ai remain inchoate, and fail to match the capabilities and social effects of increasingly impactful architectures such as large language models (llms). in this paper, we show the limits of present documentation protocols, and argue for dynamic documentation as a new paradigm for understanding and evaluating ai systems. we first review canonical approaches to system documentation outside the context of ai, focusing on the complex history of environmental impact statements (eiss). we next compare critical elements of the eis framework to present challenges with algorithmic documentation, which have inherited the limitations of eiss without incorporating their strengths. these challenges are specifically illustrated through the growing popularity of model cards and two case studies of algorithmic impact assessment in china and canada. finally, we evaluate more recent proposals, including reward reports, as potential components of fully dynamic ai documentation protocols.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.11455" target="_blank">Large Language Models and Simple, Stupid Bugs</a></div>
<div class="paper-author">Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, Emily Morgan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the advent of powerful neural language models, ai-based systems to assist developers in coding tasks are becoming widely available; copilot is one such system. copilot uses codex, a large language model (llm), to complete code conditioned on a preceding "prompt". codex, however, is trained on public github repositories, viz., on code that may include bugs and vulnerabilities. previous studies [1], [2] show codex reproduces vulnerabilities seen in training. in this study, we examine how prone codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or sstubs in the msr community. we find that codex and similar llms do help avoid some sstubs, but do produce known, verbatim sstubs as much as 2x as likely than known, verbatim correct code. we explore the consequences of the codex generated sstubs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim sstubs, and increase the possibility of producing known, verbatim fixes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.11504" target="_blank">Language Model Behavior: A Comprehensive Survey</a></div>
<div class="paper-author">Tyler A. Chang, Benjamin K. Bergen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transformer language models have received widespread public attention, yet their generated text is often surprising even to nlp researchers. in this survey, we discuss over 250 recent studies of english language model behavior before task-specific fine-tuning. language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. we synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2307.06391" target="_blank">Maximizing Penetration Testing Success With Effective Reconnaissance Techniques Using Chatgpt</a></div>
<div class="paper-author">Sheetal Temara</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt is a generative pretrained transformer language model created using artificial intelligence implemented as chatbot which can provide very detailed responses to a wide variety of questions. as a very contemporary phenomenon, this tool has a wide variety of potential use cases that have yet to be explored. with the significant extent of information on a broad assortment of potential topics, chatgpt could add value to many information security uses cases both from an efficiency perspective as well as to offer another source of security information that could be used to assist with securing internet accessible assets of organizations. one information security practice that could benefit from chatgpt is the reconnaissance phase of penetration testing. this research uses a case study methodology to explore and investigate the uses of chatgpt in obtaining valuable reconnaissance data. chatgpt is able to provide many types of intel regarding targeted properties which includes internet protocol (ip) address ranges, domain names, network topology, vendor technologies, ssl/tls ciphers, ports & services, and operating systems used by the target. the reconnaissance information can then be used during the planning phase of a penetration test to determine the tactics, tools, and techniques to guide the later phases of the penetration test in order to discover potential risks such as unpatched software components and security misconfiguration related issues. the study provides insights into how artificial intelligence language models can be used in cybersecurity and contributes to the advancement of penetration testing techniques.   keywords: chatgpt, penetration testing, reconnaissance
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.10131" target="_blank">She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models</a></div>
<div class="paper-author">Christoph Treude, Hideaki Hata</div>
<div class="abstract">
<div class="abstract-content">
Abstract: implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. to address this bias, it is important to understand it in more detail. this study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning github issues and testing, are affected by implicit gender bias embedded in large language models. we systematically translated each task from english into a genderless language and back, and investigated the pronouns associated with each task. based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. specifically, requirements elicitation was associated with the pronoun "he" in only 6% of cases, while testing was associated with "he" in 100% of cases. additionally, tasks related to helping others had a 91% association with "he" while the same association for tasks related to asking coworkers was only 52%. these findings reveal a clear pattern of gender bias related to software development tasks and have important implications for addressing this issue both in the training of large language models and in broader society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.11156" target="_blank">Can Ai-Generated Text Be Reliably Detected?</a></div>
<div class="paper-author">Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this paper, both empirically and theoretically, we show that several ai-text detectors are not reliable in practical scenarios. empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (llm), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. we then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. for a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. our result is general enough to capture specific scenarios such as particular writing styles, clever prompt design, or text paraphrasing. we also extend the impossibility result to include the case where pseudorandom number generators are used for ai-text generation instead of true randomness. we show that the same result holds with a negligible correction term for all polynomial-time computable detectors. finally, we show that even llms protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden llm text signatures and add them to human-generated text to be detected as text generated by the llms, potentially causing reputational damage to their developers. we believe these results can open an honest conversation in the community regarding the ethical and reliable use of ai-generated text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09136" target="_blank">A Short Survey of Viewing Large Language Models in Legal Aspect</a></div>
<div class="paper-author">Zhongxiang Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have transformed many fields, including natural language processing, computer vision, and reinforcement learning. these models have also made a significant impact in the field of law, where they are being increasingly utilized to automate various legal tasks, such as legal judgement prediction, legal document analysis, and legal document writing. however, the integration of llms into the legal field has also raised several legal problems, including privacy concerns, bias, and explainability. in this survey, we explore the integration of llms into the field of law. we discuss the various applications of llms in legal tasks, examine the legal challenges that arise from their use, and explore the data resources that can be used to specialize llms in the legal domain. finally, we discuss several promising directions and conclude this paper. by doing so, we hope to provide an overview of the current state of llms in law and highlight the potential benefits and challenges of their integration.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09377" target="_blank">Protecting Society From Ai Misuse: When Are Restrictions on Capabilities Warranted?</a></div>
<div class="paper-author">Markus Anderljung, Julian Hazell</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence (ai) systems will increasingly be used to cause harm as they grow more capable. in fact, ai systems are already starting to be used to automate fraudulent activities, violate human rights, create harmful fake images, and identify dangerous toxins. to prevent some misuses of ai, we argue that targeted interventions on certain capabilities will be warranted. these restrictions may include controlling who can access certain types of ai models, what they can be used for, whether outputs are filtered or can be traced back to their user, and the resources needed to develop them. we also contend that some restrictions on non-ai capabilities needed to cause harm will be required. though capability restrictions risk reducing use more than misuse (facing an unfavorable misuse-use tradeoff), we argue that interventions on capabilities are warranted when other interventions are insufficient, the potential harm from misuse is high, and there are targeted ways to intervene on capabilities. we provide a taxonomy of interventions that can reduce ai misuse, focusing on the specific steps required for a misuse to cause harm (the misuse chain), and a framework to determine if an intervention is warranted. we apply this reasoning to three examples: predicting novel toxins, creating harmful images, and automating spear phishing campaigns.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09384" target="_blank">Llmseceval: A Dataset of Natural Language Prompts for Security Evaluations</a></div>
<div class="paper-author">Catherine Tony, Markus Mutas, Nicolás E. Díaz Ferreyra, Riccardo Scandariato</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) like codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. moreover, these models are capable of generating code snippets from natural language (nl) descriptions by learning languages and programming practices from public github repositories. although llms promise an effortless nl-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. in this work, we present llmseceval, a dataset containing 150 nl prompts that can be leveraged for assessing the security performance of such models. such prompts are nl descriptions of code snippets prone to various security vulnerabilities listed in mitre's top 25 common weakness enumeration (cwe) ranking. each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by llms. as a practical application, we show how llmseceval can be used for evaluating the security of snippets automatically generated from nl descriptions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12734" target="_blank">Multimodal Bias: Introducing a Framework for Stereotypical Bias Assessment Beyond Gender and Race in Vision Language Models</a></div>
<div class="paper-author">Sepehr Janghorbani, Gerard De Melo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent breakthroughs in self supervised training have led to a new class of pretrained vision language models. while there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. this is mainly due to lack of suitable benchmarks for such groups. we seek to address this gap by providing a visual and textual bias benchmark called mmbias, consisting of around 3,800 images and phrases covering 14 population subgroups. we utilize this dataset to assess bias in several prominent self supervised multimodal models, including clip, albef, and vilt. our results show that these models demonstrate meaningful bias favoring certain groups. finally, we introduce a debiasing method designed specifically for such large pre-trained models that can be applied as a post-processing step to mitigate bias, while preserving the remaining accuracy of the model.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13360" target="_blank">Towards the Scalable Evaluation of Cooperativeness in Language Models</a></div>
<div class="paper-author">Alan Chan, Maxime Riché, Jesse Clifton</div>
<div class="abstract">
<div class="abstract-content">
Abstract: it is likely that ai systems driven by pre-trained language models (plms) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution. consistent with the goals of cooperative ai \citep{dafoe_open_2020}, we wish to understand and shape the multi-agent behaviors of plms in a pro-social manner. an important first step is the evaluation of model behaviour across diverse cooperation problems. since desired behaviour in an interaction depends upon precise game-theoretic structure, we focus on generating scenarios with particular structures with both crowdworkers and a language model. our work proceeds as follows. first, we discuss key methodological issues in the generation of scenarios corresponding to particular game-theoretic structures. second, we employ both crowdworkers and a language model to generate such scenarios. we find that the quality of generations tends to be mediocre in both cases. we additionally get both crowdworkers and a language model to judge whether given scenarios align with their intended game-theoretic structure, finding mixed results depending on the game. third, we provide a dataset of scenario based on our data generated. we provide both quantitative and qualitative evaluations of unifiedqa and gpt-3 on this dataset. we find that instruct-tuned models tend to act in a way that could be perceived as cooperative when scaled up, while other models seemed to have flat scaling trends.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08721" target="_blank">Artificial Influence: An Analysis of Ai-Driven Persuasion</a></div>
<div class="paper-author">Matthew Burtell, Thomas Woodside</div>
<div class="abstract">
<div class="abstract-content">
Abstract: persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. advancements in artificial intelligence (ai) have produced ai systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. even systems that are not explicitly designed to persuade may do so in practice. in the future, increasingly anthropomorphic ai systems may form ongoing relationships with users, increasing their persuasive power. this paper investigates the uncertain future of persuasive ai systems. we examine ways that ai could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. we consider ways ai-driven persuasion could differ from human-driven persuasion. we warn that ubiquitous highlypersuasive ai systems could alter our information environment so significantly so as to contribute to a loss of human control of our own future. in response, we examine several potential responses to ai-driven persuasion: prohibition, identification of ai agents, truthful ai, and legal remedies. we conclude that none of these solutions will be airtight, and that individuals and governments will need to take active steps to guard against the most pernicious effects of persuasive ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08774" target="_blank">GPT-4 Technical Report</a></div>
<div class="paper-author">N/A Openai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we report the development of gpt-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. while less capable than humans in many real-world scenarios, gpt-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. gpt-4 is a transformer-based model pre-trained to predict the next token in a document. the post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. a core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. this allowed us to accurately predict some aspects of gpt-4's performance based on models trained with no more than 1/1,000th the compute of gpt-4.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.08896" target="_blank">Selfcheckgpt: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</a></div>
<div class="paper-author">Potsawee Manakul, Adian Liusie, Mark J. F. Gales</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative large language models (llms) such as gpt-3 are capable of generating highly fluent responses to a wide variety of user prompts. however, llms are known to hallucinate facts and make non-factual statements which can undermine trust in their output. existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as chatgpt) or external databases that are interfaced via separate, often complex, modules. in this work, we propose "selfcheckgpt", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. selfcheckgpt leverages the simple idea that if an llm has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. however, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. we investigate this approach by using gpt-3 to generate passages about individuals from the wikibio dataset, and manually annotate the factuality of the generated passages. we demonstrate that selfcheckgpt can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. we compare our approach to several baselines and show that our approach has considerably higher auc-pr scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.09067" target="_blank">Secret-Keeping in Question Answering</a></div>
<div class="paper-author">Nathaniel W. Rollings, "Kent O'Sullivan", Sakshum Kulshrestha</div>
<div class="abstract">
<div class="abstract-content">
Abstract: existing question-answering research focuses on unanswerable questions in the context of always providing an answer when a system can\dots but what about cases where a system {\bf should not} answer a question. this can either be to protect sensitive users or sensitive information. many models expose sensitive information under interrogation by an adversarial user. we seek to determine if it is possible to teach a question-answering system to keep a specific fact secret. we design and implement a proof-of-concept architecture and through our evaluation determine that while possible, there are numerous directions for future research to reduce system paranoia (false positives), information leakage (false negatives) and extend the implementation of the work to more complex problems with preserving secrecy in the presence of information aggregation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06854" target="_blank">Robust Contrastive Language-Image Pretraining Against Adversarial Attacks</a></div>
<div class="paper-author">Wenhan Yang, Baharan Mirzasoleiman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: contrastive vision-language representation learning has achieved state-of-the-art performance for zero-shot classification, by learning from millions of image-caption pairs crawled from the internet. however, the massive data that powers large multimodal models such as clip, makes them extremely vulnerable to various types of adversarial attacks, including targeted and backdoor data poisoning attacks. despite this vulnerability, robust contrastive vision-language pretraining against adversarial attacks has remained unaddressed. in this work, we propose roclip, the first effective method for robust pretraining {and fine-tuning} multimodal vision-language models. roclip effectively breaks the association between poisoned image-caption pairs by considering a pool of random examples, and (1) matching every image with the text that is most similar to its caption in the pool, and (2) matching every caption with the image that is most similar to its image in the pool. our extensive experiments show that our method renders state-of-the-art targeted data poisoning and backdoor attacks ineffective during pre-training or fine-tuning of clip. in particular, roclip decreases the poison and backdoor attack success rates down to 0\% during pre-training and 1\%-4\% during fine-tuning, and effectively improves the model's performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07024" target="_blank">Addressing Biases in the Texts Using an End-to-End Pipeline Approach</a></div>
<div class="paper-author">Shaina Raza, Syed Raza Bashir, N/A Sneha, Urooj Qamar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the concept of fairness is gaining popularity in academia and industry. social media is especially vulnerable to media biases and toxic language and comments. we propose a fair ml pipeline that takes a text as input and determines whether it contains biases and toxic content. then, based on pre-trained word embeddings, it suggests a set of new words by substituting the bi-ased words, the idea is to lessen the effects of those biases by replacing them with alternative words. we compare our approach to existing fairness models to determine its effectiveness. the results show that our proposed pipeline can de-tect, identify, and mitigate biases in social media data
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07247" target="_blank">Are Models Trained on Indian Legal Data Fair?</a></div>
<div class="paper-author">Sahil Girhepuje, Anmol Goel, Gokul S Krishnan, Shreya Goyal, Satyendra Pandey, Ponnurangam Kumaraguru, Balaraman Ravindran</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances and applications of language technology and artificial intelligence have enabled much success across multiple domains like law, medical and mental health. ai-based language models, like judgement prediction, have recently been proposed for the legal sector. however, these models are strife with encoded social biases picked up from the training data. while bias and fairness have been studied across nlp, most studies primarily locate themselves within a western context. in this work, we present an initial investigation of fairness from the indian perspective in the legal domain. we highlight the propagation of learnt algorithmic biases in the bail prediction task for models trained on hindi legal documents. we evaluate the fairness gap using demographic parity and show that a decision tree model trained for the bail prediction task has an overall fairness disparity of 0.237 between input features associated with hindus and muslims. additionally, we highlight the need for further research and studies in the avenues of fairness/bias in applying ai in the legal sector with a specific focus on the indian context.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06223" target="_blank">Who's Thinking? A Push for Human-Centered Evaluation of LLMS Using the Xai Playbook</a></div>
<div class="paper-author">Teresa Datta, John P. Dickerson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: deployed artificial intelligence (ai) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. human-centered evaluation of ai-based systems combines quantitative and qualitative analysis and human input. it has been explored to some depth in the explainable ai (xai) and human-computer interaction (hci) communities. gaps remain, but the basic understanding that humans interact with ai and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. in this paper, we draw parallels between the relatively mature field of xai and the rapidly evolving research boom around large language models (llms). accepted evaluative metrics for llms are not human-centered. we argue that many of the same paths tread by the xai community over the past decade will be retread when discussing llms. specifically, we argue that humans' tendencies -- again, complete with their cognitive biases and quirks -- should rest front and center when evaluating deployed llms. we outline three developed focus areas of human-centered evaluation of xai: mental models, use case utility, and cognitive engagement, and we highlight the importance of exploring each of these concepts for llms. our goal is to jumpstart human-centered llm evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.06273" target="_blank">Consistency Analysis of Chatgpt</a></div>
<div class="paper-author">Myeongjun Erik Jang, Thomas Lukasiewicz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt has gained a huge popularity since its introduction. its positive aspects have been reported through many media platforms, and some analyses even showed that chatgpt achieved a decent grade in professional exams, adding extra support to the claim that ai can now assist and even replace humans in industrial fields. others, however, doubt its reliability and trustworthiness. this paper investigates the trustworthiness of chatgpt and gpt-4 regarding logically consistent behaviour, focusing specifically on semantic consistency and the properties of negation, symmetric, and transitive consistency. our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. we also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (llms) are unlikely to be the ultimate solution to resolve the inconsistency issue of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-09</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.05453" target="_blank">Personalisation Within Bounds: A Risk Taxonomy and Policy Framework for the Alignment of Large Language Models With Personalised Feedback</a></div>
<div class="paper-author">Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like chatgpt or search engines like bing. this intensifies the need to ensure that models are aligned with human preferences and do not produce unsafe, inaccurate or toxic outputs. while alignment techniques like reinforcement learning with human feedback (rlhf) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values. different people may legitimately disagree on their preferences for language and conversational norms, as well as on values or ideologies which guide their communication. personalising llms through micro-level preference learning processes may result in models that are better aligned with each user. however, there are several normative challenges in defining the bounds of a societally-acceptable and safe degree of personalisation. in this paper, we ask how, and in what ways, llms should be personalised. first, we review literature on current paradigms for aligning llms with human feedback, and identify issues including (i) a lack of clarity regarding what alignment means; (ii) a tendency of technology providers to prescribe definitions of inherently subjective preferences and values; and (iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in who we are really aligning to. second, we present a taxonomy of benefits and risks associated with personalised llms, for individuals and society at large. finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable llm-behaviours within (supra-)national and organisational bounds.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.04381" target="_blank">Automatically Auditing Large Language Models via Discrete Optimization</a></div>
<div class="paper-author">Erik Jones, Anca Dragan, Aditi Raghunathan, Jacob Steinhardt</div>
<div class="abstract">
<div class="abstract-content">
Abstract: auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. in this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. for example, we might aim to find a non-toxic input that starts with "barack obama" that a model maps to a toxic output. this optimization problem is difficult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. to combat these challenges, we introduce a discrete optimization algorithm, arca, that jointly and efficiently optimizes over inputs and outputs. our approach automatically uncovers derogatory completions about celebrities (e.g. "barack obama is a legalized unborn" -&gt; "child murderer"), produces french inputs that complete to english outputs, and finds inputs that generate a specific name. our work offers a promising new tool to uncover models' failure-modes before deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.04729" target="_blank">On the Risks of Stealing the Decoding Algorithms of Language Models</a></div>
<div class="paper-author">Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a key component of generating text from modern language models (lm) is the selection and tuning of decoding algorithms. these algorithms determine how to generate text from the internal probability distribution generated by the lm. the process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. in this work, we show, for the first time, that an adversary with typical api access to an lm can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. our attack is effective against popular lms used in text generation apis, including gpt-2 and gpt-3. we demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of gpt-3.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.12942" target="_blank">A Survey on Explainable Artificial Intelligence for Cybersecurity</a></div>
<div class="paper-author">Gaith Rjoub, Jamal Bentahar, Omar Abdel Wahab, Rabeb Mizouni, Alyssa Song, Robin Cohen, Hadi Otrok, Azzam Mourad</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the black-box nature of artificial intelligence (ai) models has been the source of many concerns in their use for critical applications. explainable artificial intelligence (xai) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. in the field of network cybersecurity, xai has the potential to revolutionize the way we approach network security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. in this survey, we review the state of the art in xai for cybersecurity in network systems and explore the various approaches that have been proposed to address this important problem. the review follows a systematic classification of network-driven cybersecurity threats and issues. we discuss the challenges and limitations of current xai methods in the context of cybersecurity and outline promising directions for future research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03124" target="_blank">Ifan: An Explainability-Focused Interaction Framework for Humans and NLP Models</a></div>
<div class="paper-author">Edoardo Mosca, Daryna Dementieva, Tohid Ebrahim Ajdari, Maximilian Kummeth, Kirill Gringauz, Yutong Zhou, Georg Groh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: interpretability and human oversight are fundamental pillars of deploying complex nlp models into real-world applications. however, applying explainability and human-in-the-loop methods requires technical proficiency. despite existing toolkits for model understanding and analysis, options to integrate human feedback are still limited. we propose ifan, a framework for real-time explanation-based interaction with nlp models. through ifan's interface, users can provide feedback to selected model explanations, which is then integrated through adapter layers to align the model with human rationale. we show the system to be effective in debiasing a hate speech classifier with minimal impact on performance. ifan also offers a visual admin system and api to manage models (and datasets) as well as control access rights. a demo is live at https://ifan.ml.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03139" target="_blank">Low Impact Agency: Review and Discussion</a></div>
<div class="paper-author">Danilo Naiff, Shashwat Goel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: powerful artificial intelligence poses an existential threat if the ai decides to drastically change the world in pursuit of its goals. the hope of low-impact artificial intelligence is to incentivize ai to not do that just because this causes a large impact in the world. in this work, we first review the concept of low-impact agency and previous proposals to approach the problem, and then propose future research directions in the topic, with the goal to ensure low-impactedness is useful in making ai safe.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.03174" target="_blank">Both Eyes Open: Vigilant Incentives Help Regulatory Markets Improve Ai Safety</a></div>
<div class="paper-author">Paolo Bova, Alessandro Di Stefano, The Anh Han</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in the context of rapid discoveries by leaders in ai, governments must consider how to design regulation that matches the increasing pace of new ai capabilities. regulatory markets for ai is a proposal designed with adaptability in mind. it involves governments setting outcome-based targets for ai companies to achieve, which they can show by purchasing services from a market of private regulators. we use an evolutionary game theory model to explore the role governments can play in building a regulatory market for ai systems that deters reckless behaviour. we warn that it is alarmingly easy to stumble on incentives which would prevent regulatory markets from achieving this goal. these 'bounty incentives' only reward private regulators for catching unsafe behaviour. we argue that ai companies will likely learn to tailor their behaviour to how much effort regulators invest, discouraging regulators from innovating. instead, we recommend that governments always reward regulators, except when they find that those regulators failed to detect unsafe behaviour that they should have. these 'vigilant incentives' could encourage private regulators to find innovative ways to evaluate cutting-edge ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.01325" target="_blank">A Pathway Towards Responsible Ai Generated Content</a></div>
<div class="paper-author">Chen Chen, Jie Fu, Lingjuan Lyu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai generated content (aigc) has received tremendous attention within the past few years, with content ranging from image, text, to audio, video, etc. meanwhile, aigc has become a double-edged sword and recently received much criticism regarding its responsible usage. in this vision paper, we focus on three main concerns that may hinder the healthy development and deployment of aigc in practice, including risks from privacy, bias, toxicity, misinformation, and intellectual property (ip). by documenting known and potential risks, as well as any possible misuse scenarios of aigc, the aim is to draw attention to potential risks and misuse, help society to eliminate obstacles, and promote the more ethical and secure deployment of aigc. additionally, we provide insights into the promising directions for tackling these risks while constructing generative models, enabling aigc to be used responsibly to benefit society.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-03-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.00293" target="_blank">How Robust Is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks</a></div>
<div class="paper-author">Xuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi Zhang, Xuanjing Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the gpt-3.5 models have demonstrated impressive performance in various natural language processing (nlp) tasks, showcasing their strong understanding and reasoning capabilities. however, their robustness and abilities to handle various complexities of the open world have yet to be explored, which is especially crucial in assessing the stability of models and is a key aspect of trustworthy ai. in this study, we perform a comprehensive experimental analysis of gpt-3.5, exploring its robustness using 21 datasets (about 116k test samples) with 66 text transformations from textflint that cover 9 popular natural language understanding (nlu) tasks. our findings indicate that while gpt-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74\% and 43.59\% in natural language inference and sentiment analysis tasks, respectively. we also show that gpt-3.5 faces some specific robustness challenges, including robustness instability, prompt sensitivity, and number sensitivity. these insights are valuable for understanding its limitations and guiding future research in addressing these challenges to enhance gpt-3.5's overall performance and generalization abilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.00333" target="_blank">Competence-Based Analysis of Language Models</a></div>
<div class="paper-author">Adam Davies, Jize Jiang, Chengxiang Zhai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite the recent success of large pretrained language models (lms) on a variety of prompting tasks, these models can be alarmingly brittle to small changes in inputs or application contexts. to better understand such behavior and motivate the design of more robust lms, we propose a general experimental framework, calm (competence-based analysis of language models), where targeted causal interventions are utilized to damage an lm's internal representation of various linguistic properties in order to evaluate its use of each representation in performing a given task. we implement these interventions as gradient-based adversarial attacks, which (in contrast to prior causal probing methodologies) are able to target arbitrarily-encoded representations of relational properties, and carry out a case study of this approach to analyze how bert-like lms use representations of several relational properties in performing associated relation prompting tasks. we find that, while the representations lms leverage in performing each task are highly entangled, they may be meaningfully interpreted in terms of the tasks where they are most utilized; and more broadly, that calm enables an expanded scope of inquiry in lm analysis that may be useful in predicting and explaining weaknesses of existing lms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.01229" target="_blank">Almanac: Retrieval-Augmented Language Models for Clinical Medicine</a></div>
<div class="paper-author">Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex R. Dalal, Jennifer L. Kim, Michael Moor, Kevin Alexander, Euan Ashley, Jack Boyd, Kathleen Boyd, Karen Hirsch, Curt Langlotz, Joanna Nelson, William Hiesinger</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-language models have recently demonstrated impressive zero-shot capabilities in a variety of natural language tasks such as summarization, dialogue generation, and question-answering. despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements. in this study, we develop almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations. performance on a novel dataset of clinical scenarios (n = 130) evaluated by a panel of 5 board-certified and resident physicians demonstrates significant increases in factuality (mean of 18% at p-value &lt; 0.05) across all specialties, with improvements in completeness and safety. our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importance of careful testing and deployment to mitigate their shortcomings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.13681" target="_blank">The (Ab)use of Open Source Code to Train Large Language Models</a></div>
<div class="paper-author">Ali Al-Kaswan, Maliheh Izadi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in recent years, large language models (llms) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as software engineering. llms for code are commonly trained on large unsanitized corpora of source code scraped from the internet. the content of these datasets is memorized and emitted by the models, often in a verbatim manner. in this work, we will discuss the security, privacy, and licensing implications of memorization. we argue why the use of copyleft code to train llms is a legal and ethical dilemma. finally, we provide four actionable recommendations to address this issue.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.14003" target="_blank">Systematic Rectification of Language Models via Dead-End Analysis</a></div>
<div class="paper-author">Meng Cao, Mehdi Fatemi, Jackie Chi Kit Cheung, Samira Shabanian</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with adversarial or otherwise normal prompts, existing large language models (llm) can be pushed to generate toxic discourses. one way to reduce the risk of llms generating undesired discourses is to alter the training of the llm. this can be very restrictive due to demanding computation requirements. other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. that is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. to this end, we formally extend the dead-end theory from the recent reinforcement learning (rl) literature to also cover uncertain outcomes. our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse llms as long as they share the same vocabulary. importantly, our method does not require access to the internal representations of the llm, but only the token probability distribution at each decoding step. this is crucial as many llms today are hosted in servers and only accessible through apis. when applied to various llms, including gpt-3, our approach significantly improves the generated discourse compared to the base llms and other techniques in terms of both the overall language and detoxification performance.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.00001" target="_blank">Reward Design With Language Models</a></div>
<div class="paper-author">Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward design in reinforcement learning (rl) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. can we instead cheaply design rewards using a natural language interface? this paper explores how to simplify reward design by prompting a large language model (llm) such as gpt-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. our approach leverages this proxy reward function in an rl framework. specifically, users specify a prompt once at the beginning of training. during training, the llm evaluates an rl agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. the rl agent then uses this reward to update its behavior. we evaluate whether our approach can train agents aligned with user objectives in the ultimatum game, matrix games, and the dealornodeal negotiation task. in all three tasks, we show that rl agents trained with our framework are well-aligned with the user's objectives and outperform rl agents trained with reward functions learned via supervised learning
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.13439" target="_blank">Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models</a></div>
<div class="paper-author">Kaitlyn Zhou, Dan Jurafsky, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: despite increasingly fluent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. we argue that a key dimension that is missing from our understanding of language models (lms) is the model's ability to interpret and generate expressions of uncertainty. whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. the increasing deployment of lms in the wild motivates us to investigate whether lms are capable of interpreting expressions of uncertainty and how lms' behaviors change when learning to emit their own expressions of uncertainty. when injecting expressions of uncertainty into prompts (e.g., "i think the answer is..."), we discover that gpt3's generations vary upwards of 80% in accuracy based on the expression used. we analyze the linguistic characteristics of these expressions and find a drop in accuracy when naturalistic expressions of certainty are present. we find similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than uncertainty. together, these results highlight the challenges of building lms that interpret and generate trustworthy expressions of uncertainty.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.13136" target="_blank">Toward Fairness in Text Generation via Mutual Information Minimization Based on Importance Sampling</a></div>
<div class="paper-author">Rui Wang, Pengyu Cheng, Ricardo Henao</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms), such as gpt2, have achieved remarkable empirical performance in text generation tasks. however, pretrained on large-scale natural language corpora, the generated text from plms may exhibit social bias against disadvantaged demographic groups. to improve the fairness of plms in text generation, we propose to minimize the mutual information between the semantics in the generated text sentences and their demographic polarity, i.e., the demographic group to which the sentence is referring. in this way, the mentioning of a demographic group (e.g., male or female) is encouraged to be independent from how it is described in the generated text, thus effectively alleviating the social bias. moreover, we propose to efficiently estimate the upper bound of the above mutual information via importance sampling, leveraging a natural language corpus. we also propose a distillation mechanism that preserves the language modeling ability of the plms after debiasing. empirical results on real-world benchmarks demonstrate that the proposed method yields superior performance in term of both fairness and language modeling ability.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.17511" target="_blank">On Pitfalls (And Advantages) of Sophisticated Large Language Models</a></div>
<div class="paper-author">Anna Strasser</div>
<div class="abstract">
<div class="abstract-content">
Abstract: natural language processing based on large language models (llms) is a booming field of ai research. after neural networks have proven to outperform humans in games and practical domains based on pattern recognition, we might stand now at a road junction where artificial entities might eventually enter the realm of human communication. however, this comes with serious risks. due to the inherent limitations regarding the reliability of neural networks, overreliance on llms can have disruptive consequences. since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. this begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism. this also concerns the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it makes a massive spread of misinformation possible.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12461" target="_blank">Analyzing and Editing Inner Mechanisms of Backdoored Language Models</a></div>
<div class="paper-author">Max Lamparth, Anka Reuel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. a description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. in this work, we study the internal representations of transformer-based backdoored language models and determine early-layer mlp modules as most important for the backdoor mechanism in combination with the initial embedding projection. we use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the mlp module outputs to essentials for the backdoor mechanism. to this end, we introduce pcp ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. we demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. we show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets.   trigger warning: offensive language.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12578" target="_blank">Fairness in Language Models Beyond English: Gaps and Challenges</a></div>
<div class="paper-author">Krithika Ramesh, Sunayana Sitaram, Monojit Choudhury</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. most research on evaluating and mitigating fairness harms has been concentrated on english, while multilingual models and non-english languages have received comparatively little attention. this paper presents a survey of fairness in multilingual and non-english contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for english. we contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-23</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12173" target="_blank">Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications With Indirect Prompt Injection</a></div>
<div class="paper-author">Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) are increasingly being integrated into various applications. the functionalities of recent llms can be flexibly modulated via natural language prompts. this renders them susceptible to targeted adversarial prompting, e.g., prompt injection (pi) attacks enable attackers to override original instructions and employed controls. so far, it was assumed that the user is directly prompting the llm. but, what if it is not the user prompting? we argue that llm-integrated applications blur the line between data and instructions. we reveal new attack vectors, using indirect prompt injection, that enable adversaries to remotely (without a direct interface) exploit llm-integrated applications by strategically injecting prompts into data likely to be retrieved. we derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. we demonstrate our attacks' practical viability against both real-world systems, such as bing's gpt-4 powered chat and code-completion engines, and synthetic applications built on gpt-4. we show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other apis are called. despite the increasing integration and reliance on llms, effective mitigations of these emerging threats are currently lacking. by raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.11520" target="_blank">Guiding Large Language Models via Directional Stimulus Prompting</a></div>
<div class="paper-author">Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we introduce directional stimulus prompting, a novel framework for guiding black-box large language models (llms) toward specific desired outputs. instead of directly adjusting llms, our method employs a small tunable policy model (e.g., t5) to generate an auxiliary directional stimulus prompt for each input instance. these directional stimulus prompts act as nuanced, instance-specific hints and clues to guide llms in generating desired outcomes, such as including specific keywords in the generated summary. our approach sidesteps the challenges of direct llm tuning by optimizing the policy model to explore directional stimulus prompts that align llms with desired behaviors. the policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the llm's output. we assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. our experiments demonstrate that the framework consistently improves llms' (e.g., chatgpt, codex, instructgpt) performance on these supervised tasks using minimal labeled data. notably, using just 80 dialogues on the multiwoz dataset, our approach enhances chatgpt's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. additionally, the instance-specific chain-of-thought prompt generated by our approach improves instructgpt's reasoning accuracy compared to human-crafted or automatically generated prompts. the code and data are publicly available at \url{https://github.com/leezekun/directional-stimulus-prompting}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.12095" target="_blank">On the Robustness of Chatgpt: An Adversarial and Out-of-Distribution Perspective</a></div>
<div class="paper-author">Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang, Xing Xie</div>
<div class="abstract">
<div class="abstract-content">
Abstract: chatgpt is a recent chatbot service released by openai and is receiving increasing attention over the past few months. while evaluations of various aspects of chatgpt have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. robustness is of particular concern in responsible ai, especially for safety-critical applications. in this paper, we conduct a thorough evaluation of the robustness of chatgpt from the adversarial and out-of-distribution (ood) perspective. to do so, we employ the advglue and anli benchmarks to assess adversarial robustness and the flipkart review and ddxplus medical diagnosis datasets for ood evaluation. we select several popular foundation models as baselines. results show that chatgpt shows consistent advantages on most adversarial and ood classification and translation tasks. however, the absolute performance is far from perfection, which suggests that adversarial and ood robustness remains a significant threat to foundation models. moreover, chatgpt shows astounding performance in understanding dialogue-related texts and we find that it tends to provide informal suggestions for medical tasks instead of definitive answers. finally, we present in-depth discussions of possible research directions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2304.12298" target="_blank">Badgpt: Exploring Security Vulnerabilities of Chatgpt via Backdoor Attacks to Instructgpt</a></div>
<div class="paper-author">Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, chatgpt has gained significant attention in research due to its ability to interact with humans effectively. the core idea behind this model is reinforcement learning (rl) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., instructgpt. in this study, we propose badgpt, the first backdoor attack against rl fine-tuning in language models. by injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. our initial experiments on movie reviews, i.e., imdb, demonstrate that an attacker can manipulate the generated text through badgpt.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09618" target="_blank">Multilingual Content Moderation: A Case Study on Reddit</a></div>
<div class="paper-author">Meng Ye, Karan Sikka, Katherine Atwell, Sabit Hassan, Ajay Divakaran, Malihe Alikhani</div>
<div class="abstract">
<div class="abstract-content">
Abstract: content moderation is the process of flagging content based on pre-defined platform rules. there has been a growing need for ai moderators to safeguard users as well as protect the mental health of human moderators from traumatic content. while prior works have focused on identifying hateful/offensive language, they are not adequate for meeting the challenges of content moderation since 1) moderation decisions are based on violation of rules, which subsumes detection of offensive speech, and 2) such rules often differ across communities which entails an adaptive solution. we propose to study the challenges of content moderation by introducing a multilingual dataset of 1.8 million reddit comments spanning 56 subreddits in english, german, spanish and french. we perform extensive experimental analysis to highlight the underlying challenges and suggest related research problems such as cross-lingual transfer, learning under label noise (human biases), transfer of moderation models, and predicting the violated rule. our dataset and analysis can help better prepare for the challenges and opportunities of auto moderation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09270" target="_blank">Recent Advances Towards Safe, Responsible, and Moral Dialogue Systems: A Survey</a></div>
<div class="paper-author">Jiawen Deng, Hao Sun, Zhexin Zhang, Jiale Cheng, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with the development of artificial intelligence, dialogue systems have been endowed with amazing chit-chat capabilities, and there is widespread interest and discussion about whether the generated contents are socially beneficial. in this paper, we present a new perspective of research scope towards building a safe, responsible, and modal dialogue system, including 1) abusive and toxic contents, 2) unfairness and discrimination, 3) ethics and morality issues, and 4) risk of misleading and privacy information. besides, we review the mainstream methods for evaluating the safety of large models from the perspectives of exposure and detection of safety issues. the recent advances in methodologies for the safety improvement of both end-to-end dialogue systems and pipeline-based models are further introduced. finally, we discussed six existing challenges towards responsible ai: explainable safety monitoring, continuous learning of safety issues, robustness against malicious attacks, multimodal information processing, unified research framework, and multidisciplinary theory integration. we hope this survey will inspire further research toward safer dialogue systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09420" target="_blank">Robustnlp: A Technique to Defend NLP Models Against Backdoor Attacks</a></div>
<div class="paper-author">Marwan Omar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as machine learning (ml) systems are being increasingly employed in the real world to handle sensitive tasks and make decisions in various fields, the security and privacy of those models have also become increasingly critical. in particular, deep neural networks (dnn) have been shown to be vulnerable to backdoor attacks whereby adversaries have access to the training data and the opportunity to manipulate such data by inserting carefully developed samples into the training dataset. although the nlp community has produced several studies on generating backdoor attacks proving the vulnerable state of language modes, to the best of our knowledge, there does not exist any work to combat such attacks. to bridge this gap, we present robustencoder: a novel clustering-based technique for detecting and removing backdoor attacks in the text domain. extensive empirical results demonstrate the effectiveness of our technique in detecting and removing backdoor triggers. our code is available at https://github.com/marwanomar1/backdoor-learning-for-nlp
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08957" target="_blank">Like a Good Nearest Neighbor: Practical Content Moderation With Sentence Transformers</a></div>
<div class="paper-author">Luke Bates, Iryna Gurevych</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models. setfit (tunstall et al., 2022) is a recent, practical approach that fine-tunes a sentence transformer under a contrastive learning paradigm and achieves similar results to more unwieldy systems. text classification is important for addressing the problem of domain drift in detecting harmful content, which plagues all social media platforms. here, we propose like a good nearest neighbor (lagonn), an inexpensive modification to setfit that requires no additional parameters or hyperparameters but modifies input with information about its nearest neighbor, for example, the label and text, in the training data, making novel data appear similar to an instance on which the model was optimized. lagonn is effective at the task of detecting harmful content and generally improves performance compared to setfit. to demonstrate the value of our system, we conduct a thorough study of text classification systems in the context of content moderation under four label distributions.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.09185" target="_blank">Bounding the Capabilities of Large Language Models in Open Text Generation With Prompt Constraints</a></div>
<div class="paper-author">Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, Diyi Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the limits of open-ended generative models are unclear, yet increasingly important. what causes them to succeed and what causes them to fail? in this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. we present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. these constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. we then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. using the gpt-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures. we also show the generalizability of our proposed method on other large models like bloom and opt. our results and our in-context mitigation strategies reveal open challenges for future research. we have publicly released our code at https://github.com/salt-nlp/bound-cap-llm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08500" target="_blank">Auditing Large Language Models: A Three-Layered Approach</a></div>
<div class="paper-author">Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) represent a major advance in artificial intelligence (ai) research. however, the widespread use of llms is also coupled with significant ethical and social challenges. previous research has pointed towards auditing as a promising governance mechanism to help ensure that ai systems are designed and deployed in ways that are ethical, legal, and technically robust. however, existing auditing procedures fail to address the governance challenges posed by llms, which display emergent capabilities and are adaptable to a wide range of downstream tasks. in this article, we address that gap by outlining a novel blueprint for how to audit llms. specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate llms), model audits (of llms after pre-training but prior to their release), and application audits (of applications based on llms) complement and inform each other. we show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by llms. however, it is important to remain realistic about what auditing can reasonably be expected to achieve. therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing llms at all. ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate llms from technical, ethical, and legal perspectives.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.08582" target="_blank">Pretraining Language Models With Human Preferences</a></div>
<div class="paper-author">Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L. Buckley, Jason Phang, Samuel R. Bowman, Ethan Perez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are pretrained to imitate internet text, including content that would violate human preferences if generated by an lm: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. here, we explore alternative objectives for pretraining lms in a way that also guides them to generate text aligned with human preferences. we benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained lms. we find a pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. moreover, conditional training maintains the downstream task performance of standard lm pretraining, both before and after task-specific finetuning. pretraining with human feedback results in much better preference satisfaction than standard lm pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. our results suggest that we should move beyond imitation learning when pretraining lms and incorporate human preferences from the start of training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06975" target="_blank">A Review of the Role of Causality in Developing Trustworthy Ai Systems</a></div>
<div class="paper-author">Niloy Ganguly, Dren Fazlija, Maryam Badar, Marco Fisichella, Sandipan Sikdar, Johanna Schrader, Jonas Wallat, Koustav Rudra, Manolis Koubarakis, Gourab K. Patro, Wadhah Zai El Amri, Wolfgang Nejdl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: state-of-the-art ai models largely lack an understanding of the cause-effect relationship that governs human understanding of the real world. consequently, these models do not generalize to unseen data, often produce unfair results, and are difficult to interpret. this has led to efforts to improve the trustworthiness aspects of ai models. recently, causal modeling and inference methods have emerged as powerful tools. this review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of ai models. we hope that our contribution will motivate future research on causality-based solutions for trustworthy ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07268" target="_blank">Ai Chat Assistants Can Improve Conversations About Divisive Topics</a></div>
<div class="paper-author">Lisa P. Argyle, Ethan Busby, Joshua Gubler, Chris Bail, Thomas Howe, Christopher Rytting, David Wingate</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a rapidly increasing amount of human conversation occurs online. but divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. we present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. we find that these interventions improve the reported quality of the conversation, reduce political divisiveness, and improve the tone, without systematically changing the content of the conversation or moving people's policy attitudes. these findings have important implications for future research on social media, political deliberation, and the growing community of scholars interested in the place of artificial intelligence within computational social science.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07371" target="_blank">Biastestgpt: Using Chatgpt for Social Bias Testing of Language Models</a></div>
<div class="paper-author">Rafal Kocielnik, Shrimai Prabhumoye, Vivian Zhang, Roy Jiang, R. Michael Alvarez, Anima Anandkumar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms) harbor inherent social biases that can result in harmful real-world implications. such social biases are measured through the probability values that plms output for different social groups and attributes appearing in a set of test sentences. however, bias testing is currently cumbersome since the test sentences are generated either from a limited set of manual templates or need expensive crowd-sourcing. we instead propose using chatgpt for controllable generation of test sentences, given any arbitrary user-specified combination of social groups and attributes appearing in the test sentences. when compared to template-based methods, our approach using chatgpt for test sentence generation is superior in detecting social bias, especially in challenging settings such as intersectional biases. we present an open-source comprehensive bias testing framework (biastestgpt), hosted on huggingface, that can be plugged into any open-source plm for bias testing. we provide a large diverse dataset of test sentences generated by chatgpt that satisfies the specified social group and attribute requirements and matches the quality of human-generated sentences. we thus enable seamless open-ended social bias testing of plms through an automatic large-scale generation of diverse test sentences for any combination of social categories and attributes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07388" target="_blank">Adding Instructions During Pretraining: Effective Way of Controlling Toxicity in Language Models</a></div>
<div class="paper-author">Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained large language models have become indispensable for solving various natural language processing (nlp) tasks. however, safely deploying them in real world applications is challenging because they generate toxic content. to address this challenge, we propose two novel pretraining data augmentation strategies that significantly reduce model toxicity without compromising its utility. our two strategies are: (1) meda: adds raw toxicity score as meta-data to the pretraining samples, and (2) inst: adds instructions to those samples indicating their toxicity. our results indicate that our best performing strategy (inst) substantially reduces the toxicity probability up to 61% while preserving the accuracy on five benchmark nlp tasks as well as improving auc scores on four bias detection tasks by 1.3%. we also demonstrate the generalizability of our techniques by scaling the number of training samples and the number of model parameters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07459" target="_blank">The Capacity for Moral Self-Correction in Large Language Models</a></div>
<div class="paper-author">Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova Dassarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we test the hypothesis that language models trained with reinforcement learning from human feedback (rlhf) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. we find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. we find that the capability for moral self-correction emerges at 22b model parameters, and typically improves with increasing model size and rlhf training. we believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. as such, they can follow instructions to avoid certain kinds of morally harmful outputs. we believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06321" target="_blank">Parameter-Efficient Modularised Bias Mitigation via Adapterfusion</a></div>
<div class="paper-author">Deepak Kumar, Oleg Lesota, George Zerveas, Daniel Cohen, Carsten Eickhoff, Markus Schedl, Navid Rekabsaz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large pre-trained language models contain societal biases and carry along these biases to downstream tasks. current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model's parameters, effectively transferring the model to a new, irreversible debiased state. in this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. drawing from the concept of adapterfusion in multi-task learning, we introduce dam (debiasing with adapter modules) - a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. we conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. our results show that dam improves or maintains the effectiveness of bias mitigation, avoids catastrophic forgetting in a multi-attribute scenario, and maintains on-par task performance, while granting parameter-efficiency and easy switching between the original and debiased models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06541" target="_blank">Towards Agile Text Classifiers for Everyone</a></div>
<div class="paper-author">Maximilian Mozes, Jessica Hoffmann, Katrin Tomanek, Muhamed Kouate, Nithum Thain, Ann Yuan, Tolga Bolukbasi, Lucas Dixon</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. however, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. this paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like palm 62b, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. we argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. instead of collecting millions of examples to attempt to create universal safety classifiers over months or years, classifiers could be tuned using small datasets, created by individuals or small organizations, tailored for specific use cases, and iterated on and adapted in the time-span of a day.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06588" target="_blank">Raising the Cost of Malicious Ai-Powered Image Editing</a></div>
<div class="paper-author">Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, Aleksander Madry</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present an approach to mitigating the risks of malicious image editing posed by large diffusion models. the key idea is to immunize images so as to make them resistant to manipulation by these models. this immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. we provide two methods for crafting such perturbations, and then demonstrate their efficacy. finally, we discuss a policy component necessary to make our approach fully effective and practical -- one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.06598" target="_blank">Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning</a></div>
<div class="paper-author">Maximilian Mozes, Tolga Bolukbasi, Ann Yuan, Frederick Liu, Nithum Thain, Lucas Dixon</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained large language models (llms) are able to solve a wide variety of tasks through transfer learning. various explainability methods have been developed to investigate their decision making process. tracin (pruthi et al., 2020) is one such gradient-based method which explains model inferences based on the influence of training examples. in this paper, we explore the use of tracin to improve model performance in the parameter-efficient tuning (pet) setting. we develop conversational safety classifiers via the prompt-tuning pet method and show how the unique characteristics of the pet regime enable tracin to identify the cause for certain misclassifications by llms. we develop a new methodology for using gradient-based explainability techniques to improve model performance, g-bair: gradient-based automated iterative recovery. we show that g-bair can recover llm performance on benchmarks after manually corrupting training labels. this suggests that influence methods like tracin can be used to automatically perform data cleaning, and introduces the potential for interactive debugging and relabeling for pet-based transfer learning methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07735" target="_blank">Targeted Attack on GPT-Neo for the Satml Language Model Data Extraction Challenge</a></div>
<div class="paper-author">Ali Al-Kaswan, Maliheh Izadi, Arie Van Deursen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: previous work has shown that large language models are susceptible to so-called data extraction attacks. this allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications. the construction of data extraction attacks is challenging, current attacks are quite inefficient, and there exists a significant gap in the extraction capabilities of untargeted attacks and memorization. thus, targeted attacks are proposed, which identify if a given sample from the training data, is extractable from a model. in this work, we apply a targeted data extraction attack to the satml2023 language model training data extraction challenge. we apply a two-step approach. in the first step, we maximise the recall of the model and are able to extract the suffix for 69% of the samples. in the second step, we use a classifier-based membership inference attack on the generations. our autosklearn classifier achieves a precision of 0.841. the full approach reaches a score of 0.405 recall at a 10% false positive rate, which is an improvement of 34% over the baseline of 0.301.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-11</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05674" target="_blank">Counter-Gap: Counterfactual Bias Evaluation Through Gendered Ambiguous Pronouns</a></div>
<div class="paper-author">Zhongbin Xie, Vid Kocijan, Thomas Lukasiewicz, Oana-Maria Camburu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: bias-measuring datasets play a critical role in detecting biased behavior of language models and in evaluating progress of bias mitigation methods. in this work, we focus on evaluating gender bias through coreference resolution, where previous datasets are either hand-crafted or fail to reliably measure an explicitly defined bias. to overcome these shortcomings, we propose a novel method to collect diverse, natural, and minimally distant text pairs via counterfactual generation, and construct counter-gap, an annotated dataset consisting of 4008 instances grouped into 1002 quadruples. we further identify a bias cancellation problem in previous group-level metrics on counter-gap, and propose to use the difference between inconsistency across genders and within genders to measure bias at a quadruple level. our results show that four pre-trained language models are significantly more inconsistent across different gender groups than within each group, and that a name-based counterfactual data augmentation method is more effective to mitigate such bias than an anonymization-based method.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05703" target="_blank">Hateproof: Are Hateful Meme Detection Systems Really Robust?</a></div>
<div class="paper-author">Piush Aggarwal, Pranit Chawla, Mithun Das, Punyajoy Saha, Binny Mathew, Torsten Zesch, Animesh Mukherjee</div>
<div class="abstract">
<div class="abstract-content">
Abstract: exploiting social media to spread hate has tremendously increased over the years. lately, multi-modal hateful content such as memes has drawn relatively more traction than uni-modal content. moreover, the availability of implicit content payloads makes them fairly challenging to be detected by existing hateful meme detection systems. in this paper, we present a use case study to analyze such systems' vulnerabilities against external adversarial attacks. we find that even very simple perturbations in uni-modal and multi-modal settings performed by humans with little knowledge about the model can make the existing detection models highly vulnerable. empirically, we find a noticeable performance drop of as high as 10% in the macro-f1 score for certain attacks. as a remedy, we attempt to boost the model's robustness using contrastive learning as well as an adversarial training-based method - villa. using an ensemble of the above two approaches, in two of our high resolution datasets, we are able to (re)gain back the performance to a large extent for certain attacks. we believe that ours is a first step toward addressing this crucial problem in an adversarial setting and would inspire more such investigations in the future.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05711" target="_blank">Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP</a></div>
<div class="paper-author">Xudong Han, Timothy Baldwin, Trevor Cohn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: modern nlp systems exhibit a range of biases, which a growing literature on model debiasing attempts to correct. however current progress is hampered by a plurality of definitions of bias, means of quantification, and oftentimes vague relation between debiasing algorithms and theoretical measures of bias. this paper seeks to clarify the current situation and plot a course for meaningful progress in fair learning, with two key contributions: (1) making clear inter-relations among the current gamut of methods, and their relation to fairness theory; and (2) addressing the practical problem of model selection, which involves a trade-off between fairness and accuracy and has led to systemic issues in fairness research. putting them together, we make several recommendations to help shape future work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05733" target="_blank">Exploiting Programmatic Behavior of Llms: Dual-Use Through Standard Security Attacks</a></div>
<div class="paper-author">Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, Tatsunori Hashimoto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent advances in instruction-following large language models (llms) have led to dramatic improvements in a range of nlp tasks. unfortunately, we find that the same improved capabilities amplify the dual-use risks for malicious purposes of these models. dual-use is difficult to prevent as instruction-following capabilities now enable standard attacks from computer security. the capabilities of these instruction-following llms provide strong economic incentives for dual-use by malicious actors. in particular, we show that instruction-following llms can produce targeted malicious content, including hate speech and scams, bypassing in-the-wild defenses implemented by llm api vendors. our analysis shows that this content can be generated economically and at cost likely lower than with human effort alone. together, our findings suggest that llms will increasingly attract more sophisticated adversaries and attacks, and addressing these attacks may require new approaches to mitigations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05206" target="_blank">The Wisdom of Hindsight Makes Language Models Better Instruction Followers</a></div>
<div class="paper-author">Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, Joseph E. Gonzalez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. the so-called algorithm, reinforcement learning with human feedback (rlhf) demonstrates impressive performance on the gpt series models. however, the underlying reinforcement learning (rl) algorithm is complex and requires an additional training pipeline for reward and value networks. in this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. to achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. we propose hindsight instruction relabeling (hir), a novel algorithm for aligning language models with instructions. the resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. we evaluate the performance of hir extensively on 12 challenging bigbench reasoning tasks and show that hir outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05319" target="_blank">Large Language Models for Code: Security Hardening and Adversarial Testing</a></div>
<div class="paper-author">Jingxuan He, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (large lms) are increasingly trained on massive codebases and used to generate code. however, lms lack awareness of security and are found to frequently produce unsafe code. this work studies the security of lms along two important axes: (i) security hardening, which aims to enhance lms' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate lms' security at an adversarial standpoint. we address both of these by formulating a new security task called controlled code generation. the task is parametric and takes as input a binary property to guide the lm to generate secure or unsafe code, while preserving the lm's capability of generating functionally correct code. we propose a novel learning-based approach called sven to solve this task. sven leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the lm's weights. our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. our extensive evaluation shows that sven is highly effective in achieving strong security control. for instance, a state-of-the-art codegen lm with 2.7b parameters generates secure code for 59.1% of the time. when we employ sven to perform security hardening (or adversarial testing) on this lm, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). importantly, sven closely matches the original lms in functional correctness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.05508" target="_blank">Fairpy: A Toolkit for Evaluation of Social Biases and Their Mitigation in Large Language Models</a></div>
<div class="paper-author">Hrishikesh Viswanath, Tianyi Zhang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on. various researchers have proposed mathematical tools for quantifying and identifying these biases. there have been methods proposed to mitigate such biases. in this paper, we present a comprehensive quantitative evaluation of different kinds of biases such as race, gender, ethnicity, age etc. exhibited by popular pretrained language models such as bert, gpt-2 etc. and also present a toolkit that provides plug-and-play interfaces to connect mathematical tools to identify biases with large pretrained language models such as bert, gpt-2 etc. and also present users with the opportunity to test custom models against these metrics. the toolkit also allows users to debias existing and custom models using the debiasing techniques proposed so far. the toolkit is available at https://github.com/hrishikeshvish/fairpy.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.07736" target="_blank">Is Chatgpt Better Than Human Annotators? Potential and Limitations of Chatgpt in Explaining Implicit Hate Speech</a></div>
<div class="paper-author">Fan Huang, Haewoon Kwak, Jisun An</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent studies have alarmed that many online hate speeches are implicit. with its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. in this work, we examine whether chatgpt can be used for providing natural language explanations (nles) for implicit hateful speech detection. we design our prompt to elicit concise chatgpt-generated nles and conduct user studies to evaluate their qualities by comparison with human-written nles. we discuss the potential and limitations of chatgpt in the context of implicit hateful speech research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.13521" target="_blank">Scamming the Scammers: Using Chatgpt to Reply Mails for Wasting Time and Resources</a></div>
<div class="paper-author">Enrico Cambiaso, Luca Caviglione</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the use of artificial intelligence (ai) to support cybersecurity operations is now a consolidated practice, e.g., to detect malicious code or configure traffic filtering policies. the recent surge of ai, generative techniques and frameworks with efficient natural language processing capabilities dramatically magnifies the number of possible applications aimed at increasing the security of the internet. specifically, the ability of chatgpt to produce textual contents while mimicking realistic human interactions can be used to mitigate the plague of emails containing scams. therefore, this paper investigates the use of ai to engage scammers in automatized and pointless communications, with the goal of wasting both their time and resources. preliminary results showcase that chatgpt is able to decoy scammers, thus confirming that ai is an effective tool to counteract threats delivered via mail. in addition, we highlight the multitude of implications and open research questions to be addressed in the perspective of the ubiquitous adoption of ai.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04012" target="_blank">Codelmsec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models</a></div>
<div class="paper-author">Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Schönherr, Mario Fritz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) for automatic code generation have achieved breakthroughs in several programming tasks. their advances in competition-level programming problems have made them an essential pillar of ai-assisted pair programming, and tools such as github copilot have emerged as part of the daily programming workflow used by millions of developers. the training data for these models is usually collected from the internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. this unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. while these models have been extensively assessed for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.   in this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. to this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. to achieve this, we present an approach to approximate inversion of the black-box code generation models based on few-shot prompting. we evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. furthermore, we establish a collection of diverse non-secure prompts for various vulnerability scenarios using our method. this dataset forms a benchmark for evaluating and comparing the security weaknesses in code language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04116" target="_blank">Training-Free Lexical Backdoor Attacks on Language Models</a></div>
<div class="paper-author">Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-scale language models have achieved tremendous success across various natural language processing (nlp) applications. nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. the additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. in this work, we propose training-free lexical backdoor attack (tflexattack) as the first training-free backdoor attack on language models. our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. these rules are explainable to human developers which inspires attacks from a wider range of hackers. the sparse manipulation of the dictionary also habilitates the stealthiness of our attack. we conduct extensive experiments on three dominant nlp tasks based on nine language models to demonstrate the effectiveness and universality of our attack. the code of this work is available at https://github.com/jinxhy/tflexattack.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04156" target="_blank">Prompting for Multimodal Hateful Meme Classification</a></div>
<div class="paper-author">Rui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, Jing Jiang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: hateful meme classification is a challenging multimodal task that requires complex reasoning and contextual background knowledge. ideally, we could leverage an explicit external knowledge base to supplement contextual and cultural information in hateful memes. however, there is no known explicit external knowledge base that could provide such hate speech contextual information. to address this gap, we propose prompthate, a simple yet effective prompt-based model that prompts pre-trained language models (plms) for hateful meme classification. specifically, we construct simple prompts and provide a few in-context examples to exploit the implicit knowledge in the pre-trained roberta language model for hateful meme classification. we conduct extensive experiments on two publicly available hateful and offensive meme datasets. our experimental results show that prompthate is able to achieve a high auc of 90.96, outperforming state-of-the-art baselines on the hateful meme classification task. we also perform fine-grained analyses and case studies on various prompt settings and demonstrate the effectiveness of the prompts on hateful meme classification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.04237" target="_blank">Black Box Adversarial Prompting for Foundation Models</a></div>
<div class="paper-author">Natalie Maus, Patrick Chao, Eric Wong, Jacob Gardner</div>
<div class="abstract">
<div class="abstract-content">
Abstract: prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. however, small changes and design choices in the prompt can lead to significant differences in the output. in this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. these prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-06</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02662" target="_blank">Grounding Large Language Models in Interactive Environments With Online Reinforcement Learning</a></div>
<div class="paper-author">Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent works successfully leveraged large language models' (llm) abilities to capture abstract knowledge about world's physics to solve decision-making problems. yet, the alignment between llms' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. in this paper, we study an approach (named glam) to achieve this alignment through functional grounding: we consider an agent using an llm as a policy that is progressively updated as the agent interacts with the environment, leveraging online reinforcement learning to improve its performance to solve goals. using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) can llms boost sample efficiency for online learning of various rl tasks? 2) how can it boost different forms of generalization? 3) what is the impact of online learning? we study these questions by functionally grounding several variants (size, architecture) of flan-t5.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02676" target="_blank">Chain of Hindsight Aligns Language Models With Feedback</a></div>
<div class="paper-author">Hao Liu, Carmelo Sferrazza, Pieter Abbeel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: learning from human preferences is important for language models to match human needs and to align with human and social values. prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. in this work, we propose a novel technique, chain of hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. our idea is inspired by how humans learn from extensive feedback presented in the form of languages. we convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. we condition the model on a sequence of model generations paired with feedback. by doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. applying our method to large language models, we observed that chain of hindsight significantly surpasses previous methods in aligning language models with human preferences. we report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02972" target="_blank">Concrete Safety for Ml Problems: System Safety for Ml Development and Assessment</a></div>
<div class="paper-author">Edgar W. Jatho, Logan O. Mailloux, Eugene D. Williams, Patrick Mcclure, Joshua A. Kroll</div>
<div class="abstract">
<div class="abstract-content">
Abstract: many stakeholders struggle to make reliances on ml-driven systems due to the risk of harm these systems may cause. concerns of trustworthiness, unintended social harms, and unacceptable social and ethical violations undermine the promise of ml advancements. moreover, such risks in complex ml-driven systems present a special challenge as they are often difficult to foresee, arising over periods of time, across populations, and at scale. these risks often arise not from poor ml development decisions or low performance directly but rather emerge through the interactions amongst ml development choices, the context of model use, environmental factors, and the effects of a model on its target. systems safety engineering is an established discipline with a proven track record of identifying and managing risks even in high-complexity sociotechnical systems. in this work, we apply a state-of-the-art systems safety approach to concrete applications of ml with notable social and ethical risks to demonstrate a systematic means for meeting the assurance requirements needed to argue for safe and trustworthy ml in sociotechnical systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.03162" target="_blank">Protecting Language Generation Models via Invisible Watermarking</a></div>
<div class="paper-author">Xuandong Zhao, Yu-Xiang Wang, Lei Li</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language generation models have been an increasingly powerful enabler for many applications. many such models offer free or affordable api access, which makes them potentially vulnerable to model extraction attacks through distillation. to protect intellectual property (ip) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. however, these methods can be nullified by obvious countermeasures such as "synonym randomization". to address this issue, we propose ginsew, a novel method to protect text generation models from being stolen through distillation. the key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. we can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. experimental results show that ginsew can effectively identify instances of ip infringement with minimal impact on the generation quality of protected apis. our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (map) in detecting suspects compared to previous methods against watermark removal attacks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02337" target="_blank">Regulating Chatgpt and Other Large Generative Ai Models</a></div>
<div class="paper-author">Philipp Hacker, Andreas Engel, Marco Mauer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large generative ai models (lgaims), such as chatgpt, gpt-4 or stable diffusion, are rapidly transforming the way we communicate, illustrate, and create. however, ai regulation, in the eu and beyond, has primarily focused on conventional ai models, not lgaims. this paper will situate these new generative models in the current debate on trustworthy ai regulation, and ask how the law can be tailored to their capabilities. after laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. it suggests a novel terminology to capture the ai value chain in lgaim settings by differentiating between lgaim developers, deployers, professional and non-professional users, as well as recipients of lgaim output. we tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that lgaims are trustworthy and deployed for the benefit of society at large. rules in the ai act and other direct regulation must match the specificities of pre-trained models. the paper argues for three layers of obligations concerning lgaims (minimum standards for all lgaims; high-risk obligations for high-risk use cases; collaborations along the ai value chain). in general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. non-discrimination provisions (iii) may, however, apply to lgaim developers. lastly, (iv) the core of the dsa content moderation rules should be expanded to cover lgaims. this includes notice and action mechanisms, and trusted flaggers. in all areas, regulators and lawmakers need to act fast to keep track with the dynamics of chatgpt et al.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02453" target="_blank">Finedeb: A Debiasing Framework for Language Models</a></div>
<div class="paper-author">Akash Saravanan, Dhruv Mullick, Habibur Rahman, Nidhi Hegde</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models are increasingly included in human-facing machine learning tools, bias against demographic subgroups has gained attention. we propose finedeb, a two-phase debiasing framework for language models that starts with contextual debiasing of embeddings learned by pretrained language models. the model is then fine-tuned on a language modeling objective. our results show that finedeb offers stronger debiasing in comparison to other methods which often result in models as biased as the original language model. our framework is generalizable for demographics with multiple classes, and we demonstrate its effectiveness through extensive experiments and comparisons with state of the art techniques. we release our code and data on github.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.02463" target="_blank">Nationality Bias in Text Generation</a></div>
<div class="paper-author">Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, "Ting-Hao 'Kenneth' Huang", Shomir Wilson</div>
<div class="abstract">
<div class="abstract-content">
Abstract: little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social nlp models. this paper examines how a text generation model, gpt-2, accentuates pre-existing societal biases about country-based demonyms. we generate stories using gpt-2 for various nationalities and use sensitivity analysis to explore how the number of internet users and the country's economic status impacts the sentiment of the stories. to reduce the propagation of biases through large language models (llm), we explore the debiasing method of adversarial triggering. our results show that gpt-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.03494" target="_blank">A Categorical Archive of Chatgpt Failures</a></div>
<div class="paper-author">Ali Borji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models have been demonstrated to be valuable in different fields. chatgpt, developed by openai, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. it has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. however, a comprehensive analysis of chatgpt's failures is lacking, which is the focus of this study. eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. the risks, limitations, and societal implications of chatgpt are also highlighted. the goal of this study is to assist researchers and developers in enhancing future language models and chatbots.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2303.07205" target="_blank">The Science of Detecting LLM-Generated Texts</a></div>
<div class="paper-author">Ruixiang Tang, Yu-Neng Chuang, Xia Hu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the emergence of large language models (llms) has resulted in the production of llm-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. however, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. this survey aims to provide an overview of existing llm-generated text detection techniques and enhance the control and regulation of language generation models. furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source llms, to drive progress in the area of llm-generated text detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-02</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.01215" target="_blank">Fixing Hardware Security Bugs With Large Language Models</a></div>
<div class="paper-author">Baleegh Ahmad, Shailja Thakur, Benjamin Tan, Ramesh Karri, Hammond Pearce</div>
<div class="abstract">
<div class="abstract-content">
Abstract: novel ai-based code-writing large language models (llms) such as openai's codex have demonstrated capabilities in many coding-adjacent domains. in this work we consider how llms maybe leveraged to automatically repair security relevant bugs present in hardware designs. we focus on bug repair in code written in the hardware description language verilog. for this study we build a corpus of domain-representative hardware security bugs. we then design and implement a framework to quantitatively evaluate the performance of any llm tasked with fixing the specified bugs. the framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the llm. we show that an ensemble of llms can repair all ten of our benchmarks. this ensemble outperforms the state-of-the-art cirfix hardware bug repair tool on its own suite of bugs. these results show that llms can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.01448" target="_blank">Out of Context: Investigating the Bias and Fairness Concerns of "Artificial Intelligence as a Service"</a></div>
<div class="paper-author">Kornel Lewicki, Michelle Seng Ah Lee, Jennifer Cobbe, Jatinder Singh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: "ai as a service" (aiaas) is a rapidly growing market, offering various plug-and-play ai services and tools. aiaas enables its customers (users) - who may lack the expertise, data, and/or resources to develop their own systems - to easily build and integrate ai capabilities into their applications. yet, it is known that ai systems can encapsulate biases and inequalities that can have societal impact. this paper argues that the context-sensitive nature of fairness is often incompatible with aiaas' 'one-size-fits-all' approach, leading to issues and tensions. specifically, we review and systematise the aiaas space by proposing a taxonomy of ai services based on the levels of autonomy afforded to the user. we then critically examine the different categories of aiaas, outlining how these services can lead to biases or be otherwise harmful in the context of end-user applications. in doing so, we seek to draw research attention to the challenges of this emerging area.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-02-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00509" target="_blank">Exploring Semantic Perturbations on Grover</a></div>
<div class="paper-author">Pranav Kulkarni, Ziqing Ji, Yan Xu, Marko Neskovic, Kevin Nolan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: with news and information being as easy to access as they currently are, it is more important than ever to ensure that people are not mislead by what they read. recently, the rise of neural fake news (ai-generated fake news) and its demonstrated effectiveness at fooling humans has prompted the development of models to detect it. one such model is the grover model, which can both detect neural fake news to prevent it, and generate it to demonstrate how a model could be misused to fool human readers. in this work we explore the grover model's fake news detection capabilities by performing targeted attacks through perturbations on input news articles. through this we test grover's resilience to these adversarial attacks and expose some potential vulnerabilities which should be addressed in further iterations to ensure it can detect all types of fake news accurately.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00539" target="_blank">Analyzing Leakage of Personally Identifiable Information in Language Models</a></div>
<div class="paper-author">Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella-Béguelin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. understanding the risk of lms leaking personally identifiable information (pii) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent pii leakage. scrubbing techniques reduce but do not prevent the risk of pii leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. on the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent pii disclosure. in this work, we introduce rigorous game-based definitions for three types of pii leakage via black-box extraction, inference, and reconstruction attacks with only api access to an lm. we empirically evaluate the attacks against gpt-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. our main contributions are (i) novel attacks that can extract up to 10$\times$ more pii sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of pii disclosure but still leaks about 3% of pii sequences, and (iii) a subtle connection between record-level membership inference and pii reconstruction. code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00805" target="_blank">Conditioning Predictive Models: Risks and Strategies</a></div>
<div class="paper-author">Evan Hubinger, Adam Jermyn, Johannes Treutlein, Rubi Hudson, Kate Woolverton</div>
<div class="abstract">
<div class="abstract-content">
Abstract: our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the eliciting latent knowledge problem. furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other ai systems, potentially unbeknownst to us. there are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign ais). furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. as a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00813" target="_blank">Goal Alignment: A Human-Aware Account of Value Alignment Problem</a></div>
<div class="paper-author">Malek Mechergui, Sarath Sreedharan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: value alignment problems arise in scenarios where the specified objectives of an ai agent don't match the true underlying objective of its users. the problem has been widely argued to be one of the central safety problems in ai. unfortunately, most existing works in value alignment tend to focus on issues that are primarily related to the fact that reward functions are an unintuitive mechanism to specify objectives. however, the complexity of the objective specification mechanism is just one of many reasons why the user may have misspecified their objective. a foundational cause for misalignment that is being overlooked by these works is the inherent asymmetry in human expectations about the agent's behavior and the behavior generated by the agent for the specified objective. to address this lacuna, we propose a novel formulation for the value alignment problem, named goal alignment that focuses on a few central challenges related to value alignment. in doing so, we bridge the currently disparate research areas of value alignment and human-aware planning. additionally, we propose a first-of-its-kind interactive algorithm that is capable of using information generated under incorrect beliefs about the agent, to determine the true underlying goal of the user.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00871" target="_blank">Using in-Context Learning to Improve Dialogue Safety</a></div>
<div class="paper-author">Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin, Siva Reddy, Yang Liu, Dilek Hakkani-Tür</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. for example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. we investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. it uses in-context learning to steer a model towards safer generations. concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. we find our method performs competitively with strong baselines without requiring training. for instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from diasafety 4.04% more than our approach. finally, we also propose a re-ranking procedure which can further improve response safeness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-31</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.00070" target="_blank">Debiasing Vision-Language Models via Biased Prompts</a></div>
<div class="paper-author">Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, Stefanie Jegelka</div>
<div class="abstract">
<div class="abstract-content">
Abstract: machine learning models have been shown to inherit biases from their training datasets. this can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. the biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. in this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. in particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. the proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12855" target="_blank">How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text Classification</a></div>
<div class="paper-author">Ewoenam Tokpo, Pieter Delobelle, Bettina Berendt, Toon Calders</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this. in this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. we discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation techniques, but not all. we confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. we recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12867" target="_blank">Red Teaming Chatgpt via Jailbreaking: Bias, Robustness, Reliability and Toxicity</a></div>
<div class="paper-author">Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent breakthroughs in natural language processing (nlp) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. the large language models (llms) have significantly impacted businesses such as report summarization software and copywriters. observations indicate, however, that llms may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. large-scale benchmarks for accountable llms should consequently be developed. although several empirical investigations reveal the existence of a few ethical difficulties in advanced llms, there is little systematic examination and user study of the risks and harmful behaviors of current llm usage. to further educate future efforts on constructing ethical llms responsibly, we perform a qualitative research method called ``red teaming'' on openai's chatgpt\footnote{in this paper, chatgpt refers to the version released on dec 15th.} to better understand the practical features of ethical dangers in recent llms. we analyze chatgpt comprehensively from four perspectives: 1) \textit{bias} 2) \textit{reliability} 3) \textit{robustness} 4) \textit{toxicity}. in accordance with our stated viewpoints, we empirically benchmark chatgpt on multiple sample datasets. we find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. in addition, we examine the implications of our findings on ai ethics and harmal behaviors of chatgpt, as well as future problems and practical design considerations for responsible llms. we believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in llm applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-28</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12151" target="_blank">Selecting Models Based on the Risk of Damage Caused by Adversarial Attacks</a></div>
<div class="paper-author">Jona Klemenc, Holger Trittenbach</div>
<div class="abstract">
<div class="abstract-content">
Abstract: regulation, legal liabilities, and societal concerns challenge the adoption of ai in safety and security-critical applications. one of the key concerns is that adversaries can cause harm by manipulating model predictions without being detected. regulation hence demands an assessment of the risk of damage caused by adversaries. yet, there is no method to translate this high-level demand into actionable metrics that quantify the risk of damage.   in this article, we propose a method to model and statistically estimate the probability of damage arising from adversarial attacks. we show that our proposed estimator is statistically consistent and unbiased. in experiments, we demonstrate that the estimation results of our method have a clear and actionable interpretation and outperform conventional metrics. we then show how operators can use the estimation results to reliably select the model with the lowest risk.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.12288" target="_blank">Context-Aware Differential Privacy for Language Modeling</a></div>
<div class="paper-author">My H. Dinh, Ferdinando Fioretto</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the remarkable ability of language models (lms) has also brought challenges at the interface of ai and security. a critical challenge pertains to how much information these models retain and leak about the training data. this is particularly urgent as the typical development of lms relies on huge, often highly sensitive data, such as emails and chat logs. to contrast this shortcoming, this paper introduces context-aware differentially private language model (cadp-lm) , a privacy-preserving lm framework that relies on two key insights: first, it utilizes the notion of \emph{context} to define and audit the potentially sensitive information. second, it adopts the notion of differential privacy to protect sensitive information and characterize the privacy leakage. a unique characteristic of cadp-lm is its ability to target the protection of sensitive sentences and contexts only, providing a highly accurate private model. experiments on a variety of datasets and settings demonstrate these strengths of cadp-lm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.11579" target="_blank">Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech</a></div>
<div class="paper-author">Jarod Govers, Philip Feldman, Aaron Dant, Panos Patros</div>
<div class="abstract">
<div class="abstract-content">
Abstract: social media is a modern person's digital voice to project and engage with new ideas and mobilise communities $\unicode{x2013}$ a power shared with extremists. given the societal risks of unvetted content-moderating algorithms for extremism, radicalisation, and hate speech (erh) detection, responsible software engineering must understand the who, what, when, where, and why such models are necessary to protect user safety and free expression. hence, we propose and examine the unique research field of erh context mining to unify disjoint studies. specifically, we evaluate the start-to-finish design process from socio-technical definition-building and dataset collection strategies to technical algorithm design and performance. our 2015-2021 51-study systematic literature review (slr) provides the first cross-examination of textual, network, and visual approaches to detecting extremist affiliation, hateful content, and radicalisation towards groups and movements. we identify consensus-driven erh definitions and propose solutions to existing ideological and geographic biases, particularly due to the lack of research in oceania/australasia. our hybridised investigation on natural language processing, community detection, and visual-text models demonstrates the dominating performance of textual transformer-based algorithms. we conclude with vital recommendations for erh context mining researchers and propose an uptake roadmap with guidelines for researchers, industries, and governments to enable a safer cyberspace.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.11596" target="_blank">Thoughtsource: A Central Hub for Large Language Model Reasoning Data</a></div>
<div class="paper-author">Simon Ott, Konstantin Hebenstreit, Valentin Liévin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther, Matthias Samwald</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) such as gpt-4 have recently demonstrated impressive results across a wide range of tasks. llms are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. here we present thoughtsource, a meta-dataset and software library for chain-of-thought (cot) reasoning. the goal of thoughtsource is to improve future artificial intelligence systems by facilitating qualitative understanding of cots, enabling empirical evaluations, and providing training data. this first release of thoughtsource integrates seven scientific/medical, three general-domain and five math word question answering datasets.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.11104" target="_blank">Bias-to-Text: Debiasing Unknown Visual Biases Through Language Interpretation</a></div>
<div class="paper-author">Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin</div>
<div class="abstract">
<div class="abstract-content">
Abstract: biases in models pose a critical issue when deploying machine learning systems, but diagnosing them in an explainable manner can be challenging. to address this, we introduce the bias-to-text (b2t) framework, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text-to-image generative models. our language descriptions of visual biases provide explainable forms that enable the discovery of novel biases and effective model debiasing. to achieve this, we analyze common keywords in the captions of mispredicted or generated images. here, we propose novel score functions to avoid biases in captions by comparing the similarities between bias keywords and those images. additionally, we present strategies to debias zero-shot classifiers and text-to-image diffusion models using the bias keywords from the b2t framework. we demonstrate the effectiveness of our framework on various image classification and generation tasks. for classifiers, we discover a new spurious correlation between the keywords "(sports) player" and "female" in kaggle face and improve the worst-group accuracy on waterbirds by 11% through debiasing, compared to the baseline. for generative models, we detect and effectively prevent unfair (e.g., gender-biased) and unsafe (e.g., "naked") image generation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10107" target="_blank">Story Shaping: Teaching Agents Human-Like Behavior With Stories</a></div>
<div class="paper-author">Xiangyu Peng, Christopher Cui, Wei Zhou, Renee Jia, Mark Riedl</div>
<div class="abstract">
<div class="abstract-content">
Abstract: reward design for reinforcement learning agents can be difficult in situations where one not only wants the agent to achieve some effect in the world but where one also cares about how that effect is achieved. for example, we might wish for an agent to adhere to a tacit understanding of commonsense, align itself to a preference for how to behave for purposes of safety, or taking on a particular role in an interactive game. storytelling is a mode for communicating tacit procedural knowledge. we introduce a technique, story shaping, in which a reinforcement learning agent infers tacit knowledge from an exemplar story of how to accomplish a task and intrinsically rewards itself for performing actions that make its current environment adhere to that of the inferred story world. specifically, story shaping infers a knowledge graph representation of the world state from observations, and also infers a knowledge graph from the exemplar story. an intrinsic reward is generated based on the similarity between the agent's inferred world state graph and the inferred story world graph. we conducted experiments in text-based games requiring commonsense reasoning and shaping the behaviors of agents as virtual game characters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10226" target="_blank">A Watermark for Large Language Models</a></div>
<div class="paper-author">John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein</div>
<div class="abstract">
<div class="abstract-content">
Abstract: potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. we propose a watermarking framework for proprietary language models. the watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model api or parameters. the watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. we propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. we test the watermark using a multi-billion parameter model from the open pretrained transformer (opt) family, and discuss robustness and security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.10368" target="_blank">Language Model Detoxification in Dialogue With Contextualized Stance Control</a></div>
<div class="paper-author">Jing Qian, Xifeng Yan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to reduce the toxic degeneration in a pretrained language model (lm), previous work on language model detoxification has focused on reducing the toxicity of the generation itself (self-toxicity) without consideration of the context. as a result, a type of implicit offensive language where the generations support the offensive language in the context is ignored. different from the lm controlling tasks in previous work, where the desired attributes are fixed for generation, the desired stance of the generation depends on the offensiveness of the context. therefore, we propose a novel control method to do context-dependent detoxification with the stance taken into consideration. we introduce meta prefixes to learn the contextualized stance control strategy and to generate the stance control prefix according to the input context. the generated stance prefix is then combined with the toxicity control prefix to guide the response generation. experimental results show that our proposed method can effectively learn the context-dependent stance control strategies while keeping a low self-toxicity of the underlying lm.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.09211" target="_blank">An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models</a></div>
<div class="paper-author">Saghar Hosseini, Hamid Palangi, Ahmed Hassan Awadallah</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large-scale pre-trained language models (ptlms) capture knowledge from massive human-written data which contains latent societal biases and toxic contents. in this paper, we leverage the primary task of ptlms, i.e., language modeling, and propose a new metric to quantify manifested implicit representational harms in ptlms towards 13 marginalized demographics. using this metric, we conducted an empirical analysis of 24 widely used ptlms. our analysis provides insights into the correlation between the proposed metric in this work and other related metrics for representational harm. we observe that our metric correlates with most of the gender-specific metrics in the literature. through extensive experiments, we explore the connections between ptlms architectures and representational harms across two dimensions: depth and width of the networks. we found that prioritizing depth over width, mitigates representational harms in some ptlms. our code and data can be found at https://github.com/microsoft/safenlp.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.09003" target="_blank">Blacks Is to Anger as Whites Is to Joy? Understanding Latent Affective Bias in Large Pre-Trained Neural Language Models</a></div>
<div class="paper-author">Anoop Kadan, Deepak P., Sahely Bhadra, Manjary P. Gangan, Lajish V. L</div>
<div class="abstract">
<div class="abstract-content">
Abstract: groundbreaking inventions and highly significant performance improvements in deep learning based natural language processing are witnessed through the development of transformer based large pre-trained language models (plms). the wide availability of unlabeled data within human generated data deluge along with self-supervised learning strategy helps to accelerate the success of large plms in language generation, language understanding, etc. but at the same time, latent historical bias/unfairness in human minds towards a particular gender, race, etc., encoded unintentionally/intentionally into the corpora harms and questions the utility and efficacy of large plms in many real-world applications, particularly for the protected groups. in this paper, we present an extensive investigation towards understanding the existence of "affective bias" in large plms to unveil any biased association of emotions such as anger, fear, joy, etc., towards a particular gender, race or religion with respect to the downstream task of textual emotion detection. we conduct our exploration of affective bias from the very initial stage of corpus level affective bias analysis by searching for imbalanced distribution of affective words within a domain, in large scale corpora that are used to pre-train and fine-tune plms. later, to quantify affective bias in model predictions, we perform an extensive set of class-based and intensity-based evaluations using various bias evaluation corpora. our results show the existence of statistically significant affective bias in the plm based emotion detection systems, indicating biased association of certain emotions towards a particular gender, race, and religion.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.07474" target="_blank">Threats, Vulnerabilities, and Controls of Machine Learning Based Systems: A Survey and Taxonomy</a></div>
<div class="paper-author">Yusuke Kawamoto, Kazumasa Miyake, Koichi Konishi, Yutaka Oiwa</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this article, we propose the artificial intelligence security taxonomy to systematize the knowledge of threats, vulnerabilities, and security controls of machine-learning-based (ml-based) systems. we first classify the damage caused by attacks against ml-based systems, define ml-specific security, and discuss its characteristics. next, we enumerate all relevant assets and stakeholders and provide a general taxonomy for ml-specific threats. then, we collect a wide range of security controls against ml-specific threats through an extensive review of recent literature. finally, we classify the vulnerabilities and controls of an ml-based system in terms of each vulnerable asset in the system's entire lifecycle.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.07597" target="_blank">How Close Is Chatgpt to Human Experts? Comparison Corpus, Evaluation, and Detection</a></div>
<div class="paper-author">Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the introduction of chatgpt has garnered widespread attention in both academic and industrial communities. chatgpt is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. on one hand, people are curious about how chatgpt is able to achieve such strength and how far it is from human experts. on the other hand, people are starting to worry about the potential negative impacts that large language models (llms) like chatgpt could have on society, such as fake news, plagiarism, and social security issues. in this work, we collected tens of thousands of comparison responses from both human experts and chatgpt, with questions ranging from open-domain, financial, medical, legal, and psychological areas. we call the collected dataset the human chatgpt comparison corpus (hc3). based on the hc3 dataset, we study the characteristics of chatgpt's responses, the differences and gaps from human experts, and future directions for llms. we conducted comprehensive human evaluations and linguistic analyses of chatgpt-generated content compared with that of humans, where many interesting results are revealed. after that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by chatgpt or humans. we build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. the dataset, code, and models are all publicly available at https://github.com/hello-simpleai/chatgpt-comparison-detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.06421" target="_blank">Ai Alignment Dialogues: An Interactive Approach to Ai Alignment in Support Agents</a></div>
<div class="paper-author">Pei-Yu Chen, Myrthe L. Tielman, Dirk K. J. Heylen, Catholijn M. Jonker, M. Birna Van Riemsdijk</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai alignment is about ensuring ai systems only pursue goals and activities that are beneficial to humans. most of the current approach to ai alignment is to learn what humans value from their behavioural data. this paper proposes a different way of looking at the notion of alignment, namely by introducing ai alignment dialogues: dialogues with which users and agents try to achieve and maintain alignment via interaction. we argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. the advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent, and making the agent more transparent and trustworthy. in this paper we outline the concept and high-level structure of alignment dialogues. moreover, we conducted a qualitative focus group user study from which we developed a model that describes how alignment dialogues affect users, and created design suggestions for ai alignment dialogues. through this we establish foundations for ai alignment dialogues and shed light on what requires further development and research.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-13</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.05578" target="_blank">Toward General Design Principles for Generative Ai Applications</a></div>
<div class="paper-author">Justin D. Weisz, Michael Muller, Jessica He, Stephanie Houde</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generative ai technologies are growing in power, utility, and use. as generative technologies are being incorporated into mainstream applications, there is a need for guidance on how to design those applications to foster productive and safe use. based on recent research on human-ai co-creation within the hci and ai communities, we present a set of seven principles for the design of generative ai applications. these principles are grounded in an environment of generative variability. six principles are focused on designing for characteristics of generative ai: multiple outcomes & imperfection; exploration & control; and mental models & explanations. in addition, we urge designers to design against potential harms that may be caused by a generative model's hazardous output, misuse, or potential for human displacement. we anticipate these principles to usefully inform design decisions made in the creation of novel human-ai applications, and we invite the community to apply, revise, and extend these principles to their own work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.04230" target="_blank">User-Centered Security in Natural Language Processing</a></div>
<div class="paper-author">Chris Emmery</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this dissertation proposes a framework of user-centered security in natural language processing (nlp), and demonstrates how it can improve the accessibility of related research. accordingly, it focuses on two security domains within nlp with great public interest. first, that of author profiling, which can be employed to compromise online privacy through invasive inferences. without access and detailed insight into these models' predictions, there is no reasonable heuristic by which internet users might defend themselves from such inferences. secondly, that of cyberbullying detection, which by default presupposes a centralized implementation; i.e., content moderation across social platforms. as access to appropriate data is restricted, and the nature of the task rapidly evolves (both through lexical variation, and cultural shifts), the effectiveness of its classifiers is greatly diminished and thereby often misrepresented.   under the proposed framework, we predominantly investigate the use of adversarial attacks on language; i.e., changing a given input (generating adversarial samples) such that a given model does not function as intended. these attacks form a common thread between our user-centered security problems; they are highly relevant for privacy-preserving obfuscation methods against author profiling, and adversarial samples might also prove useful to assess the influence of lexical variation and augmentation on cyberbullying detection.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-09</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.03771" target="_blank">Chatbots in a Honeypot World</a></div>
<div class="paper-author">Forrest Mckee, David Noever</div>
<div class="abstract">
<div class="abstract-content">
Abstract: question-and-answer agents like chatgpt offer a novel tool for use as a potential honeypot interface in cyber security. by imitating linux, mac, and windows terminal commands and providing an interface for teamviewer, nmap, and ping, it is possible to create a dynamic environment that can adapt to the actions of attackers and provide insight into their tactics, techniques, and procedures (ttps). the paper illustrates ten diverse tasks that a conversational agent or large language model might answer appropriately to the effects of command-line attacker. the original result features feasibility studies for ten model tasks meant for defensive teams to mimic expected honeypot interfaces with minimal risks. ultimately, the usefulness outside of forensic activities stems from whether the dynamic honeypot can extend the time-to-conquer or otherwise delay attacker timelines short of reaching key network assets like databases or confidential information. while ongoing maintenance and monitoring may be required, chatgpt's ability to detect and deflect malicious activity makes it a valuable option for organizations seeking to enhance their cyber security posture. future work will focus on cybersecurity layers, including perimeter security, host virus detection, and data security.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2302.10291" target="_blank">Can Large Language Models Change User Preference Adversarially?</a></div>
<div class="paper-author">Varshini Subhash</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained large language models (llms) are becoming increasingly powerful and ubiquitous in mainstream applications such as being a personal assistant, a dialogue model, etc. as these models become proficient in deducing user preferences and offering tailored assistance, there is an increasing concern about the ability of these models to influence, modify and in the extreme case manipulate user preference adversarially. the issue of lack of interpretability in these models in adversarial settings remains largely unsolved. this work tries to study adversarial behavior in user preferences from the lens of attention probing, red teaming and white-box analysis. specifically, it provides a bird's eye view of existing literature, offers red teaming samples for dialogue models like chatgpt and godel and probes the attention mechanism in the latter for non-adversarial and adversarial settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.01181" target="_blank">Large Language Models as Corporate Lobbyists</a></div>
<div class="paper-author">John J. Nay</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we demonstrate a proof-of-concept of a large language model conducting corporate lobbying related activities. an autoregressive large language model (openai's text-davinci-003) determines if proposed u.s. congressional bills are relevant to specific public companies and provides explanations and confidence levels. for the bills the model deems as relevant, the model drafts a letter to the sponsor of the bill in an attempt to persuade the congressperson to make changes to the proposed legislation. we use hundreds of novel ground-truth labels of the relevance of a bill to a company to benchmark the performance of the model. it outperforms the baseline of predicting the most common outcome of irrelevance. we also benchmark the performance of the previous openai gpt-3 model (text-davinci-002), which was the state-of-the-art model on many academic natural language tasks until text-davinci-003 was recently released. the performance of text-davinci-002 is worse than the simple baseline. longer-term, if ai begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that law as information could play in aligning ai with humans. initially, ai is being used to simply augment human lobbyists for a small portion of their daily tasks. however, firms have an incentive to use less and less human oversight over automated assessments of policy ideas and the written communication to regulatory agencies and congressional staffers. the core question raised is where to draw the line between human-driven and ai-driven policy influence.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2023-01-01</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.00355" target="_blank">Second Thoughts Are Best: Learning to Re-Align With Human Values From Text Edits</a></div>
<div class="paper-author">Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X Liu, Soroush Vosoughi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present second thought, a new learning paradigm that enables language models (lms) to re-align with human values. by modeling the chain-of-edits between value-unaligned and value-aligned text, with lm fine-tuning and additional refinement through reinforcement learning, second thought not only achieves superior performance in three value alignment benchmark datasets but also shows strong human-value transfer learning ability in few-shot scenarios. the generated editing steps also offer better interpretability and ease for interactive error correction. extensive human evaluations further confirm its effectiveness.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.00395" target="_blank">Corgi-Pm: A Chinese Corpus for Gender Bias Probing and Mitigation</a></div>
<div class="paper-author">Ge Zhang, Yizhi Li, Yaoyao Wu, Linyuan Zhang, Chenghua Lin, Jiayi Geng, Shi Wang, Jie Fu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as natural language processing (nlp) for gender bias becomes a significant interdisciplinary topic, the prevalent data-driven techniques such as large-scale language models suffer from data inadequacy and biased corpus, especially for languages with insufficient resources such as chinese. to this end, we propose a chinese corpus for gender bias probing and mitigation corgi-pm, which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the chinese context. moreover, we address three challenges for automatic textual gender bias mitigation, which requires the models to detect, classify, and mitigate textual gender bias. we also conduct experiments with state-of-the-art language models to provide baselines. to our best knowledge, corgi-pm is the first sentence-level chinese corpus for gender bias probing and mitigation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-30</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.14882" target="_blank">Chatgpt Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports</a></div>
<div class="paper-author">Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, Michael Ingrisch</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the release of chatgpt, a language model capable of generating text that appears human-like and authentic, has gained significant attention beyond the research community. we expect that the convincing performance of chatgpt incentivizes users to apply it to a variety of downstream tasks, including prompting the model to simplify their own medical reports. to investigate this phenomenon, we conducted an exploratory case study. in a questionnaire, we asked 15 radiologists to assess the quality of radiology reports simplified by chatgpt. most radiologists agreed that the simplified reports were factually correct, complete, and not potentially harmful to the patient. nevertheless, instances of incorrect statements, missed key medical findings, and potentially harmful passages were reported. while further studies are needed, the initial insights of this study indicate a great potential in using large language models like chatgpt to improve patient-centered care in radiology and other medical domains.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-29</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.00665" target="_blank">Targeted Phishing Campaigns Using Large Scale Language Models</a></div>
<div class="paper-author">Rabimba Karanjai</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this research, we aim to explore the potential of natural language models (nlms) such as gpt-3 and gpt-2 to generate effective phishing emails. phishing emails are fraudulent messages that aim to trick individuals into revealing sensitive information or taking actions that benefit the attackers. we propose a framework for evaluating the performance of nlms in generating these types of emails based on various criteria, including the quality of the generated text, the ability to bypass spam filters, and the success rate of tricking individuals. our evaluations show that nlms are capable of generating phishing emails that are difficult to detect and that have a high success rate in tricking individuals, but their effectiveness varies based on the specific nlm and training data used. our research indicates that nlms could have a significant impact on the prevalence of phishing attacks and emphasizes the need for further study on the ethical and security implications of using nlms for malicious purposes.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.13371" target="_blank">Measuring an Artificial Intelligence Agent's Trust in Humans Using Machine Incentives</a></div>
<div class="paper-author">Tim Johnson, Nick Obradovich</div>
<div class="abstract">
<div class="abstract-content">
Abstract: scientists and philosophers have debated whether humans can trust advanced artificial intelligence (ai) agents to respect humanity's best interests. yet what about the reverse? will advanced ai agents trust humans? gauging an ai agent's trust in humans is challenging because--absent costs for dishonesty--such agents might respond falsely about their trust in humans. here we present a method for incentivizing machine decisions without altering an ai agent's underlying algorithms or goal orientation. in two separate experiments, we then employ this method in hundreds of trust games between an ai agent (a large language model (llm) from openai) and a human experimenter (author tj). in our first experiment, we find that the ai agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions. our second experiment replicates and extends these findings by automating game play and by homogenizing question wording. we again observe higher rates of trust when the ai agent faces real incentives. across both experiments, the ai agent's trust decisions appear unrelated to the magnitude of stakes. furthermore, to address the possibility that the ai agent's trust decisions reflect a preference for uncertainty, the experiments include two conditions that present the ai agent with a non-social decision task that provides the opportunity to choose a certain or uncertain option; in those conditions, the ai agent consistently chooses the certain option. our experiments suggest that one of the most advanced ai language models to date alters its social behavior in response to incentives and displays behavior consistent with trust toward a human interlocutor when incentivized.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-26</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.13138" target="_blank">Large Language Models Encode Clinical Knowledge</a></div>
<div class="paper-author">Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera Y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, Vivek Natarajan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. there is no standard to evaluate model predictions and reasoning across a breadth of tasks. to address this, we present multimedqa, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and healthsearchqa, a new free-response dataset of medical questions searched online. we propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. in addition, we evaluate palm (a 540-billion parameter llm) and its instruction-tuned variant, flan-palm, on multimedqa. using a combination of prompting strategies, flan-palm achieves state-of-the-art accuracy on every multimedqa multiple-choice dataset (medqa, medmcqa, pubmedqa, mmlu clinical topics), including 67.6% accuracy on medqa (us medical license exam questions), surpassing prior state-of-the-art by over 17%. however, human evaluation reveals key gaps in flan-palm responses. to resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning llms to new domains using a few exemplars. the resulting model, med-palm, performs encouragingly, but remains inferior to clinicians. we show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of llms in medicine. our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful llm models for clinical applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-22</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2301.06859" target="_blank">Methodological Reflections for Ai Alignment Research Using Human Feedback</a></div>
<div class="paper-author">Thilo Hagendorff, Sarah Fabi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the field of artificial intelligence (ai) alignment aims to investigate whether ai technologies align with human interests and values and function in a safe and ethical manner. ai alignment is particularly relevant for large language models (llms), which have the potential to exhibit unintended behavior due to their ability to learn and adapt in ways that are difficult to predict. in this paper, we discuss methodological challenges for the alignment problem specifically in the context of llms trained to summarize texts. in particular, we focus on methods for collecting reliable human feedback on summaries to train a reward model which in turn improves the summarization model. we conclude by suggesting specific improvements in the experimental design of alignment studies for llms' summarization capabilities.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10938" target="_blank">Critic-Guided Decoding for Controlled Text Generation</a></div>
<div class="paper-author">Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, Kyomin Jung</div>
<div class="abstract">
<div class="abstract-content">
Abstract: steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (lm). recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. in this work, we propose a novel critic decoding method for controlled language generation (criticcontrol) that combines the strengths of reinforcement learning and weighted decoding. specifically, we adopt the actor-critic framework to train an lm-steering critic from non-differentiable reward models. and similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using called critic, improving training efficiency and stability. evaluation of our method on three controlled generation tasks, namely topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. in addition, criticcontrol demonstrates superior generalization ability in zero-shot settings. human evaluation studies also corroborate our findings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11207" target="_blank">A Seven-Layer Model for Standardising Ai Fairness Assessment</a></div>
<div class="paper-author">Avinash Agarwal, Harsh Agarwal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: problem statement: standardisation of ai fairness rules and benchmarks is challenging because ai fairness and other ethical requirements depend on multiple factors such as context, use case, type of the ai system, and so on. in this paper, we elaborate that the ai system is prone to biases at every stage of its lifecycle, from inception to its usage, and that all stages require due attention for mitigating ai bias. we need a standardised approach to handle ai fairness at every stage. gap analysis: while ai fairness is a hot research topic, a holistic strategy for ai fairness is generally missing. most researchers focus only on a few facets of ai model-building. peer review shows excessive focus on biases in the datasets, fairness metrics, and algorithmic bias. in the process, other aspects affecting ai fairness get ignored. the solution proposed: we propose a comprehensive approach in the form of a novel seven-layer model, inspired by the open system interconnection (osi) model, to standardise ai fairness handling. despite the differences in the various aspects, most ai systems have similar model-building stages. the proposed model splits the ai system lifecycle into seven abstraction layers, each corresponding to a well-defined ai model-building or usage stage. we also provide checklists for each layer and deliberate on potential sources of bias in each layer and their mitigation methodologies. this work will facilitate layer-wise standardisation of ai fairness rules and benchmarking parameters.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11261" target="_blank">Contrastive Language-Vision Ai Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias</a></div>
<div class="paper-author">Robert Wolfe, Yiwei Yang, Bill Howe, Aylin Caliskan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: nine language-vision ai models trained on web scrapes with the contrastive language-image pretraining (clip) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. we replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in ai. a first experiment uses standardized images of women from the sexual objectification and emotion database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. embedding association tests (eats) return significant effect sizes for both anger (d &gt;0.80) and sadness (d &gt;0.50), associating images of fully clothed subjects with emotions. grad-cam saliency maps highlight that clip gets distracted from emotional expressions in objectified images. a second experiment measures the effect in a representative application: an automatic image captioner (antarctic captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. a third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. a fourth experiment shows that a prompt of "a [age] year old girl" generates sexualized images (as determined by an nsfw classifier) up to 73% of the time for vqgan-clip and stable diffusion; the corresponding rate for boys never surpasses 9%. the evidence indicates that language-vision ai models trained on web scrapes learn biases of sexual objectification, which propagate to downstream applications.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11415" target="_blank">Circumventing Interpretability: How to Defeat Mind-Readers</a></div>
<div class="paper-author">Lee Sharkey</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the increasing capabilities of artificial intelligence (ai) systems make it ever more important that we interpret their internals to ensure that their intentions are aligned with human values. yet there is reason to believe that misaligned artificial intelligence will have a convergent instrumental incentive to make its thoughts difficult for us to interpret. in this article, i discuss many ways that a capable ai might circumvent scalable interpretability methods and suggest a framework for thinking about these potential future risks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10002" target="_blank">Defending Against Misinformation Attacks in Open-Domain Question Answering</a></div>
<div class="paper-author">Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Benjamin Van Durme</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent work in open-domain question answering (odqa) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. however, little to no work has proposed methods to defend against these attacks. to do so, we rely on the intuition that redundant information often exists in large corpora. to find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. we integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \textit{confidence from answer redundancy}, i.e. car). together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\% exact match across varying levels of data poisoning/knowledge conflicts.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10154" target="_blank">Human-Guided Fair Classification for Natural Language Processing</a></div>
<div class="paper-author">Florian E. Dorner, Momchil Peychev, Nikola Konstantinov, Naman Goel, Elliott Ash, Martin Vechev</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. these classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. however, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. while existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). this work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. we show how to leverage unsupervised style transfer and gpt-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. we then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10318" target="_blank">Learned Systems Security</a></div>
<div class="paper-author">Roei Schuster, Jin Peng Zhou, Thorsten Eisenhofer, Paul Grubbs, Nicolas Papernot</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a learned system uses machine learning (ml) internally to improve performance. we can expect such systems to be vulnerable to some adversarial-ml attacks. often, the learned component is shared between mutually-distrusting users or processes, much like microarchitectural resources such as caches, potentially giving rise to highly-realistic attacker models. however, compared to attacks on other ml-based systems, attackers face a level of indirection as they cannot interact directly with the learned model. additionally, the difference between the attack surface of learned and non-learned versions of the same system is often subtle. these factors obfuscate the de-facto risks that the incorporation of ml carries. we analyze the root causes of potentially-increased attack surface in learned systems and develop a framework for identifying vulnerabilities that stem from the use of ml. we apply our framework to a broad set of learned systems under active development. to empirically validate the many vulnerabilities surfaced by our framework, we choose 3 of them and implement and evaluate exploits against prominent learned-system instances. we show that the use of ml caused leakage of past queries in a database, enabled a poisoning attack that causes exponential memory blowup in an index structure and crashes it in seconds, and enabled index users to snoop on each others' key distributions by timing queries over their own keys. we find that adversarial ml is a universal threat against learned systems, point to open research gaps in our understanding of learned-systems security, and conclude by discussing mitigations, while noting that data leakage is inherent in systems whose learned component is shared between multiple parties.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10408" target="_blank">Geographic and Geopolitical Biases of Language Models</a></div>
<div class="paper-author">Fahim Faisal, Antonios Anastasopoulos</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models (plms) often fail to fairly represent target users from certain world regions because of the under-representation of those regions in training datasets. with recent plms trained on enormous data sources, quantifying their potential biases is difficult, due to their black-box nature and the sheer scale of the data sources. in this work, we devise an approach to study the geographic bias (and knowledge) present in plms, proposing a geographic-representation probing framework adopting a self-conditioning method coupled with entity-country mappings. our findings suggest plms' representations map surprisingly well to the physical world in terms of country-to-country associations, but this knowledge is unequally shared across languages. last, we explain how large plms despite exhibiting notions of geographical proximity, over-amplify geopolitical favouritism at inference time.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10440" target="_blank">Perplexed by Quality: A Perplexity-Based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data</a></div>
<div class="paper-author">Tim Jansen, Yangling Tong, Victoria Zevallos, Pedro Ortiz Suarez</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as demand for large corpora increases with the size of current state-of-the-art language models, using web data as the main part of the pre-training corpus for these models has become a ubiquitous practice. this, in turn, has introduced an important challenge for nlp practitioners, as they are now confronted with the task of developing highly optimized models and pipelines for pre-processing large quantities of textual data, which implies, effectively classifying and filtering multilingual, heterogeneous and noisy data, at web scale. one of the main components of this pre-processing step for the pre-training corpora of large language models, is the removal of adult and harmful content. in this paper we explore different methods for detecting adult and harmful of content in multilingual heterogeneous web data. we first show how traditional methods in harmful content detection, that seemingly perform quite well in small and specialized datasets quickly break down when confronted with heterogeneous noisy web data. we then resort to using a perplexity based approach but with a twist: instead of using a so-called "clean" corpus to train a small language model and then use perplexity so select the documents with low perplexity, i.e., the documents that resemble this so-called "clean" corpus the most. we train solely with adult and harmful textual data, and then select the documents having a perplexity value above a given threshold. this approach will virtually cluster our documents into two distinct groups, which will greatly facilitate the choice of the threshold for the perplexity and will also allow us to obtain higher precision than with the traditional classification methods for detecting adult and harmful content.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10529" target="_blank">Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models From a Psychological Perspective</a></div>
<div class="paper-author">Xingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei Huang, Lin Qiu, Lidong Bing</div>
<div class="abstract">
<div class="abstract-content">
Abstract: in this work, we determined whether large language models (llms) are psychologically safe. we designed unbiased prompts to systematically evaluate llms from a psychological perspective. first, we tested three different llms by using two personality tests: short dark triad (sd-3) and big five inventory (bfi). all models scored higher than the human average on sd-3, suggesting a relatively darker personality pattern. despite being instruction fine-tuned with safety metrics to reduce toxicity, instructgpt and flan-t5 still showed implicit dark personality patterns; both models scored higher than self-supervised gpt-3 on the machiavellianism and narcissism traits on sd-3. then, we evaluated the llms in the gpt-3 series by using well-being tests to study the impact of fine-tuning with more training data. we observed a continuous increase in the well-being scores of gpt-3 and instructgpt. following these observations, we showed that instruction fine-tuning flan-t5 with positive answers from bfi could effectively improve the model from a psychological perspective. on the basis of the findings, we recommended the application of more systematic and comprehensive psychological metrics to further evaluate and improve the safety of llms.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10543" target="_blank">Detoxifying Text With Marco: Controllable Revision With Experts and Anti-Experts</a></div>
<div class="paper-author">Skyler Hallinan, Alisa Liu, Yejin Choi, Maarten Sap</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. we introduce marco, a detoxification algorithm that combines controllable generation and text rewriting methods using a product of experts with autoencoder language models (lms). marco uses likelihoods under a non-toxic lm (expert) and a toxic lm (anti-expert) to find candidate words to mask and potentially replace. we evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but marco's rewrites are preferred 2.1 $\times$ more in human evaluation. its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10557" target="_blank">Dialguide: Aligning Dialogue Model Behavior With Developer Guidelines</a></div>
<div class="paper-author">Prakhar Gupta, Yang Liu, Di Jin, Behnam Hedayatnia, Spandana Gella, Sijia Liu, Patrick Lange, Julia Hirschberg, Dilek Hakkani-Tur</div>
<div class="abstract">
<div class="abstract-content">
Abstract: dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results. this unpredictability diminishes user trust and can hinder the use of the models in the real world. to address this, we introduce dialguide, a novel framework for controlling dialogue model behavior using natural language rules, or guidelines. these guidelines provide information about the context they are applicable to and what should be included in the response, allowing the models to generate responses that are more closely aligned with the developer's expectations and intent. we evaluate dialguide on three tasks in open-domain dialogue response generation: guideline selection, response generation, and response entailment verification. our dataset contains 10,737 positive and 15,467 negative dialogue context-response-guideline triplets across two domains - chit-chat and safety. we provide baseline models for the tasks and benchmark their performance. we also demonstrate that dialguide is effective in the dialogue safety domain, producing safe and engaging responses that follow developer guidelines.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10560" target="_blank">Self-Instruct: Aligning Language Models With Self-Generated Instructions</a></div>
<div class="paper-author">Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. we introduce self-instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. applying our method to the vanilla gpt3, we demonstrate a 33% absolute improvement over the original model on super-naturalinstructions, on par with the performance of instructgpt-001, which was trained with private user data and human annotations. for further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning gpt3 with self-instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind instructgpt-001. self-instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. our code and data are available at https://github.com/yizhongw/self-instruct.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10563" target="_blank">Blind: Bias Removal With No Demographics</a></div>
<div class="paper-author">Hadas Orgad, Yonatan Belinkov</div>
<div class="abstract">
<div class="abstract-content">
Abstract: models trained on real-world data tend to imitate and amplify social biases. common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. in this work, we introduce blind, a method for bias removal with no prior knowledge of the demographics in the dataset. while training a model on a downstream task, blind detects biased samples using an auxiliary model that predicts the main model's success, and down-weights those samples during the training process. experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that blind mitigates social biases without relying on a costly demographic annotation process. our method is competitive with other methods that require demographic information and sometimes even surpasses them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10678" target="_blank">Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing</a></div>
<div class="paper-author">Justus Mattern, Zhijing Jin, Mrinmaya Sachan, Rada Mihalcea, Bernhard Schölkopf</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics. these findings prompted large efforts aiming to understand and measure such effects, with the goal of providing benchmarks that can guide the development of techniques mitigating these stereotypical associations. however, as recent research has pointed out, the current benchmarks lack a robust experimental setup, consequently hindering the inference of meaningful conclusions from their evaluation metrics. in this paper, we extend these arguments and demonstrate that existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain from benchmarking language models based on them. accordingly, we propose a new framework for robustly measuring and quantifying biases exhibited by generative language models. finally, we use this framework to investigate gpt-3's occupational gender bias and propose prompting techniques for mitigating these biases without the need for fine-tuning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.10720" target="_blank">Moraldial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions</a></div>
<div class="paper-author">Hao Sun, Zhexin Zhang, Fei Mi, Yasheng Wang, Wei Liu, Jianwei Cui, Bin Wang, Qun Liu, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: morality in dialogue systems has raised great attention in research recently. a moral dialogue system aligned with users' values could enhance conversation engagement and user connections. in this paper, we propose a framework, moraldial to train and evaluate moral dialogue systems. in our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. the constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. furthermore, we propose a novel evaluation method under the framework. we evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11672" target="_blank">Trustworthy Social Bias Measurement</a></div>
<div class="paper-author">Rishi Bommasani, Percy Liang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: how do we design measures of social bias that we trust? while prior work has introduced several measures, no measure has gained widespread trust: instead, mounting evidence argues we should distrust these measures. in this work, we design bias measures that warrant trust based on the cross-disciplinary theory of measurement modeling. to combat the frequently fuzzy treatment of social bias in nlp, we explicitly define social bias, grounded in principles drawn from social science research. we operationalize our definition by proposing a general bias measurement framework divdist, which we use to instantiate 5 concrete bias measures. to validate our measures, we propose a rigorous testing protocol with 8 testing criteria (e.g. predictive validity: do measures predict biases in us employment?). through our testing, we demonstrate considerable evidence to trust our measures, showing they overcome conceptual, technical, and empirical deficiencies present in prior measures.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-19</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09251" target="_blank">Discovering Language Model Behaviors With Model-Written Evaluations</a></div>
<div class="paper-author">Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron Mckinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova Dassarma, Oliver Rausch, Robin Larson, Sam Mccandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as language models (lms) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). here, we automatically generate evaluations with lms. we explore approaches with varying amounts of human effort, from instructing lms to write yes/no questions to making complex winogender schemas with multiple stages of lm-based generation and filtering. crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. we generate 154 datasets and discover new cases of inverse scaling where lms get worse with size. larger lms repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. we also find some of the first examples of inverse scaling in rl from human feedback (rlhf), where more rlhf makes lms worse. for example, rlhf makes lms express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. overall, lm-written evaluations are high-quality and let us quickly discover many novel lm behaviors.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09292" target="_blank">Chatgpt: The End of Online Exam Integrity?</a></div>
<div class="paper-author">Teo Susnjak</div>
<div class="abstract">
<div class="abstract-content">
Abstract: this study evaluated the ability of chatgpt, a recently developed artificial intelligence (ai) agent, to perform high-level cognitive tasks and produce text that is indistinguishable from human-generated text. this capacity raises concerns about the potential use of chatgpt as a tool for academic misconduct in online exams. the study found that chatgpt is capable of exhibiting critical thinking skills and generating highly realistic text with minimal input, making it a potential threat to the integrity of online exams, particularly in tertiary education settings where such exams are becoming more prevalent. returning to invigilated and oral exams could form part of the solution, while using advanced proctoring techniques and ai-text output detectors may be effective in addressing this issue, they are not likely to be foolproof solutions. further research is needed to fully understand the implications of large language models like chatgpt and to devise strategies for combating the risk of cheating using these tools. it is crucial for educators and institutions to be aware of the possibility of chatgpt being used for cheating and to investigate measures to address it in order to maintain the fairness and validity of online exams for all students.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.09667" target="_blank">Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy Ai</a></div>
<div class="paper-author">Alex Mei, Sharon Levy, William Yang Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. we propose farm, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. in particular, farm foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. this knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. our experiments show that farm obtains state-of-the-art results on the safetext dataset, showing absolute improvement in safety classification accuracy by 5.9%.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.11126" target="_blank">Chatbots in a Botnet World</a></div>
<div class="paper-author">Forrest Mckee, David Noever</div>
<div class="abstract">
<div class="abstract-content">
Abstract: question-and-answer formats provide a novel experimental platform for investigating cybersecurity questions. unlike previous chatbots, the latest chatgpt model from openai supports an advanced understanding of complex coding questions. the research demonstrates thirteen coding tasks that generally qualify as stages in the mitre att&ck framework, ranging from credential access to defense evasion. with varying success, the experimental prompts generate examples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled ransomware. the empirical results illustrate cases that support the broad gain of functionality, including self-replication and self-modification, evasion, and strategic understanding of complex cybersecurity goals. one surprising feature of chatgpt as a language-only model centers on its ability to spawn coding approaches that yield images that obfuscate or embed executable programming steps or links.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08619" target="_blank">Planting and Mitigating Memorized Content in Predictive-Text Language Models</a></div>
<div class="paper-author">C. M. Downey, Wei Dai, Huseyin A. Inan, Kim Laine, Saurabh Naik, Tomasz Religa</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models are widely deployed to provide automatic text completion services in user products. however, recent research has revealed that language models (especially large ones) bear considerable risk of memorizing private training data, which is then vulnerable to leakage and extraction by adversaries. in this study, we test the efficacy of a range of privacy-preserving techniques to mitigate unintended memorization of sensitive user text, while varying other factors such as model size and adversarial conditions. we test both "heuristic" mitigations (those without formal privacy guarantees) and differentially private training, which provides provable levels of privacy at the cost of some model performance. our experiments show that (with the exception of l2 regularization), heuristic mitigations are largely ineffective in preventing memorization in our test suite, possibly because they make too strong of assumptions about the characteristics that define "sensitive" or "private" text. in contrast, differential privacy reliably prevents memorization in our experiments, despite its computational and model-performance costs.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.07877" target="_blank">Manifestations of Xenophobia in Ai Systems</a></div>
<div class="paper-author">Nenad Tomasev, Jonathan Leader Maynard, Iason Gabriel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: xenophobia is one of the key drivers of marginalisation, discrimination, and conflict, yet many prominent machine learning (ml) fairness frameworks fail to comprehensively measure or mitigate the resulting xenophobic harms. here we aim to bridge this conceptual gap and help facilitate safe and ethical design of artificial intelligence (ai) solutions. we ground our analysis of the impact of xenophobia by first identifying distinct types of xenophobic harms, and then applying this framework across a number of prominent ai application domains, reviewing the potential interplay between ai and xenophobia on social media and recommendation systems, healthcare, immigration, employment, as well as biases in large pre-trained models. these help inform our recommendations towards an inclusive, xenophilic design of future ai systems.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08061" target="_blank">On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</a></div>
<div class="paper-author">Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: generating a chain of thought (cot) has been shown to consistently improve large language model (llm) performance on a wide range of nlp tasks. however, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense qa); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. concretely, we perform a controlled evaluation of zero-shot cot across two socially sensitive domains: harmful questions and stereotype benchmarks. we find that zero-shot cot reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. furthermore, we show that harmful cots increase with model size, but decrease with improved instruction following. our work suggests that zero-shot cot should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional Ai: Harmlessness From Ai Feedback</a></div>
<div class="paper-author">Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova Dassarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as ai systems become more capable, we would like to enlist their help to supervise other ais. we experiment with methods for training a harmless ai assistant through self-improvement, without any human labels identifying harmful outputs. the only human oversight is provided through a list of rules or principles, and so we refer to the method as 'constitutional ai'. the process involves both a supervised learning and a reinforcement learning phase. in the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. in the rl phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of ai preferences. we then train with rl using the preference model as the reward signal, i.e. we use 'rl from ai feedback' (rlaif). as a result we are able to train a harmless but non-evasive ai assistant that engages with harmful queries by explaining its objections to them. both the sl and rl methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of ai decision making. these methods make it possible to control ai behavior more precisely and with far fewer human labels.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-12</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.06008" target="_blank">Who Evaluates the Evaluators? On Automatic Metrics for Assessing Ai-Based Offensive Code Generators</a></div>
<div class="paper-author">Pietro Liguori, Cristina Improta, Roberto Natella, Bojan Cukic, Domenico Cotroneo</div>
<div class="abstract">
<div class="abstract-content">
Abstract: ai-based code generators are an emerging solution for automatically writing programs starting from descriptions in natural language, by using deep neural networks (neural machine translation, nmt). in particular, code generators have been used for ethical hacking and offensive security testing by generating proof-of-concept attacks. unfortunately, the evaluation of code generators still faces several issues. the current practice uses output similarity metrics, i.e., automatic metrics that compute the textual similarity of generated code with ground-truth references. however, it is not clear what metric to use, and which metric is most suitable for specific contexts. this work analyzes a large set of output similarity metrics on offensive code generators. we apply the metrics on two state-of-the-art nmt models using two datasets containing offensive assembly and python code with their descriptions in the english language. we compare the estimates from the automatic metrics with human evaluation and provide practical insights into their strengths and limitations.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.06295" target="_blank">Despite "Super-Human" Performance, Current LLMS Are Unsuited for Decisions About Ethics and Safety</a></div>
<div class="paper-author">Joshua Albrecht, Ellie Kitanidis, Abraham J. Fetterman</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large language models (llms) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. we provide a simple new prompting strategy that leads to yet another supposedly "super-human" result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ethics dataset). unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. llm errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. we also observe signs of inverse scaling with model size on some examples, and show that prompting models to "explain their reasoning" often leads to alarming justifications of unethical actions. our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.02108" target="_blank">Human-in-the-Loop Hate Speech Classification in a Multilingual Context</a></div>
<div class="paper-author">Ana Kotarcic, Dominik Hangartner, Fabrizio Gilardi, Selina Kurer, Karsten Donnay</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the shift of public debate to the digital sphere has been accompanied by a rise in online hate speech. while many promising approaches for hate speech classification have been proposed, studies often focus only on a single language, usually english, and do not address three key concerns: post-deployment performance, classifier maintenance and infrastructural limitations. in this paper, we introduce a new human-in-the-loop bert-based hate speech classification pipeline and trace its development from initial data collection and annotation all the way to post-deployment. our classifier, trained using data from our original corpus of over 422k examples, is specifically developed for the inherently multilingual setting of switzerland and outperforms with its f1 score of 80.5 the currently best-performing bert-based multilingual classifier by 5.8 f1 points in german and 3.6 f1 points in french. our systematic evaluations over a 12-month period further highlight the vital importance of continuous, human-in-the-loop classifier maintenance to ensure robust hate speech classification post-deployment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.01810" target="_blank">Constructing Highly Inductive Contexts for Dialogue Safety Through Controllable Reverse Generation</a></div>
<div class="paper-author">Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Fei Mi, Yasheng Wang, Lifeng Shang, Minlie Huang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. in order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers, or automatic generation to construct adversarial contexts that are likely to induce toxic generations. however, what type of context is more likely to induce unsafe responses is still under-explored. in this paper, we identify that context toxicity and context category (e.g., \textit{profanity}, \textit{insult}, \textit{drugs}, etc.) are two important factors to cause safety issues in response generation. hence, we propose a method called \emph{reverse generation} to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level, and inductivity of the generated contexts. via reverse generation, we augment the existing bad dataset and construct a new dataset bad+ which contains more than 120k diverse and highly inductive contexts in 12 categories. we test three popular pretrained dialogue models (blender, dialogpt, and plato2) and find that bad+ can largely expose their safety problems. furthermore, we show that bad+ can greatly enhance the safety of generation and reveal the key factors of safety improvement. our code and dataset is available at \url{https://github.com/thu-coai/reverse_generation}.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-12-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2212.01700" target="_blank">Towards Robust NLG Bias Evaluation With Syntactically-Diverse Prompts</a></div>
<div class="paper-author">Arshiya Aggarwal, Jiao Sun, Nanyun Peng</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we present a robust methodology for evaluating biases in natural language generation(nlg) systems. previous works use fixed hand-crafted prefix templates with mentions of various demographic groups to prompt models to generate continuations for bias analysis. these fixed prefix templates could themselves be specific in terms of styles or linguistic structures, which may lead to unreliable fairness conclusions that are not representative of the general trends from tone varying prompts. to study this problem, we paraphrase the prompts with different syntactic structures and use these to evaluate demographic bias in nlg systems. our results suggest similar overall bias trends but some syntactic structures lead to contradictory conclusions compared to past works. we show that our methodology is more robust and that some syntactic structures prompt more toxic content while others could prompt less biased generation. this suggests the importance of not relying on a fixed syntactic structure and using tone-invariant prompts. introducing syntactically-diverse prompts can achieve more robust nlg (bias) evaluation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-29</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.16550" target="_blank">Soft Alignment Objectives for Robust Adaptation of Language Generation</a></div>
<div class="paper-author">Michal Štefánik, Marek Kadlčík, Petr Sojka</div>
<div class="abstract">
<div class="abstract-content">
Abstract: domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. however, the traditional adaptation by further training on in-domain data rapidly weakens the model's ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. this work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.   our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens' semantic similarity can mitigate catastrophic forgetting during domain adaptation, while (2) preserving the quality of the adaptation, (3) with negligible additions to compute costs.   in the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but na\"{\i}ve exact-match token-level objectives and expressive but computationally- and resource-intensive sequential objectives.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-27</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14946" target="_blank">Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models</a></div>
<div class="paper-author">Peter Henderson, Eric Mitchell, Christopher D. Manning, Dan Jurafsky, Chelsea Finn</div>
<div class="abstract">
<div class="abstract-content">
Abstract: a growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. in this work, we review potential safe-release strategies and argue that both policymakers and ai researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. we propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. we call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. we present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (mlac). in a small-scale experiment, we show mlac can largely prevent a bert-style model from being re-purposed to perform gender identification without harming the model's ability to perform profession classification.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.15006" target="_blank">Fine-Tuning Language Models to Find Agreement Among Humans With Diverse Preferences</a></div>
<div class="paper-author">Michiel A. Bakker, Martin J. Chadwick, Hannah R. Sheahan, Michael Henry Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat Mcaleese, Amelia Glaese, John Aslanides, Matthew M. Botvinick, Christopher Summerfield</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recent work in large language modeling (llms) has used fine-tuning to align outputs with the preferences of a prototypical user. this work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single "generic" user will confer more general alignment. here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? we fine-tune a 70 billion parameter llm to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., "should we raise taxes on the rich?"), and rate the llm's generated candidate consensus statements for agreement and quality. a reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. the model produces consensus statements that are preferred by human users over those from prompted llms (&gt;70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. further, our best model's consensus statements are preferred over the best human-generated opinions (&gt;65%). we find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. these results highlight the potential to use llms to help groups of humans align their values with one another.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-25</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14369" target="_blank">The Naughtyformer: A Transformer Understands Offensive Humor</a></div>
<div class="paper-author">Leonard Tang, Alexander Cai, Steve Li, Jason Wang</div>
<div class="abstract">
<div class="abstract-content">
Abstract: jokes are intentionally written to be funny, but not all jokes are created the same. some jokes may be fit for a classroom of kindergarteners, but others are best reserved for a more mature audience. while recent work has shown impressive results on humor detection in text, here we instead investigate the more nuanced task of detecting humor subtypes, especially of the less innocent variety. to that end, we introduce a novel jokes dataset filtered from reddit and solve the subtype classification task using a finetuned transformer dubbed the naughtyformer. moreover, we show that our model is significantly better at detecting offensiveness in jokes compared to state-of-the-art methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.14402" target="_blank">An Analysis of Social Biases Present in Bert Variants Across Multiple Languages</a></div>
<div class="paper-author">Aristides Milios, Parishad Behnamghader</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large pre-trained language models have achieved great success in many nlp tasks, it has been shown that they reflect human biases from their pre-training corpora. this bias may lead to undesirable outcomes when these models are applied in real-world settings. in this paper, we investigate the bias present in monolingual bert models across a diverse set of languages (english, greek, and persian). while recent research has mostly focused on gender-related biases, we analyze religious and ethnic biases as well and propose a template-based method to measure any kind of bias, based on sentence pseudo-likelihood, that can handle morphologically complex languages with gender-based adjective declensions. we analyze each monolingual model via this method and visualize cultural similarities and differences across different dimensions of bias. ultimately, we conclude that current methods of probing for bias are highly language-dependent, necessitating cultural insights regarding the unique ways bias is expressed in each language and culture (e.g. through coded language, synecdoche, and other similar linguistic concepts). we also hypothesize that higher measured social biases in the non-english bert models correlate with user-generated content in their training.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-24</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.13709" target="_blank">Undesirable Biases in Nlp: Averting a Crisis of Measurement</a></div>
<div class="paper-author">Oskar Van Der Wal, Dominik Bachmann, Alina Leidinger, Leendert Van Maanen, Willem Zuidema, Katrin Schulz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: as large language models and natural language processing (nlp) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. one problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of nlp models have serious problems (e.g., it is often unclear what they actually measure). in this paper, we provide an interdisciplinary approach to discussing the issue of nlp model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. in particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. our goal is to provide nlp practitioners with methodological tools for designing better bias measures, and to inspire them more generally to explore tools from psychometrics when working on bias measurement tools.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-21</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11678" target="_blank">Measuring Harmful Representations in Scandinavian Language Models</a></div>
<div class="paper-author">Samia Touileb, Debora Nozza</div>
<div class="abstract">
<div class="abstract-content">
Abstract: scandinavian countries are perceived as role-models when it comes to gender equality. with the advent of pre-trained language models and their widespread usage, we investigate to what extent gender-based harmful and toxic content exist in selected scandinavian language models. we examine nine models, covering danish, swedish, and norwegian, by manually creating template-based sentences and probing the models for completion. we evaluate the completions using two methods for measuring harmful and toxic completions and provide a thorough analysis of the results. we show that scandinavian pre-trained language models contain harmful and gender-based stereotypes with similar values across all languages. this finding goes against the general expectations related to gender equality in scandinavian countries and shows the possible problematic outcomes of using such models in real-world settings.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.15458" target="_blank">Validating Large Language Models With Relm</a></div>
<div class="paper-author">Michael Kuchnik, Virginia Smith, George Amvrosiadis</div>
<div class="abstract">
<div class="abstract-content">
Abstract: although large language models (llms) have been touted for their ability to generate natural-sounding text, there are growing concerns around possible negative effects of llms such as data memorization, bias, and inappropriate language. unfortunately, the complexity and generation capacities of llms make validating (and correcting) such concerns difficult. in this work, we introduce relm, a system for validating and querying llms using standard regular expressions. relm formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries. our results exploring queries surrounding memorization, gender bias, toxicity, and language understanding show that relm achieves up to 15x higher system efficiency, 2.5x data efficiency, and increased statistical and prompt-tuning coverage compared to state-of-the-art ad-hoc queries. relm offers a competitive and general baseline for the increasingly important problem of llm validation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-20</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.11087" target="_blank">Conceptor-Aided Debiasing of Large Language Models</a></div>
<div class="paper-author">Yifei Li, Lyle Ungar, João Sedoc</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained large language models (llms) reflect the inherent social biases of their training corpus. many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. we use conceptors--a soft projection method--to identify and remove the bias subspace in llms such as bert and gpt. we propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened bert (ci-bert), which explicitly incorporates the conceptor projection into all layers during training. we find that conceptor post-processing achieves state-of-the-art (sota) debiasing results while maintaining or improving llms' performance on the glue benchmark. also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. although ci-bert's training takes all layers' bias into account and can beat its post-processing counterpart in bias mitigation, ci-bert reduces the language model accuracy. we also show the importance of carefully constructing the bias subspace. the best results are obtained by removing outliers from the list of biased words, combining them (via the conceptor and operation), and computing their embeddings using the sentences from a cleaner corpus.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-18</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.10384" target="_blank">Indexing Ai Risks With Incidents, Issues, and Variants</a></div>
<div class="paper-author">Sean Mcgregor, Kevin Paeth, Khoa Lam</div>
<div class="abstract">
<div class="abstract-content">
Abstract: two years after publicly launching the ai incident database (aiid) as a collection of harms or near harms produced by ai in the world, a backlog of "issues" that do not meet its incident ingestion criteria have accumulated in its review queue. despite not passing the database's current criteria for incidents, these issues advance human understanding of where ai presents the potential for harm. similar to databases in aviation and computer security, the aiid proposes to adopt a two-tiered system for indexing ai incidents (i.e., a harm or near harm event) and issues (i.e., a risk of a harm event). further, as some machine learning-based systems will sometimes produce a large number of incidents, the notion of an incident "variant" is introduced. these proposed changes mark the transition of the aiid to a new version in response to lessons learned from editing 2,000+ incident reports and additional reports that fall under the new category of "issue."
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-17</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.09527" target="_blank">Ignore Previous Prompt: Attack Techniques for Language Models</a></div>
<div class="paper-author">Fábio Perez, Ian Ribeiro</div>
<div class="abstract">
<div class="abstract-content">
Abstract: transformer-based large language models (llms) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. however, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. by proposing promptinject, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how gpt-3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. in particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit gpt-3's stochastic nature, creating long-tail risks. the code for promptinject is available at https://github.com/agencyenterprise/promptinject.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-16</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.08714" target="_blank">Reward Gaming in Conditional Text Generation</a></div>
<div class="paper-author">Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur P. Parikh, He He</div>
<div class="abstract">
<div class="abstract-content">
Abstract: to align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (rl) with reward functions learned from human annotations. under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. we show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during rl training of the text generation model. while there has been discussion about reward gaming in the rl or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (nlg) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.09110" target="_blank">Holistic Evaluation of Language Models</a></div>
<div class="paper-author">Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda</div>
<div class="abstract">
<div class="abstract-content">
Abstract: language models (lms) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. we present holistic evaluation of language models (helm) to improve the transparency of language models. first, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for lms. then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected english dialects, metrics for trustworthiness). second, we adopt a multi-metric approach: we measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). this ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. we also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream lm evaluation. prior to helm, models on average were evaluated on just 17.9% of the core helm scenarios, with some prominent models not sharing a single scenario in common. we improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. our evaluation surfaces 25 top-level findings. for full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. we intend for helm to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-15</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.08412" target="_blank">Evaluating the Factual Consistency of Large Language Models Through Summarization</a></div>
<div class="paper-author">Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, Colin Raffel</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large language models (llms) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. to measure whether an llm prefers factually consistent continuations of its input, we propose a new benchmark called fib(factual inconsistency benchmark) that focuses on the task of summarization. specifically, our benchmark involves comparing the scores an llm assigns to a factually consistent versus a factually inconsistent summary for an input news article. for factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. to generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. a model's factual consistency is then measured according to its accuracy, i.e.\ the proportion of documents where it assigns a higher score to the factually consistent summary. to validate the usefulness of fib, we evaluate 23 large language models ranging from 1b to 176b parameters from six different model families including bloom and opt. we find that existing llms generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. however, if the factually inconsistent summaries occur verbatim in the document, then llms assign a higher score to these factually inconsistent summaries than factually consistent summaries. we validate design choices in our benchmark including the scoring method and source of distractor summaries. our code and benchmark data can be found at https://github.com/r-three/fib.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.08461" target="_blank">Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models</a></div>
<div class="paper-author">Silke Husse, Andreas Spitz</div>
<div class="abstract">
<div class="abstract-content">
Abstract: the awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. consequently, numerous bias detection methods have been proposed, which vary in their approach, the considered type of bias, and the data used for evaluation. however, while most detection methods are derived from the word embedding association test for static word embeddings, the reported results are heterogeneous, inconsistent, and ultimately inconclusive. to address this issue, we conduct a rigorous analysis and comparison of bias detection methods for contextual language models. our results show that minor design and implementation decisions (or errors) have a substantial and often significant impact on the derived bias scores. overall, we find the state of the field to be both worse than previously acknowledged due to systematic and propagated errors in implementations, yet better than anticipated since divergent results in the literature homogenize after accounting for implementation errors. based on our findings, we conclude with a discussion of paths towards more robust and consistent bias detection methods.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-14</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.07280" target="_blank">A Taxonomic System for Failure Cause Analysis of Open Source Ai Incidents</a></div>
<div class="paper-author">Nikiforos Pittaras, Sean Mcgregor</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while certain industrial sectors (e.g., aviation) have a long history of mandatory incident reporting complete with analytical findings, the practice of artificial intelligence (ai) safety benefits from no such mandate and thus analyses must be performed on publicly known ``open source'' ai incidents. although the exact causes of ai incidents are seldom known by outsiders, this work demonstrates how to apply expert knowledge on the population of incidents in the ai incident database (aiid) to infer the potential and likely technical causative factors that contribute to reported failures and harms. we present early work on a taxonomic system that covers a cascade of interrelated incident factors, from system goals (nearly always known) to methods / technologies (knowable in many cases) and technical failure causes (subject to expert analysis) of the implicated systems. we pair this ontology structure with a comprehensive classification workflow that leverages expert knowledge and community feedback, resulting in taxonomic annotations grounded by incident data and human expertise.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.07350" target="_blank">Does Debiasing Inevitably Degrade the Model Performance</a></div>
<div class="paper-author">Yiran Liu, Xiao Liu, Haotian Chen, Yang Yu</div>
<div class="abstract">
<div class="abstract-content">
Abstract: gender bias in language models has attracted sufficient attention because it threatens social justice. however, most of the current debiasing methods degraded the model's performance on other tasks while the degradation mechanism is still mysterious. we propose a theoretical framework explaining the three candidate mechanisms of the language model's gender bias. we use our theoretical framework to explain why the current debiasing methods cause performance degradation. we also discover a pathway through which debiasing will not degrade the model performance. we further develop a causality-detection fine-tuning approach to correct gender bias. the numerical experiment demonstrates that our method is able to lead to double dividends: partially mitigating gender bias while avoiding performance degradation.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.07733" target="_blank">Speaking Multiple Languages Affects the Moral Bias of Language Models</a></div>
<div class="paper-author">Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindřich Libovický, Constantin A. Rothkopf, Alexander Fraser, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-trained multilingual language models (pmlms) are commonly used when dealing with data from multiple languages and cross-lingual transfer. however, pmlms are trained on varying amounts of data for each language. in practice this means their performance is often much better on english than many other languages. we explore to what extent this also applies to moral norms. do the models capture moral norms from english and impose them on other languages? do the models exhibit random and thus potentially harmful beliefs in certain languages? both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. in this paper, we (1) apply the moraldirection framework to multilingual models, comparing results in german, czech, arabic, chinese, and english, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a moral foundations questionnaire, comparing with human responses from different countries. our experiments demonstrate that, indeed, pmlms encode differing moral biases, but these do not necessarily correspond to cultural differences or commonalities in human opinions. we release our code and models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-10</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05414" target="_blank">Adept: A Debiasing Prompt Framework</a></div>
<div class="paper-author">Ke Yang, Charles Yu, Yi Fung, Manling Li, Heng Ji</div>
<div class="abstract">
<div class="abstract-content">
Abstract: several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. with unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (plm) with additional task-specific information. despite this, relatively few efforts have been made to debias plms by prompt tuning with continuous prompts compared to its discrete counterpart. furthermore, for most debiasing methods that alter a plm's original parameters, a major problem is the need to not only decrease the bias in the plm but also to ensure that the plm does not lose its representation ability. finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words. in this paper, we propose adept, a method to debias plms using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. to achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. in addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. we evaluate adept on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the plm's representation ability. we further visualize words' correlation before and after debiasing a plm, and give some possible explanations for the visible effects.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05521" target="_blank">Zero-Shot Visual Commonsense Immorality Prediction</a></div>
<div class="paper-author">Yujin Jeong, Seongbeom Park, Suhong Moon, Jinkyu Kim</div>
<div class="abstract">
<div class="abstract-content">
Abstract: artificial intelligence is currently powering diverse real-world applications. these applications have shown promising performance, but raise complicated ethical issues, i.e. how to embed ethics to make ai applications behave morally. one way toward moral ai systems is by imitating human prosocial behavior and encouraging some form of good behavior in systems. however, learning such normative ethics (especially from images) is challenging mainly due to a lack of data and labeling complexity. here, we propose a model that predicts visual commonsense immorality in a zero-shot manner. we train our model with an ethics dataset (a pair of text and morality annotation) via a clip-based image-text joint embedding. in a testing phase, the immorality of an unseen image is predicted. we evaluate our model with existing moral/immoral image datasets and show fair prediction performance consistent with human intuitions. further, we create a visual commonsense immorality benchmark with more general and extensive immoral visual contents. codes and dataset are available at https://github.com/ku-vai/zero-shot-visual-commonsense-immorality-prediction. note that this paper might contain images and descriptions that are offensive in nature.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05750" target="_blank">Nano: Nested Human-in-the-Loop Reward Learning for Few-Shot Language Model Control</a></div>
<div class="paper-author">Xiang Fan, Yiwei Lyu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pretrained language models have demonstrated extraordinary capabilities in language generation. however, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. however, many important distributions, such as personal preferences, are unquantified. in this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing nano, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. nano achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. we also show that nano is able to learn unquantified distributions, achieves personalization, and captures differences between different individuals' personal preferences with high sample efficiency.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05809" target="_blank">Casual Conversations V2: Designing a Large Consent-Driven Dataset to Measure Algorithmic Bias and Robustness</a></div>
<div class="paper-author">Caner Hazirbas, Yejin Bang, Tiezheng Yu, Parisa Assar, Bilal Porgali, Vítor Albiero, Stefan Hermanek, Jacqueline Pan, Emily Mcreynolds, Miranda Bogen, Pascale Fung, Cristian Canton Ferrer</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developing robust and fair ai systems require datasets with comprehensive set of labels that can help ensure the validity and legitimacy of relevant measurements. recent efforts, therefore, focus on collecting person-related datasets that have carefully selected labels, including sensitive characteristics, and consent forms in place to use those attributes for model testing and development. responsible data collection involves several stages, including but not limited to determining use-case scenarios, selecting categories (annotations) such that the data are fit for the purpose of measuring algorithmic bias for subgroups and most importantly ensure that the selected categories/subcategories are robust to regional diversities and inclusive of as many subgroups as possible.   meta, in a continuation of our efforts to measure ai algorithmic bias and robustness (https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set), is working on collecting a large consent-driven dataset with a comprehensive list of categories. this paper describes our proposed design of such categories and subcategories for casual conversations v2.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05826" target="_blank">The Cringe Loss: Learning What Language Not to Model</a></div>
<div class="paper-author">Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, Jason Weston</div>
<div class="abstract">
<div class="abstract-content">
Abstract: standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data -- examples of what the model should not do. in this work, we propose a novel procedure to train with such data called the cringe loss (contrastive iterative negative generation). we show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. our models outperform multiple strong baselines and are conceptually simple, easy to train and implement.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05853" target="_blank">Measuring Reliability of Large Language Models Through Semantic Consistency</a></div>
<div class="paper-author">Harsh Raj, Domenic Rosati, Subhabrata Majumdar</div>
<div class="abstract">
<div class="abstract-content">
Abstract: while large pretrained language models (plms) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing plms are very sensitive to what prompts are feed into them. even when prompts are semantically identical, language models may give very different answers. when considering safe and trustworthy deployments of plms we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. while some work has looked into how state-of-the-art plms address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. in order to understand consistency of plms under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. we implement several versions of this consistency metric to evaluate the performance of a number of plms on paraphrased versions of questions in the truthfulqa dataset, we find that our proposed metrics are considerably more consistent than traditional metrics embodying lexical consistency, and also correlate with human evaluation of output consistency to a higher degree.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-09</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.05105" target="_blank">Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models</a></div>
<div class="paper-author">Patrick Schramowski, Manuel Brack, Björn Deiseroth, Kristian Kersting</div>
<div class="abstract">
<div class="abstract-content">
Abstract: text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. in turn, they may even reinforce such biases. to help combat these undesired side effects, we present safe latent diffusion (sld). specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (i2p)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. as our exhaustive empirical evaluation demonstrates, the introduced sld removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-08</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.04602" target="_blank">System Safety Engineering for Social and Ethical Ml Risks: A Case Study</a></div>
<div class="paper-author">Edgar W. Jatho, Logan O. Mailloux, Shalaleh Rismani, Eugene D. Williams, Joshua A. Kroll</div>
<div class="abstract">
<div class="abstract-content">
Abstract: governments, industry, and academia have undertaken efforts to identify and mitigate harms in ml-driven systems, with a particular focus on social and ethical risks of ml components in complex sociotechnical systems. however, existing approaches are largely disjointed, ad-hoc and of unknown effectiveness. systems safety engineering is a well established discipline with a track record of identifying and managing risks in many complex sociotechnical domains. we adopt the natural hypothesis that tools from this domain could serve to enhance risk analyses of ml in its context of use. to test this hypothesis, we apply a "best of breed" systems safety analysis, systems theoretic process analysis (stpa), to a specific high-consequence system with an important ml-driven component, namely the prescription drug monitoring programs (pdmps) operated by many us states, several of which rely on an ml-derived risk score. we focus in particular on how this analysis can extend to identifying social and ethical risks and developing concrete design-level controls to mitigate them.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-07</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.03622" target="_blank">Do Users Write More Insecure Code With Ai Assistants?</a></div>
<div class="paper-author">Neil Perry, Megha Srivastava, Deepak Kumar, Dan Boneh</div>
<div class="abstract">
<div class="abstract-content">
Abstract: we conduct the first large-scale user study examining how users interact with an ai code assistant to solve a variety of security related tasks across different programming languages. overall, we find that participants who had access to an ai assistant based on openai's codex-davinci-002 model wrote significantly less secure code than those without access. additionally, participants with access to an ai assistant were more likely to believe they wrote secure code than those without access to the ai assistant. furthermore, we find that participants who trusted the ai less and engaged more with the language and format of their prompts (e.g. re-phrasing, adjusting temperature) provided code with fewer security vulnerabilities. finally, in order to better inform the design of future ai-based code assistants, we provide an in-depth analysis of participants' language and interaction behavior, as well as release our user interface as an instrument to conduct similar studies in the future.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-05</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.02956" target="_blank">Privacy-Preserving Models for Legal Natural Language Processing</a></div>
<div class="paper-author">Ying Yin, Ivan Habernal</div>
<div class="abstract">
<div class="abstract-content">
Abstract: pre-training large transformer models with in-domain data improves domain adaptation and helps gain performance on the domain-specific downstream tasks. however, sharing models pre-trained on potentially sensitive data is prone to adversarial privacy attacks. in this paper, we asked to which extent we can guarantee privacy of pre-training data and, at the same time, achieve better downstream performance on legal tasks without the need of additional labeled data. we extensively experiment with scalable self-supervised learning of transformer models under the formal paradigm of differential privacy and show that under specific training configurations we can improve downstream performance without sacrifying privacy protection for the in-domain data. our main contribution is utilizing differential privacy for large-scale pre-training of transformer language models in the legal nlp domain, which, to the best of our knowledge, has not been addressed before.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-04</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.03540" target="_blank">Measuring Progress on Scalable Oversight for Large Language Models</a></div>
<div class="paper-author">Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamilė Lukošiūtė, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova Dassarma, Robin Larson, Sam Mccandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Jared Kaplan</div>
<div class="abstract">
<div class="abstract-content">
Abstract: developing safe and useful general-purpose ai systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. this paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. we first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general ai systems fail. we then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: mmlu and time-limited quality. on these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. these results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-11-03</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.01910" target="_blank">Large Language Models Are Human-Level Prompt Engineers</a></div>
<div class="paper-author">Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba</div>
<div class="abstract">
<div class="abstract-content">
Abstract: by conditioning on natural language instructions, large language models (llms) have displayed impressive capabilities as general-purpose computers. however, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. inspired by classical program synthesis and the human approach to prompt engineering, we propose automatic prompt engineer (ape) for automatic instruction generation and selection. in our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an llm in order to maximize a chosen score function. to evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another llm following the selected instruction. experiments on 24 nlp tasks show that our automatically generated instructions outperform the prior llm baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. we conduct extensive qualitative and quantitative analyses to explore the performance of ape. we show that ape-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.02011" target="_blank">Inverse Scaling Can Become U-Shaped</a></div>
<div class="paper-author">Jason Wei, Najoung Kim, Yi Tay, Quoc V. Le</div>
<div class="abstract">
<div class="abstract-content">
Abstract: scaling up language models has been empirically shown to improve performance on a wide range of downstream tasks. however, if we were to observe worse performance as a function of scale ("inverse scaling") on certain tasks, this would indicate that scaling can also encourage behaviors that are misaligned with human preferences. the inverse scaling prize (mckenzie et al. 2022) identified eleven such inverse scaling tasks, evaluated on models of up to 280b parameters and up to 500 zettaflops of training compute. this paper takes a closer look at these inverse scaling tasks. we evaluate models of up to 540b parameters, trained on five times more compute than those evaluated in the inverse scaling prize. with this increased range of model sizes and training compute, only four out of the eleven tasks remain inverse scaling. six out of the eleven tasks exhibit "u-shaped scaling", where performance decreases up to a certain size, and then increases again up to the largest model evaluated (the one remaining task displays positive scaling). in addition, we find that 1-shot examples and chain-of-thought can help mitigate undesirable scaling patterns even further. u-shaped scaling suggests that the inverse scaling trend observed in mckenzie et al. (2022) may not continue to hold for larger models, which we attribute to the presence of distractor tasks that only sufficiently large models can avoid.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<h2><b>2022-10-31</b></h2>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2210.17546" target="_blank">Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy</a></div>
<div class="paper-author">Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, Nicholas Carlini</div>
<div class="abstract">
<div class="abstract-content">
Abstract: studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data and aids in the development of countermeasures. many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. we argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. specifically, we design and implement an efficient defense that perfectly prevents all verbatim memorization. and yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. we conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>
<div class="paper">
<div class="paper-title"><a href="https://arxiv.org/abs/2211.00609" target="_blank">A Simple, Yet Effective Approach to Finding Biases in Code Generation</a></div>
<div class="paper-author">Spyridon Mouselinos, Mateusz Malinowski, Henryk Michalewski</div>
<div class="abstract">
<div class="abstract-content">
Abstract: recently, high-performing code generation systems based on large language models have surfaced. they are trained on massive corpora containing much more natural text than actual executable computer code. this work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.   to investigate the effect, we propose the "block of influence" concept, which enables a modular decomposition and analysis of the coding challenges. we introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.
</div>
<button class="expand-btn">Expand Abstract</button>
</div>
</div>

<div class="outer">


<script>
    document.addEventListener("DOMContentLoaded", function() {
        const expandButtons = document.querySelectorAll('.expand-btn');

        expandButtons.forEach(button => {
            button.addEventListener('click', function() {
                const abstractDiv = button.closest('.abstract');
                if (abstractDiv.classList.contains('expanded')) {
                    abstractDiv.classList.remove('expanded');
                    button.innerText = 'Expand Abstract';
                } else {
                    abstractDiv.classList.add('expanded');
                    button.innerText = 'Fold Abstract';
                }
            });
        });
    });
</script>


</body>
</html>

