---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---





Hi there! I am Xiangyu Qi (漆翔宇), a Ph.D. candidate in the department of [Electrical and Computer Engineering](https://ece.princeton.edu) at Princeton University. I am fortunate to be advised by [Prof. Prateek Mittal](https://www.princeton.edu/~pmittal/index.html). Before that, I received my B.S. degree from the CS Department at [Zhejiang University](http://www.zju.edu.cn/english/) (June 2021). I also worked with [Prof. Bo Li](https://aisecure.github.io/) as a research intern at [UIUC Secure Learning Lab](https://aisecure.github.io).

My interests extend across Artificial Intelligence, Machine Learning, and Philosophy. Currently, my primary focus lies within the intriguing realm of [Adversarial Machine Learning](https://en.wikipedia.org/wiki/Adversarial_machine_learning) (Adv ML). My research endeavors are centered around two main objectives: (1) To decipher the fundamental vulnerabilities prevalent in ML systems, and (2) To devise strategies that can counter these vulnerabilities, thereby contributing to the development of robust and trustworthy ML systems. I am of the firm belief that Adv ML offers a unique vantage point for critically examining current ML methodologies, and it carries the potential to lead us towards the creation of fundamentally improved ML systems.

Please feel free to reach out via [xiangyuqi@princeton.edu]().

<br>



## Publications & Manuscripts

**( $ ^* $ denotes equal contributions)**

<br>

* **Preprint**

  <br>

  * **[Visual Adversarial Examples Jailbreak Large Language Models](https://arxiv.org/abs/2306.13213)**

    **Xiangyu Qi$^* $**, Kaixuan Huang$^* $, Ashwinee Panda, Mengdi Wang, Prateek Mittal

    ***[AdvML-Frontiers 2023](https://advml-frontier.github.io/) @ ICML***, 2023 <font color="red"> **(oral presentation)** </font>

<br>

* **Publications**

  <br>

  * **[Towards A Proactive ML Approach for Detecting Backdoor Poison Samples](https://arxiv.org/abs/2205.13616)**

    **Xiangyu Qi**, Tinghao Xie, Jiachen T. Wang, Tong Wu, Saeed Mahloujifar, Prateek Mittal

    ***[USENIX Security](https://www.usenix.org/conference/usenixsecurity23), 2023***

    <br>

  * **[Uncovering Adversarial Risks of Test-Time Adaptation](https://arxiv.org/abs/2301.12576)**

    Tong Wu, Feiran Jia, **Xiangyu Qi**, Jiachen T. Wang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal

    ***[ICML](https://icml.cc/Conferences/2023/Dates), 2023***

    <br>

  * **[Revisiting the Assumption of Latent Separability for Backdoor Defenses](https://openreview.net/forum?id=_wSHsgrVali)**

    **Xiangyu Qi$^* $**, Tinghao Xie$^* $, Yiming Li, Saeed Mahloujifar, Prateek Mittal

    ***[ICLR](https://iclr.cc/Conferences/2023), 2023***

    <br>

  * **[Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks](https://arxiv.org/abs/2111.12965)**

    **Xiangyu Qi$^* $**, Tinghao Xie$^* $, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu

    ***[CVPR](https://cvpr2022.thecvf.com/), 2022***  <font color="red"> **(oral presentation, 4.2%)** </font>

    <br>

  * **[Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks](https://arxiv.org/abs/2106.06235)**

    Nezihe Merve Gürel$^*$, **Xiangyu Qi$^* $**, Luka Rimanic, Ce Zhang, Bo Li

    ***[ICML](https://icml.cc/Conferences/2021), 2021***

<br>

* **Workshop Papers**

  <br>

  * **[Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting](https://arxiv.org/abs/2107.07240)**

    **Xiangyu Qi**, Jifeng Zhu, Chulin Xie, Yong Yang

    ***[ICLR Workshop on Security and Safety in Machine Learning Systems](https://aisecure-workshop.github.io/aml-iclr2021/), 2021***
